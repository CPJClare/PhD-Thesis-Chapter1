{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 3 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/rosen.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Rosenbrock'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Rosenbrock':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = 0\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = -2.048 \n",
    "    ub = +2.048 \n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 2\n",
    "\n",
    "# 2-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test)\n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training):\n",
    "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [-0.22246619 -0.23774559]. \t  \u001b[92m-9.744921046208802\u001b[0m \t -9.744921046208802\n",
      "2      \t [-0.44259375 -0.41212289]. \t  -39.0489503873149 \t -9.744921046208802\n",
      "3      \t [ 1.38387448 -0.09997246]. \t  -406.2025216458289 \t -9.744921046208802\n",
      "4      \t [-0.16665514  0.08681738]. \t  \u001b[92m-1.7096970021626978\u001b[0m \t -1.7096970021626978\n",
      "5      \t [ 1.53746021 -2.048     ]. \t  -1946.6725713151652 \t -1.7096970021626978\n",
      "6      \t [0.0419833  0.29493492]. \t  -9.512796927906653 \t -1.7096970021626978\n",
      "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -1.7096970021626978\n",
      "8      \t [1.53108318 1.0603856 ]. \t  -165.10401995611414 \t -1.7096970021626978\n",
      "9      \t [0.58686429 0.11291367]. \t  -5.529721991981178 \t -1.7096970021626978\n",
      "10     \t [-1.46718824  0.91875406]. \t  -158.33479736047443 \t -1.7096970021626978\n",
      "11     \t [-1.74048967  1.90210614]. \t  -134.5678515165159 \t -1.7096970021626978\n",
      "12     \t [-0.94792724  1.03619755]. \t  -5.688663462388562 \t -1.7096970021626978\n",
      "13     \t [ 0.24301181 -0.13160576]. \t  -4.208173503027023 \t -1.7096970021626978\n",
      "14     \t [0.9684561 1.7270646]. \t  -62.27793206271763 \t -1.7096970021626978\n",
      "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -1.7096970021626978\n",
      "16     \t [-0.43805569  0.3064678 ]. \t  -3.380747551456502 \t -1.7096970021626978\n",
      "17     \t [1.02381932 2.048     ]. \t  -99.95937133528344 \t -1.7096970021626978\n",
      "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -1.7096970021626978\n",
      "19     \t [-1.27748654  1.27771342]. \t  -17.736848720672292 \t -1.7096970021626978\n",
      "20     \t [1.58184171 1.11098514]. \t  -193.89287342355146 \t -1.7096970021626978\n",
      "21     \t [0.84228194 0.70428355]. \t  \u001b[92m-0.027532719372608695\u001b[0m \t -0.027532719372608695\n",
      "22     \t [0.80534723 0.54015208]. \t  -1.2136413443257084 \t -0.027532719372608695\n",
      "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -0.027532719372608695\n",
      "24     \t [-1.92372447 -0.91331464]. \t  -2137.475875853538 \t -0.027532719372608695\n",
      "25     \t [-0.53298446  0.06339036]. \t  -7.2200991789372235 \t -0.027532719372608695\n",
      "26     \t [-0.08235995  0.39477545]. \t  -16.22530470798019 \t -0.027532719372608695\n",
      "27     \t [-1.13241259  1.7134821 ]. \t  -23.133958372920567 \t -0.027532719372608695\n",
      "28     \t [-1.02080879 -0.13940014]. \t  -143.66625118755718 \t -0.027532719372608695\n",
      "29     \t [1.03733951 1.09386625]. \t  -0.033053331883434386 \t -0.027532719372608695\n",
      "30     \t [0.95956921 0.90708699]. \t  \u001b[92m-0.020365513722093033\u001b[0m \t -0.020365513722093033\n",
      "31     \t [0.9692753  0.93453261]. \t  \u001b[92m-0.003406144889479281\u001b[0m \t -0.003406144889479281\n",
      "32     \t [0.53386496 0.02594546]. \t  -6.928818537696993 \t -0.003406144889479281\n",
      "33     \t [-1.39253331 -0.59085225]. \t  -645.8148494698042 \t -0.003406144889479281\n",
      "34     \t [-0.85840066  1.22147162]. \t  -26.939300528904813 \t -0.003406144889479281\n",
      "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.003406144889479281\n",
      "36     \t [-1.23462122  0.38938055]. \t  -133.79537733532314 \t -0.003406144889479281\n",
      "37     \t [0.68281647 0.45551651]. \t  -0.11210114314710315 \t -0.003406144889479281\n",
      "38     \t [-0.26683513 -0.66227277]. \t  -55.40324609566223 \t -0.003406144889479281\n",
      "39     \t [0.96814601 1.03308968]. \t  -0.9184525702177563 \t -0.003406144889479281\n",
      "40     \t [-1.53740048  1.2231408 ]. \t  -136.50317180697115 \t -0.003406144889479281\n",
      "41     \t [1.9426309 1.9633803]. \t  -328.65586656807943 \t -0.003406144889479281\n",
      "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.003406144889479281\n",
      "43     \t [-1.16223016 -1.42849377]. \t  -777.1109183233424 \t -0.003406144889479281\n",
      "44     \t [-1.04316404  1.15873988]. \t  -4.672230813198803 \t -0.003406144889479281\n",
      "45     \t [0.74327672 1.96714473]. \t  -200.19911334585981 \t -0.003406144889479281\n",
      "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.003406144889479281\n",
      "47     \t [-1.21082569  1.68390588]. \t  -9.631740102570523 \t -0.003406144889479281\n",
      "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.003406144889479281\n",
      "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.003406144889479281\n",
      "50     \t [0.66014216 1.64749383]. \t  -146.93868689891644 \t -0.003406144889479281\n",
      "51     \t [-0.43226986  1.33958364]. \t  -134.92921491596502 \t -0.003406144889479281\n",
      "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.003406144889479281\n",
      "53     \t [ 0.16555488 -0.53612718]. \t  -32.4535352858786 \t -0.003406144889479281\n",
      "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.003406144889479281\n",
      "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.003406144889479281\n",
      "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.003406144889479281\n",
      "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.003406144889479281\n",
      "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.003406144889479281\n",
      "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.003406144889479281\n",
      "60     \t [1.02250523 1.72285278]. \t  -45.878888676329034 \t -0.003406144889479281\n",
      "61     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.003406144889479281\n",
      "62     \t [0.25962153 1.26514291]. \t  -144.0061684773042 \t -0.003406144889479281\n",
      "63     \t [-1.12854391  1.29562523]. \t  -4.57916022324821 \t -0.003406144889479281\n",
      "64     \t [-1.00705448  1.16548517]. \t  -6.318237027567069 \t -0.003406144889479281\n",
      "65     \t [-0.21515472  1.99157176]. \t  -379.8881065908075 \t -0.003406144889479281\n",
      "66     \t [0.97254944 0.91457515]. \t  -0.0985801857821006 \t -0.003406144889479281\n",
      "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.003406144889479281\n",
      "68     \t [-1.95064235 -0.88388403]. \t  -2207.274882433743 \t -0.003406144889479281\n",
      "69     \t [0.16198391 0.10840563]. \t  -1.3774100518971923 \t -0.003406144889479281\n",
      "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.003406144889479281\n",
      "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.003406144889479281\n",
      "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.003406144889479281\n",
      "73     \t [0.59019269 0.36939939]. \t  -0.21234486038223965 \t -0.003406144889479281\n",
      "74     \t [0.44485672 0.22472667]. \t  -0.38016447122718194 \t -0.003406144889479281\n",
      "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.003406144889479281\n",
      "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.003406144889479281\n",
      "77     \t [ 0.215878   -0.57694162]. \t  -39.495675767076335 \t -0.003406144889479281\n",
      "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.003406144889479281\n",
      "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.003406144889479281\n",
      "80     \t [1.0448602  1.09298009]. \t  \u001b[92m-0.002168003450219745\u001b[0m \t -0.002168003450219745\n",
      "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.002168003450219745\n",
      "82     \t [1.03815875 1.07881697]. \t  \u001b[92m-0.0015649549045195304\u001b[0m \t -0.0015649549045195304\n",
      "83     \t [0.80777678 0.66365278]. \t  -0.049380795879166925 \t -0.0015649549045195304\n",
      "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.0015649549045195304\n",
      "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.0015649549045195304\n",
      "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.0015649549045195304\n",
      "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.0015649549045195304\n",
      "88     \t [0.60703817 0.37768558]. \t  -0.16286504638093893 \t -0.0015649549045195304\n",
      "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.0015649549045195304\n",
      "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.0015649549045195304\n",
      "91     \t [1.10148711 1.21372887]. \t  -0.010320337327623346 \t -0.0015649549045195304\n",
      "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.0015649549045195304\n",
      "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.0015649549045195304\n",
      "94     \t [-1.17118342  0.76949043]. \t  -40.976133562082566 \t -0.0015649549045195304\n",
      "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.0015649549045195304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.0015649549045195304\n",
      "97     \t [1.00007027 0.99805295]. \t  \u001b[92m-0.00043581245498646804\u001b[0m \t -0.00043581245498646804\n",
      "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.00043581245498646804\n",
      "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.00043581245498646804\n",
      "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.00043581245498646804\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.38382722 -0.20002744]. \t  -12.444925217581003 \t -1.3013277264983028\n",
      "2      \t [0.74575449 0.06127072]. \t  -24.555167089118154 \t -1.3013277264983028\n",
      "3      \t [0.86803262 1.82156653]. \t  -114.09816275934874 \t -1.3013277264983028\n",
      "4      \t [ 0.46185146 -0.65999866]. \t  -76.5558415660385 \t -1.3013277264983028\n",
      "5      \t [0.5105136  1.17310968]. \t  -83.5025837806539 \t -1.3013277264983028\n",
      "6      \t [1.18411797 1.28058677]. \t  -1.5113054183090753 \t -1.3013277264983028\n",
      "7      \t [0.96695624 1.09506056]. \t  -2.5628902934529054 \t -1.3013277264983028\n",
      "8      \t [1.8774407 2.048    ]. \t  -218.85888120416146 \t -1.3013277264983028\n",
      "9      \t [-0.35320399 -0.14491695]. \t  -9.103352584475711 \t -1.3013277264983028\n",
      "10     \t [-0.10103105  0.24295934]. \t  -6.629622007414247 \t -1.3013277264983028\n",
      "11     \t [-1.84652963 -2.03028032]. \t  -2967.410518416418 \t -1.3013277264983028\n",
      "12     \t [1.05930778 1.01952393]. \t  \u001b[92m-1.0563789641769186\u001b[0m \t -1.0563789641769186\n",
      "13     \t [0.56365564 1.79548012]. \t  -218.5715331592075 \t -1.0563789641769186\n",
      "14     \t [ 0.30627648 -0.14910958]. \t  -6.382015315679451 \t -1.0563789641769186\n",
      "15     \t [-0.08960228 -0.17361579]. \t  -4.4867005802384305 \t -1.0563789641769186\n",
      "16     \t [0.21334486 0.17524857]. \t  -2.3018794648950673 \t -1.0563789641769186\n",
      "17     \t [-0.61156446  0.86155167]. \t  -26.366721503073624 \t -1.0563789641769186\n",
      "18     \t [-1.19063296  1.74954686]. \t  -15.81729039775714 \t -1.0563789641769186\n",
      "19     \t [-0.93346442  1.42219356]. \t  -34.080506807049005 \t -1.0563789641769186\n",
      "20     \t [-0.3413371  -0.26077155]. \t  -16.033398685166116 \t -1.0563789641769186\n",
      "21     \t [-1.7213462  2.048    ]. \t  -91.13421482691442 \t -1.0563789641769186\n",
      "22     \t [-1.8893095  -0.46960809]. \t  -1639.7797752822478 \t -1.0563789641769186\n",
      "23     \t [1.25096243 0.99435341]. \t  -32.616121404735196 \t -1.0563789641769186\n",
      "24     \t [0.93081782 0.10040177]. \t  -58.68345579354755 \t -1.0563789641769186\n",
      "25     \t [1.1025212  1.23366077]. \t  \u001b[92m-0.043299717652430215\u001b[0m \t -0.043299717652430215\n",
      "26     \t [-0.32829337 -1.80462139]. \t  -367.49094444375294 \t -0.043299717652430215\n",
      "27     \t [0.46466716 1.54737497]. \t  -177.56499598933985 \t -0.043299717652430215\n",
      "28     \t [1.08781736 1.18314713]. \t  \u001b[92m-0.00771586898309462\u001b[0m \t -0.00771586898309462\n",
      "29     \t [-1.20169539  2.048     ]. \t  -41.3203893046589 \t -0.00771586898309462\n",
      "30     \t [-0.71700127  0.34279055]. \t  -5.882471910940302 \t -0.00771586898309462\n",
      "31     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.00771586898309462\n",
      "32     \t [1.08855921 1.18143373]. \t  -0.009087008296729085 \t -0.00771586898309462\n",
      "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.00771586898309462\n",
      "34     \t [0.66841581 0.44191857]. \t  -0.11231112905277234 \t -0.00771586898309462\n",
      "35     \t [0.73005674 0.51754951]. \t  -0.09668813696464773 \t -0.00771586898309462\n",
      "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.00771586898309462\n",
      "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.00771586898309462\n",
      "38     \t [0.82481015 1.10117661]. \t  -17.743411896257598 \t -0.00771586898309462\n",
      "39     \t [-0.98073994 -2.00640186]. \t  -884.9757352175648 \t -0.00771586898309462\n",
      "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.00771586898309462\n",
      "41     \t [1.03165866 0.66417548]. \t  -16.01253240286429 \t -0.00771586898309462\n",
      "42     \t [-0.34286574  1.94133637]. \t  -334.4204397773866 \t -0.00771586898309462\n",
      "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.00771586898309462\n",
      "44     \t [-1.22092697  1.33908676]. \t  -7.230042157302588 \t -0.00771586898309462\n",
      "45     \t [ 1.99501367 -1.7679637 ]. \t  -3304.990185125095 \t -0.00771586898309462\n",
      "46     \t [0.79806246 0.64724868]. \t  -0.051480636842797745 \t -0.00771586898309462\n",
      "47     \t [-1.92446685  2.01946897]. \t  -292.1730256583349 \t -0.00771586898309462\n",
      "48     \t [-0.79094099 -1.53717212]. \t  -470.96045277147226 \t -0.00771586898309462\n",
      "49     \t [-0.57036757 -1.33478685]. \t  -278.0612514402123 \t -0.00771586898309462\n",
      "50     \t [0.21598433 0.09697089]. \t  -0.8679074804196298 \t -0.00771586898309462\n",
      "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
      "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.0037764840180860245\n",
      "53     \t [2.00029358 1.68210447]. \t  -538.8091297035712 \t -0.0037764840180860245\n",
      "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.0037764840180860245\n",
      "55     \t [1.06225137 1.12584387]. \t  -0.004517398255667639 \t -0.0037764840180860245\n",
      "56     \t [1.31403595 0.46630098]. \t  -158.95678900240353 \t -0.0037764840180860245\n",
      "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.0037764840180860245\n",
      "58     \t [1.33818709 1.95798627]. \t  -2.9113447674351685 \t -0.0037764840180860245\n",
      "59     \t [-1.32427512  0.34521736]. \t  -203.78588019736992 \t -0.0037764840180860245\n",
      "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.0037764840180860245\n",
      "61     \t [1.57259458 0.00595459]. \t  -608.9856696190132 \t -0.0037764840180860245\n",
      "62     \t [1.85709005 1.58285552]. \t  -348.9033138942158 \t -0.0037764840180860245\n",
      "63     \t [-0.58054926 -0.49970011]. \t  -72.51110784024942 \t -0.0037764840180860245\n",
      "64     \t [0.84829766 0.72116891]. \t  -0.02325695956156507 \t -0.0037764840180860245\n",
      "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.0037764840180860245\n",
      "66     \t [ 0.08406173 -2.02016692]. \t  -411.80642514590784 \t -0.0037764840180860245\n",
      "67     \t [ 0.60840368 -1.00349936]. \t  -188.84598804941893 \t -0.0037764840180860245\n",
      "68     \t [-0.04636727 -0.07615456]. \t  -1.7080436883233414 \t -0.0037764840180860245\n",
      "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.0037764840180860245\n",
      "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.0037764840180860245\n",
      "71     \t [-0.96363475  0.86302257]. \t  -4.2857956363608976 \t -0.0037764840180860245\n",
      "72     \t [1.31178016 1.69705082]. \t  -0.15345341524354045 \t -0.0037764840180860245\n",
      "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.0037764840180860245\n",
      "74     \t [0.88155552 0.77067489]. \t  -0.018209025228376006 \t -0.0037764840180860245\n",
      "75     \t [0.84361066 0.69447202]. \t  -0.05406546866753627 \t -0.0037764840180860245\n",
      "76     \t [1.38178252 1.96641555]. \t  -0.4717144597903842 \t -0.0037764840180860245\n",
      "77     \t [-1.18605588  1.00865149]. \t  -20.62537567388703 \t -0.0037764840180860245\n",
      "78     \t [ 0.59733697 -0.66741213]. \t  -105.06553138860436 \t -0.0037764840180860245\n",
      "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.0037764840180860245\n",
      "80     \t [-1.03425542  0.62109077]. \t  -24.261808152218876 \t -0.0037764840180860245\n",
      "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.0037764840180860245\n",
      "82     \t [1.4277567 2.048    ]. \t  -0.19202133139501618 \t -0.0037764840180860245\n",
      "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.0037764840180860245\n",
      "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.0037764840180860245\n",
      "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.0037764840180860245\n",
      "86     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.0037764840180860245\n",
      "87     \t [1.889115   0.21074836]. \t  -1128.411706159457 \t -0.0037764840180860245\n",
      "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.0037764840180860245\n",
      "89     \t [0.74091969 0.55119627]. \t  -0.06762180313179203 \t -0.0037764840180860245\n",
      "90     \t [0.82631999 0.68359394]. \t  -0.030227031754108093 \t -0.0037764840180860245\n",
      "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.0037764840180860245\n",
      "92     \t [-0.28495107 -1.45635923]. \t  -238.05904995480884 \t -0.0037764840180860245\n",
      "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.0037764840180860245\n",
      "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.0037764840180860245\n",
      "95     \t [1.24586252 1.53227738]. \t  -0.10003360765156546 \t -0.0037764840180860245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.24293068 1.52543964]. \t  -0.09679515991743427 \t -0.0037764840180860245\n",
      "97     \t [1.2432263  1.52651501]. \t  -0.09562714401257486 \t -0.0037764840180860245\n",
      "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.0037764840180860245\n",
      "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.0037764840180860245\n",
      "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.0037764840180860245\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.41275174 -0.97286658]. \t  -132.69348247966587 \t -1.118465165857483\n",
      "2      \t [ 0.20874147 -0.31693142]. \t  -13.622434004259702 \t -1.118465165857483\n",
      "3      \t [-0.49114702  1.07775462]. \t  -72.20163389946413 \t -1.118465165857483\n",
      "4      \t [-0.16863905  0.31002998]. \t  -9.2950576707444 \t -1.118465165857483\n",
      "5      \t [-0.50104472  0.20894064]. \t  -2.4304197416885485 \t -1.118465165857483\n",
      "6      \t [-2.02343475  1.58463911]. \t  -638.9749977990072 \t -1.118465165857483\n",
      "7      \t [-0.27229474  0.03287976]. \t  -1.789011151972603 \t -1.118465165857483\n",
      "8      \t [1.97427299 1.65111495]. \t  -505.6878315179781 \t -1.118465165857483\n",
      "9      \t [ 1.7223881  -0.75279135]. \t  -1383.9245050803652 \t -1.118465165857483\n",
      "10     \t [-1.50727822 -0.74652847]. \t  -917.3700151977407 \t -1.118465165857483\n",
      "11     \t [ 0.45756182 -0.02415879]. \t  -5.747473493975391 \t -1.118465165857483\n",
      "12     \t [-0.37864797  0.25280462]. \t  -3.0981699608693862 \t -1.118465165857483\n",
      "13     \t [-0.09116721  2.048     ]. \t  -417.22357969734804 \t -1.118465165857483\n",
      "14     \t [ 0.23888875 -0.01007463]. \t  \u001b[92m-1.030101366771608\u001b[0m \t -1.030101366771608\n",
      "15     \t [0.17293491 1.45901441]. \t  -204.91898369264595 \t -1.030101366771608\n",
      "16     \t [ 0.8520427 -0.3307208]. \t  -111.6828633500786 \t -1.030101366771608\n",
      "17     \t [0.8575366  1.97026682]. \t  -152.51755456713286 \t -1.030101366771608\n",
      "18     \t [-0.81755149 -1.84721376]. \t  -636.1299433662065 \t -1.030101366771608\n",
      "19     \t [-0.95370942  0.69716305]. \t  -8.328297141371449 \t -1.030101366771608\n",
      "20     \t [-0.38415023 -0.51128198]. \t  -45.324649641757965 \t -1.030101366771608\n",
      "21     \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.030101366771608\n",
      "22     \t [ 0.04936119 -1.5908052 ]. \t  -254.74563352649488 \t -1.030101366771608\n",
      "23     \t [-0.83722349  0.80377519]. \t  -4.432832532058066 \t -1.030101366771608\n",
      "24     \t [-0.86774288  0.63332426]. \t  -4.920158121310172 \t -1.030101366771608\n",
      "25     \t [1.560757   1.33396179]. \t  -121.75498398698987 \t -1.030101366771608\n",
      "26     \t [ 0.86474998 -1.06134866]. \t  -327.31747625263705 \t -1.030101366771608\n",
      "27     \t [-0.17477802  1.48873115]. \t  -214.0101020827603 \t -1.030101366771608\n",
      "28     \t [1.38753524 1.99555813]. \t  \u001b[92m-0.6444500539681968\u001b[0m \t -0.6444500539681968\n",
      "29     \t [1.11792007 1.30692342]. \t  \u001b[92m-0.340839128169687\u001b[0m \t -0.340839128169687\n",
      "30     \t [-1.88548814  0.78108077]. \t  -777.8251909301468 \t -0.340839128169687\n",
      "31     \t [ 1.13060795 -1.47916085]. \t  -760.3619397240727 \t -0.340839128169687\n",
      "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.340839128169687\n",
      "33     \t [-0.67499788  1.75886749]. \t  -172.6504632335947 \t -0.340839128169687\n",
      "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.340839128169687\n",
      "35     \t [1.32419917 1.54971625]. \t  -4.258027089273375 \t -0.340839128169687\n",
      "36     \t [1.20598647 1.56484595]. \t  -1.2621869560688832 \t -0.340839128169687\n",
      "37     \t [-1.1211341   1.26739421]. \t  -4.510135427706172 \t -0.340839128169687\n",
      "38     \t [-1.94580847 -0.71166838]. \t  -2031.7333252118815 \t -0.340839128169687\n",
      "39     \t [-1.03861437 -1.56074171]. \t  -700.8316600995099 \t -0.340839128169687\n",
      "40     \t [1.63843289 1.36021904]. \t  -175.76962581466148 \t -0.340839128169687\n",
      "41     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.340839128169687\n",
      "42     \t [1.1996008  1.42033209]. \t  \u001b[92m-0.07484681006073628\u001b[0m \t -0.07484681006073628\n",
      "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.07484681006073628\n",
      "44     \t [-0.54203671 -1.29930436]. \t  -256.1772363256532 \t -0.07484681006073628\n",
      "45     \t [-1.83494523 -0.90244545]. \t  -1830.873844658998 \t -0.07484681006073628\n",
      "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.07484681006073628\n",
      "47     \t [0.89277581 0.8005285 ]. \t  \u001b[92m-0.012707961825170618\u001b[0m \t -0.012707961825170618\n",
      "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.012707961825170618\n",
      "49     \t [0.98434196 0.97976621]. \t  \u001b[92m-0.011989492271941809\u001b[0m \t -0.011989492271941809\n",
      "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.011989492271941809\n",
      "51     \t [-1.45985298  2.03008325]. \t  -7.0727447641879655 \t -0.011989492271941809\n",
      "52     \t [-1.32246083  2.00388105]. \t  -11.89522283234405 \t -0.011989492271941809\n",
      "53     \t [1.2380622  1.52779857]. \t  -0.059173061193383475 \t -0.011989492271941809\n",
      "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.011989492271941809\n",
      "55     \t [1.2891623  1.69196965]. \t  -0.1737962096053322 \t -0.011989492271941809\n",
      "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.011989492271941809\n",
      "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.011989492271941809\n",
      "58     \t [1.11547996 1.21235946]. \t  -0.11532694725225504 \t -0.011989492271941809\n",
      "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.011989492271941809\n",
      "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.011989492271941809\n",
      "61     \t [0.85438349 0.72719617]. \t  -0.021974218416746995 \t -0.011989492271941809\n",
      "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.011989492271941809\n",
      "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.011989492271941809\n",
      "64     \t [0.85532453 0.75972732]. \t  -0.10015786780147526 \t -0.011989492271941809\n",
      "65     \t [-1.21261842 -1.9313326 ]. \t  -1162.1036949545671 \t -0.011989492271941809\n",
      "66     \t [1.66604392 0.39622405]. \t  -566.6353090913572 \t -0.011989492271941809\n",
      "67     \t [1.196341   1.40582986]. \t  -0.1030756069623869 \t -0.011989492271941809\n",
      "68     \t [ 0.19994297 -1.39459727]. \t  -206.44047973584387 \t -0.011989492271941809\n",
      "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.011989492271941809\n",
      "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.011989492271941809\n",
      "71     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.011989492271941809\n",
      "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.011989492271941809\n",
      "73     \t [0.68488567 0.48532686]. \t  -0.12573086419628893 \t -0.011989492271941809\n",
      "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.011989492271941809\n",
      "75     \t [ 1.39075807 -1.89625902]. \t  -1467.4004652258593 \t -0.011989492271941809\n",
      "76     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.011989492271941809\n",
      "77     \t [-1.13297187 -0.62810777]. \t  -370.021887695414 \t -0.011989492271941809\n",
      "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.011989492271941809\n",
      "79     \t [0.79676663 0.71758119]. \t  -0.7259629876842714 \t -0.011989492271941809\n",
      "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.011989492271941809\n",
      "81     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.011989492271941809\n",
      "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.011989492271941809\n",
      "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.011989492271941809\n",
      "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.011989492271941809\n",
      "85     \t [-1.7717308   2.02961981]. \t  -130.761591605953 \t -0.011989492271941809\n",
      "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.011989492271941809\n",
      "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.011989492271941809\n",
      "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.011989492271941809\n",
      "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.011989492271941809\n",
      "90     \t [-0.9563154  -0.45374215]. \t  -191.04654219354157 \t -0.011989492271941809\n",
      "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.011989492271941809\n",
      "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.011989492271941809\n",
      "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.011989492271941809\n",
      "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.011989492271941809\n",
      "95     \t [0.76579775 1.19033201]. \t  -36.5226583551558 \t -0.011989492271941809\n",
      "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.011989492271941809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.25274348  1.20739645]. \t  -18.17706518631196 \t -0.011989492271941809\n",
      "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.011989492271941809\n",
      "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.011989492271941809\n",
      "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.011989492271941809\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
      "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
      "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
      "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
      "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
      "1      \t [-0.00933519 -1.72218378]. \t  -297.64047024942215 \t -12.122423820878506\n",
      "2      \t [0.55438246 1.26879645]. \t  -92.63844192618087 \t -12.122423820878506\n",
      "3      \t [1.4102362 0.3572151]. \t  -266.3641717959405 \t -12.122423820878506\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -12.122423820878506\n",
      "5      \t [0.9509412  0.96688936]. \t  \u001b[92m-0.3942852631873592\u001b[0m \t -0.3942852631873592\n",
      "6      \t [0.74756226 0.81859824]. \t  -6.810674116298312 \t -0.3942852631873592\n",
      "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -0.3942852631873592\n",
      "8      \t [-1.256087  2.048   ]. \t  -27.203005810605532 \t -0.3942852631873592\n",
      "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -0.3942852631873592\n",
      "10     \t [1.35128497 0.97742276]. \t  -72.12682233348824 \t -0.3942852631873592\n",
      "11     \t [0.92697914 2.048     ]. \t  -141.3083993154159 \t -0.3942852631873592\n",
      "12     \t [0.64998767 0.17858348]. \t  -6.071253778808317 \t -0.3942852631873592\n",
      "13     \t [-2.03015267  2.048     ]. \t  -439.1302929844424 \t -0.3942852631873592\n",
      "14     \t [-0.9025929  1.8022134]. \t  -101.14327863611892 \t -0.3942852631873592\n",
      "15     \t [0.72473896 0.44654373]. \t  -0.6951823581833387 \t -0.3942852631873592\n",
      "16     \t [1.13234936 1.4403697 ]. \t  -2.5188053345230363 \t -0.3942852631873592\n",
      "17     \t [ 1.73164586 -0.32150232]. \t  -1102.8415029977223 \t -0.3942852631873592\n",
      "18     \t [1.11463274 1.25423886]. \t  \u001b[92m-0.027141986001183165\u001b[0m \t -0.027141986001183165\n",
      "19     \t [ 1.12210002 -1.56110415]. \t  -795.3748184470774 \t -0.027141986001183165\n",
      "20     \t [-0.61916606 -0.43957734]. \t  -70.34537379874358 \t -0.027141986001183165\n",
      "21     \t [-0.31508739  0.34382049]. \t  -7.709456709289073 \t -0.027141986001183165\n",
      "22     \t [-0.11945395 -0.38997863]. \t  -17.59481156165661 \t -0.027141986001183165\n",
      "23     \t [-0.17127353  0.00822367]. \t  -1.4164489203904496 \t -0.027141986001183165\n",
      "24     \t [1.83420092 1.92805264]. \t  -206.97453394502486 \t -0.027141986001183165\n",
      "25     \t [1.41537596 2.048     ]. \t  -0.3724435264845297 \t -0.027141986001183165\n",
      "26     \t [-1.47557584  0.22933391]. \t  -385.5950403975736 \t -0.027141986001183165\n",
      "27     \t [1.35807506 1.81672227]. \t  -0.2046456703022251 \t -0.027141986001183165\n",
      "28     \t [-0.55290055  1.30781461]. \t  -102.83506472840799 \t -0.027141986001183165\n",
      "29     \t [-1.07206331  1.34781616]. \t  -8.233529643120526 \t -0.027141986001183165\n",
      "30     \t [-0.97184682  1.01589148]. \t  -4.398050767233996 \t -0.027141986001183165\n",
      "31     \t [ 1.48633283 -0.77964418]. \t  -893.5466803786309 \t -0.027141986001183165\n",
      "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.027141986001183165\n",
      "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.027141986001183165\n",
      "34     \t [-0.01587209 -0.58220972]. \t  -34.958152717907716 \t -0.027141986001183165\n",
      "35     \t [-1.70714828 -1.19774021]. \t  -1698.2615538946 \t -0.027141986001183165\n",
      "36     \t [0.85401762 0.70594319]. \t  -0.07608043211474838 \t -0.027141986001183165\n",
      "37     \t [1.34122372 1.78401813]. \t  -0.13852429536418476 \t -0.027141986001183165\n",
      "38     \t [0.8472206  0.70417018]. \t  -0.04187173874758621 \t -0.027141986001183165\n",
      "39     \t [-1.12292196  1.80136345]. \t  -33.71106413908456 \t -0.027141986001183165\n",
      "40     \t [ 0.82992569 -0.41506505]. \t  -121.87557525235378 \t -0.027141986001183165\n",
      "41     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.027141986001183165\n",
      "42     \t [1.19079437 0.37061068]. \t  -109.7370036372121 \t -0.027141986001183165\n",
      "43     \t [0.67382959 0.44672827]. \t  -0.1117425145908162 \t -0.027141986001183165\n",
      "44     \t [0.68886396 0.4655227 ]. \t  -0.10492518129202977 \t -0.027141986001183165\n",
      "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.027141986001183165\n",
      "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.027141986001183165\n",
      "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.027141986001183165\n",
      "48     \t [0.26502029 1.06660612]. \t  -99.81558628550876 \t -0.027141986001183165\n",
      "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.027141986001183165\n",
      "50     \t [-1.91655642 -0.80527284]. \t  -2014.1679084374393 \t -0.027141986001183165\n",
      "51     \t [-0.69945851  0.39396279]. \t  -3.795975973719777 \t -0.027141986001183165\n",
      "52     \t [1.96827608 0.51385253]. \t  -1130.0710754655745 \t -0.027141986001183165\n",
      "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.027141986001183165\n",
      "54     \t [-1.2761108   1.32629013]. \t  -14.311269225975838 \t -0.027141986001183165\n",
      "55     \t [0.30291503 0.11430787]. \t  -0.5367793351067386 \t -0.027141986001183165\n",
      "56     \t [0.52351276 0.29023773]. \t  -0.2531938719557695 \t -0.027141986001183165\n",
      "57     \t [0.52180079 0.33005145]. \t  -0.5624739375058474 \t -0.027141986001183165\n",
      "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.027141986001183165\n",
      "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.027141986001183165\n",
      "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.027141986001183165\n",
      "61     \t [1.38831163 1.92221886]. \t  -0.15347985766694422 \t -0.027141986001183165\n",
      "62     \t [ 1.0450378  -1.71499994]. \t  -787.9852810986221 \t -0.027141986001183165\n",
      "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.027141986001183165\n",
      "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.027141986001183165\n",
      "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.027141986001183165\n",
      "66     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.027141986001183165\n",
      "67     \t [1.04061146 1.08656081]. \t  \u001b[92m-0.0030098714087796576\u001b[0m \t -0.0030098714087796576\n",
      "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.0030098714087796576\n",
      "69     \t [0.9302396  0.89922877]. \t  -0.11967264793471892 \t -0.0030098714087796576\n",
      "70     \t [-1.47252835  0.50797933]. \t  -281.7930650445806 \t -0.0030098714087796576\n",
      "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.0030098714087796576\n",
      "72     \t [0.13545758 1.00238521]. \t  -97.58020718432583 \t -0.0030098714087796576\n",
      "73     \t [-0.29352304 -0.09093165]. \t  -4.8091973629883595 \t -0.0030098714087796576\n",
      "74     \t [-0.84245666 -1.64991733]. \t  -560.1897177737782 \t -0.0030098714087796576\n",
      "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.0030098714087796576\n",
      "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.0030098714087796576\n",
      "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.0030098714087796576\n",
      "78     \t [1.74876814 1.68137527]. \t  -190.12253463601016 \t -0.0030098714087796576\n",
      "79     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.0030098714087796576\n",
      "80     \t [-1.6419876  -1.70374082]. \t  -1942.8604978327307 \t -0.0030098714087796576\n",
      "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.0030098714087796576\n",
      "82     \t [1.08616483 1.14274229]. \t  -0.14441133113668123 \t -0.0030098714087796576\n",
      "83     \t [0.15927452 0.02955016]. \t  -0.7085680687742651 \t -0.0030098714087796576\n",
      "84     \t [1.01586111 1.01434984]. \t  -0.03131192845511335 \t -0.0030098714087796576\n",
      "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.0030098714087796576\n",
      "86     \t [ 0.62086675 -1.99945441]. \t  -568.9328188065471 \t -0.0030098714087796576\n",
      "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.0030098714087796576\n",
      "88     \t [ 2.00197735 -0.98261503]. \t  -2491.5412798331467 \t -0.0030098714087796576\n",
      "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.0030098714087796576\n",
      "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.0030098714087796576\n",
      "91     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.0030098714087796576\n",
      "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.0030098714087796576\n",
      "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.0030098714087796576\n",
      "94     \t [0.97817863 0.96346351]. \t  -0.0048719716429606475 \t -0.0030098714087796576\n",
      "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0030098714087796576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0030098714087796576\n",
      "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0030098714087796576\n",
      "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0030098714087796576\n",
      "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0030098714087796576\n",
      "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0030098714087796576\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-0.20787038  0.38221562]. \t  -12.951425052058712 \t -1.9278091788796494\n",
      "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
      "3      \t [ 1.01950798 -1.24720923]. \t  -522.85696842208 \t -1.9278091788796494\n",
      "4      \t [ 0.07100164 -0.05570017]. \t  \u001b[92m-1.2319897893378482\u001b[0m \t -1.2319897893378482\n",
      "5      \t [-0.31009759  1.36182696]. \t  -161.9075115269378 \t -1.2319897893378482\n",
      "6      \t [1.84256052 1.9255108 ]. \t  -216.6583564474458 \t -1.2319897893378482\n",
      "7      \t [-0.36629948 -0.2342708 ]. \t  -15.44202785243899 \t -1.2319897893378482\n",
      "8      \t [0.57953198 0.81222119]. \t  -22.869047410351204 \t -1.2319897893378482\n",
      "9      \t [0.65694789 0.32591047]. \t  -1.2343010204164098 \t -1.2319897893378482\n",
      "10     \t [-1.45782604 -0.16358394]. \t  -529.920081399145 \t -1.2319897893378482\n",
      "11     \t [-1.08106709  0.51892388]. \t  -46.552527354468715 \t -1.2319897893378482\n",
      "12     \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.2319897893378482\n",
      "13     \t [-1.87846585  0.86816008]. \t  -716.0976929688445 \t -1.2319897893378482\n",
      "14     \t [-1.1702916   1.79222005]. \t  -22.572421994235256 \t -1.2319897893378482\n",
      "15     \t [-1.10742968  1.36657059]. \t  -6.406025499802244 \t -1.2319897893378482\n",
      "16     \t [-0.68365498  0.19131149]. \t  -10.456304765846308 \t -1.2319897893378482\n",
      "17     \t [-1.27730525  1.58969233]. \t  -5.36098004471088 \t -1.2319897893378482\n",
      "18     \t [-1.56321837  0.0803017 ]. \t  -565.1123929380714 \t -1.2319897893378482\n",
      "19     \t [-0.82359378  0.69999699]. \t  -3.372541060140701 \t -1.2319897893378482\n",
      "20     \t [-0.73274214 -0.48359735]. \t  -107.14613451292593 \t -1.2319897893378482\n",
      "21     \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -1.2319897893378482\n",
      "22     \t [-0.00355248 -1.6214444 ]. \t  -263.91940341438124 \t -1.2319897893378482\n",
      "23     \t [-0.34012153  0.38623137]. \t  -9.115586071104392 \t -1.2319897893378482\n",
      "24     \t [ 0.81500145 -0.12707556]. \t  -62.650256265456946 \t -1.2319897893378482\n",
      "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -1.2319897893378482\n",
      "26     \t [-0.15807809  0.05456156]. \t  -1.4286003721129885 \t -1.2319897893378482\n",
      "27     \t [0.8013301  0.17053804]. \t  -22.279359836832548 \t -1.2319897893378482\n",
      "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -1.2319897893378482\n",
      "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -1.2319897893378482\n",
      "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -1.2319897893378482\n",
      "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -1.2319897893378482\n",
      "32     \t [-1.18032486 -1.07964242]. \t  -616.2323504683599 \t -1.2319897893378482\n",
      "33     \t [2.04191633 1.94083269]. \t  -497.7467544769756 \t -1.2319897893378482\n",
      "34     \t [1.42561837 2.048     ]. \t  \u001b[92m-0.20552528304956458\u001b[0m \t -0.20552528304956458\n",
      "35     \t [1.32113337 1.60272788]. \t  -2.1384712079062433 \t -0.20552528304956458\n",
      "36     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.20552528304956458\n",
      "37     \t [1.39165245 1.85940615]. \t  -0.7507722241897082 \t -0.20552528304956458\n",
      "38     \t [-0.75365745 -1.90976445]. \t  -617.0067622668932 \t -0.20552528304956458\n",
      "39     \t [-1.12527059  1.17804048]. \t  -5.29458284839629 \t -0.20552528304956458\n",
      "40     \t [0.51026574 0.26985254]. \t  -0.24882936367099734 \t -0.20552528304956458\n",
      "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.20552528304956458\n",
      "42     \t [1.08338442 1.21664741]. \t  \u001b[92m-0.1912136837522159\u001b[0m \t -0.1912136837522159\n",
      "43     \t [-1.06064946  0.22065185]. \t  -86.02672492343893 \t -0.1912136837522159\n",
      "44     \t [-1.4610998   1.76453335]. \t  -19.767685852850942 \t -0.1912136837522159\n",
      "45     \t [-0.25640987  0.21172911]. \t  -3.7096718553017576 \t -0.1912136837522159\n",
      "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.1912136837522159\n",
      "47     \t [0.92467799 0.87198466]. \t  \u001b[92m-0.034421572121576025\u001b[0m \t -0.034421572121576025\n",
      "48     \t [0.4302708  0.17511064]. \t  -0.3346360517196895 \t -0.034421572121576025\n",
      "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.034421572121576025\n",
      "50     \t [1.12805075 1.41851326]. \t  -2.1484280874112036 \t -0.034421572121576025\n",
      "51     \t [-0.15643341 -0.63785765]. \t  -45.20531701437525 \t -0.034421572121576025\n",
      "52     \t [1.04133418 1.11839936]. \t  -0.1174615126135806 \t -0.034421572121576025\n",
      "53     \t [-0.57842583 -1.50280606]. \t  -340.0888766860873 \t -0.034421572121576025\n",
      "54     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.034421572121576025\n",
      "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.034421572121576025\n",
      "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.034421572121576025\n",
      "57     \t [0.93196297 0.90382006]. \t  -0.1289916174950372 \t -0.034421572121576025\n",
      "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.034421572121576025\n",
      "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.034421572121576025\n",
      "60     \t [ 1.54664472 -0.85143791]. \t  -1052.3590489943044 \t -0.034421572121576025\n",
      "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.034421572121576025\n",
      "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.034421572121576025\n",
      "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.034421572121576025\n",
      "64     \t [ 0.53230149 -1.91970845]. \t  -485.56313408956726 \t -0.034421572121576025\n",
      "65     \t [1.39449448 0.02403067]. \t  -369.01998822142326 \t -0.034421572121576025\n",
      "66     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.034421572121576025\n",
      "67     \t [-0.41301572  0.94702602]. \t  -62.28314763695713 \t -0.034421572121576025\n",
      "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.034421572121576025\n",
      "69     \t [1.08108308 1.15926859]. \t  \u001b[92m-0.015546421776397694\u001b[0m \t -0.015546421776397694\n",
      "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.015546421776397694\n",
      "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.015546421776397694\n",
      "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.015546421776397694\n",
      "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.015546421776397694\n",
      "74     \t [ 1.44126017 -1.05174425]. \t  -979.2432473247527 \t -0.015546421776397694\n",
      "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.015546421776397694\n",
      "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.015546421776397694\n",
      "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.015546421776397694\n",
      "78     \t [-0.80366534  0.34169506]. \t  -12.505933083898942 \t -0.015546421776397694\n",
      "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.015546421776397694\n",
      "80     \t [ 0.20382741 -0.87757499]. \t  -85.11215914353261 \t -0.015546421776397694\n",
      "81     \t [ 0.92531161 -0.60711948]. \t  -214.1364263200481 \t -0.015546421776397694\n",
      "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.015546421776397694\n",
      "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.015546421776397694\n",
      "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.015546421776397694\n",
      "85     \t [-0.93470523  0.42585779]. \t  -23.79700799426778 \t -0.015546421776397694\n",
      "86     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.015546421776397694\n",
      "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.015546421776397694\n",
      "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.015546421776397694\n",
      "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.015546421776397694\n",
      "90     \t [-1.46338831 -1.42839435]. \t  -1280.4866590917438 \t -0.015546421776397694\n",
      "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.015546421776397694\n",
      "92     \t [ 1.77148003 -1.40361085]. \t  -2063.34662094588 \t -0.015546421776397694\n",
      "93     \t [-1.54232992  0.34864832]. \t  -418.6075536786673 \t -0.015546421776397694\n",
      "94     \t [-1.09556612  1.43234486]. \t  -9.777497992412677 \t -0.015546421776397694\n",
      "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.015546421776397694\n",
      "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.015546421776397694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.015546421776397694\n",
      "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.015546421776397694\n",
      "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.015546421776397694\n",
      "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.015546421776397694\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [-0.00884268  0.57735456]. \t  -34.3425644814277 \t -3.0269049669752817\n",
      "2      \t [-0.41056426 -0.32434024]. \t  -26.285052479462937 \t -3.0269049669752817\n",
      "3      \t [-0.18259276  0.05642632]. \t  \u001b[92m-1.451822910191488\u001b[0m \t -1.451822910191488\n",
      "4      \t [-0.69918937  1.52001968]. \t  -109.21508306962447 \t -1.451822910191488\n",
      "5      \t [-0.49491752  0.74319853]. \t  -27.06060131172997 \t -1.451822910191488\n",
      "6      \t [1.25253705 1.38194386]. \t  -3.5571302781407854 \t -1.451822910191488\n",
      "7      \t [0.94735483 0.96480224]. \t  \u001b[92m-0.4559842442259708\u001b[0m \t -0.4559842442259708\n",
      "8      \t [-0.52290982  0.33972368]. \t  -2.7586773885486493 \t -0.4559842442259708\n",
      "9      \t [1.63300715 2.048     ]. \t  -38.681197246392834 \t -0.4559842442259708\n",
      "10     \t [1.01522948 1.32032244]. \t  -8.388875513891564 \t -0.4559842442259708\n",
      "11     \t [0.0982027  0.04786953]. \t  -0.9593592644593685 \t -0.4559842442259708\n",
      "12     \t [-0.03614372 -0.06604734]. \t  -1.5272460498346703 \t -0.4559842442259708\n",
      "13     \t [-0.51042121 -1.54998546]. \t  -330.07792889119304 \t -0.4559842442259708\n",
      "14     \t [-1.4958887  2.048    ]. \t  -9.827424105787463 \t -0.4559842442259708\n",
      "15     \t [0.70156348 0.6219094 ]. \t  -1.7717425513018328 \t -0.4559842442259708\n",
      "16     \t [1.17722931 0.04356452]. \t  -180.20950318768865 \t -0.4559842442259708\n",
      "17     \t [-1.27021696  1.63855742]. \t  -5.216917632880278 \t -0.4559842442259708\n",
      "18     \t [1.13837128 0.90777876]. \t  -15.082115815769942 \t -0.4559842442259708\n",
      "19     \t [-0.23267667 -1.00058447]. \t  -112.76353295205944 \t -0.4559842442259708\n",
      "20     \t [2.01114585 1.80204342]. \t  -503.9766974406107 \t -0.4559842442259708\n",
      "21     \t [1.32272775 2.048     ]. \t  -9.00789058785884 \t -0.4559842442259708\n",
      "22     \t [-1.2777204  2.048    ]. \t  -22.446267520276574 \t -0.4559842442259708\n",
      "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.4559842442259708\n",
      "24     \t [1.13585219 1.29946512]. \t  \u001b[92m-0.02711395550337144\u001b[0m \t -0.02711395550337144\n",
      "25     \t [1.44161565 2.048     ]. \t  -0.28656499806897306 \t -0.02711395550337144\n",
      "26     \t [0.11354968 1.01192361]. \t  -100.59190414532765 \t -0.02711395550337144\n",
      "27     \t [1.12747574 1.2727106 ]. \t  \u001b[92m-0.01647778719065169\u001b[0m \t -0.01647778719065169\n",
      "28     \t [0.10568514 0.11910562]. \t  -1.9648230673134135 \t -0.01647778719065169\n",
      "29     \t [-1.16042126 -0.48243   ]. \t  -339.1942622308875 \t -0.01647778719065169\n",
      "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.01647778719065169\n",
      "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.01647778719065169\n",
      "32     \t [-1.1389719   1.10184857]. \t  -8.393645889324755 \t -0.01647778719065169\n",
      "33     \t [1.7123502  0.67173204]. \t  -511.4533086722718 \t -0.01647778719065169\n",
      "34     \t [ 1.01819213 -0.91463989]. \t  -380.7790056557882 \t -0.01647778719065169\n",
      "35     \t [ 1.72965811 -0.3611323 ]. \t  -1124.692361971671 \t -0.01647778719065169\n",
      "36     \t [0.7715961  0.54976729]. \t  -0.26004276824002376 \t -0.01647778719065169\n",
      "37     \t [1.30363716 1.71805819]. \t  -0.126748190186758 \t -0.01647778719065169\n",
      "38     \t [0.71253773 0.50293746]. \t  -0.08491228159486391 \t -0.01647778719065169\n",
      "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.01647778719065169\n",
      "40     \t [ 0.32029479 -1.97746247]. \t  -433.1233074488358 \t -0.01647778719065169\n",
      "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.01647778719065169\n",
      "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.01647778719065169\n",
      "43     \t [-1.54621797  1.68497034]. \t  -56.30136737596896 \t -0.01647778719065169\n",
      "44     \t [ 1.58689748 -1.98885517]. \t  -2031.7383830266047 \t -0.01647778719065169\n",
      "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.01647778719065169\n",
      "46     \t [0.61907672 0.37675687]. \t  -0.1493263881047734 \t -0.01647778719065169\n",
      "47     \t [1.1860111  0.69200666]. \t  -51.10215552529505 \t -0.01647778719065169\n",
      "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.01647778719065169\n",
      "49     \t [0.07221105 0.65215451]. \t  -42.71393786596149 \t -0.01647778719065169\n",
      "50     \t [0.58884561 0.34929577]. \t  -0.1697015622203558 \t -0.01647778719065169\n",
      "51     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.01647778719065169\n",
      "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.01647778719065169\n",
      "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.01647778719065169\n",
      "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.01647778719065169\n",
      "55     \t [ 1.26456752 -1.16271586]. \t  -762.8498168433456 \t -0.01647778719065169\n",
      "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.01647778719065169\n",
      "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.01647778719065169\n",
      "58     \t [0.65318575 1.34301448]. \t  -84.09236747648043 \t -0.01647778719065169\n",
      "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.01647778719065169\n",
      "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.01647778719065169\n",
      "61     \t [0.42215344 0.18138096]. \t  -0.33490990235066975 \t -0.01647778719065169\n",
      "62     \t [1.17688397 1.38796941]. \t  -0.032136803507514675 \t -0.01647778719065169\n",
      "63     \t [ 0.89735443 -0.11846241]. \t  -85.33406945143894 \t -0.01647778719065169\n",
      "64     \t [1.16666026 1.36111467]. \t  -0.027775677164163383 \t -0.01647778719065169\n",
      "65     \t [-1.91902188 -1.03299139]. \t  -2232.243308095427 \t -0.01647778719065169\n",
      "66     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.01647778719065169\n",
      "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.01647778719065169\n",
      "68     \t [-0.55155062  0.27051137]. \t  -2.520856175085368 \t -0.01647778719065169\n",
      "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.01647778719065169\n",
      "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.01647778719065169\n",
      "71     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.01647778719065169\n",
      "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.01647778719065169\n",
      "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.01647778719065169\n",
      "74     \t [ 0.39624454 -1.5080447 ]. \t  -277.60514967002473 \t -0.01647778719065169\n",
      "75     \t [-1.13532691  1.12907122]. \t  -7.11629317883051 \t -0.01647778719065169\n",
      "76     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.01647778719065169\n",
      "77     \t [1.43432384 2.048     ]. \t  -0.19725809382739182 \t -0.01647778719065169\n",
      "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.01647778719065169\n",
      "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.01647778719065169\n",
      "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.01647778719065169\n",
      "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.01647778719065169\n",
      "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.01647778719065169\n",
      "83     \t [-1.72488578  0.24382631]. \t  -753.4821381841246 \t -0.01647778719065169\n",
      "84     \t [0.85061931 0.72472308]. \t  -0.022451450323285087 \t -0.01647778719065169\n",
      "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.01647778719065169\n",
      "86     \t [0.99042542 0.98749623]. \t  \u001b[92m-0.004386775852060192\u001b[0m \t -0.004386775852060192\n",
      "87     \t [1.82363278 1.78019387]. \t  -239.51767098091653 \t -0.004386775852060192\n",
      "88     \t [1.00944389 1.02531674]. \t  \u001b[92m-0.004108453769549359\u001b[0m \t -0.004108453769549359\n",
      "89     \t [1.37944416 1.87282225]. \t  -0.23424169414195317 \t -0.004108453769549359\n",
      "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.004108453769549359\n",
      "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.004108453769549359\n",
      "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.004108453769549359\n",
      "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.004108453769549359\n",
      "94     \t [-0.0111504  1.0522277]. \t  -111.71457518049557 \t -0.004108453769549359\n",
      "95     \t [1.35707418 1.8092718 ]. \t  -0.23233890330060425 \t -0.004108453769549359\n",
      "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.004108453769549359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.004108453769549359\n",
      "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.004108453769549359\n",
      "99     \t [0.95872571 0.92954807]. \t  -0.01250519048231594 \t -0.004108453769549359\n",
      "100    \t [-1.45895949 -1.65195926]. \t  -1435.2811721917542 \t -0.004108453769549359\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.47273281  1.16874249]. \t  -91.52175707418381 \t -2.0077595729598063\n",
      "2      \t [-0.02450474  0.04538531]. \t  \u001b[92m-1.2501780197199097\u001b[0m \t -1.2501780197199097\n",
      "3      \t [1.68585264 0.78240463]. \t  -424.70453514476327 \t -1.2501780197199097\n",
      "4      \t [-0.42007355  0.486865  ]. \t  -11.651624457822306 \t -1.2501780197199097\n",
      "5      \t [-1.44885198  1.39505365]. \t  -55.57515125944981 \t -1.2501780197199097\n",
      "6      \t [ 0.24875508 -1.16743118]. \t  -151.68474322135503 \t -1.2501780197199097\n",
      "7      \t [0.93084749 2.03834578]. \t  -137.33241456947934 \t -1.2501780197199097\n",
      "8      \t [1.06444123 1.42879822]. \t  -8.75173381079799 \t -1.2501780197199097\n",
      "9      \t [-0.98099823  0.95588429]. \t  -3.928544263348288 \t -1.2501780197199097\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.2501780197199097\n",
      "11     \t [0.62288294 0.18463974]. \t  -4.277071860372212 \t -1.2501780197199097\n",
      "12     \t [-0.69809527  0.50777279]. \t  -2.925289656046603 \t -1.2501780197199097\n",
      "13     \t [1.47313155 1.22820186]. \t  -88.94418548055519 \t -1.2501780197199097\n",
      "14     \t [0.47519087 0.42362014]. \t  -4.188453817223986 \t -1.2501780197199097\n",
      "15     \t [ 0.26461578 -0.15711781]. \t  -5.700016984471431 \t -1.2501780197199097\n",
      "16     \t [-0.06599621 -0.01552053]. \t  \u001b[92m-1.1758535835385266\u001b[0m \t -1.1758535835385266\n",
      "17     \t [1.24180127 1.48658269]. \t  \u001b[92m-0.3663562110243889\u001b[0m \t -0.3663562110243889\n",
      "18     \t [0.47256492 1.92336581]. \t  -289.29457645503635 \t -0.3663562110243889\n",
      "19     \t [-0.92606685  0.78687317]. \t  -4.209959456551836 \t -0.3663562110243889\n",
      "20     \t [ 1.118485   -1.11392497]. \t  -559.3051604113011 \t -0.3663562110243889\n",
      "21     \t [-0.61359581 -2.048     ]. \t  -590.4236287757818 \t -0.3663562110243889\n",
      "22     \t [0.44493233 0.14422343]. \t  -0.5969133387990369 \t -0.3663562110243889\n",
      "23     \t [ 0.41310572 -1.76305071]. \t  -374.26673998424303 \t -0.3663562110243889\n",
      "24     \t [-0.40424352  0.18651119]. \t  -2.0252533407865503 \t -0.3663562110243889\n",
      "25     \t [-0.23527315  1.89308588]. \t  -339.25194620375197 \t -0.3663562110243889\n",
      "26     \t [1.28518478 1.62368908]. \t  \u001b[92m-0.15979097945501483\u001b[0m \t -0.15979097945501483\n",
      "27     \t [-1.51502555 -0.57082521]. \t  -827.7941113195253 \t -0.15979097945501483\n",
      "28     \t [-1.87469876 -0.93454369]. \t  -1987.6588142819153 \t -0.15979097945501483\n",
      "29     \t [-0.46501394 -0.4439421 ]. \t  -45.730037758046485 \t -0.15979097945501483\n",
      "30     \t [-1.93949334  1.69699813]. \t  -434.9129180923196 \t -0.15979097945501483\n",
      "31     \t [-1.2491973   1.36576703]. \t  -8.850743886580085 \t -0.15979097945501483\n",
      "32     \t [ 1.54002649 -1.83787344]. \t  -1772.3269971951645 \t -0.15979097945501483\n",
      "33     \t [-1.8431759  -1.77951983]. \t  -2688.0272945478355 \t -0.15979097945501483\n",
      "34     \t [-1.280005   1.5420202]. \t  -6.127575986568125 \t -0.15979097945501483\n",
      "35     \t [ 0.2736799  -0.01522965]. \t  -1.3398885876565299 \t -0.15979097945501483\n",
      "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.15979097945501483\n",
      "37     \t [-0.73283867  0.87632897]. \t  -14.513580664318035 \t -0.15979097945501483\n",
      "38     \t [-1.26947568  2.048     ]. \t  -24.19776588850033 \t -0.15979097945501483\n",
      "39     \t [-1.72517692 -0.55026939]. \t  -1251.0501918294053 \t -0.15979097945501483\n",
      "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.15979097945501483\n",
      "41     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.15979097945501483\n",
      "42     \t [1.4493702  1.88796319]. \t  -4.72652160481193 \t -0.15979097945501483\n",
      "43     \t [0.8199031  1.16600161]. \t  -24.41237951303199 \t -0.15979097945501483\n",
      "44     \t [-0.86486021  0.97471617]. \t  -8.61848851357872 \t -0.15979097945501483\n",
      "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.15979097945501483\n",
      "46     \t [1.4062769 2.048    ]. \t  -0.6604695406562099 \t -0.15979097945501483\n",
      "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.15979097945501483\n",
      "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.15979097945501483\n",
      "49     \t [1.34714981 1.81726515]. \t  \u001b[92m-0.1211144876496195\u001b[0m \t -0.1211144876496195\n",
      "50     \t [0.7791266  0.59108038]. \t  \u001b[92m-0.07425045027656486\u001b[0m \t -0.07425045027656486\n",
      "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.07425045027656486\n",
      "52     \t [ 0.58352362 -0.23479548]. \t  -33.2699195494349 \t -0.07425045027656486\n",
      "53     \t [1.2308309  1.49906477]. \t  -0.07850010578585372 \t -0.07425045027656486\n",
      "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.07425045027656486\n",
      "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.07425045027656486\n",
      "56     \t [0.24163853 0.08129877]. \t  -0.6275970524778705 \t -0.07425045027656486\n",
      "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.07425045027656486\n",
      "58     \t [0.70000036 0.47705868]. \t  -0.10674884830409836 \t -0.07425045027656486\n",
      "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.07425045027656486\n",
      "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.07425045027656486\n",
      "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.07425045027656486\n",
      "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.07425045027656486\n",
      "63     \t [ 1.97666469 -0.70501524]. \t  -2128.209855573222 \t -0.07425045027656486\n",
      "64     \t [1.07958268 1.14061527]. \t  \u001b[92m-0.06825221475838486\u001b[0m \t -0.06825221475838486\n",
      "65     \t [-1.58125633  0.36595552]. \t  -462.2360771499493 \t -0.06825221475838486\n",
      "66     \t [1.11681299 1.23270021]. \t  \u001b[92m-0.034876787138852644\u001b[0m \t -0.034876787138852644\n",
      "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.034876787138852644\n",
      "68     \t [1.06543941 1.11951553]. \t  \u001b[92m-0.028760793969299633\u001b[0m \t -0.028760793969299633\n",
      "69     \t [1.35648952 1.84591095]. \t  -0.13050367421886694 \t -0.028760793969299633\n",
      "70     \t [-1.453335    0.70777001]. \t  -203.2563341481985 \t -0.028760793969299633\n",
      "71     \t [1.55375794 0.04422647]. \t  -561.9669176444024 \t -0.028760793969299633\n",
      "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.028760793969299633\n",
      "73     \t [0.59921347 0.36182552]. \t  -0.16139643161135042 \t -0.028760793969299633\n",
      "74     \t [1.36052246 1.85931937]. \t  -0.13686212611877535 \t -0.028760793969299633\n",
      "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.028760793969299633\n",
      "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.028760793969299633\n",
      "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.028760793969299633\n",
      "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.028760793969299633\n",
      "79     \t [1.03613652 1.03388204]. \t  -0.15888987957400896 \t -0.028760793969299633\n",
      "80     \t [ 1.05092279 -1.43042672]. \t  -642.5568651619753 \t -0.028760793969299633\n",
      "81     \t [0.20377698 0.76973986]. \t  -53.66365112401365 \t -0.028760793969299633\n",
      "82     \t [1.0043492  1.01149819]. \t  \u001b[92m-0.0007922403871347229\u001b[0m \t -0.0007922403871347229\n",
      "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.0007922403871347229\n",
      "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.0007922403871347229\n",
      "85     \t [1.10113693 1.20180191]. \t  -0.021679006356357104 \t -0.0007922403871347229\n",
      "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.0007922403871347229\n",
      "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.0007922403871347229\n",
      "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.0007922403871347229\n",
      "89     \t [1.35936773 0.81886257]. \t  -106.01696195155297 \t -0.0007922403871347229\n",
      "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.0007922403871347229\n",
      "91     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0007922403871347229\n",
      "92     \t [ 0.14583905 -0.10642548]. \t  -2.3601797106240374 \t -0.0007922403871347229\n",
      "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.0007922403871347229\n",
      "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.0007922403871347229\n",
      "95     \t [0.63363781 0.41420535]. \t  -0.15037179969733092 \t -0.0007922403871347229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.0007922403871347229\n",
      "97     \t [1.36083287 1.87749507]. \t  -0.1958847502149283 \t -0.0007922403871347229\n",
      "98     \t [0.63313443 0.41442596]. \t  -0.15299603144374702 \t -0.0007922403871347229\n",
      "99     \t [1.14033305 1.29017386]. \t  -0.03006803698557653 \t -0.0007922403871347229\n",
      "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.0007922403871347229\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.6164745  0.32466281]. \t  \u001b[92m-0.45376407586037554\u001b[0m \t -0.45376407586037554\n",
      "2      \t [ 0.50908955 -0.18347256]. \t  -19.834429029373293 \t -0.45376407586037554\n",
      "3      \t [-1.32333068  0.93303667]. \t  -72.33765762594129 \t -0.45376407586037554\n",
      "4      \t [1.44508292 0.8206238 ]. \t  -160.8894336881283 \t -0.45376407586037554\n",
      "5      \t [-0.62676042  0.76344834]. \t  -16.382247344473406 \t -0.45376407586037554\n",
      "6      \t [-1.032139    0.43005276]. \t  -44.48488220039099 \t -0.45376407586037554\n",
      "7      \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.45376407586037554\n",
      "8      \t [-0.56185309  0.40368017]. \t  -3.213807568923504 \t -0.45376407586037554\n",
      "9      \t [0.88835827 0.23244623]. \t  -31.00775999418488 \t -0.45376407586037554\n",
      "10     \t [0.43882728 1.55440029]. \t  -185.7732570546789 \t -0.45376407586037554\n",
      "11     \t [ 0.64467069 -0.79401276]. \t  -146.44263235022598 \t -0.45376407586037554\n",
      "12     \t [0.2852873  0.61793147]. \t  -29.298613771896225 \t -0.45376407586037554\n",
      "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.45376407586037554\n",
      "14     \t [-1.07680911 -0.66245332]. \t  -336.27103308075306 \t -0.45376407586037554\n",
      "15     \t [-1.39359154  0.69519013]. \t  -161.20705136277712 \t -0.45376407586037554\n",
      "16     \t [ 1.9323183 -1.5303425]. \t  -2772.0457234584587 \t -0.45376407586037554\n",
      "17     \t [-1.09598532  1.29103451]. \t  -5.200469214996014 \t -0.45376407586037554\n",
      "18     \t [-1.54848356 -1.82686842]. \t  -1791.2782240225968 \t -0.45376407586037554\n",
      "19     \t [-0.79976726  0.61285556]. \t  -3.310836720784585 \t -0.45376407586037554\n",
      "20     \t [-0.17881907 -1.96887286]. \t  -401.7293350850919 \t -0.45376407586037554\n",
      "21     \t [-0.09416479 -2.01791579]. \t  -411.98204758939727 \t -0.45376407586037554\n",
      "22     \t [-0.25896759  1.05540738]. \t  -99.26722111254946 \t -0.45376407586037554\n",
      "23     \t [-0.24884643  1.86338313]. \t  -326.08491946804156 \t -0.45376407586037554\n",
      "24     \t [-0.96524174 -1.36581502]. \t  -531.715849495629 \t -0.45376407586037554\n",
      "25     \t [0.48291786 0.24483966]. \t  \u001b[92m-0.28089963048330086\u001b[0m \t -0.28089963048330086\n",
      "26     \t [-1.13686556  0.57091361]. \t  -56.629591495046235 \t -0.28089963048330086\n",
      "27     \t [0.04232686 0.05822716]. \t  -1.2356354817148862 \t -0.28089963048330086\n",
      "28     \t [0.2767406  0.46772662]. \t  -15.82225228302519 \t -0.28089963048330086\n",
      "29     \t [-1.00237446  1.05328022]. \t  -4.244977486898595 \t -0.28089963048330086\n",
      "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.28089963048330086\n",
      "31     \t [-0.45283819  1.95181393]. \t  -307.22482120864316 \t -0.28089963048330086\n",
      "32     \t [ 1.98022044 -0.46929244]. \t  -1928.6673011391758 \t -0.28089963048330086\n",
      "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.28089963048330086\n",
      "34     \t [ 0.66061502 -0.95589505]. \t  -193.9671294202154 \t -0.28089963048330086\n",
      "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.28089963048330086\n",
      "36     \t [-1.80825474  0.3658329 ]. \t  -851.1801983666131 \t -0.28089963048330086\n",
      "37     \t [-2.04007732  1.73210541]. \t  -599.6397734779823 \t -0.28089963048330086\n",
      "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.28089963048330086\n",
      "39     \t [-0.11215858  1.13031619]. \t  -126.17041680603957 \t -0.28089963048330086\n",
      "40     \t [0.91482339 0.79423007]. \t  \u001b[92m-0.18934306041291585\u001b[0m \t -0.18934306041291585\n",
      "41     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.18934306041291585\n",
      "42     \t [1.32620756 1.89312966]. \t  -1.9101456898767162 \t -0.18934306041291585\n",
      "43     \t [1.24394179 1.51526971]. \t  \u001b[92m-0.16268653948898826\u001b[0m \t -0.16268653948898826\n",
      "44     \t [0.31095434 0.0873135 ]. \t  -0.48358067086360496 \t -0.16268653948898826\n",
      "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.16268653948898826\n",
      "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.16268653948898826\n",
      "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.16268653948898826\n",
      "48     \t [0.82402371 0.6225959 ]. \t  -0.34928005198783274 \t -0.16268653948898826\n",
      "49     \t [ 0.18357043 -0.01507331]. \t  -0.9044222849638495 \t -0.16268653948898826\n",
      "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.16268653948898826\n",
      "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.16268653948898826\n",
      "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.16268653948898826\n",
      "53     \t [1.24601934 1.69185633]. \t  -2.000755285254035 \t -0.16268653948898826\n",
      "54     \t [-1.18082414  1.75523799]. \t  -17.78032166588062 \t -0.16268653948898826\n",
      "55     \t [ 1.25588263 -1.705961  ]. \t  -1078.0071343372058 \t -0.16268653948898826\n",
      "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.16268653948898826\n",
      "57     \t [1.05624264 1.06021813]. \t  -0.3104159927602636 \t -0.16268653948898826\n",
      "58     \t [0.88169297 1.8141501 ]. \t  -107.50270556616614 \t -0.16268653948898826\n",
      "59     \t [-1.26352511  0.09110064]. \t  -231.7449777164809 \t -0.16268653948898826\n",
      "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.16268653948898826\n",
      "61     \t [-0.26935638 -0.24773242]. \t  -11.86953142243642 \t -0.16268653948898826\n",
      "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.16268653948898826\n",
      "63     \t [0.99005936 0.96303684]. \t  \u001b[92m-0.029616457175806413\u001b[0m \t -0.029616457175806413\n",
      "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.029616457175806413\n",
      "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.029616457175806413\n",
      "66     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.029616457175806413\n",
      "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.029616457175806413\n",
      "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.029616457175806413\n",
      "69     \t [0.88375916 0.93238652]. \t  -2.3043837843615584 \t -0.029616457175806413\n",
      "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.029616457175806413\n",
      "71     \t [ 1.92973463 -0.0818242 ]. \t  -1449.1996114104088 \t -0.029616457175806413\n",
      "72     \t [1.32948596 1.71639491]. \t  -0.37007060608890185 \t -0.029616457175806413\n",
      "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.029616457175806413\n",
      "74     \t [1.28473937 1.60313041]. \t  -0.30598814683547887 \t -0.029616457175806413\n",
      "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.029616457175806413\n",
      "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.029616457175806413\n",
      "77     \t [-1.21131546e+00 -4.31045776e-04]. \t  -220.30899682730103 \t -0.029616457175806413\n",
      "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.029616457175806413\n",
      "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.029616457175806413\n",
      "80     \t [1.40959923 2.048     ]. \t  -0.5402378130625347 \t -0.029616457175806413\n",
      "81     \t [1.30629721 1.66037838]. \t  -0.305730977917589 \t -0.029616457175806413\n",
      "82     \t [1.30180202 1.64962359]. \t  -0.29416903650652076 \t -0.029616457175806413\n",
      "83     \t [ 1.45351392 -0.62312236]. \t  -748.6795639055398 \t -0.029616457175806413\n",
      "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.029616457175806413\n",
      "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.029616457175806413\n",
      "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.029616457175806413\n",
      "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.029616457175806413\n",
      "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.029616457175806413\n",
      "89     \t [0.69718911 0.49831639]. \t  -0.10668536903475861 \t -0.029616457175806413\n",
      "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.029616457175806413\n",
      "91     \t [0.66891714 0.45693919]. \t  -0.11862008224737726 \t -0.029616457175806413\n",
      "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.029616457175806413\n",
      "93     \t [0.68434031 0.4798637 ]. \t  -0.11296290571470906 \t -0.029616457175806413\n",
      "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.029616457175806413\n",
      "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.029616457175806413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.029616457175806413\n",
      "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.029616457175806413\n",
      "98     \t [-0.03460129  1.98045929]. \t  -392.81822179015063 \t -0.029616457175806413\n",
      "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.029616457175806413\n",
      "100    \t [1.18931045 1.35118693]. \t  -0.43617834160686086 \t -0.029616457175806413\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [ 0.43606844 -1.67369118]. \t  -347.71053213071485 \t -29.9831488845538\n",
      "2      \t [-0.13839446  0.7147904 ]. \t  -49.68707676672544 \t -29.9831488845538\n",
      "3      \t [-1.8510361   2.03742245]. \t  -201.03611268756 \t -29.9831488845538\n",
      "4      \t [ 1.36280034 -0.74164998]. \t  -675.5466126021265 \t -29.9831488845538\n",
      "5      \t [0.42078817 0.31547559]. \t  \u001b[92m-2.2512996056908405\u001b[0m \t -2.2512996056908405\n",
      "6      \t [-1.19704929  0.13798041]. \t  -172.5156939449146 \t -2.2512996056908405\n",
      "7      \t [-1.33704579  1.04920576]. \t  -59.99789396651467 \t -2.2512996056908405\n",
      "8      \t [-0.15828987  1.24297839]. \t  -149.67520799598637 \t -2.2512996056908405\n",
      "9      \t [-0.68251278  1.25763828]. \t  -65.527882489692 \t -2.2512996056908405\n",
      "10     \t [-2.01346326 -1.00148458]. \t  -2564.9080532207845 \t -2.2512996056908405\n",
      "11     \t [-0.61670841  0.32014372]. \t  -2.9759760760009555 \t -2.2512996056908405\n",
      "12     \t [-0.92944254  0.60020216]. \t  -10.674475932592117 \t -2.2512996056908405\n",
      "13     \t [0.26659492 0.21386467]. \t  -2.576833295854134 \t -2.2512996056908405\n",
      "14     \t [-1.84498644 -1.0743048 ]. \t  -2013.5929248868647 \t -2.2512996056908405\n",
      "15     \t [ 0.3754054  -1.02590489]. \t  -136.54030096647267 \t -2.2512996056908405\n",
      "16     \t [-0.63587684 -0.38921785]. \t  -65.64939686735737 \t -2.2512996056908405\n",
      "17     \t [ 1.63480771 -0.56566643]. \t  -1049.0374887651844 \t -2.2512996056908405\n",
      "18     \t [2.048      1.43539746]. \t  -762.2548335919013 \t -2.2512996056908405\n",
      "19     \t [1.83310149 1.59724977]. \t  -311.51493996050334 \t -2.2512996056908405\n",
      "20     \t [1.24346466 2.048     ]. \t  -25.239161082041527 \t -2.2512996056908405\n",
      "21     \t [1.16346116 1.29482541]. \t  \u001b[92m-0.372657204635996\u001b[0m \t -0.372657204635996\n",
      "22     \t [1.5556845 2.048    ]. \t  -14.158664625390305 \t -0.372657204635996\n",
      "23     \t [1.35470952 1.74065179]. \t  -1.020471893124139 \t -0.372657204635996\n",
      "24     \t [1.05690445 0.86672317]. \t  -6.269440678154241 \t -0.372657204635996\n",
      "25     \t [-1.41959775 -1.09305594]. \t  -972.0158623952941 \t -0.372657204635996\n",
      "26     \t [-1.88587156  0.91344747]. \t  -706.9070146986345 \t -0.372657204635996\n",
      "27     \t [-0.64415618  0.12182517]. \t  -11.294714704146086 \t -0.372657204635996\n",
      "28     \t [-1.11146794  2.048     ]. \t  -70.49651406754407 \t -0.372657204635996\n",
      "29     \t [-1.40918017  1.68587243]. \t  -14.79912853253838 \t -0.372657204635996\n",
      "30     \t [-1.19989204  1.47928733]. \t  -4.995917044029757 \t -0.372657204635996\n",
      "31     \t [0.58181308 1.3615467 ]. \t  -104.83601437806144 \t -0.372657204635996\n",
      "32     \t [1.19426837 1.56898936]. \t  -2.0744237819649807 \t -0.372657204635996\n",
      "33     \t [0.85032359 0.68430792]. \t  \u001b[92m-0.1724995416864819\u001b[0m \t -0.1724995416864819\n",
      "34     \t [1.82219672 1.80090893]. \t  -231.56158444838957 \t -0.1724995416864819\n",
      "35     \t [0.72676339 0.43461712]. \t  -0.9501534583661697 \t -0.1724995416864819\n",
      "36     \t [ 1.63076731 -1.45423047]. \t  -1692.5951079213537 \t -0.1724995416864819\n",
      "37     \t [-0.33797863  0.41658592]. \t  -10.932124152684748 \t -0.1724995416864819\n",
      "38     \t [-0.36329081  1.32777155]. \t  -144.85025260403364 \t -0.1724995416864819\n",
      "39     \t [0.35214645 0.08158721]. \t  -0.5996591297057909 \t -0.1724995416864819\n",
      "40     \t [-0.98511057  1.00135647]. \t  -4.036229232607207 \t -0.1724995416864819\n",
      "41     \t [-0.59836164 -1.03523576]. \t  -196.67556195811156 \t -0.1724995416864819\n",
      "42     \t [-0.15119024 -1.71306133]. \t  -302.6670012948018 \t -0.1724995416864819\n",
      "43     \t [1.25165206 1.51651917]. \t  -0.31446702373142194 \t -0.1724995416864819\n",
      "44     \t [0.99127909 1.00634457]. \t  \u001b[92m-0.05629401970937036\u001b[0m \t -0.05629401970937036\n",
      "45     \t [-1.2051055   1.08983678]. \t  -17.998945242101303 \t -0.05629401970937036\n",
      "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.05629401970937036\n",
      "47     \t [-0.0868701  -0.07454394]. \t  -1.8551691418678062 \t -0.05629401970937036\n",
      "48     \t [0.64058855 1.94804941]. \t  -236.57998974621958 \t -0.05629401970937036\n",
      "49     \t [-1.72312315 -0.3832916 ]. \t  -1131.3041455848916 \t -0.05629401970937036\n",
      "50     \t [ 1.98436426 -0.06430682]. \t  -1602.5760280513484 \t -0.05629401970937036\n",
      "51     \t [ 0.88625641 -1.55731416]. \t  -548.8675271403135 \t -0.05629401970937036\n",
      "52     \t [-0.25987559 -0.36331888]. \t  -20.150820827709044 \t -0.05629401970937036\n",
      "53     \t [-1.54536301  0.97857225]. \t  -205.16892502915746 \t -0.05629401970937036\n",
      "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.05629401970937036\n",
      "55     \t [1.22428949 1.46928835]. \t  -0.13790057367087188 \t -0.05629401970937036\n",
      "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.05629401970937036\n",
      "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.05629401970937036\n",
      "58     \t [-1.49595357 -0.91197093]. \t  -998.3840314171645 \t -0.05629401970937036\n",
      "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.05629401970937036\n",
      "60     \t [0.94958602 0.91021023]. \t  \u001b[92m-0.00976080774599615\u001b[0m \t -0.00976080774599615\n",
      "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.00976080774599615\n",
      "62     \t [0.91207372 0.84021571]. \t  -0.014681990172699682 \t -0.00976080774599615\n",
      "63     \t [0.46873837 1.96743036]. \t  -305.7329054062828 \t -0.00976080774599615\n",
      "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.00976080774599615\n",
      "65     \t [1.60269915 1.7340784 ]. \t  -70.01331296577641 \t -0.00976080774599615\n",
      "66     \t [0.00140331 1.36362061]. \t  -186.94277558703737 \t -0.00976080774599615\n",
      "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.00976080774599615\n",
      "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.00976080774599615\n",
      "69     \t [-0.55761127 -1.33865854]. \t  -274.54049687577555 \t -0.00976080774599615\n",
      "70     \t [ 1.2980682  -1.25427281]. \t  -864.0101684671865 \t -0.00976080774599615\n",
      "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.00976080774599615\n",
      "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.00976080774599615\n",
      "73     \t [ 0.23754772 -0.61027856]. \t  -45.03121945417927 \t -0.00976080774599615\n",
      "74     \t [-1.66305092 -0.65620777]. \t  -1178.0633699721684 \t -0.00976080774599615\n",
      "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.00976080774599615\n",
      "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.00976080774599615\n",
      "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.00976080774599615\n",
      "78     \t [-0.69654673 -1.22160209]. \t  -294.18787606883797 \t -0.00976080774599615\n",
      "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.00976080774599615\n",
      "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.00976080774599615\n",
      "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.00976080774599615\n",
      "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.00976080774599615\n",
      "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.00976080774599615\n",
      "84     \t [0.84865451 0.72737381]. \t  -0.028031062039438847 \t -0.00976080774599615\n",
      "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.00976080774599615\n",
      "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.00976080774599615\n",
      "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.00976080774599615\n",
      "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.00976080774599615\n",
      "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.00976080774599615\n",
      "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.00976080774599615\n",
      "91     \t [0.59657037 0.36192193]. \t  -0.16638641108880176 \t -0.00976080774599615\n",
      "92     \t [1.28316479 1.59543828]. \t  -0.3410335449016928 \t -0.00976080774599615\n",
      "93     \t [0.59182538 0.35597083]. \t  -0.16987097899495665 \t -0.00976080774599615\n",
      "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.00976080774599615\n",
      "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.00976080774599615\n",
      "96     \t [ 1.98571538 -1.49840793]. \t  -2961.9350309063184 \t -0.00976080774599615\n",
      "97     \t [-1.03840028 -0.95345358]. \t  -416.9472377976473 \t -0.00976080774599615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.64266637 0.42010077]. \t  -0.13270096373467086 \t -0.00976080774599615\n",
      "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.00976080774599615\n",
      "100    \t [ 1.19856704 -0.13210183]. \t  -246.11034989199715 \t -0.00976080774599615\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
      "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
      "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
      "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
      "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
      "1      \t [-0.85767015  2.048     ]. \t  -175.69081547649077 \t -8.580376531587937\n",
      "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
      "3      \t [-1.87496607  1.74203438]. \t  -322.7826713148287 \t -8.580376531587937\n",
      "4      \t [-0.02273896 -1.98197652]. \t  -394.07407427702145 \t -8.580376531587937\n",
      "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
      "6      \t [1.82395491 1.22407741]. \t  -442.82797197898645 \t -8.580376531587937\n",
      "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
      "8      \t [0.27906648 0.14523119]. \t  \u001b[92m-0.9733889581797339\u001b[0m \t -0.9733889581797339\n",
      "9      \t [0.53651394 0.48570281]. \t  -4.129503229441299 \t -0.9733889581797339\n",
      "10     \t [0.5075319  0.16639316]. \t  -1.0741861547286604 \t -0.9733889581797339\n",
      "11     \t [0.82681696 0.51407135]. \t  -2.9048798393068798 \t -0.9733889581797339\n",
      "12     \t [-0.41232715  0.43404141]. \t  -8.965731880345292 \t -0.9733889581797339\n",
      "13     \t [0.09685213 1.14732323]. \t  -130.30708024312105 \t -0.9733889581797339\n",
      "14     \t [-0.05292324 -0.03230472]. \t  -1.2318875908055162 \t -0.9733889581797339\n",
      "15     \t [0.67024592 0.41040153]. \t  \u001b[92m-0.25949962289603734\u001b[0m \t -0.25949962289603734\n",
      "16     \t [ 0.25042996 -0.16555912]. \t  -5.7727700106054725 \t -0.25949962289603734\n",
      "17     \t [0.90326441 1.1188775 ]. \t  -9.189706511440672 \t -0.25949962289603734\n",
      "18     \t [0.80699632 0.783624  ]. \t  -1.7897214840504985 \t -0.25949962289603734\n",
      "19     \t [-0.08358053  0.1756553 ]. \t  -4.019090099792217 \t -0.25949962289603734\n",
      "20     \t [-1.0616581   1.04308552]. \t  -4.956578467866227 \t -0.25949962289603734\n",
      "21     \t [ 0.10767256 -0.01557173]. \t  -0.8700425774512041 \t -0.25949962289603734\n",
      "22     \t [-0.54406621  0.55178079]. \t  -8.926110786722774 \t -0.25949962289603734\n",
      "23     \t [1.40780934 1.88021852]. \t  -1.2007728814974612 \t -0.25949962289603734\n",
      "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.25949962289603734\n",
      "25     \t [1.36484112 2.048     ]. \t  -3.563336267724233 \t -0.25949962289603734\n",
      "26     \t [1.45733524 0.6290517 ]. \t  -223.64417807412963 \t -0.25949962289603734\n",
      "27     \t [-0.24544502  1.24586148]. \t  -142.12018927681916 \t -0.25949962289603734\n",
      "28     \t [1.24267316 1.59589791]. \t  -0.32577948061250184 \t -0.25949962289603734\n",
      "29     \t [1.82728722 0.31366993]. \t  -915.9336552102571 \t -0.25949962289603734\n",
      "30     \t [1.13205766 1.17048564]. \t  -1.2510692927073752 \t -0.25949962289603734\n",
      "31     \t [0.98187324 0.92603428]. \t  \u001b[92m-0.14503875645377873\u001b[0m \t -0.14503875645377873\n",
      "32     \t [-0.31793576  0.82467337]. \t  -54.09523653176086 \t -0.14503875645377873\n",
      "33     \t [0.83897536 0.68296945]. \t  \u001b[92m-0.06965261921905326\u001b[0m \t -0.06965261921905326\n",
      "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.06965261921905326\n",
      "35     \t [ 2.04545002 -1.72379955]. \t  -3491.1439590465025 \t -0.06965261921905326\n",
      "36     \t [0.98717698 0.86455261]. \t  -1.2094115911311565 \t -0.06965261921905326\n",
      "37     \t [-0.49143264  0.98704416]. \t  -57.80707919856127 \t -0.06965261921905326\n",
      "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.06965261921905326\n",
      "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.06965261921905326\n",
      "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.06965261921905326\n",
      "41     \t [1.01964144 1.02597538]. \t  \u001b[92m-0.019136384783796413\u001b[0m \t -0.019136384783796413\n",
      "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.019136384783796413\n",
      "43     \t [ 1.09423273 -1.03254202]. \t  -497.2486072284271 \t -0.019136384783796413\n",
      "44     \t [-1.87395402  0.49278705]. \t  -919.6453670223667 \t -0.019136384783796413\n",
      "45     \t [-0.39397142  0.06160881]. \t  -2.8193397143106482 \t -0.019136384783796413\n",
      "46     \t [ 1.89366666 -1.78216789]. \t  -2882.4927331825857 \t -0.019136384783796413\n",
      "47     \t [1.4471799 2.048    ]. \t  -0.41461353561270375 \t -0.019136384783796413\n",
      "48     \t [1.0304227  1.05837174]. \t  \u001b[92m-0.002080999024356565\u001b[0m \t -0.002080999024356565\n",
      "49     \t [1.02664626 1.0496866 ]. \t  -0.002572769217248201 \t -0.002080999024356565\n",
      "50     \t [1.36812328 1.89993209]. \t  -0.21487408127456034 \t -0.002080999024356565\n",
      "51     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.002080999024356565\n",
      "52     \t [-0.84502623 -1.83974622]. \t  -655.6015088704874 \t -0.002080999024356565\n",
      "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.002080999024356565\n",
      "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.002080999024356565\n",
      "55     \t [-1.49630139  1.46255007]. \t  -66.50621539864875 \t -0.002080999024356565\n",
      "56     \t [-0.77159888  0.08083053]. \t  -29.613117616473627 \t -0.002080999024356565\n",
      "57     \t [-1.44741732  2.048     ]. \t  -6.21091046371543 \t -0.002080999024356565\n",
      "58     \t [-1.34492302  1.79667011]. \t  -5.513420942184953 \t -0.002080999024356565\n",
      "59     \t [1.31417954 1.98169538]. \t  -6.582225985731442 \t -0.002080999024356565\n",
      "60     \t [1.22557584 1.48303182]. \t  -0.08700091094830492 \t -0.002080999024356565\n",
      "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.002080999024356565\n",
      "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.002080999024356565\n",
      "63     \t [-1.81823939  1.2990678 ]. \t  -410.71794193666307 \t -0.002080999024356565\n",
      "64     \t [0.47241056 1.13269949]. \t  -83.00242487243595 \t -0.002080999024356565\n",
      "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.002080999024356565\n",
      "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.002080999024356565\n",
      "67     \t [-1.0497881   1.62391789]. \t  -31.43571377729516 \t -0.002080999024356565\n",
      "68     \t [-0.34965343 -1.95956872]. \t  -435.221610964645 \t -0.002080999024356565\n",
      "69     \t [ 1.51152057 -1.89587699]. \t  -1747.9794032094064 \t -0.002080999024356565\n",
      "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.002080999024356565\n",
      "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.002080999024356565\n",
      "72     \t [ 0.30886344 -0.26784294]. \t  -13.67196772240941 \t -0.002080999024356565\n",
      "73     \t [ 1.63415265 -1.63483115]. \t  -1853.9509454567055 \t -0.002080999024356565\n",
      "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.002080999024356565\n",
      "75     \t [-1.73400181  1.72905703]. \t  -170.72783899706164 \t -0.002080999024356565\n",
      "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.002080999024356565\n",
      "77     \t [1.13639142 1.29070187]. \t  -0.018649345714716378 \t -0.002080999024356565\n",
      "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.002080999024356565\n",
      "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.002080999024356565\n",
      "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.002080999024356565\n",
      "81     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.002080999024356565\n",
      "82     \t [0.93199488 0.87309423]. \t  -0.006631539674353539 \t -0.002080999024356565\n",
      "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.002080999024356565\n",
      "84     \t [0.93293501 0.90391066]. \t  -0.11701051986194459 \t -0.002080999024356565\n",
      "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.002080999024356565\n",
      "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.002080999024356565\n",
      "87     \t [0.91340862 0.83880666]. \t  -0.009515283152860002 \t -0.002080999024356565\n",
      "88     \t [ 1.45792617 -0.2476256 ]. \t  -563.4053281955346 \t -0.002080999024356565\n",
      "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.002080999024356565\n",
      "90     \t [0.36724721 0.11381873]. \t  -0.444693862019727 \t -0.002080999024356565\n",
      "91     \t [1.25236481 1.57561538]. \t  -0.06886878243164578 \t -0.002080999024356565\n",
      "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.002080999024356565\n",
      "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.002080999024356565\n",
      "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.002080999024356565\n",
      "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.002080999024356565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.002080999024356565\n",
      "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.002080999024356565\n",
      "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.002080999024356565\n",
      "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.002080999024356565\n",
      "100    \t [2.01579377 0.71686644]. \t  -1120.9769394817645 \t -0.002080999024356565\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.59060318  0.68440247]. \t  \u001b[92m-13.792107070246617\u001b[0m \t -13.792107070246617\n",
      "2      \t [-0.43261271  0.05287406]. \t  \u001b[92m-3.8554828095625155\u001b[0m \t -3.8554828095625155\n",
      "3      \t [-1.09092491  0.68223928]. \t  -30.165961943368167 \t -3.8554828095625155\n",
      "4      \t [ 1.36856297 -1.03958351]. \t  -848.4294834321007 \t -3.8554828095625155\n",
      "5      \t [0.52611459 0.12330121]. \t  \u001b[92m-2.580649747779165\u001b[0m \t -2.580649747779165\n",
      "6      \t [ 0.39211637 -1.33428971]. \t  -221.7973022262781 \t -2.580649747779165\n",
      "7      \t [-0.76397583  0.37859956]. \t  -7.316550643665399 \t -2.580649747779165\n",
      "8      \t [-0.21806941  1.95448521]. \t  -365.1222538947518 \t -2.580649747779165\n",
      "9      \t [ 0.03858641 -0.43829987]. \t  -20.265732970797348 \t -2.580649747779165\n",
      "10     \t [ 0.32150197 -0.02041078]. \t  \u001b[92m-1.992367327735597\u001b[0m \t -1.992367327735597\n",
      "11     \t [ 0.13972863 -0.02640609]. \t  \u001b[92m-0.9510250140105745\u001b[0m \t -0.9510250140105745\n",
      "12     \t [0.16392502 0.69941313]. \t  -45.93025761343199 \t -0.9510250140105745\n",
      "13     \t [ 1.18952872 -1.53357098]. \t  -869.4303719620717 \t -0.9510250140105745\n",
      "14     \t [-1.55073664 -0.69812067]. \t  -969.3080665040666 \t -0.9510250140105745\n",
      "15     \t [-1.2386734  -1.13925684]. \t  -719.8085800301743 \t -0.9510250140105745\n",
      "16     \t [1.12681131 0.78354499]. \t  -23.651112251327692 \t -0.9510250140105745\n",
      "17     \t [ 0.99977521 -0.76676678]. \t  -311.9876671060406 \t -0.9510250140105745\n",
      "18     \t [0.46131648 0.31414458]. \t  -1.316991041470905 \t -0.9510250140105745\n",
      "19     \t [ 1.98042259 -0.98573901]. \t  -2409.6237150913776 \t -0.9510250140105745\n",
      "20     \t [-0.28283639 -1.32666863]. \t  -199.5163276605285 \t -0.9510250140105745\n",
      "21     \t [0.8374439 0.655215 ]. \t  \u001b[92m-0.23892052057330462\u001b[0m \t -0.23892052057330462\n",
      "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.23892052057330462\n",
      "23     \t [0.81434629 0.55060573]. \t  -1.3013111020120771 \t -0.23892052057330462\n",
      "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.23892052057330462\n",
      "25     \t [0.346047   0.12361642]. \t  -0.4291505806098731 \t -0.23892052057330462\n",
      "26     \t [ 1.29236218 -0.97473783]. \t  -699.6550875729368 \t -0.23892052057330462\n",
      "27     \t [-1.70805982 -0.79601716]. \t  -1386.3310485804243 \t -0.23892052057330462\n",
      "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.23892052057330462\n",
      "29     \t [-1.47304488 -0.75853993]. \t  -863.669276188972 \t -0.23892052057330462\n",
      "30     \t [-1.65700941  1.77131254]. \t  -101.99893166096496 \t -0.23892052057330462\n",
      "31     \t [-1.09782657 -0.40581759]. \t  -263.94610895647355 \t -0.23892052057330462\n",
      "32     \t [-1.18098345  1.27770353]. \t  -6.126018932441506 \t -0.23892052057330462\n",
      "33     \t [-1.94199151 -1.42106607]. \t  -2704.754071546131 \t -0.23892052057330462\n",
      "34     \t [-1.92237694  1.06979801]. \t  -697.9887655770525 \t -0.23892052057330462\n",
      "35     \t [-1.41815585  2.048     ]. \t  -5.983151996989663 \t -0.23892052057330462\n",
      "36     \t [-0.88702882 -0.79713069]. \t  -254.45089311807683 \t -0.23892052057330462\n",
      "37     \t [-1.0154422   1.16980679]. \t  -5.985330813903839 \t -0.23892052057330462\n",
      "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.23892052057330462\n",
      "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.23892052057330462\n",
      "40     \t [1.09113823 1.40072801]. \t  -4.4244134725728665 \t -0.23892052057330462\n",
      "41     \t [ 1.22778986 -1.53194722]. \t  -923.8563456372658 \t -0.23892052057330462\n",
      "42     \t [1.01888929 1.1315487 ]. \t  -0.8729615407864829 \t -0.23892052057330462\n",
      "43     \t [-0.3219115  0.186892 ]. \t  -2.4407558565841576 \t -0.23892052057330462\n",
      "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.23892052057330462\n",
      "45     \t [-0.08912908  0.32218779]. \t  -11.061118648132027 \t -0.23892052057330462\n",
      "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.23892052057330462\n",
      "47     \t [1.92883444 0.33470046]. \t  -1147.160415538827 \t -0.23892052057330462\n",
      "48     \t [-1.30210954  1.78000982]. \t  -6.014080835379149 \t -0.23892052057330462\n",
      "49     \t [-0.5681301   0.61439423]. \t  -10.963395698427673 \t -0.23892052057330462\n",
      "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.23892052057330462\n",
      "51     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.23892052057330462\n",
      "52     \t [-1.88092602 -1.71952514]. \t  -2772.333433014948 \t -0.23892052057330462\n",
      "53     \t [ 1.77670957 -0.01402205]. \t  -1005.9491539234891 \t -0.23892052057330462\n",
      "54     \t [ 0.9735238  -1.26120847]. \t  -487.9498313854103 \t -0.23892052057330462\n",
      "55     \t [ 1.13312633 -1.69041578]. \t  -884.7179340438174 \t -0.23892052057330462\n",
      "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.23892052057330462\n",
      "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.23892052057330462\n",
      "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.23892052057330462\n",
      "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.23892052057330462\n",
      "60     \t [ 1.86779441 -1.8241196 ]. \t  -2823.3114968565624 \t -0.23892052057330462\n",
      "61     \t [-1.61216036 -0.94126898]. \t  -1260.2170238403203 \t -0.23892052057330462\n",
      "62     \t [-1.64117344 -1.89599003]. \t  -2113.2720115420593 \t -0.23892052057330462\n",
      "63     \t [ 1.31580711 -1.11887593]. \t  -812.477579666318 \t -0.23892052057330462\n",
      "64     \t [1.59027407 2.048     ]. \t  -23.48179444233881 \t -0.23892052057330462\n",
      "65     \t [-0.03637786  1.38300939]. \t  -191.97971095134145 \t -0.23892052057330462\n",
      "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.23892052057330462\n",
      "67     \t [-0.59792941 -0.37098712]. \t  -55.62557857024103 \t -0.23892052057330462\n",
      "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.23892052057330462\n",
      "69     \t [-0.04818242  0.19123297]. \t  -4.667439202146498 \t -0.23892052057330462\n",
      "70     \t [ 1.96677042 -1.23243807]. \t  -2602.5710986536355 \t -0.23892052057330462\n",
      "71     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.23892052057330462\n",
      "72     \t [1.28228069 1.61054021]. \t  \u001b[92m-0.19327533756548054\u001b[0m \t -0.19327533756548054\n",
      "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.19327533756548054\n",
      "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.19327533756548054\n",
      "75     \t [1.21962555 1.47605679]. \t  \u001b[92m-0.06129920361782816\u001b[0m \t -0.06129920361782816\n",
      "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.06129920361782816\n",
      "77     \t [ 1.98216118 -0.56707881]. \t  -2022.4037849155825 \t -0.06129920361782816\n",
      "78     \t [ 0.6256004  -1.77182998]. \t  -468.0861277393353 \t -0.06129920361782816\n",
      "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.06129920361782816\n",
      "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.06129920361782816\n",
      "81     \t [1.1813951  1.37179861]. \t  -0.09000498434428916 \t -0.06129920361782816\n",
      "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.06129920361782816\n",
      "83     \t [1.79282834 0.64597849]. \t  -660.2219442582725 \t -0.06129920361782816\n",
      "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.06129920361782816\n",
      "85     \t [0.81909119 0.68004149]. \t  \u001b[92m-0.04106571157579786\u001b[0m \t -0.04106571157579786\n",
      "86     \t [0.81343408 0.67005565]. \t  -0.041830366349827765 \t -0.04106571157579786\n",
      "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.04106571157579786\n",
      "88     \t [-0.88361966 -0.74654485]. \t  -236.82127108162658 \t -0.04106571157579786\n",
      "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.04106571157579786\n",
      "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.04106571157579786\n",
      "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.04106571157579786\n",
      "92     \t [1.21418723 1.45050123]. \t  -0.10227956434504437 \t -0.04106571157579786\n",
      "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.04106571157579786\n",
      "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.04106571157579786\n",
      "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.04106571157579786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.04106571157579786\n",
      "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.04106571157579786\n",
      "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.04106571157579786\n",
      "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.04106571157579786\n",
      "100    \t [ 1.99729274 -0.68967908]. \t  -2190.1652376019097 \t -0.04106571157579786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-1.3787018  2.048    ]. \t  \u001b[92m-7.824456843181751\u001b[0m \t -7.824456843181751\n",
      "2      \t [-1.16736243  1.64664033]. \t  -12.75768117752235 \t -7.824456843181751\n",
      "3      \t [-0.82476779  1.48115298]. \t  -67.47563363670153 \t -7.824456843181751\n",
      "4      \t [-2.048       1.85692053]. \t  -555.6264525326193 \t -7.824456843181751\n",
      "5      \t [-0.12642315  0.45531769]. \t  -20.57034283559652 \t -7.824456843181751\n",
      "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -7.824456843181751\n",
      "7      \t [0.49818184 1.06984657]. \t  -67.76457057592293 \t -7.824456843181751\n",
      "8      \t [0.5651675  0.63849688]. \t  -10.370448682858372 \t -7.824456843181751\n",
      "9      \t [ 1.76108119 -1.81331805]. \t  -2416.0314336997494 \t -7.824456843181751\n",
      "10     \t [ 0.63661893 -0.25202025]. \t  -43.33688910841324 \t -7.824456843181751\n",
      "11     \t [ 1.05910733 -0.24097216]. \t  -185.69330959029529 \t -7.824456843181751\n",
      "12     \t [-1.18036067  2.048     ]. \t  -47.62355568324965 \t -7.824456843181751\n",
      "13     \t [0.26009262 0.6052131 ]. \t  -29.44506856087163 \t -7.824456843181751\n",
      "14     \t [ 0.29740787 -0.78677559]. \t  -77.0958711997293 \t -7.824456843181751\n",
      "15     \t [ 0.51522622 -0.87850331]. \t  -131.09976624440213 \t -7.824456843181751\n",
      "16     \t [ 0.0009418  -0.14297283]. \t  \u001b[92m-3.0422656212001553\u001b[0m \t -3.0422656212001553\n",
      "17     \t [ 0.53875338 -0.81057991]. \t  -121.39654403776179 \t -3.0422656212001553\n",
      "18     \t [0.28821005 0.07242824]. \t  \u001b[92m-0.5179590750516427\u001b[0m \t -0.5179590750516427\n",
      "19     \t [-0.53669937  0.25529521]. \t  -2.468707731627618 \t -0.5179590750516427\n",
      "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.5179590750516427\n",
      "21     \t [-1.8898996  -1.52770608]. \t  -2608.766667606483 \t -0.5179590750516427\n",
      "22     \t [-0.32995286 -1.77276659]. \t  -355.8239814493661 \t -0.5179590750516427\n",
      "23     \t [-0.57465484 -0.70543142]. \t  -109.73861986473887 \t -0.5179590750516427\n",
      "24     \t [-0.58847674  0.39507782]. \t  -2.7611384287272727 \t -0.5179590750516427\n",
      "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.5179590750516427\n",
      "26     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -0.5179590750516427\n",
      "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.5179590750516427\n",
      "28     \t [0.53067473 0.26930859]. \t  \u001b[92m-0.2354126426440179\u001b[0m \t -0.2354126426440179\n",
      "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.2354126426440179\n",
      "30     \t [ 0.53610384 -1.32590034]. \t  -260.4913625993307 \t -0.2354126426440179\n",
      "31     \t [-0.25784332 -1.26042859]. \t  -177.651652670226 \t -0.2354126426440179\n",
      "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.2354126426440179\n",
      "33     \t [1.11805115 1.34213172]. \t  -0.862054686914709 \t -0.2354126426440179\n",
      "34     \t [1.05307778 1.06445676]. \t  \u001b[92m-0.20098509202956166\u001b[0m \t -0.20098509202956166\n",
      "35     \t [-1.25457954 -0.90390996]. \t  -619.0719516289206 \t -0.20098509202956166\n",
      "36     \t [-1.53137776 -1.39676552]. \t  -1406.5769775535598 \t -0.20098509202956166\n",
      "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.20098509202956166\n",
      "38     \t [-0.87070096  1.76451424]. \t  -104.78242689426114 \t -0.20098509202956166\n",
      "39     \t [1.12772407 2.048     ]. \t  -60.27092156868604 \t -0.20098509202956166\n",
      "40     \t [0.51853573 0.27057188]. \t  -0.2320943206474129 \t -0.20098509202956166\n",
      "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.20098509202956166\n",
      "42     \t [0.52804319 0.28339341]. \t  -0.22482606252578782 \t -0.20098509202956166\n",
      "43     \t [ 0.1722737  -1.09339274]. \t  -126.81397133518486 \t -0.20098509202956166\n",
      "44     \t [ 1.97086788 -1.38085597]. \t  -2773.1505780850493 \t -0.20098509202956166\n",
      "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.20098509202956166\n",
      "46     \t [ 0.03298517 -0.06661061]. \t  -1.3934281580334764 \t -0.20098509202956166\n",
      "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.20098509202956166\n",
      "48     \t [-1.59011409 -1.17205592]. \t  -1376.0925958070266 \t -0.20098509202956166\n",
      "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.20098509202956166\n",
      "50     \t [1.48328002 2.048     ]. \t  -2.547597290106557 \t -0.20098509202956166\n",
      "51     \t [0.88069297 0.74146317]. \t  \u001b[92m-0.13090375402891008\u001b[0m \t -0.13090375402891008\n",
      "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.13090375402891008\n",
      "53     \t [1.34664239 1.76742198]. \t  -0.33197943541040853 \t -0.13090375402891008\n",
      "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.13090375402891008\n",
      "55     \t [-1.19694562  1.28218297]. \t  -7.0914699026767956 \t -0.13090375402891008\n",
      "56     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -0.13090375402891008\n",
      "57     \t [-0.38189943 -0.10655757]. \t  -8.280461489813375 \t -0.13090375402891008\n",
      "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.13090375402891008\n",
      "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.13090375402891008\n",
      "60     \t [ 0.34967597 -0.49509189]. \t  -38.53689694008928 \t -0.13090375402891008\n",
      "61     \t [1.12547775 1.25767639]. \t  \u001b[92m-0.023887506892937962\u001b[0m \t -0.023887506892937962\n",
      "62     \t [1.11703736 1.24008273]. \t  \u001b[92m-0.019610944445885056\u001b[0m \t -0.019610944445885056\n",
      "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.019610944445885056\n",
      "64     \t [1.1914228  1.38583014]. \t  -0.1499298584385004 \t -0.019610944445885056\n",
      "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.019610944445885056\n",
      "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.019610944445885056\n",
      "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.019610944445885056\n",
      "68     \t [0.94819772 0.93037814]. \t  -0.10064755503383545 \t -0.019610944445885056\n",
      "69     \t [1.16429039 1.33762416]. \t  -0.05920427022767664 \t -0.019610944445885056\n",
      "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.019610944445885056\n",
      "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.019610944445885056\n",
      "72     \t [1.27200548 0.21893352]. \t  -195.81210967711115 \t -0.019610944445885056\n",
      "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.019610944445885056\n",
      "74     \t [ 0.1920062  -1.81452282]. \t  -343.41704935735197 \t -0.019610944445885056\n",
      "75     \t [0.7026122  1.41582394]. \t  -85.12635272420111 \t -0.019610944445885056\n",
      "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.019610944445885056\n",
      "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.019610944445885056\n",
      "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.019610944445885056\n",
      "79     \t [1.02366783 1.02484337]. \t  -0.05370174572629335 \t -0.019610944445885056\n",
      "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.019610944445885056\n",
      "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.019610944445885056\n",
      "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.019610944445885056\n",
      "83     \t [1.52288195 0.32830641]. \t  -396.62696437497647 \t -0.019610944445885056\n",
      "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.019610944445885056\n",
      "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.019610944445885056\n",
      "86     \t [1.89917358 1.78193519]. \t  -333.84367620804505 \t -0.019610944445885056\n",
      "87     \t [0.95115313 0.90638088]. \t  \u001b[92m-0.0026711546260692227\u001b[0m \t -0.0026711546260692227\n",
      "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.0026711546260692227\n",
      "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.0026711546260692227\n",
      "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.0026711546260692227\n",
      "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.0026711546260692227\n",
      "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.0026711546260692227\n",
      "93     \t [-0.5821073  -1.55784162]. \t  -362.24656034397196 \t -0.0026711546260692227\n",
      "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.0026711546260692227\n",
      "95     \t [1.41382446 2.048     ]. \t  -0.4123356273755938 \t -0.0026711546260692227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.0026711546260692227\n",
      "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.0026711546260692227\n",
      "98     \t [1.29858931 1.68016183]. \t  -0.09296537477530241 \t -0.0026711546260692227\n",
      "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.0026711546260692227\n",
      "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.0026711546260692227\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.37995778 0.83552072]. \t  \u001b[92m-48.153672694682946\u001b[0m \t -48.153672694682946\n",
      "2      \t [1.55860765 1.62153229]. \t  -65.55409148154004 \t -48.153672694682946\n",
      "3      \t [0.46023823 0.17397552]. \t  \u001b[92m-0.43455737399098016\u001b[0m \t -0.43455737399098016\n",
      "4      \t [0.95160054 1.01537238]. \t  -1.2085789839255758 \t -0.43455737399098016\n",
      "5      \t [-1.08607807  2.048     ]. \t  -79.76955836944353 \t -0.43455737399098016\n",
      "6      \t [0.76140714 0.61041897]. \t  \u001b[92m-0.15104134638093003\u001b[0m \t -0.15104134638093003\n",
      "7      \t [1.00337308 0.90654961]. \t  -1.004174379817866 \t -0.15104134638093003\n",
      "8      \t [0.07709175 2.048     ]. \t  -417.85138275712023 \t -0.15104134638093003\n",
      "9      \t [-2.01971806  2.00733586]. \t  -438.40608785864225 \t -0.15104134638093003\n",
      "10     \t [0.50166341 0.03248154]. \t  -5.052529742057543 \t -0.15104134638093003\n",
      "11     \t [0.20173424 0.10176067]. \t  -1.0101090063818985 \t -0.15104134638093003\n",
      "12     \t [-0.8193735   0.64318483]. \t  -3.3895768510701876 \t -0.15104134638093003\n",
      "13     \t [-0.53350943  0.4830196 ]. \t  -6.2874026566035255 \t -0.15104134638093003\n",
      "14     \t [0.87696467 0.81183928]. \t  -0.19808426552069966 \t -0.15104134638093003\n",
      "15     \t [-0.90946616  0.89026662]. \t  -4.044700648927882 \t -0.15104134638093003\n",
      "16     \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.15104134638093003\n",
      "17     \t [-1.79116532  0.39368635]. \t  -799.9805187663303 \t -0.15104134638093003\n",
      "18     \t [0.27147229 0.11980829]. \t  -0.743375801871843 \t -0.15104134638093003\n",
      "19     \t [ 1.39317566 -0.10390501]. \t  -418.29305163403814 \t -0.15104134638093003\n",
      "20     \t [0.94413113 0.89997394]. \t  \u001b[92m-0.0105007464795961\u001b[0m \t -0.0105007464795961\n",
      "21     \t [-0.52856026 -1.00599816]. \t  -167.55515641751728 \t -0.0105007464795961\n",
      "22     \t [0.64779313 0.36590906]. \t  -0.41270749627960684 \t -0.0105007464795961\n",
      "23     \t [1.68411474 0.68570093]. \t  -462.9508923919746 \t -0.0105007464795961\n",
      "24     \t [-0.75113232  0.73522321]. \t  -5.991366067000058 \t -0.0105007464795961\n",
      "25     \t [-0.36435495 -2.048     ]. \t  -477.4304949063629 \t -0.0105007464795961\n",
      "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.0105007464795961\n",
      "27     \t [-1.21394464  0.86163408]. \t  -42.359319193002385 \t -0.0105007464795961\n",
      "28     \t [-1.41188748  0.3343338 ]. \t  -281.07598340222137 \t -0.0105007464795961\n",
      "29     \t [ 1.97957486 -1.68024005]. \t  -3135.791165885233 \t -0.0105007464795961\n",
      "30     \t [0.72586704 0.37811578]. \t  -2.2883160295173526 \t -0.0105007464795961\n",
      "31     \t [1.45952525 2.048     ]. \t  -0.887076764915745 \t -0.0105007464795961\n",
      "32     \t [-1.63774371  0.69143832]. \t  -403.2726747555389 \t -0.0105007464795961\n",
      "33     \t [-0.50864684  0.05615614]. \t  -6.379292253906455 \t -0.0105007464795961\n",
      "34     \t [-1.30842607 -1.26558062]. \t  -891.9148312871883 \t -0.0105007464795961\n",
      "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.0105007464795961\n",
      "36     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.0105007464795961\n",
      "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.0105007464795961\n",
      "38     \t [-1.8437376   0.34724672]. \t  -939.6314711821524 \t -0.0105007464795961\n",
      "39     \t [-0.16795991  0.1217925 ]. \t  -2.2398888638748584 \t -0.0105007464795961\n",
      "40     \t [ 1.19278592 -1.8498044 ]. \t  -1070.9907086052215 \t -0.0105007464795961\n",
      "41     \t [-1.28994606 -1.18169978]. \t  -815.0222927572457 \t -0.0105007464795961\n",
      "42     \t [ 0.4781391  -1.74497403]. \t  -389.7784949090759 \t -0.0105007464795961\n",
      "43     \t [-1.65106047  1.61048312]. \t  -131.46606497698292 \t -0.0105007464795961\n",
      "44     \t [-1.27142181  1.62577656]. \t  -5.167937620004863 \t -0.0105007464795961\n",
      "45     \t [0.05342914 0.77434006]. \t  -60.41496633178125 \t -0.0105007464795961\n",
      "46     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.0105007464795961\n",
      "47     \t [-1.92765507 -0.28267287]. \t  -1607.3929445715162 \t -0.0105007464795961\n",
      "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.0105007464795961\n",
      "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.0105007464795961\n",
      "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.0105007464795961\n",
      "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.0105007464795961\n",
      "52     \t [1.32952403 1.73909692]. \t  -0.19002333551530137 \t -0.0105007464795961\n",
      "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.0105007464795961\n",
      "54     \t [-1.15543014 -0.80314546]. \t  -461.8205277732385 \t -0.0105007464795961\n",
      "55     \t [-1.41795336  2.048     ]. \t  -5.98643638227141 \t -0.0105007464795961\n",
      "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.0105007464795961\n",
      "57     \t [1.52907419 2.048     ]. \t  -8.693857138901325 \t -0.0105007464795961\n",
      "58     \t [0.74548014 1.42810719]. \t  -76.16712000504089 \t -0.0105007464795961\n",
      "59     \t [ 0.12827911 -0.10851908]. \t  -2.3217626101374083 \t -0.0105007464795961\n",
      "60     \t [1.13506205 1.28160793]. \t  -0.022808723654615434 \t -0.0105007464795961\n",
      "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.0105007464795961\n",
      "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.0105007464795961\n",
      "63     \t [0.96238417 0.93918237]. \t  -0.018312564324216435 \t -0.0105007464795961\n",
      "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.0105007464795961\n",
      "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.0105007464795961\n",
      "66     \t [1.09210023 1.19998299]. \t  -0.01381154921942595 \t -0.0105007464795961\n",
      "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.0105007464795961\n",
      "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.0105007464795961\n",
      "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.0105007464795961\n",
      "70     \t [-1.58563016  1.17593792]. \t  -185.78617810527686 \t -0.0105007464795961\n",
      "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.0105007464795961\n",
      "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.0105007464795961\n",
      "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.0105007464795961\n",
      "74     \t [0.77181157 0.30847436]. \t  -8.301530181457267 \t -0.0105007464795961\n",
      "75     \t [1.21385442 1.46075685]. \t  -0.061826425282636384 \t -0.0105007464795961\n",
      "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.0105007464795961\n",
      "77     \t [1.15363453 1.32068037]. \t  -0.03399177213583851 \t -0.0105007464795961\n",
      "78     \t [1.14062434 1.29181335]. \t  -0.028258585702706535 \t -0.0105007464795961\n",
      "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.0105007464795961\n",
      "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.0105007464795961\n",
      "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.0105007464795961\n",
      "82     \t [1.10070234 1.20525414]. \t  -0.014099260547559497 \t -0.0105007464795961\n",
      "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.0105007464795961\n",
      "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.0105007464795961\n",
      "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.0105007464795961\n",
      "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.0105007464795961\n",
      "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.0105007464795961\n",
      "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.0105007464795961\n",
      "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.0105007464795961\n",
      "90     \t [0.71029964 0.38376261]. \t  -1.5422957760661784 \t -0.0105007464795961\n",
      "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.0105007464795961\n",
      "92     \t [1.1202444  1.24692499]. \t  -0.020894794702690515 \t -0.0105007464795961\n",
      "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.0105007464795961\n",
      "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.0105007464795961\n",
      "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.0105007464795961\n",
      "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.0105007464795961\n",
      "97     \t [0.55016118 0.30517732]. \t  -0.20297995737585128 \t -0.0105007464795961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.0105007464795961\n",
      "99     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.0105007464795961\n",
      "100    \t [2.0161086 1.2859591]. \t  -773.1691858866905 \t -0.0105007464795961\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [1.24752015 0.50762648]. \t  -110.03425123612769 \t -4.306489127802793\n",
      "2      \t [0.40905629 0.43067185]. \t  -7.284263221937693 \t -4.306489127802793\n",
      "3      \t [0.58960722 0.36786723]. \t  \u001b[92m-0.20934976693068252\u001b[0m \t -0.20934976693068252\n",
      "4      \t [1.50467572 1.85422513]. \t  -17.050259355443057 \t -0.20934976693068252\n",
      "5      \t [0.3895931  0.08380615]. \t  -0.8346788131126889 \t -0.20934976693068252\n",
      "6      \t [1.04319355 1.15819176]. \t  -0.4910117331202918 \t -0.20934976693068252\n",
      "7      \t [0.74112232 1.67535394]. \t  -126.87525800509205 \t -0.20934976693068252\n",
      "8      \t [1.81101547 1.70033276]. \t  -250.12216497315885 \t -0.20934976693068252\n",
      "9      \t [-1.39501341  1.96440775]. \t  -5.769744345411383 \t -0.20934976693068252\n",
      "10     \t [-1.11817079  1.50613291]. \t  -11.031392810522732 \t -0.20934976693068252\n",
      "11     \t [-1.51846679 -1.34386354]. \t  -1338.30428218148 \t -0.20934976693068252\n",
      "12     \t [0.84108936 1.35159914]. \t  -41.52047090373132 \t -0.20934976693068252\n",
      "13     \t [1.3550637 2.048    ]. \t  -4.612095149890622 \t -0.20934976693068252\n",
      "14     \t [-1.36955464  0.96694894]. \t  -88.19398624809719 \t -0.20934976693068252\n",
      "15     \t [1.27185276 1.7394737 ]. \t  -1.5589936860446165 \t -0.20934976693068252\n",
      "16     \t [-0.44917478  0.37965668]. \t  -5.264902319297189 \t -0.20934976693068252\n",
      "17     \t [-1.24756713  2.03358832]. \t  -27.820161630826867 \t -0.20934976693068252\n",
      "18     \t [-1.42485847  1.7555478 ]. \t  -13.424511198869107 \t -0.20934976693068252\n",
      "19     \t [-0.75252175  0.77409873]. \t  -7.389821627924743 \t -0.20934976693068252\n",
      "20     \t [0.36778814 0.19018003]. \t  -0.7012236754080831 \t -0.20934976693068252\n",
      "21     \t [1.28496257 1.04363211]. \t  -36.98642785984151 \t -0.20934976693068252\n",
      "22     \t [-0.20172275 -0.83419603]. \t  -77.98705544652253 \t -0.20934976693068252\n",
      "23     \t [1.45747924 2.048     ]. \t  -0.790628312566114 \t -0.20934976693068252\n",
      "24     \t [-1.71316048  0.26234352]. \t  -721.6271162631949 \t -0.20934976693068252\n",
      "25     \t [-0.50565574 -0.16992768]. \t  -20.381846978906378 \t -0.20934976693068252\n",
      "26     \t [-0.71991397  0.34792118]. \t  -5.8601846697037985 \t -0.20934976693068252\n",
      "27     \t [-1.13379775  1.22251051]. \t  -4.94982702281166 \t -0.20934976693068252\n",
      "28     \t [0.08710804 1.04892057]. \t  -109.27076301525734 \t -0.20934976693068252\n",
      "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.20934976693068252\n",
      "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.20934976693068252\n",
      "31     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.20934976693068252\n",
      "32     \t [1.62780113 0.05607663]. \t  -673.101292312616 \t -0.20934976693068252\n",
      "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.20934976693068252\n",
      "34     \t [-0.92423914  0.76059627]. \t  -4.579198957579712 \t -0.20934976693068252\n",
      "35     \t [0.7476354  1.17273707]. \t  -37.736077303702444 \t -0.20934976693068252\n",
      "36     \t [ 0.77205549 -1.24169292]. \t  -337.7890947457284 \t -0.20934976693068252\n",
      "37     \t [0.95678488 0.89197313]. \t  \u001b[92m-0.05692429160489036\u001b[0m \t -0.05692429160489036\n",
      "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.05692429160489036\n",
      "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.05692429160489036\n",
      "40     \t [0.96823241 0.92580636]. \t  \u001b[92m-0.014622547272366887\u001b[0m \t -0.014622547272366887\n",
      "41     \t [0.45014774 1.91013468]. \t  -291.8585425470606 \t -0.014622547272366887\n",
      "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.014622547272366887\n",
      "43     \t [1.38584151 1.92644639]. \t  -0.15234251260283777 \t -0.014622547272366887\n",
      "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.014622547272366887\n",
      "45     \t [ 0.32514034 -1.28652307]. \t  -194.28846499360978 \t -0.014622547272366887\n",
      "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.014622547272366887\n",
      "47     \t [ 1.91370402 -1.69217447]. \t  -2867.835007825963 \t -0.014622547272366887\n",
      "48     \t [-1.73379435 -0.73116528]. \t  -1404.1460948150188 \t -0.014622547272366887\n",
      "49     \t [-0.03095389 -0.07465241]. \t  -1.6345614317988844 \t -0.014622547272366887\n",
      "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.014622547272366887\n",
      "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.014622547272366887\n",
      "52     \t [ 0.29888104 -1.46818963]. \t  -243.07826902833955 \t -0.014622547272366887\n",
      "53     \t [-1.65210435 -0.0990571 ]. \t  -807.0782056667098 \t -0.014622547272366887\n",
      "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.014622547272366887\n",
      "55     \t [-1.6060395  -1.08628769]. \t  -1350.4908402004257 \t -0.014622547272366887\n",
      "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.014622547272366887\n",
      "57     \t [0.52884692 0.2721379 ]. \t  -0.22767214315800485 \t -0.014622547272366887\n",
      "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.014622547272366887\n",
      "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.014622547272366887\n",
      "60     \t [-1.32088165  0.18350019]. \t  -249.12981988210257 \t -0.014622547272366887\n",
      "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.014622547272366887\n",
      "62     \t [0.50452257 0.22088028]. \t  -0.35881591436259663 \t -0.014622547272366887\n",
      "63     \t [1.38889767 1.93392397]. \t  -0.1536299034083029 \t -0.014622547272366887\n",
      "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.014622547272366887\n",
      "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.014622547272366887\n",
      "66     \t [1.1008322  1.20953052]. \t  \u001b[92m-0.01069660301017262\u001b[0m \t -0.01069660301017262\n",
      "67     \t [ 1.33553529 -1.94373514]. \t  -1389.4559372659976 \t -0.01069660301017262\n",
      "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.01069660301017262\n",
      "69     \t [0.56708569 0.31248964]. \t  -0.19568950560666498 \t -0.01069660301017262\n",
      "70     \t [ 1.67728183 -0.22584165]. \t  -924.0813148512133 \t -0.01069660301017262\n",
      "71     \t [1.38924299 1.93427859]. \t  -0.1533440992702526 \t -0.01069660301017262\n",
      "72     \t [-1.19208035 -1.95536668]. \t  -1144.827932674559 \t -0.01069660301017262\n",
      "73     \t [-2.02108705  1.69906427]. \t  -578.2970575781673 \t -0.01069660301017262\n",
      "74     \t [1.07956962 1.16235714]. \t  \u001b[92m-0.00730066986647933\u001b[0m \t -0.00730066986647933\n",
      "75     \t [-0.67274238 -1.56222325]. \t  -408.7422158525488 \t -0.00730066986647933\n",
      "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.00730066986647933\n",
      "77     \t [ 0.65265246 -1.88587041]. \t  -534.5744305473129 \t -0.00730066986647933\n",
      "78     \t [-1.69721438 -1.92572466]. \t  -2317.2897352832306 \t -0.00730066986647933\n",
      "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.00730066986647933\n",
      "80     \t [-1.18472947  0.67165453]. \t  -58.34510613785976 \t -0.00730066986647933\n",
      "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.00730066986647933\n",
      "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.00730066986647933\n",
      "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.00730066986647933\n",
      "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.00730066986647933\n",
      "85     \t [1.38753819 1.92259198]. \t  -0.1508988694596794 \t -0.00730066986647933\n",
      "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.00730066986647933\n",
      "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.00730066986647933\n",
      "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.00730066986647933\n",
      "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.00730066986647933\n",
      "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.00730066986647933\n",
      "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
      "92     \t [1.31011778 1.71844037]. \t  -0.09658584701169247 \t -0.00598628680283637\n",
      "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.00598628680283637\n",
      "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.00598628680283637\n",
      "95     \t [0.51232447 0.86448493]. \t  -36.479259130122884 \t -0.00598628680283637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
      "97     \t [1.06233966 1.11046425]. \t  -0.0366519316149771 \t -0.00598628680283637\n",
      "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.00598628680283637\n",
      "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.00598628680283637\n",
      "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.00598628680283637\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-1.81759642  0.69595325]. \t  -687.9505990059421 \t -6.867717811955245\n",
      "2      \t [-0.42723667  0.39237061]. \t  \u001b[92m-6.440263368743487\u001b[0m \t -6.440263368743487\n",
      "3      \t [-0.68138009  0.77658894]. \t  -12.58079932586099 \t -6.440263368743487\n",
      "4      \t [1.30644486 2.048     ]. \t  -11.735777297828214 \t -6.440263368743487\n",
      "5      \t [-0.63826815  0.03497162]. \t  -16.55318639544221 \t -6.440263368743487\n",
      "6      \t [1.19322693 0.38983581]. \t  -106.94356890411369 \t -6.440263368743487\n",
      "7      \t [-0.279325    0.00863746]. \t  \u001b[92m-2.1181002226537706\u001b[0m \t -2.1181002226537706\n",
      "8      \t [-0.60558284  0.34534271]. \t  -2.6236403205886574 \t -2.1181002226537706\n",
      "9      \t [-1.21471885  1.51432563]. \t  -5.055397447061675 \t -2.1181002226537706\n",
      "10     \t [ 1.69570655 -2.048     ]. \t  -2424.4911648953716 \t -2.1181002226537706\n",
      "11     \t [0.37314826 0.06116715]. \t  \u001b[92m-1.0024742369439816\u001b[0m \t -1.0024742369439816\n",
      "12     \t [0.20502039 1.38416858]. \t  -180.76468746907915 \t -1.0024742369439816\n",
      "13     \t [ 0.15533006 -0.02400782]. \t  \u001b[92m-0.9451674680240623\u001b[0m \t -0.9451674680240623\n",
      "14     \t [-1.06147738  1.21644242]. \t  -5.054444803060877 \t -0.9451674680240623\n",
      "15     \t [-1.35058142  2.048     ]. \t  -10.539690329788293 \t -0.9451674680240623\n",
      "16     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.9451674680240623\n",
      "17     \t [0.46087486 0.32701969]. \t  -1.604294043954632 \t -0.9451674680240623\n",
      "18     \t [0.92606754 2.048     ]. \t  -141.7104228359623 \t -0.9451674680240623\n",
      "19     \t [-1.00960544 -1.69868823]. \t  -742.7862207882696 \t -0.9451674680240623\n",
      "20     \t [ 0.00721609 -0.30368694]. \t  -10.211358856081647 \t -0.9451674680240623\n",
      "21     \t [0.28443126 0.17799633]. \t  -1.4547862680960644 \t -0.9451674680240623\n",
      "22     \t [1.17563233 1.44384491]. \t  \u001b[92m-0.41194952080250125\u001b[0m \t -0.41194952080250125\n",
      "23     \t [-1.28718132  1.3649696 ]. \t  -13.749783661926397 \t -0.41194952080250125\n",
      "24     \t [0.72818365 1.90109329]. \t  -187.99462602351747 \t -0.41194952080250125\n",
      "25     \t [1.89352989 1.45619731]. \t  -454.1724149262043 \t -0.41194952080250125\n",
      "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -0.41194952080250125\n",
      "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -0.41194952080250125\n",
      "28     \t [1.27654895 0.99584516]. \t  -40.2381133304632 \t -0.41194952080250125\n",
      "29     \t [0.97727194 1.05809814]. \t  -1.062193377121161 \t -0.41194952080250125\n",
      "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.41194952080250125\n",
      "31     \t [0.20243607 1.33620308]. \t  -168.3962972454647 \t -0.41194952080250125\n",
      "32     \t [ 1.86202408 -0.41052955]. \t  -1504.3702930072584 \t -0.41194952080250125\n",
      "33     \t [-1.87844345  1.70426946]. \t  -341.08531238943704 \t -0.41194952080250125\n",
      "34     \t [-0.69688178  0.87300274]. \t  -17.884070497534147 \t -0.41194952080250125\n",
      "35     \t [0.830135   0.61139408]. \t  -0.6330501033647371 \t -0.41194952080250125\n",
      "36     \t [0.55182392 0.26399945]. \t  \u001b[92m-0.3649692831176702\u001b[0m \t -0.3649692831176702\n",
      "37     \t [ 0.94346704 -0.1780317 ]. \t  -114.10015004857898 \t -0.3649692831176702\n",
      "38     \t [-0.20019587  0.38612538]. \t  -13.415322219203915 \t -0.3649692831176702\n",
      "39     \t [-0.9926861   0.51774171]. \t  -25.843628897374533 \t -0.3649692831176702\n",
      "40     \t [1.15381854 1.09155764]. \t  -5.77116687300655 \t -0.3649692831176702\n",
      "41     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.3649692831176702\n",
      "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.3649692831176702\n",
      "43     \t [0.98128879 0.91133416]. \t  \u001b[92m-0.26653920343508974\u001b[0m \t -0.26653920343508974\n",
      "44     \t [0.17414949 0.71603838]. \t  -47.70189533400468 \t -0.26653920343508974\n",
      "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.26653920343508974\n",
      "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.26653920343508974\n",
      "47     \t [ 0.07770192 -0.08699998]. \t  -1.7162325778799612 \t -0.26653920343508974\n",
      "48     \t [1.35710706 1.88281138]. \t  -0.29621477368128035 \t -0.26653920343508974\n",
      "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.26653920343508974\n",
      "50     \t [1.28738432 1.76000009]. \t  -1.136121701875308 \t -0.26653920343508974\n",
      "51     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.26653920343508974\n",
      "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.26653920343508974\n",
      "53     \t [-1.98750058 -0.69836789]. \t  -2169.804975144425 \t -0.26653920343508974\n",
      "54     \t [ 0.05025156 -1.881929  ]. \t  -356.01879130785295 \t -0.26653920343508974\n",
      "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.26653920343508974\n",
      "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.26653920343508974\n",
      "57     \t [0.64401272 0.36646301]. \t  -0.359913286067773 \t -0.26653920343508974\n",
      "58     \t [-0.42475128  1.02612634]. \t  -73.55291025568293 \t -0.26653920343508974\n",
      "59     \t [-0.89795156 -0.02881667]. \t  -73.34704401720438 \t -0.26653920343508974\n",
      "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.26653920343508974\n",
      "61     \t [0.49676181 0.3065502 ]. \t  -0.6105884926302754 \t -0.26653920343508974\n",
      "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.26653920343508974\n",
      "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.26653920343508974\n",
      "64     \t [ 0.59307306 -0.6666248 ]. \t  -103.87139014582638 \t -0.26653920343508974\n",
      "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.26653920343508974\n",
      "66     \t [0.83582424 0.69112011]. \t  \u001b[92m-0.032551796759837756\u001b[0m \t -0.032551796759837756\n",
      "67     \t [0.71328651 0.49137061]. \t  -0.11250513742056896 \t -0.032551796759837756\n",
      "68     \t [0.85295802 0.7190723 ]. \t  \u001b[92m-0.028787118217305247\u001b[0m \t -0.028787118217305247\n",
      "69     \t [0.99943212 1.00428487]. \t  \u001b[92m-0.0029382945008131363\u001b[0m \t -0.0029382945008131363\n",
      "70     \t [1.00630484 1.01727835]. \t  \u001b[92m-0.0021824393687718793\u001b[0m \t -0.0021824393687718793\n",
      "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.0021824393687718793\n",
      "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.0021824393687718793\n",
      "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.0021824393687718793\n",
      "74     \t [-1.9100255   0.72643737]. \t  -862.1364214861749 \t -0.0021824393687718793\n",
      "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.0021824393687718793\n",
      "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.0021824393687718793\n",
      "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.0021824393687718793\n",
      "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.0021824393687718793\n",
      "79     \t [-0.22101996 -1.38284999]. \t  -206.46732600589422 \t -0.0021824393687718793\n",
      "80     \t [0.6547521  0.44081045]. \t  -0.1338616449430302 \t -0.0021824393687718793\n",
      "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.0021824393687718793\n",
      "82     \t [0.93562776 0.86152371]. \t  -0.023396993412763373 \t -0.0021824393687718793\n",
      "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.0021824393687718793\n",
      "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.0021824393687718793\n",
      "85     \t [0.39495926 1.89434764]. \t  -302.55382616645153 \t -0.0021824393687718793\n",
      "86     \t [0.79810778 0.68684508]. \t  -0.28945273468813654 \t -0.0021824393687718793\n",
      "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.0021824393687718793\n",
      "88     \t [0.73203987 0.55168381]. \t  -0.09677117549492316 \t -0.0021824393687718793\n",
      "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.0021824393687718793\n",
      "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.0021824393687718793\n",
      "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.0021824393687718793\n",
      "92     \t [-0.83237225  0.07587352]. \t  -41.422790641693126 \t -0.0021824393687718793\n",
      "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.0021824393687718793\n",
      "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.0021824393687718793\n",
      "95     \t [1.43160823 2.048     ]. \t  -0.18651129808219385 \t -0.0021824393687718793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.0021824393687718793\n",
      "97     \t [0.87720362 0.79027476]. \t  -0.05829541102554585 \t -0.0021824393687718793\n",
      "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.0021824393687718793\n",
      "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.0021824393687718793\n",
      "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.0021824393687718793\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
      "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
      "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
      "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
      "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
      "1      \t [ 0.01925236 -1.03263826]. \t  -107.67260830173007 \t -21.690996320546372\n",
      "2      \t [0.24241732 0.64168896]. \t  -34.55383093366369 \t -21.690996320546372\n",
      "3      \t [ 0.32122442 -0.08712064]. \t  \u001b[92m-4.0823647468742\u001b[0m \t -4.0823647468742\n",
      "4      \t [-2.048       0.58434354]. \t  -1312.4717561464336 \t -4.0823647468742\n",
      "5      \t [-0.67957819  0.06097666]. \t  -18.889043986844815 \t -4.0823647468742\n",
      "6      \t [ 0.44883143 -2.048     ]. \t  -506.3061619406781 \t -4.0823647468742\n",
      "7      \t [-0.57582745  0.77096427]. \t  -21.789327264593773 \t -4.0823647468742\n",
      "8      \t [ 0.42941509 -0.34734148]. \t  -28.600181717840076 \t -4.0823647468742\n",
      "9      \t [0.51073814 0.28090377]. \t  \u001b[92m-0.2795786907744215\u001b[0m \t -0.2795786907744215\n",
      "10     \t [-1.36096431 -1.46986084]. \t  -1109.1988316865343 \t -0.2795786907744215\n",
      "11     \t [0.22563357 0.02868629]. \t  -0.6490349679910357 \t -0.2795786907744215\n",
      "12     \t [-0.55348631  0.39689309]. \t  -3.2331774270464315 \t -0.2795786907744215\n",
      "13     \t [-0.34728917  2.048     ]. \t  -373.2984981879673 \t -0.2795786907744215\n",
      "14     \t [0.38766628 0.15411658]. \t  -0.37642057905717796 \t -0.2795786907744215\n",
      "15     \t [0.44758652 1.78406243]. \t  -251.12483395628925 \t -0.2795786907744215\n",
      "16     \t [-1.52930951 -1.01038496]. \t  -1128.0930833336422 \t -0.2795786907744215\n",
      "17     \t [-0.48561958 -1.32420417]. \t  -245.5765963866809 \t -0.2795786907744215\n",
      "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.2795786907744215\n",
      "19     \t [1.64144774 2.048     ]. \t  -42.18837383697442 \t -0.2795786907744215\n",
      "20     \t [1.14214396 1.42611369]. \t  -1.4993683564463458 \t -0.2795786907744215\n",
      "21     \t [0.68986958 1.53523052]. \t  -112.31005072817484 \t -0.2795786907744215\n",
      "22     \t [-1.09066884  1.80179203]. \t  -41.85388301514736 \t -0.2795786907744215\n",
      "23     \t [1.25311608 1.8460984 ]. \t  -7.670547811694383 \t -0.2795786907744215\n",
      "24     \t [1.26689908 1.61194891]. \t  \u001b[92m-0.07601770859797806\u001b[0m \t -0.07601770859797806\n",
      "25     \t [0.2693997  1.07008029]. \t  -100.03521822356028 \t -0.07601770859797806\n",
      "26     \t [-0.28520894  1.99131112]. \t  -366.44914700758426 \t -0.07601770859797806\n",
      "27     \t [ 0.67906291 -1.46483459]. \t  -371.0355872844689 \t -0.07601770859797806\n",
      "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.07601770859797806\n",
      "29     \t [-1.08543823  1.22942607]. \t  -4.611708008148876 \t -0.07601770859797806\n",
      "30     \t [-0.83656932  0.66120205]. \t  -3.522339594473351 \t -0.07601770859797806\n",
      "31     \t [ 0.79193639 -1.45821261]. \t  -434.9225345066252 \t -0.07601770859797806\n",
      "32     \t [-0.40342255 -1.92282347]. \t  -436.9311602155533 \t -0.07601770859797806\n",
      "33     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.07601770859797806\n",
      "34     \t [-1.22722795 -0.38785209]. \t  -363.66161351077665 \t -0.07601770859797806\n",
      "35     \t [-1.50651717  1.90888302]. \t  -19.293867303130988 \t -0.07601770859797806\n",
      "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.07601770859797806\n",
      "37     \t [-0.29731575  0.14691306]. \t  -2.0254451751424076 \t -0.07601770859797806\n",
      "38     \t [-1.12743965  1.91984752]. \t  -46.610718111637404 \t -0.07601770859797806\n",
      "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.07601770859797806\n",
      "40     \t [ 0.07572925 -2.0285615 ]. \t  -414.69046851086 \t -0.07601770859797806\n",
      "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.07601770859797806\n",
      "42     \t [1.51433978 2.00771194]. \t  -8.416314764337956 \t -0.07601770859797806\n",
      "43     \t [ 1.27618886 -1.3273556 ]. \t  -873.8779230470094 \t -0.07601770859797806\n",
      "44     \t [-1.46050973  2.048     ]. \t  -6.7781160869402655 \t -0.07601770859797806\n",
      "45     \t [-0.61286995 -1.65116907]. \t  -413.3845153643779 \t -0.07601770859797806\n",
      "46     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.07601770859797806\n",
      "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.07601770859797806\n",
      "48     \t [1.35015396 2.04109485]. \t  -4.8828212273132925 \t -0.07601770859797806\n",
      "49     \t [1.39051078 1.8553051 ]. \t  -0.7642594133898644 \t -0.07601770859797806\n",
      "50     \t [-0.03078867 -0.10215075]. \t  -2.1254592892524284 \t -0.07601770859797806\n",
      "51     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.07601770859797806\n",
      "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.07601770859797806\n",
      "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.07601770859797806\n",
      "54     \t [ 0.82517819 -1.33677215]. \t  -407.1383404308359 \t -0.07601770859797806\n",
      "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.07601770859797806\n",
      "56     \t [ 1.44253993 -0.83813951]. \t  -852.2875350360233 \t -0.07601770859797806\n",
      "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.07601770859797806\n",
      "58     \t [ 1.74567009 -1.89656171]. \t  -2444.796244157968 \t -0.07601770859797806\n",
      "59     \t [0.02346288 1.9066815 ]. \t  -364.2871597262516 \t -0.07601770859797806\n",
      "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.07601770859797806\n",
      "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.07601770859797806\n",
      "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.07601770859797806\n",
      "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.07601770859797806\n",
      "64     \t [-1.88164839 -0.66326923]. \t  -1775.5560972745068 \t -0.07601770859797806\n",
      "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.07601770859797806\n",
      "66     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.07601770859797806\n",
      "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.07601770859797806\n",
      "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.07601770859797806\n",
      "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.07601770859797806\n",
      "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.07601770859797806\n",
      "71     \t [1.02552828 1.05397326]. \t  \u001b[92m-0.001164720931605901\u001b[0m \t -0.001164720931605901\n",
      "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.001164720931605901\n",
      "73     \t [1.10154235 1.21162875]. \t  -0.010623004296493281 \t -0.001164720931605901\n",
      "74     \t [1.09899821 1.2058827 ]. \t  -0.010167130240381185 \t -0.001164720931605901\n",
      "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.001164720931605901\n",
      "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.001164720931605901\n",
      "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.001164720931605901\n",
      "78     \t [-0.0209748   0.02390357]. \t  -1.097443739468415 \t -0.001164720931605901\n",
      "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.001164720931605901\n",
      "80     \t [1.91253661 0.98388592]. \t  -715.8123979341622 \t -0.001164720931605901\n",
      "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.001164720931605901\n",
      "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.001164720931605901\n",
      "83     \t [ 1.67192921 -1.86254559]. \t  -2170.0480973974777 \t -0.001164720931605901\n",
      "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.001164720931605901\n",
      "85     \t [-0.72281118 -1.29658803]. \t  -333.8601983413895 \t -0.001164720931605901\n",
      "86     \t [0.56832542 0.35812604]. \t  -0.3097705252017462 \t -0.001164720931605901\n",
      "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.001164720931605901\n",
      "88     \t [0.49183668 0.24545175]. \t  -0.259489104774664 \t -0.001164720931605901\n",
      "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.001164720931605901\n",
      "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.001164720931605901\n",
      "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.001164720931605901\n",
      "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.001164720931605901\n",
      "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.001164720931605901\n",
      "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.001164720931605901\n",
      "95     \t [-0.6762613   1.75609806]. \t  -171.48987023474416 \t -0.001164720931605901\n",
      "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.001164720931605901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.16529491  1.41904107]. \t  -5.062175677754481 \t -0.001164720931605901\n",
      "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.001164720931605901\n",
      "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.001164720931605901\n",
      "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.001164720931605901\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.52002911 0.72258891]. \t  -252.41293686650607 \t -31.22188590191926\n",
      "2      \t [ 1.67350791 -1.98930425]. \t  -2294.799412623314 \t -31.22188590191926\n",
      "3      \t [-1.38699609 -2.028939  ]. \t  -1568.0792396357292 \t -31.22188590191926\n",
      "4      \t [-0.04878061 -0.25273843]. \t  \u001b[92m-7.608458763831756\u001b[0m \t -7.608458763831756\n",
      "5      \t [ 0.98301962 -1.01648991]. \t  -393.1568044044206 \t -7.608458763831756\n",
      "6      \t [ 0.39397944 -0.54423186]. \t  -49.290523610417054 \t -7.608458763831756\n",
      "7      \t [ 1.03714737 -0.87052167]. \t  -378.7694005688127 \t -7.608458763831756\n",
      "8      \t [-0.16593024  0.33478887]. \t  -10.800020230462335 \t -7.608458763831756\n",
      "9      \t [-0.66906229 -0.37411569]. \t  -70.31472418109783 \t -7.608458763831756\n",
      "10     \t [ 0.14552358 -0.78866312]. \t  -66.31425067583925 \t -7.608458763831756\n",
      "11     \t [1.57306667 2.048     ]. \t  -18.521936747116193 \t -7.608458763831756\n",
      "12     \t [0.27474746 0.2298561 ]. \t  \u001b[92m-2.908998860402267\u001b[0m \t -2.908998860402267\n",
      "13     \t [0.01774961 0.0851444 ]. \t  \u001b[92m-1.6844177853784323\u001b[0m \t -1.6844177853784323\n",
      "14     \t [-1.67099134  1.44532198]. \t  -188.54548800317096 \t -1.6844177853784323\n",
      "15     \t [-0.20917042  1.10710212]. \t  -114.53338518007739 \t -1.6844177853784323\n",
      "16     \t [-1.64824302 -0.21314914]. \t  -865.4177536465476 \t -1.6844177853784323\n",
      "17     \t [-0.94649806  0.73098383]. \t  -6.507223156858549 \t -1.6844177853784323\n",
      "18     \t [-0.6353006   0.23973332]. \t  -5.359661549356731 \t -1.6844177853784323\n",
      "19     \t [1.92350377 1.85305989]. \t  -341.92241734077754 \t -1.6844177853784323\n",
      "20     \t [-1.41432614  1.892902  ]. \t  -6.982799998635764 \t -1.6844177853784323\n",
      "21     \t [-1.29830636  2.048     ]. \t  -18.415631194251745 \t -1.6844177853784323\n",
      "22     \t [1.18817453 2.048     ]. \t  -40.5157063586203 \t -1.6844177853784323\n",
      "23     \t [-1.98012465  2.04178599]. \t  -361.9857001577987 \t -1.6844177853784323\n",
      "24     \t [-1.21317419  1.66561736]. \t  -8.65498186194672 \t -1.6844177853784323\n",
      "25     \t [0.85774365 0.02614855]. \t  -50.36999313130531 \t -1.6844177853784323\n",
      "26     \t [-1.0962071   0.44784178]. \t  -61.219782983337005 \t -1.6844177853784323\n",
      "27     \t [-0.52252074 -0.35167814]. \t  -41.34383566617724 \t -1.6844177853784323\n",
      "28     \t [1.68058629 1.72362185]. \t  -121.62790411263364 \t -1.6844177853784323\n",
      "29     \t [1.24354472 1.52412822]. \t  \u001b[92m-0.10893270336515468\u001b[0m \t -0.10893270336515468\n",
      "30     \t [-1.31910352  1.21186882]. \t  -33.27409734432689 \t -0.10893270336515468\n",
      "31     \t [-0.61373384 -0.38423968]. \t  -60.50237428910913 \t -0.10893270336515468\n",
      "32     \t [-1.40875573  1.6289175 ]. \t  -18.452590617769218 \t -0.10893270336515468\n",
      "33     \t [0.24730585 0.03044269]. \t  -0.6609049324232796 \t -0.10893270336515468\n",
      "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.10893270336515468\n",
      "35     \t [1.3657064  1.85806843]. \t  -0.13876166779705879 \t -0.10893270336515468\n",
      "36     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.10893270336515468\n",
      "37     \t [-1.5044638  -1.07565267]. \t  -1121.2071802953471 \t -0.10893270336515468\n",
      "38     \t [-0.35326329 -0.03337889]. \t  -4.333218142660941 \t -0.10893270336515468\n",
      "39     \t [-0.93743739 -1.51691736]. \t  -577.6944968849152 \t -0.10893270336515468\n",
      "40     \t [0.67325011 0.42533977]. \t  -0.18475131754841478 \t -0.10893270336515468\n",
      "41     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.10893270336515468\n",
      "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.10893270336515468\n",
      "43     \t [-0.09976551  1.90814023]. \t  -361.5209006963194 \t -0.10893270336515468\n",
      "44     \t [-0.99610818  0.19881197]. \t  -66.93590511449064 \t -0.10893270336515468\n",
      "45     \t [0.50362477 0.21983198]. \t  -0.36067241414956924 \t -0.10893270336515468\n",
      "46     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -0.10893270336515468\n",
      "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.10893270336515468\n",
      "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.10893270336515468\n",
      "49     \t [-0.3415817  1.4835984]. \t  -188.6469634120826 \t -0.10893270336515468\n",
      "50     \t [ 0.80102667 -0.25698711]. \t  -80.79332915429475 \t -0.10893270336515468\n",
      "51     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.10893270336515468\n",
      "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.10893270336515468\n",
      "53     \t [1.43037403 2.048     ]. \t  -0.18563395016082818 \t -0.10893270336515468\n",
      "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.10893270336515468\n",
      "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.10893270336515468\n",
      "56     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.10893270336515468\n",
      "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.10893270336515468\n",
      "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.10893270336515468\n",
      "59     \t [ 0.00784076 -0.97890301]. \t  -96.821526355202 \t -0.10893270336515468\n",
      "60     \t [-1.12769857  1.00116836]. \t  -11.846057670026436 \t -0.10893270336515468\n",
      "61     \t [1.7954402  0.87939316]. \t  -550.1658746391516 \t -0.10893270336515468\n",
      "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.10893270336515468\n",
      "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.10893270336515468\n",
      "64     \t [1.02191855 1.02091545]. \t  \u001b[92m-0.055246093826293696\u001b[0m \t -0.055246093826293696\n",
      "65     \t [0.605327   0.36611034]. \t  -0.15577641270288978 \t -0.055246093826293696\n",
      "66     \t [1.12581227 1.25147644]. \t  \u001b[92m-0.04135465339877588\u001b[0m \t -0.04135465339877588\n",
      "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.04135465339877588\n",
      "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.04135465339877588\n",
      "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.04135465339877588\n",
      "70     \t [-0.03744522  1.13452397]. \t  -129.47279988809532 \t -0.04135465339877588\n",
      "71     \t [1.16794486 1.37444498]. \t  \u001b[92m-0.0389172856546883\u001b[0m \t -0.0389172856546883\n",
      "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.0389172856546883\n",
      "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.0389172856546883\n",
      "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.0389172856546883\n",
      "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.0389172856546883\n",
      "76     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.0389172856546883\n",
      "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.0389172856546883\n",
      "78     \t [-1.01927465  1.69726372]. \t  -47.41900715611397 \t -0.0389172856546883\n",
      "79     \t [1.92040529 0.19413672]. \t  -1221.52480784025 \t -0.0389172856546883\n",
      "80     \t [1.00947053 0.26619503]. \t  -56.67625089578919 \t -0.0389172856546883\n",
      "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.0389172856546883\n",
      "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.0389172856546883\n",
      "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.0389172856546883\n",
      "84     \t [-1.60862375  1.17313899]. \t  -206.89481918097397 \t -0.0389172856546883\n",
      "85     \t [1.942115   1.69760421]. \t  -431.12082590076807 \t -0.0389172856546883\n",
      "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.0389172856546883\n",
      "87     \t [-0.45545945  1.64796546]. \t  -209.62876938362476 \t -0.0389172856546883\n",
      "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.0389172856546883\n",
      "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.0389172856546883\n",
      "90     \t [0.49538467 0.25080293]. \t  -0.2575493460016409 \t -0.0389172856546883\n",
      "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.0389172856546883\n",
      "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.0389172856546883\n",
      "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.0389172856546883\n",
      "94     \t [0.48206965 0.23766799]. \t  -0.2710363611556548 \t -0.0389172856546883\n",
      "95     \t [1.1647046  1.34701788]. \t  \u001b[92m-0.036188593283794895\u001b[0m \t -0.036188593283794895\n",
      "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.036188593283794895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.036188593283794895\n",
      "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.036188593283794895\n",
      "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.036188593283794895\n",
      "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.036188593283794895\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-0.5838592   0.42107488]. \t  -3.151546324395398 \t -1.7663579664225912\n",
      "2      \t [-0.25047818  0.02972492]. \t  \u001b[92m-1.6726906902126766\u001b[0m \t -1.6726906902126766\n",
      "3      \t [-1.31652822  1.37453189]. \t  -18.23392347915735 \t -1.6726906902126766\n",
      "4      \t [ 0.21720923 -0.54450454]. \t  -35.62180259164732 \t -1.6726906902126766\n",
      "5      \t [0.39893756 2.048     ]. \t  -357.13626394766504 \t -1.6726906902126766\n",
      "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.6726906902126766\n",
      "7      \t [-1.6592448   0.61224628]. \t  -465.3941770270854 \t -1.6726906902126766\n",
      "8      \t [1.16380436 1.47061005]. \t  \u001b[92m-1.3763662634648268\u001b[0m \t -1.3763662634648268\n",
      "9      \t [-0.77922126  1.10699248]. \t  -28.146303051691017 \t -1.3763662634648268\n",
      "10     \t [-1.7520212  2.048    ]. \t  -111.93583684829115 \t -1.3763662634648268\n",
      "11     \t [1.21848011 2.048     ]. \t  -31.779122161790156 \t -1.3763662634648268\n",
      "12     \t [-0.96084287  1.00652407]. \t  -4.5388778732143615 \t -1.3763662634648268\n",
      "13     \t [-1.21805618 -0.74376773]. \t  -501.0635869544814 \t -1.3763662634648268\n",
      "14     \t [1.47996912 1.30758724]. \t  -78.15006826502355 \t -1.3763662634648268\n",
      "15     \t [1.32517167 1.71572575]. \t  \u001b[92m-0.2685827692038145\u001b[0m \t -0.2685827692038145\n",
      "16     \t [0.70899277 0.70790763]. \t  -4.296902837614261 \t -0.2685827692038145\n",
      "17     \t [1.0223785  1.10788087]. \t  -0.3926656474370077 \t -0.2685827692038145\n",
      "18     \t [0.00842168 1.21627265]. \t  -148.89789227870298 \t -0.2685827692038145\n",
      "19     \t [-0.06107749 -0.04401587]. \t  -1.3538566565061956 \t -0.2685827692038145\n",
      "20     \t [0.87621562 1.02633812]. \t  -6.701906934254677 \t -0.2685827692038145\n",
      "21     \t [1.36465468 1.79040632]. \t  -0.6495901307152054 \t -0.2685827692038145\n",
      "22     \t [0.37102679 0.20769264]. \t  -0.886052037304184 \t -0.2685827692038145\n",
      "23     \t [ 1.78433219 -1.76828436]. \t  -2452.970093434316 \t -0.2685827692038145\n",
      "24     \t [1.42976536 2.048     ]. \t  \u001b[92m-0.186120315685995\u001b[0m \t -0.186120315685995\n",
      "25     \t [0.61275137 0.31832019]. \t  -0.47650571394358315 \t -0.186120315685995\n",
      "26     \t [-0.50301419  0.51412736]. \t  -9.076586301267312 \t -0.186120315685995\n",
      "27     \t [0.58674508 0.36628632]. \t  -0.21925235875589705 \t -0.186120315685995\n",
      "28     \t [-1.7750829  0.9792554]. \t  -479.3134904673417 \t -0.186120315685995\n",
      "29     \t [ 1.30212754 -1.04704589]. \t  -752.2668928051232 \t -0.186120315685995\n",
      "30     \t [1.2241202  1.43137903]. \t  -0.5003533030563203 \t -0.186120315685995\n",
      "31     \t [-2.00155577 -1.73617063]. \t  -3306.5206802629314 \t -0.186120315685995\n",
      "32     \t [-0.51379973 -1.63690855]. \t  -363.6331805481531 \t -0.186120315685995\n",
      "33     \t [-0.3086102 -1.5223783]. \t  -263.3814376804697 \t -0.186120315685995\n",
      "34     \t [ 0.70410932 -2.03075123]. \t  -638.4184743565811 \t -0.186120315685995\n",
      "35     \t [0.54120142 1.47148362]. \t  -139.1166704926515 \t -0.186120315685995\n",
      "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.186120315685995\n",
      "37     \t [-1.83696341 -1.25620347]. \t  -2152.32922596957 \t -0.186120315685995\n",
      "38     \t [1.8550285  0.74680791]. \t  -726.6686225385389 \t -0.186120315685995\n",
      "39     \t [-1.27560641  1.69777836]. \t  -5.676914496877277 \t -0.186120315685995\n",
      "40     \t [-1.44206756 -1.21484429]. \t  -1091.2728928664756 \t -0.186120315685995\n",
      "41     \t [1.068591   0.91240867]. \t  -5.270722615947526 \t -0.186120315685995\n",
      "42     \t [-0.29874947  0.69036876]. \t  -37.820977350770235 \t -0.186120315685995\n",
      "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.186120315685995\n",
      "44     \t [1.03427819 1.06711928]. \t  \u001b[92m-0.001857295385662538\u001b[0m \t -0.001857295385662538\n",
      "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.001857295385662538\n",
      "46     \t [1.01924627 1.03716979]. \t  \u001b[92m-0.0006570993390599014\u001b[0m \t -0.0006570993390599014\n",
      "47     \t [-1.16965222  1.44478982]. \t  -5.295733522302386 \t -0.0006570993390599014\n",
      "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.0006570993390599014\n",
      "49     \t [1.39119913 1.93327325]. \t  -0.15350408826844805 \t -0.0006570993390599014\n",
      "50     \t [1.01501892 1.03156513]. \t  \u001b[92m-0.00039501496185426996\u001b[0m \t -0.00039501496185426996\n",
      "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.00039501496185426996\n",
      "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.00039501496185426996\n",
      "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.00039501496185426996\n",
      "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.00039501496185426996\n",
      "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.00039501496185426996\n",
      "56     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.00039501496185426996\n",
      "57     \t [1.03697633 1.0319875 ]. \t  -0.18913702011945493 \t -0.00039501496185426996\n",
      "58     \t [1.02773691 1.05729113]. \t  -0.0008791593523144197 \t -0.00039501496185426996\n",
      "59     \t [-0.6656559   1.80874007]. \t  -189.27229594052525 \t -0.00039501496185426996\n",
      "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.00039501496185426996\n",
      "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.00039501496185426996\n",
      "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.00039501496185426996\n",
      "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.00039501496185426996\n",
      "64     \t [1.08652946 1.22040262]. \t  -0.16634022798350892 \t -0.00039501496185426996\n",
      "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.00039501496185426996\n",
      "66     \t [0.71180344 0.48073656]. \t  -0.15028119964701456 \t -0.00039501496185426996\n",
      "67     \t [ 1.94377297 -1.45285214]. \t  -2737.3371698894134 \t -0.00039501496185426996\n",
      "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.00039501496185426996\n",
      "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.00039501496185426996\n",
      "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.00039501496185426996\n",
      "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.00039501496185426996\n",
      "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.00039501496185426996\n",
      "73     \t [-1.84375537 -1.86727236]. \t  -2781.9064022820107 \t -0.00039501496185426996\n",
      "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.00039501496185426996\n",
      "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.00039501496185426996\n",
      "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.00039501496185426996\n",
      "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.00039501496185426996\n",
      "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.00039501496185426996\n",
      "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.00039501496185426996\n",
      "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.00039501496185426996\n",
      "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.00039501496185426996\n",
      "82     \t [1.07927832 1.16554396]. \t  -0.0063343698946076105 \t -0.00039501496185426996\n",
      "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.00039501496185426996\n",
      "84     \t [1.20764837 0.79814724]. \t  -43.63841506633272 \t -0.00039501496185426996\n",
      "85     \t [ 0.38269592 -1.23938671]. \t  -192.4371118418537 \t -0.00039501496185426996\n",
      "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.00039501496185426996\n",
      "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.00039501496185426996\n",
      "88     \t [-0.92679023 -1.14150193]. \t  -403.8893634877983 \t -0.00039501496185426996\n",
      "89     \t [0.78318802 0.59121723]. \t  -0.09614165737818745 \t -0.00039501496185426996\n",
      "90     \t [1.39924314 1.96076222]. \t  -0.16022501559781602 \t -0.00039501496185426996\n",
      "91     \t [-0.2896775 -0.7808088]. \t  -76.43765662694076 \t -0.00039501496185426996\n",
      "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.00039501496185426996\n",
      "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.00039501496185426996\n",
      "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.00039501496185426996\n",
      "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.00039501496185426996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.00039501496185426996\n",
      "97     \t [1.10320842 1.21263853]. \t  -0.01261472276031042 \t -0.00039501496185426996\n",
      "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.00039501496185426996\n",
      "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.00039501496185426996\n",
      "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.00039501496185426996\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
      "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
      "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
      "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
      "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
      "1      \t [ 0.4289548  -0.54692575]. \t  -53.75166279546546 \t -4.219752052396591\n",
      "2      \t [-2.01057158  1.54840386]. \t  -631.0642658014616 \t -4.219752052396591\n",
      "3      \t [-0.81640829  0.57003337]. \t  -4.230354265254039 \t -4.219752052396591\n",
      "4      \t [1.48975836 1.25183036]. \t  -93.85508647922497 \t -4.219752052396591\n",
      "5      \t [-0.08031195 -0.27675321]. \t  -9.187480026978836 \t -4.219752052396591\n",
      "6      \t [-0.20278538 -1.27079977]. \t  -173.56054293310598 \t -4.219752052396591\n",
      "7      \t [-1.00036302  0.86176304]. \t  -5.932527489682171 \t -4.219752052396591\n",
      "8      \t [0.78366374 0.40690276]. \t  -4.341066672074766 \t -4.219752052396591\n",
      "9      \t [ 1.88662176 -1.52272924]. \t  -2583.530575995578 \t -4.219752052396591\n",
      "10     \t [0.90780551 1.05472632]. \t  -5.3268501253444525 \t -4.219752052396591\n",
      "11     \t [ 1.883773   -0.96035514]. \t  -2033.8493574863483 \t -4.219752052396591\n",
      "12     \t [0.85073193 0.70907048]. \t  \u001b[92m-0.04381458802715288\u001b[0m \t -0.04381458802715288\n",
      "13     \t [-0.73934761  1.83511187]. \t  -169.04262503651273 \t -0.04381458802715288\n",
      "14     \t [-1.02326689  0.13551371]. \t  -87.18803054460024 \t -0.04381458802715288\n",
      "15     \t [-0.91779786  0.09679596]. \t  -59.26346567837915 \t -0.04381458802715288\n",
      "16     \t [ 1.22568239 -1.1696075 ]. \t  -713.9584663977281 \t -0.04381458802715288\n",
      "17     \t [ 0.32624751 -2.019548  ]. \t  -452.43535156259907 \t -0.04381458802715288\n",
      "18     \t [-0.06450438  0.16629939]. \t  -3.7620613850141296 \t -0.04381458802715288\n",
      "19     \t [1.06473705 2.048     ]. \t  -83.60504392331846 \t -0.04381458802715288\n",
      "20     \t [-0.94349482  0.99130426]. \t  -4.799733492202603 \t -0.04381458802715288\n",
      "21     \t [-1.99774998 -1.90184684]. \t  -3481.5567684995062 \t -0.04381458802715288\n",
      "22     \t [-0.39255273 -0.0113082 ]. \t  -4.6751125366412 \t -0.04381458802715288\n",
      "23     \t [1.45933798 1.02393232]. \t  -122.47598556447716 \t -0.04381458802715288\n",
      "24     \t [-1.31971342 -1.08305348]. \t  -803.2723765145327 \t -0.04381458802715288\n",
      "25     \t [ 1.38242446 -0.1308308 ]. \t  -417.0933217978137 \t -0.04381458802715288\n",
      "26     \t [0.5812708  0.38710501]. \t  -0.4176862677958069 \t -0.04381458802715288\n",
      "27     \t [0.75180032 0.57097289]. \t  -0.06493141378655598 \t -0.04381458802715288\n",
      "28     \t [0.17153394 0.02079416]. \t  -0.6938032391470994 \t -0.04381458802715288\n",
      "29     \t [-0.03142889  0.0074378 ]. \t  -1.0680058379718527 \t -0.04381458802715288\n",
      "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.04381458802715288\n",
      "31     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.04381458802715288\n",
      "32     \t [-0.62571956  1.4256426 ]. \t  -109.58289262533435 \t -0.04381458802715288\n",
      "33     \t [-0.61375131  0.89657007]. \t  -29.631652197295683 \t -0.04381458802715288\n",
      "34     \t [-0.5868938   1.12710118]. \t  -63.77340517813315 \t -0.04381458802715288\n",
      "35     \t [2.0381793 0.72216  ]. \t  -1178.9504173057226 \t -0.04381458802715288\n",
      "36     \t [1.111885   1.13756807]. \t  -0.9870855260166645 \t -0.04381458802715288\n",
      "37     \t [0.50673089 0.16869641]. \t  -1.0191193020423532 \t -0.04381458802715288\n",
      "38     \t [0.99761977 0.96281009]. \t  -0.10520937468045778 \t -0.04381458802715288\n",
      "39     \t [1.17092552 1.57273923]. \t  -4.096401225250451 \t -0.04381458802715288\n",
      "40     \t [ 0.959215  -1.3861733]. \t  -531.888274532088 \t -0.04381458802715288\n",
      "41     \t [ 0.963913   -1.67233212]. \t  -676.7609160489471 \t -0.04381458802715288\n",
      "42     \t [-0.24680134 -1.78126617]. \t  -340.9161515362546 \t -0.04381458802715288\n",
      "43     \t [-0.58202633 -0.52425901]. \t  -76.98206422017245 \t -0.04381458802715288\n",
      "44     \t [1.0893578  1.18241623]. \t  \u001b[92m-0.009820246873348799\u001b[0m \t -0.009820246873348799\n",
      "45     \t [ 0.42345259 -0.53876914]. \t  -51.896472941900235 \t -0.009820246873348799\n",
      "46     \t [ 1.79677526 -1.32817717]. \t  -2076.875614620295 \t -0.009820246873348799\n",
      "47     \t [0.79281754 0.70307785]. \t  -0.5982207655329881 \t -0.009820246873348799\n",
      "48     \t [-0.65978578 -1.76653712]. \t  -487.5711645016507 \t -0.009820246873348799\n",
      "49     \t [0.95554818 0.91196488]. \t  \u001b[92m-0.0020986079645110515\u001b[0m \t -0.0020986079645110515\n",
      "50     \t [ 1.02841419 -1.36419327]. \t  -586.5263844771054 \t -0.0020986079645110515\n",
      "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.0020986079645110515\n",
      "52     \t [1.28390872 1.65478729]. \t  -0.08465634865059796 \t -0.0020986079645110515\n",
      "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.0020986079645110515\n",
      "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.0020986079645110515\n",
      "55     \t [-0.18601702 -1.73004712]. \t  -312.8054036447344 \t -0.0020986079645110515\n",
      "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.0020986079645110515\n",
      "57     \t [1.20057254 1.42148384]. \t  -0.07979290349240192 \t -0.0020986079645110515\n",
      "58     \t [-1.46571663  2.048     ]. \t  -7.086274160521702 \t -0.0020986079645110515\n",
      "59     \t [0.76101456 0.58943797]. \t  -0.06771235104415185 \t -0.0020986079645110515\n",
      "60     \t [-1.14287721 -1.84974167]. \t  -1000.568697410595 \t -0.0020986079645110515\n",
      "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.0020986079645110515\n",
      "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.0020986079645110515\n",
      "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.0020986079645110515\n",
      "64     \t [-1.78051982  0.0386201 ]. \t  -988.4423965795083 \t -0.0020986079645110515\n",
      "65     \t [-1.74403802 -1.30179646]. \t  -1894.098633935269 \t -0.0020986079645110515\n",
      "66     \t [-1.28822841  1.7831024 ]. \t  -6.762942941341422 \t -0.0020986079645110515\n",
      "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.0020986079645110515\n",
      "68     \t [0.29070172 0.83551013]. \t  -56.90360093054751 \t -0.0020986079645110515\n",
      "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.0020986079645110515\n",
      "70     \t [-1.77269767 -1.13558288]. \t  -1837.8503994612217 \t -0.0020986079645110515\n",
      "71     \t [0.11330097 1.90764284]. \t  -359.81511194786157 \t -0.0020986079645110515\n",
      "72     \t [1.00127548 0.98242266]. \t  -0.04052304804428198 \t -0.0020986079645110515\n",
      "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.0020986079645110515\n",
      "74     \t [0.98970685 0.95722753]. \t  -0.049799799392199945 \t -0.0020986079645110515\n",
      "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.0020986079645110515\n",
      "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.0020986079645110515\n",
      "77     \t [0.94548383 0.88208101]. \t  -0.01703479846433533 \t -0.0020986079645110515\n",
      "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.0020986079645110515\n",
      "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.0020986079645110515\n",
      "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.0020986079645110515\n",
      "81     \t [0.14672588 0.87963436]. \t  -74.36264551014955 \t -0.0020986079645110515\n",
      "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.0020986079645110515\n",
      "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.0020986079645110515\n",
      "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.0020986079645110515\n",
      "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.0020986079645110515\n",
      "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.0020986079645110515\n",
      "87     \t [0.87499096 0.74409058]. \t  -0.0619322451656779 \t -0.0020986079645110515\n",
      "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.0020986079645110515\n",
      "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.0020986079645110515\n",
      "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.0020986079645110515\n",
      "91     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.0020986079645110515\n",
      "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.0020986079645110515\n",
      "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.0020986079645110515\n",
      "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.0020986079645110515\n",
      "95     \t [-0.93516627 -0.76413114]. \t  -272.2678550483519 \t -0.0020986079645110515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-0.87974617 -1.97633035]. \t  -759.9394729222546 \t -0.0020986079645110515\n",
      "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.0020986079645110515\n",
      "98     \t [-1.51314065  0.48297648]. \t  -332.70279227581244 \t -0.0020986079645110515\n",
      "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.0020986079645110515\n",
      "100    \t [-0.20069779  1.79982045]. \t  -311.04007298950785 \t -0.0020986079645110515\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [-0.1332695   0.13267709]. \t  \u001b[92m-2.6048761032215726\u001b[0m \t -2.6048761032215726\n",
      "2      \t [1.52601541 0.49362155]. \t  -337.0364385207258 \t -2.6048761032215726\n",
      "3      \t [0.06323371 1.41284359]. \t  -199.36197910212854 \t -2.6048761032215726\n",
      "4      \t [ 0.93462177 -1.1475014 ]. \t  -408.4561526610949 \t -2.6048761032215726\n",
      "5      \t [-0.37159194 -0.18318702]. \t  -12.202550786317964 \t -2.6048761032215726\n",
      "6      \t [-1.42024952  1.13391263]. \t  -83.86113639867138 \t -2.6048761032215726\n",
      "7      \t [-1.41345518  2.048     ]. \t  -6.07621247720745 \t -2.6048761032215726\n",
      "8      \t [-0.97540746  1.26259133]. \t  -13.585012192506847 \t -2.6048761032215726\n",
      "9      \t [-1.88679977  2.048     ]. \t  -236.95205926553626 \t -2.6048761032215726\n",
      "10     \t [-0.29297393  0.05695057]. \t  \u001b[92m-1.7552052274073262\u001b[0m \t -1.7552052274073262\n",
      "11     \t [-1.19530118  2.048     ]. \t  -43.16703336468801 \t -1.7552052274073262\n",
      "12     \t [0.53419478 0.81149925]. \t  -27.898798372583414 \t -1.7552052274073262\n",
      "13     \t [-1.30576919  1.59694943]. \t  -6.4847809016286755 \t -1.7552052274073262\n",
      "14     \t [-0.78860716  0.73563946]. \t  -4.492753503759733 \t -1.7552052274073262\n",
      "15     \t [-0.3983729  -1.64194288]. \t  -326.1872740695626 \t -1.7552052274073262\n",
      "16     \t [0.32302253 0.4999506 ]. \t  -16.108791642617973 \t -1.7552052274073262\n",
      "17     \t [1.31747016 2.048     ]. \t  -9.852191432704268 \t -1.7552052274073262\n",
      "18     \t [ 0.25799003 -0.24867382]. \t  -10.4877429368878 \t -1.7552052274073262\n",
      "19     \t [-1.10943324  1.18089624]. \t  -4.699167739297667 \t -1.7552052274073262\n",
      "20     \t [0.18615436 0.02312019]. \t  \u001b[92m-0.675646330080072\u001b[0m \t -0.675646330080072\n",
      "21     \t [0.95289162 1.25393753]. \t  -11.969327146174747 \t -0.675646330080072\n",
      "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.675646330080072\n",
      "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.675646330080072\n",
      "24     \t [-1.69537024 -1.39278119]. \t  -1828.0463414122016 \t -0.675646330080072\n",
      "25     \t [0.23677012 0.4709882 ]. \t  -17.799052926002027 \t -0.675646330080072\n",
      "26     \t [ 0.8501451  -0.35546572]. \t  -116.27665665103598 \t -0.675646330080072\n",
      "27     \t [ 0.14364922 -0.89507096]. \t  -84.58509496095981 \t -0.675646330080072\n",
      "28     \t [1.80318636 1.94411307]. \t  -171.56621188478246 \t -0.675646330080072\n",
      "29     \t [1.1229608  1.95377589]. \t  -48.00328822004864 \t -0.675646330080072\n",
      "30     \t [-1.1342216  -0.37006358]. \t  -278.9614916305373 \t -0.675646330080072\n",
      "31     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.675646330080072\n",
      "32     \t [1.20459609 1.79563618]. \t  -11.915702706226021 \t -0.675646330080072\n",
      "33     \t [-0.47830041 -1.52576977]. \t  -310.02680065959424 \t -0.675646330080072\n",
      "34     \t [-1.20550988 -0.05889056]. \t  -233.52241158111184 \t -0.675646330080072\n",
      "35     \t [-0.52674853  0.54617333]. \t  -9.55143065929903 \t -0.675646330080072\n",
      "36     \t [0.74304255 0.56832733]. \t  \u001b[92m-0.09232009178814232\u001b[0m \t -0.09232009178814232\n",
      "37     \t [1.32115648 1.18152797]. \t  -31.904447673878682 \t -0.09232009178814232\n",
      "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.09232009178814232\n",
      "39     \t [0.85087995 0.77008763]. \t  -0.23467415583651066 \t -0.09232009178814232\n",
      "40     \t [1.22350821 1.48006613]. \t  \u001b[92m-0.07853789152412906\u001b[0m \t -0.07853789152412906\n",
      "41     \t [0.71711674 0.58197332]. \t  -0.5385807862798063 \t -0.07853789152412906\n",
      "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.07853789152412906\n",
      "43     \t [0.17529204 1.15916279]. \t  -128.0168083834562 \t -0.07853789152412906\n",
      "44     \t [0.48382867 1.77067549]. \t  -236.37587193199764 \t -0.07853789152412906\n",
      "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.07853789152412906\n",
      "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.07853789152412906\n",
      "47     \t [ 1.27091858 -1.70898462]. \t  -1105.116363693183 \t -0.07853789152412906\n",
      "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.07853789152412906\n",
      "49     \t [-1.86573362  0.37119973]. \t  -975.2745320400953 \t -0.07853789152412906\n",
      "50     \t [0.67134476 0.46935239]. \t  -0.14279133921161213 \t -0.07853789152412906\n",
      "51     \t [1.13740342 1.29280045]. \t  \u001b[92m-0.018958216595401665\u001b[0m \t -0.018958216595401665\n",
      "52     \t [1.95650081 1.38439042]. \t  -597.9865725043503 \t -0.018958216595401665\n",
      "53     \t [ 1.71219045 -1.9877464 ]. \t  -2420.500308270193 \t -0.018958216595401665\n",
      "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.018958216595401665\n",
      "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.018958216595401665\n",
      "56     \t [1.10529464 0.8664695 ]. \t  -12.628270272037758 \t -0.018958216595401665\n",
      "57     \t [1.06989476 1.13508979]. \t  \u001b[92m-0.01407250852536994\u001b[0m \t -0.01407250852536994\n",
      "58     \t [-1.66676799  0.66994422]. \t  -451.5502786295779 \t -0.01407250852536994\n",
      "59     \t [-0.99523387  0.24252604]. \t  -59.926034914967154 \t -0.01407250852536994\n",
      "60     \t [1.17397494 1.40563929]. \t  -0.10546463932844857 \t -0.01407250852536994\n",
      "61     \t [0.56691931 0.32727801]. \t  -0.19101691259248388 \t -0.01407250852536994\n",
      "62     \t [-0.94001591  2.04528928]. \t  -138.70891112332072 \t -0.01407250852536994\n",
      "63     \t [ 0.57187407 -1.66012158]. \t  -395.0643876039098 \t -0.01407250852536994\n",
      "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.01407250852536994\n",
      "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.01407250852536994\n",
      "66     \t [-0.91082315 -1.26232069]. \t  -441.26396375882774 \t -0.01407250852536994\n",
      "67     \t [0.0821836  0.58251462]. \t  -33.99239946621883 \t -0.01407250852536994\n",
      "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.01407250852536994\n",
      "69     \t [0.59012764 0.32547946]. \t  -0.2198479842540385 \t -0.01407250852536994\n",
      "70     \t [0.5989424 0.3403831]. \t  -0.19451541762401342 \t -0.01407250852536994\n",
      "71     \t [1.13604239 1.29132354]. \t  -0.01856100064466296 \t -0.01407250852536994\n",
      "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.01407250852536994\n",
      "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.01407250852536994\n",
      "74     \t [1.52137197 0.73026457]. \t  -251.27504800338738 \t -0.01407250852536994\n",
      "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.01407250852536994\n",
      "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.01407250852536994\n",
      "77     \t [ 1.154773   -0.95401997]. \t  -523.2990278949036 \t -0.01407250852536994\n",
      "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.01407250852536994\n",
      "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.01407250852536994\n",
      "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.01407250852536994\n",
      "81     \t [0.50321713 0.25309516]. \t  -0.24679496978740006 \t -0.01407250852536994\n",
      "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.01407250852536994\n",
      "83     \t [ 0.10348957 -1.23911708]. \t  -157.01052777221707 \t -0.01407250852536994\n",
      "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.01407250852536994\n",
      "85     \t [-1.05457877  1.81289119]. \t  -53.32702467490017 \t -0.01407250852536994\n",
      "86     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.01407250852536994\n",
      "87     \t [-1.23659192 -0.58198804]. \t  -450.69676966562554 \t -0.01407250852536994\n",
      "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.01407250852536994\n",
      "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.01407250852536994\n",
      "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.01407250852536994\n",
      "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.01407250852536994\n",
      "92     \t [1.59988677 0.83088525]. \t  -299.2183568062418 \t -0.01407250852536994\n",
      "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.01407250852536994\n",
      "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.01407250852536994\n",
      "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.01407250852536994\n",
      "96     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.01407250852536994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.01407250852536994\n",
      "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.01407250852536994\n",
      "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.01407250852536994\n",
      "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.01407250852536994\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [ 0.00128835 -0.13877779]. \t  \u001b[92m-2.9233984470034695\u001b[0m \t -2.9233984470034695\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -2.9233984470034695\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -2.9233984470034695\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -2.9233984470034695\n",
      "5      \t [2.048      0.43090226]. \t  -1417.4175668339624 \t -2.9233984470034695\n",
      "6      \t [0.37668347 2.048     ]. \t  -363.71389063167976 \t -2.9233984470034695\n",
      "7      \t [-0.82134949  2.048     ]. \t  -191.93595526899637 \t -2.9233984470034695\n",
      "8      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -2.9233984470034695\n",
      "9      \t [-0.2525115  -0.97000391]. \t  -108.43599236174295 \t -2.9233984470034695\n",
      "10     \t [-0.00891402 -2.048     ]. \t  -420.4808548362627 \t -2.9233984470034695\n",
      "11     \t [-1.10112355  0.70178623]. \t  -30.494825234760764 \t -2.9233984470034695\n",
      "12     \t [ 0.92514207 -0.32804494]. \t  -140.17528930732217 \t -2.9233984470034695\n",
      "13     \t [0.80965599 0.83869288]. \t  -3.39062490514596 \t -2.9233984470034695\n",
      "14     \t [-2.048       1.13748861]. \t  -943.7023384209446 \t -2.9233984470034695\n",
      "15     \t [-0.62466053  0.29993429]. \t  -3.454325709547416 \t -2.9233984470034695\n",
      "16     \t [0.49486598 0.3293501 ]. \t  \u001b[92m-0.9684716051859179\u001b[0m \t -0.9684716051859179\n",
      "17     \t [1.23731012 2.048     ]. \t  -26.791800377780696 \t -0.9684716051859179\n",
      "18     \t [-1.09373178  1.40760174]. \t  -8.850702366358941 \t -0.9684716051859179\n",
      "19     \t [1.09195091 1.52832309]. \t  -11.295789871134433 \t -0.9684716051859179\n",
      "20     \t [ 0.44983453 -0.55123208]. \t  -57.091443498528044 \t -0.9684716051859179\n",
      "21     \t [0.61010788 1.23590384]. \t  -74.74498546275422 \t -0.9684716051859179\n",
      "22     \t [-0.85858799  0.99247069]. \t  -9.972022829867651 \t -0.9684716051859179\n",
      "23     \t [-1.41429461  2.048     ]. \t  -6.057022889315878 \t -0.9684716051859179\n",
      "24     \t [ 2.048      -0.75526463]. \t  -2450.921263340788 \t -0.9684716051859179\n",
      "25     \t [-0.57139633 -0.2747878 ]. \t  -38.62323885795433 \t -0.9684716051859179\n",
      "26     \t [ 0.72913917 -2.048     ]. \t  -665.5296438914417 \t -0.9684716051859179\n",
      "27     \t [-0.03599807  0.44658708]. \t  -20.901719238461553 \t -0.9684716051859179\n",
      "28     \t [2.048      1.36336191]. \t  -802.5216136886764 \t -0.9684716051859179\n",
      "29     \t [1.08716376 1.14365524]. \t  \u001b[92m-0.1540553470195686\u001b[0m \t -0.1540553470195686\n",
      "30     \t [0.93082982 0.40814634]. \t  -21.008472929971497 \t -0.1540553470195686\n",
      "31     \t [0.4897489  0.00889794]. \t  -5.594425751272401 \t -0.1540553470195686\n",
      "32     \t [1.4520188  1.84417715]. \t  -7.183504137030813 \t -0.1540553470195686\n",
      "33     \t [-1.33592177  1.80020968]. \t  -5.480625956271071 \t -0.1540553470195686\n",
      "34     \t [-0.78275735 -2.048     ]. \t  -711.115500662541 \t -0.1540553470195686\n",
      "35     \t [1.44522596 2.048     ]. \t  -0.36369661912921847 \t -0.1540553470195686\n",
      "36     \t [-1.2700162   1.29794806]. \t  -15.075037743000898 \t -0.1540553470195686\n",
      "37     \t [ 1.14203611 -1.29204917]. \t  -674.0952779628587 \t -0.1540553470195686\n",
      "38     \t [-0.21741431  0.08932348]. \t  -1.6589556981769382 \t -0.1540553470195686\n",
      "39     \t [1.31703037 1.76004434]. \t  -0.16540752140187426 \t -0.1540553470195686\n",
      "40     \t [-2.048      -1.16263748]. \t  -2878.9725013859315 \t -0.1540553470195686\n",
      "41     \t [0.12253836 0.07429392]. \t  -1.12133026037819 \t -0.1540553470195686\n",
      "42     \t [-0.84701821  0.68153054]. \t  -3.5404241441023805 \t -0.1540553470195686\n",
      "43     \t [1.21663062 1.47101066]. \t  \u001b[92m-0.05535499233305119\u001b[0m \t -0.05535499233305119\n",
      "44     \t [0.7108263  0.46782652]. \t  -0.22385302879199512 \t -0.05535499233305119\n",
      "45     \t [-1.29335095  2.048     ]. \t  -19.340213590968816 \t -0.05535499233305119\n",
      "46     \t [0.91510987 0.83858856]. \t  \u001b[92m-0.007341469802820256\u001b[0m \t -0.007341469802820256\n",
      "47     \t [1.04646577 1.13665852]. \t  -0.17494809779221737 \t -0.007341469802820256\n",
      "48     \t [1.37305383 1.88700429]. \t  -0.1394675758431932 \t -0.007341469802820256\n",
      "49     \t [1.08722617 1.25709863]. \t  -0.5706768994056504 \t -0.007341469802820256\n",
      "50     \t [1.3230183  1.75800195]. \t  -0.11015416056907218 \t -0.007341469802820256\n",
      "51     \t [0.82343802 0.63778641]. \t  -0.19329123608317988 \t -0.007341469802820256\n",
      "52     \t [1.40515964 2.04261992]. \t  -0.6285461312099311 \t -0.007341469802820256\n",
      "53     \t [1.31944494 1.7572808 ]. \t  -0.1287637335226166 \t -0.007341469802820256\n",
      "54     \t [0.72153892 0.48076845]. \t  -0.23634255676454202 \t -0.007341469802820256\n",
      "55     \t [-0.32384449 -1.65897042]. \t  -312.8677186581471 \t -0.007341469802820256\n",
      "56     \t [0.93381673 0.84756106]. \t  -0.06417330305338638 \t -0.007341469802820256\n",
      "57     \t [1.25241868 1.54276163]. \t  -0.13023236474300923 \t -0.007341469802820256\n",
      "58     \t [1.0100439  1.05997213]. \t  -0.15837319818446777 \t -0.007341469802820256\n",
      "59     \t [1.42132472 2.04799003]. \t  -0.2549435694752325 \t -0.007341469802820256\n",
      "60     \t [1.36658879 1.85324437]. \t  -0.1548951517468477 \t -0.007341469802820256\n",
      "61     \t [1.3486375  1.80763996]. \t  -0.13405435596044465 \t -0.007341469802820256\n",
      "62     \t [1.2346107  1.51367566]. \t  -0.06625258982205501 \t -0.007341469802820256\n",
      "63     \t [1.21631227 1.4945847 ]. \t  -0.0698013407484924 \t -0.007341469802820256\n",
      "64     \t [0.80149475 0.62906107]. \t  -0.05718060722076655 \t -0.007341469802820256\n",
      "65     \t [0.82318023 0.66052362]. \t  -0.0605132943731161 \t -0.007341469802820256\n",
      "66     \t [0.23418851 0.06639747]. \t  -0.5998149080893009 \t -0.007341469802820256\n",
      "67     \t [0.57903695 0.30471585]. \t  -0.2706498088829322 \t -0.007341469802820256\n",
      "68     \t [0.58030786 0.31242179]. \t  -0.23536278257809742 \t -0.007341469802820256\n",
      "69     \t [1.1371938  1.36088018]. \t  -0.47675109271336624 \t -0.007341469802820256\n",
      "70     \t [0.78594891 0.60135451]. \t  -0.07258670120482105 \t -0.007341469802820256\n",
      "71     \t [0.79694529 0.65416029]. \t  -0.07747763434602004 \t -0.007341469802820256\n",
      "72     \t [1.38332748 1.92270544]. \t  -0.15524012240226287 \t -0.007341469802820256\n",
      "73     \t [0.90677298 0.8331191 ]. \t  -0.02053278566031899 \t -0.007341469802820256\n",
      "74     \t [1.33209225 1.77024104]. \t  -0.1120734572344088 \t -0.007341469802820256\n",
      "75     \t [1.40738836 1.94637757]. \t  -0.28405668618492524 \t -0.007341469802820256\n",
      "76     \t [0.91143508 0.81911236]. \t  -0.02130331248597161 \t -0.007341469802820256\n",
      "77     \t [1.14712042 1.34715944]. \t  -0.11945195263932783 \t -0.007341469802820256\n",
      "78     \t [1.07365927 1.18434705]. \t  -0.10529951797545867 \t -0.007341469802820256\n",
      "79     \t [0.76179089 0.55507005]. \t  -0.12052667537774606 \t -0.007341469802820256\n",
      "80     \t [1.1606595  1.38207493]. \t  -0.1479230619380042 \t -0.007341469802820256\n",
      "81     \t [0.7444941  0.54124659]. \t  -0.08224802237813963 \t -0.007341469802820256\n",
      "82     \t [1.12004719 1.31431811]. \t  -0.3721636644985591 \t -0.007341469802820256\n",
      "83     \t [0.80777678 0.66365278]. \t  -0.049380795879166925 \t -0.007341469802820256\n",
      "84     \t [0.5960594  0.32767331]. \t  -0.23941854073712426 \t -0.007341469802820256\n",
      "85     \t [1.31643386 1.73691987]. \t  -0.1016684087892152 \t -0.007341469802820256\n",
      "86     \t [1.39153887 1.93425706]. \t  -0.15375355633736282 \t -0.007341469802820256\n",
      "87     \t [0.9416688  0.90185552]. \t  -0.02625004121077064 \t -0.007341469802820256\n",
      "88     \t [1.41931305 1.97685544]. \t  -0.31715500986741707 \t -0.007341469802820256\n",
      "89     \t [0.27272249 0.05193438]. \t  -0.5793021915390371 \t -0.007341469802820256\n",
      "90     \t [0.62158753 0.42180004]. \t  -0.2687173359229606 \t -0.007341469802820256\n",
      "91     \t [0.94708647 0.91257635]. \t  -0.027146970652951137 \t -0.007341469802820256\n",
      "92     \t [0.77961814 0.56147562]. \t  -0.2632041704618038 \t -0.007341469802820256\n",
      "93     \t [1.32053583 1.7256553 ]. \t  -0.13572020145084546 \t -0.007341469802820256\n",
      "94     \t [0.86686445 0.77041445]. \t  -0.05367503978992944 \t -0.007341469802820256\n",
      "95     \t [1.40233921 1.97818424]. \t  -0.17540017557974658 \t -0.007341469802820256\n",
      "96     \t [0.52945468 0.21971589]. \t  -0.5887261539544045 \t -0.007341469802820256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.007341469802820256\n",
      "98     \t [-0.09528591  0.03774469]. \t  -1.2818210723764543 \t -0.007341469802820256\n",
      "99     \t [0.66625589 0.4051948 ]. \t  -0.2611704980396064 \t -0.007341469802820256\n",
      "100    \t [0.68611425 0.42683727]. \t  -0.29138126283446064 \t -0.007341469802820256\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.16909841 -0.32076974]. \t  -12.895918857031877 \t -1.3013277264983028\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.3013277264983028\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.3013277264983028\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.3013277264983028\n",
      "5      \t [-0.18877374  2.048     ]. \t  -406.37426171258574 \t -1.3013277264983028\n",
      "6      \t [ 0.02375442 -2.048     ]. \t  -420.6146132499557 \t -1.3013277264983028\n",
      "7      \t [-0.53154899  0.92362894]. \t  -43.44459107771689 \t -1.3013277264983028\n",
      "8      \t [0.5577106  0.16505888]. \t  -2.3267013037182003 \t -1.3013277264983028\n",
      "9      \t [0.95317658 2.048     ]. \t  -129.83782490993693 \t -1.3013277264983028\n",
      "10     \t [-2.048     1.037194]. \t  -1006.0246619339333 \t -1.3013277264983028\n",
      "11     \t [ 0.45970195 -0.96884364]. \t  -139.57193149833145 \t -1.3013277264983028\n",
      "12     \t [-0.70196955  0.07556657]. \t  -20.301839631700783 \t -1.3013277264983028\n",
      "13     \t [1.65191273 1.12399166]. \t  -257.97100030468147 \t -1.3013277264983028\n",
      "14     \t [-1.17394589  2.048     ]. \t  -49.596084462460034 \t -1.3013277264983028\n",
      "15     \t [-0.55182903 -0.97627021]. \t  -166.44931996976183 \t -1.3013277264983028\n",
      "16     \t [-1.03028466  1.32309873]. \t  -10.966152639024052 \t -1.3013277264983028\n",
      "17     \t [1.09871184 1.46421671]. \t  -6.617162796089153 \t -1.3013277264983028\n",
      "18     \t [-2.048     -0.4739737]. \t  -2188.571968798449 \t -1.3013277264983028\n",
      "19     \t [-0.95614449  0.68860007]. \t  -8.916588626278426 \t -1.3013277264983028\n",
      "20     \t [ 0.68756961 -0.1974179 ]. \t  -45.01037899511508 \t -1.3013277264983028\n",
      "21     \t [-0.10342573 -0.94587601]. \t  -92.72071819937021 \t -1.3013277264983028\n",
      "22     \t [-0.64207407 -2.048     ]. \t  -607.9838998235775 \t -1.3013277264983028\n",
      "23     \t [0.21648673 0.57350572]. \t  -28.348779491045974 \t -1.3013277264983028\n",
      "24     \t [1.42555245 2.048     ]. \t  \u001b[92m-0.20605959262390786\u001b[0m \t -0.20605959262390786\n",
      "25     \t [-0.20983711  0.08660163]. \t  -1.6449264602574745 \t -0.20605959262390786\n",
      "26     \t [1.35008156 1.71442766]. \t  -1.295284842038365 \t -0.20605959262390786\n",
      "27     \t [-0.36903362 -0.35559509]. \t  -26.059099192034402 \t -0.20605959262390786\n",
      "28     \t [-0.86582374  1.66833765]. \t  -87.87986089751477 \t -0.20605959262390786\n",
      "29     \t [1.16907637 1.14976635]. \t  -4.73632431265295 \t -0.20605959262390786\n",
      "30     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.20605959262390786\n",
      "31     \t [ 0.79244997 -2.048     ]. \t  -716.1283436287355 \t -0.20605959262390786\n",
      "32     \t [-1.40397577  1.79323618]. \t  -8.944360033742015 \t -0.20605959262390786\n",
      "33     \t [-1.46810506  2.048     ]. \t  -7.243568125597353 \t -0.20605959262390786\n",
      "34     \t [1.17233235 0.52577422]. \t  -72.04001347585171 \t -0.20605959262390786\n",
      "35     \t [0.1543752  1.33265661]. \t  -172.01734618100141 \t -0.20605959262390786\n",
      "36     \t [1.31937435 1.88870521]. \t  -2.2911133171538656 \t -0.20605959262390786\n",
      "37     \t [-1.3036827   1.83373458]. \t  -7.106469157296621 \t -0.20605959262390786\n",
      "38     \t [ 0.09300152 -0.00995592]. \t  -0.8572615975050744 \t -0.20605959262390786\n",
      "39     \t [ 2.048      -1.11592491]. \t  -2820.9514160585686 \t -0.20605959262390786\n",
      "40     \t [0.71612387 0.41212118]. \t  -1.0948808477311505 \t -0.20605959262390786\n",
      "41     \t [1.34936619 2.04538956]. \t  -5.166593345670524 \t -0.20605959262390786\n",
      "42     \t [-1.48700888 -1.21505275]. \t  -1180.1028596619597 \t -0.20605959262390786\n",
      "43     \t [-1.13917907 -0.5174661 ]. \t  -334.06939737777645 \t -0.20605959262390786\n",
      "44     \t [ 0.28378325 -1.59078314]. \t  -279.84270988670136 \t -0.20605959262390786\n",
      "45     \t [-1.21753372  1.18540069]. \t  -13.737623803553365 \t -0.20605959262390786\n",
      "46     \t [1.15530646 1.33099127]. \t  \u001b[92m-0.02552017215751509\u001b[0m \t -0.02552017215751509\n",
      "47     \t [-0.04490241 -0.05805421]. \t  -1.4526668143334207 \t -0.02552017215751509\n",
      "48     \t [0.88292638 0.76038149]. \t  -0.05048389174162758 \t -0.02552017215751509\n",
      "49     \t [-1.31330683 -2.048     ]. \t  -1428.7343836141317 \t -0.02552017215751509\n",
      "50     \t [1.05620661 1.15934033]. \t  -0.19472232281923543 \t -0.02552017215751509\n",
      "51     \t [-1.23635374  1.51220836]. \t  -5.028050256467635 \t -0.02552017215751509\n",
      "52     \t [1.25554723 1.60470242]. \t  -0.14541363697524162 \t -0.02552017215751509\n",
      "53     \t [2.048      0.50971691]. \t  -1358.71650496125 \t -0.02552017215751509\n",
      "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.02552017215751509\n",
      "55     \t [1.23101742 1.50446889]. \t  -0.06532644899964261 \t -0.02552017215751509\n",
      "56     \t [0.42576078 0.16941876]. \t  -0.3438011888174481 \t -0.02552017215751509\n",
      "57     \t [1.08079418 1.17490641]. \t  \u001b[92m-0.011138601720350942\u001b[0m \t -0.011138601720350942\n",
      "58     \t [0.69473348 0.47128354]. \t  -0.10611775722565804 \t -0.011138601720350942\n",
      "59     \t [0.7972472  0.63101497]. \t  -0.04321379181376614 \t -0.011138601720350942\n",
      "60     \t [0.97313465 0.89456815]. \t  -0.2755378237548927 \t -0.011138601720350942\n",
      "61     \t [0.87328235 0.66413635]. \t  -0.98600086816128 \t -0.011138601720350942\n",
      "62     \t [1.01646977 1.03414945]. \t  \u001b[92m-0.0003593597674281231\u001b[0m \t -0.0003593597674281231\n",
      "63     \t [1.36134721 1.77317945]. \t  -0.7719611393197592 \t -0.0003593597674281231\n",
      "64     \t [1.11360509 1.27074828]. \t  -0.1067379609543908 \t -0.0003593597674281231\n",
      "65     \t [0.69314068 0.44003567]. \t  -0.25744602440643055 \t -0.0003593597674281231\n",
      "66     \t [0.68156913 0.41441561]. \t  -0.35260827284530105 \t -0.0003593597674281231\n",
      "67     \t [0.99058262 1.01095785]. \t  -0.08832096084938067 \t -0.0003593597674281231\n",
      "68     \t [1.14382761 1.29456176]. \t  -0.03967479223164573 \t -0.0003593597674281231\n",
      "69     \t [1.22703105 1.49592237]. \t  -0.0609188006429351 \t -0.0003593597674281231\n",
      "70     \t [1.12990053 1.20669067]. \t  -0.5066577424130932 \t -0.0003593597674281231\n",
      "71     \t [0.74365325 0.54086715]. \t  -0.08048320126887701 \t -0.0003593597674281231\n",
      "72     \t [1.02200137 1.06528378]. \t  -0.04373553566967083 \t -0.0003593597674281231\n",
      "73     \t [0.65750689 0.45129453]. \t  -0.15332260973046274 \t -0.0003593597674281231\n",
      "74     \t [0.91194093 0.81878643]. \t  -0.0242662041843959 \t -0.0003593597674281231\n",
      "75     \t [0.84361066 0.69447202]. \t  -0.05406546866753627 \t -0.0003593597674281231\n",
      "76     \t [0.62041598 0.33946451]. \t  -0.35066772336969654 \t -0.0003593597674281231\n",
      "77     \t [1.36904629 1.88524471]. \t  -0.14820067518373226 \t -0.0003593597674281231\n",
      "78     \t [1.00194544 1.01183456]. \t  -0.006307986028485669 \t -0.0003593597674281231\n",
      "79     \t [1.10388884 1.26922652]. \t  -0.2673953897277438 \t -0.0003593597674281231\n",
      "80     \t [0.98448327 0.95734244]. \t  -0.014318271835721593 \t -0.0003593597674281231\n",
      "81     \t [0.91322137 0.81426691]. \t  -0.046364599247548474 \t -0.0003593597674281231\n",
      "82     \t [1.00064555 1.06371229]. \t  -0.3896356004505905 \t -0.0003593597674281231\n",
      "83     \t [0.89333694 0.83099654]. \t  -0.11991858197658192 \t -0.0003593597674281231\n",
      "84     \t [0.92038218 0.89150219]. \t  -0.2034646639124135 \t -0.0003593597674281231\n",
      "85     \t [0.77889707 0.57003234]. \t  -0.1831963170049545 \t -0.0003593597674281231\n",
      "86     \t [0.5579758  0.30972002]. \t  -0.19564685497293105 \t -0.0003593597674281231\n",
      "87     \t [1.33286333 1.75456167]. \t  -0.1590353047162283 \t -0.0003593597674281231\n",
      "88     \t [1.11370008 1.23418443]. \t  -0.016701902852388522 \t -0.0003593597674281231\n",
      "89     \t [1.15615532 1.3401321 ]. \t  -0.025565768555978596 \t -0.0003593597674281231\n",
      "90     \t [0.87053935 0.76388366]. \t  -0.020414150695507532 \t -0.0003593597674281231\n",
      "91     \t [1.11059558 1.21427598]. \t  -0.048890502361554546 \t -0.0003593597674281231\n",
      "92     \t [1.05675007 1.09790617]. \t  -0.03861921788245524 \t -0.0003593597674281231\n",
      "93     \t [1.38022391 1.94083448]. \t  -0.2728520188991892 \t -0.0003593597674281231\n",
      "94     \t [1.23947868 1.4565175 ]. \t  -0.6939927857274035 \t -0.0003593597674281231\n",
      "95     \t [1.3142813  1.68578585]. \t  -0.27140875948051435 \t -0.0003593597674281231\n",
      "96     \t [1.2623681  1.61200262]. \t  -0.10280130476126911 \t -0.0003593597674281231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.24606001 1.53653516]. \t  -0.08656449364615296 \t -0.0003593597674281231\n",
      "98     \t [0.7386426  0.47500706]. \t  -0.5665436397191428 \t -0.0003593597674281231\n",
      "99     \t [1.16110338 1.31909634]. \t  -0.11043010956288224 \t -0.0003593597674281231\n",
      "100    \t [0.6869505  0.40208811]. \t  -0.5853837212579867 \t -0.0003593597674281231\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.89830197 -1.18801708]. \t  -401.59148800089145 \t -1.118465165857483\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.118465165857483\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.118465165857483\n",
      "4      \t [-0.00293696  2.048     ]. \t  -420.4327494538231 \t -1.118465165857483\n",
      "5      \t [-2.048       0.24991511]. \t  -1565.1106736813902 \t -1.118465165857483\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.118465165857483\n",
      "7      \t [2.048      0.42469052]. \t  -1422.0968808010853 \t -1.118465165857483\n",
      "8      \t [-0.22045224 -0.79410917]. \t  -72.5052414326178 \t -1.118465165857483\n",
      "9      \t [0.49194795 0.93639172]. \t  -48.47432691776136 \t -1.118465165857483\n",
      "10     \t [-0.59911988  1.10715872]. \t  -58.53961743836388 \t -1.118465165857483\n",
      "11     \t [-0.36503769 -2.048     ]. \t  -477.64958289376614 \t -1.118465165857483\n",
      "12     \t [-1.04521953  2.048     ]. \t  -95.48403212032858 \t -1.118465165857483\n",
      "13     \t [0.08857858 0.63085647]. \t  -39.64487362667463 \t -1.118465165857483\n",
      "14     \t [1.1255801 2.048    ]. \t  -61.02271544260576 \t -1.118465165857483\n",
      "15     \t [-0.83592353 -0.07497311]. \t  -63.2381689210113 \t -1.118465165857483\n",
      "16     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.118465165857483\n",
      "17     \t [-1.41211687  1.38418046]. \t  -43.01532697352938 \t -1.118465165857483\n",
      "18     \t [1.33042532 1.45114748]. \t  -10.277884478517802 \t -1.118465165857483\n",
      "19     \t [0.88799473 0.53372706]. \t  -6.505235361030627 \t -1.118465165857483\n",
      "20     \t [-0.87307722  0.6850153 ]. \t  -4.105151730209859 \t -1.118465165857483\n",
      "21     \t [-0.45916656  0.20801966]. \t  -2.12995907918208 \t -1.118465165857483\n",
      "22     \t [0.51893006 0.32450176]. \t  \u001b[92m-0.5362796783719685\u001b[0m \t -0.5362796783719685\n",
      "23     \t [ 2.048      -0.74073276]. \t  -2436.557085770937 \t -0.5362796783719685\n",
      "24     \t [-0.57859089 -0.501119  ]. \t  -72.36255917526168 \t -0.5362796783719685\n",
      "25     \t [1.05246729 1.11726313]. \t  \u001b[92m-0.011922294821374814\u001b[0m \t -0.011922294821374814\n",
      "26     \t [-1.16663628  1.56185001]. \t  -8.726770565433926 \t -0.011922294821374814\n",
      "27     \t [-1.47259814  2.048     ]. \t  -7.566858017475325 \t -0.011922294821374814\n",
      "28     \t [-2.048      -0.81059407]. \t  -2514.190774113243 \t -0.011922294821374814\n",
      "29     \t [1.05251459 1.61087627]. \t  -25.31264414207946 \t -0.011922294821374814\n",
      "30     \t [ 0.37269598 -0.2726865 ]. \t  -17.334043440476645 \t -0.011922294821374814\n",
      "31     \t [1.47591162 2.048     ]. \t  -1.9246947377520525 \t -0.011922294821374814\n",
      "32     \t [ 0.59832989 -2.048     ]. \t  -579.0442933732446 \t -0.011922294821374814\n",
      "33     \t [1.34338887 1.8184806 ]. \t  -0.13692392498144254 \t -0.011922294821374814\n",
      "34     \t [-1.38294242  1.80869321]. \t  -6.756616879592839 \t -0.011922294821374814\n",
      "35     \t [-1.02099199 -2.048     ]. \t  -959.156855483202 \t -0.011922294821374814\n",
      "36     \t [0.58337965 0.12448229]. \t  -4.832674486512306 \t -0.011922294821374814\n",
      "37     \t [-1.04872568  1.14188747]. \t  -4.374197444244446 \t -0.011922294821374814\n",
      "38     \t [1.20178833 1.46334869]. \t  -0.077022076823129 \t -0.011922294821374814\n",
      "39     \t [-2.048       1.26388214]. \t  -868.0275293057452 \t -0.011922294821374814\n",
      "40     \t [-0.47674435 -1.51719481]. \t  -306.5018172239077 \t -0.011922294821374814\n",
      "41     \t [0.71894868 0.55567282]. \t  -0.229422272422656 \t -0.011922294821374814\n",
      "42     \t [1.38912659 2.048     ]. \t  -1.5515547670332548 \t -0.011922294821374814\n",
      "43     \t [-0.68383503  0.5558536 ]. \t  -3.6136345419112548 \t -0.011922294821374814\n",
      "44     \t [0.87824596 0.76949745]. \t  -0.015154744884605312 \t -0.011922294821374814\n",
      "45     \t [0.18413227 0.04038239]. \t  -0.6698361951481941 \t -0.011922294821374814\n",
      "46     \t [2.048      1.30488719]. \t  -835.9712534951963 \t -0.011922294821374814\n",
      "47     \t [0.99022793 0.89485758]. \t  -0.7344377056160908 \t -0.011922294821374814\n",
      "48     \t [-1.37979786  2.048     ]. \t  -7.741586569515217 \t -0.011922294821374814\n",
      "49     \t [1.13690012 1.29607519]. \t  -0.019990072882387917 \t -0.011922294821374814\n",
      "50     \t [1.31880677 1.7705657 ]. \t  -0.19969690120012087 \t -0.011922294821374814\n",
      "51     \t [1.18604087 1.39700303]. \t  -0.044000637832842054 \t -0.011922294821374814\n",
      "52     \t [1.41976002 2.048     ]. \t  -0.28040783227068206 \t -0.011922294821374814\n",
      "53     \t [0.81408193 0.68870202]. \t  -0.1020233207884628 \t -0.011922294821374814\n",
      "54     \t [1.01152635 0.99276167]. \t  -0.09269415793913713 \t -0.011922294821374814\n",
      "55     \t [0.82556422 0.6396021 ]. \t  -0.2064430696995998 \t -0.011922294821374814\n",
      "56     \t [0.75282711 0.5763541 ]. \t  -0.07032087921961874 \t -0.011922294821374814\n",
      "57     \t [1.13477245 1.29049225]. \t  -0.01893853146513899 \t -0.011922294821374814\n",
      "58     \t [0.89558797 0.78814321]. \t  -0.03031917423143928 \t -0.011922294821374814\n",
      "59     \t [0.84333783 0.69576097]. \t  -0.04843713224742778 \t -0.011922294821374814\n",
      "60     \t [1.31829799 1.72411397]. \t  -0.12034549770059809 \t -0.011922294821374814\n",
      "61     \t [0.81456657 0.65549692]. \t  -0.04082045478636227 \t -0.011922294821374814\n",
      "62     \t [-0.65925387  0.35057096]. \t  -3.459474709131906 \t -0.011922294821374814\n",
      "63     \t [0.74473489 0.54084572]. \t  -0.08416107528138803 \t -0.011922294821374814\n",
      "64     \t [0.9839164  0.95875617]. \t  \u001b[92m-0.008973481409548685\u001b[0m \t -0.008973481409548685\n",
      "65     \t [1.16652398 1.34583482]. \t  -0.05006069560842475 \t -0.008973481409548685\n",
      "66     \t [1.37788729 1.92236527]. \t  -0.19940419675425602 \t -0.008973481409548685\n",
      "67     \t [1.16085906 1.35068609]. \t  -0.02683189292412615 \t -0.008973481409548685\n",
      "68     \t [1.19633448 1.43065046]. \t  -0.0385792328674269 \t -0.008973481409548685\n",
      "69     \t [1.18861864 1.3940623 ]. \t  -0.07074061430942605 \t -0.008973481409548685\n",
      "70     \t [1.16832883 1.3468837 ]. \t  -0.061126578834942526 \t -0.008973481409548685\n",
      "71     \t [1.26309939 1.63448311]. \t  -0.22181347144476915 \t -0.008973481409548685\n",
      "72     \t [1.36004558 1.85509004]. \t  -0.13251226977533717 \t -0.008973481409548685\n",
      "73     \t [1.23374989 1.52539061]. \t  -0.05569644591629766 \t -0.008973481409548685\n",
      "74     \t [1.26608275 1.62611699]. \t  -0.12439908178409465 \t -0.008973481409548685\n",
      "75     \t [0.86139966 0.73598239]. \t  -0.022842508230505116 \t -0.008973481409548685\n",
      "76     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.008973481409548685\n",
      "77     \t [0.91778278 0.83457788]. \t  -0.012761805646466881 \t -0.008973481409548685\n",
      "78     \t [ 0.10387396 -0.02135545]. \t  -0.9063735845224772 \t -0.008973481409548685\n",
      "79     \t [1.12586588 1.25665267]. \t  -0.027769726362926773 \t -0.008973481409548685\n",
      "80     \t [0.90112464 0.79273045]. \t  -0.04700664136283627 \t -0.008973481409548685\n",
      "81     \t [0.7481084  0.50942349]. \t  -0.31588212842592056 \t -0.008973481409548685\n",
      "82     \t [1.27781373 1.62571751]. \t  -0.08220787938060556 \t -0.008973481409548685\n",
      "83     \t [0.82427858 0.62406242]. \t  -0.3374922322462087 \t -0.008973481409548685\n",
      "84     \t [1.18711111 1.43317402]. \t  -0.09232882697113877 \t -0.008973481409548685\n",
      "85     \t [0.78662684 0.64899484]. \t  -0.13681100995555326 \t -0.008973481409548685\n",
      "86     \t [0.84323322 0.69392134]. \t  -0.05388839937040468 \t -0.008973481409548685\n",
      "87     \t [0.83317955 0.69345691]. \t  -0.02788253592688538 \t -0.008973481409548685\n",
      "88     \t [0.47391203 0.2160289 ]. \t  -0.2841022709102263 \t -0.008973481409548685\n",
      "89     \t [1.4298813  2.04677364]. \t  -0.1852877164669998 \t -0.008973481409548685\n",
      "90     \t [1.22016503 1.49341851]. \t  -0.050603211760174856 \t -0.008973481409548685\n",
      "91     \t [0.98859128 1.03202797]. \t  -0.29950600894735413 \t -0.008973481409548685\n",
      "92     \t [0.88699051 0.76493323]. \t  -0.06037772833184576 \t -0.008973481409548685\n",
      "93     \t [0.95218541 0.87845126]. \t  -0.08184295660018381 \t -0.008973481409548685\n",
      "94     \t [0.43121941 0.17681999]. \t  -0.3318474024251857 \t -0.008973481409548685\n",
      "95     \t [0.64162173 0.4169937 ]. \t  -0.13126018234566295 \t -0.008973481409548685\n",
      "96     \t [1.13958677 1.32154097]. \t  -0.07184746148185833 \t -0.008973481409548685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.80755567 0.61122167]. \t  -0.2045162022365295 \t -0.008973481409548685\n",
      "98     \t [0.4704414  0.18981105]. \t  -0.3796828569685258 \t -0.008973481409548685\n",
      "99     \t [1.4289079 2.048    ]. \t  -0.18783359138754666 \t -0.008973481409548685\n",
      "100    \t [0.44939979 0.1687033 ]. \t  -0.41376255135504225 \t -0.008973481409548685\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
      "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
      "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
      "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
      "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
      "1      \t [ 0.33600626 -2.048     ]. \t  -467.3898575531001 \t -12.122423820878506\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -12.122423820878506\n",
      "3      \t [ 2.048     -0.4625607]. \t  -2169.7371882813745 \t -12.122423820878506\n",
      "4      \t [0.13092338 2.048     ]. \t  -413.1941500252042 \t -12.122423820878506\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -12.122423820878506\n",
      "6      \t [-2.048       0.42193818]. \t  -1432.364695305647 \t -12.122423820878506\n",
      "7      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -12.122423820878506\n",
      "8      \t [-0.31611099 -1.39588302]. \t  -225.4766572385394 \t -12.122423820878506\n",
      "9      \t [-0.61908966  1.12152602]. \t  -57.12335025023076 \t -12.122423820878506\n",
      "10     \t [-0.98496736  2.048     ]. \t  -120.11385122993535 \t -12.122423820878506\n",
      "11     \t [2.048      0.93307731]. \t  -1064.6582594601052 \t -12.122423820878506\n",
      "12     \t [0.630823   1.27646274]. \t  -77.31692262300862 \t -12.122423820878506\n",
      "13     \t [1.20000845 2.048     ]. \t  -37.003938669621924 \t -12.122423820878506\n",
      "14     \t [-0.42669549 -2.048     ]. \t  -499.3562515461474 \t -12.122423820878506\n",
      "15     \t [ 0.47450234 -0.44430404]. \t  -45.09335001825961 \t -12.122423820878506\n",
      "16     \t [-0.07839525  0.16072951]. \t  \u001b[92m-3.5525482794363485\u001b[0m \t -3.5525482794363485\n",
      "17     \t [-1.31708224  1.43972819]. \t  -14.070039253890691 \t -3.5525482794363485\n",
      "18     \t [ 0.33244579 -0.00410594]. \t  \u001b[92m-1.7595439697963764\u001b[0m \t -1.7595439697963764\n",
      "19     \t [-0.91001182  0.56407089]. \t  -10.620418306880467 \t -1.7595439697963764\n",
      "20     \t [1.06529656 1.30930882]. \t  -3.0476158422688955 \t -1.7595439697963764\n",
      "21     \t [1.34719619 1.6826475 ]. \t  -1.8706113530930304 \t -1.7595439697963764\n",
      "22     \t [-1.43979001  2.048     ]. \t  -6.015051612726614 \t -1.7595439697963764\n",
      "23     \t [-0.44895245  0.58921802]. \t  -17.127469526171332 \t -1.7595439697963764\n",
      "24     \t [-2.048      -0.63425126]. \t  -2340.784898166913 \t -1.7595439697963764\n",
      "25     \t [-1.0475021   1.00113394]. \t  -5.116299403139627 \t -1.7595439697963764\n",
      "26     \t [ 0.05587184 -0.56064819]. \t  -32.67502252061367 \t -1.7595439697963764\n",
      "27     \t [1.12213383 0.29118782]. \t  -93.7166433438455 \t -1.7595439697963764\n",
      "28     \t [-0.46845202  0.00417664]. \t  -6.790496922023635 \t -1.7595439697963764\n",
      "29     \t [-1.20914777  1.6938948 ]. \t  -10.25607641249174 \t -1.7595439697963764\n",
      "30     \t [1.48416734 2.048     ]. \t  -2.629257259436843 \t -1.7595439697963764\n",
      "31     \t [0.98716846 0.79195031]. \t  -3.332660554121153 \t -1.7595439697963764\n",
      "32     \t [ 0.40369642 -1.30808139]. \t  -216.75503255306324 \t -1.7595439697963764\n",
      "33     \t [ 1.02475481 -2.048     ]. \t  -959.8368633971592 \t -1.7595439697963764\n",
      "34     \t [ 1.21903082 -1.11063219]. \t  -674.3166150831378 \t -1.7595439697963764\n",
      "35     \t [0.69810938 0.32904858]. \t  -2.5972844067694933 \t -1.7595439697963764\n",
      "36     \t [-0.14546443 -0.16242936]. \t  -4.682590370906723 \t -1.7595439697963764\n",
      "37     \t [-1.12145212 -2.048     ]. \t  -1097.23595704132 \t -1.7595439697963764\n",
      "38     \t [-2.048       1.34913148]. \t  -818.7909687213381 \t -1.7595439697963764\n",
      "39     \t [1.22114679 1.46279104]. \t  \u001b[92m-0.12960992471641278\u001b[0m \t -0.12960992471641278\n",
      "40     \t [-0.63926696  0.40281507]. \t  -2.6906151384719807 \t -0.12960992471641278\n",
      "41     \t [-1.38505415  1.83226293]. \t  -6.43001213131502 \t -0.12960992471641278\n",
      "42     \t [1.04850529 1.07432959]. \t  \u001b[92m-0.06502161591343952\u001b[0m \t -0.06502161591343952\n",
      "43     \t [-1.25917281 -0.1753228 ]. \t  -315.15924943683933 \t -0.06502161591343952\n",
      "44     \t [ 0.09218801 -0.04780637]. \t  -1.1411479056687297 \t -0.06502161591343952\n",
      "45     \t [-1.16479041  1.36607585]. \t  -4.695039501020162 \t -0.06502161591343952\n",
      "46     \t [1.09917855 1.10485796]. \t  -1.0776591513147509 \t -0.06502161591343952\n",
      "47     \t [1.40524618 2.02681054]. \t  -0.43559994337713537 \t -0.06502161591343952\n",
      "48     \t [0.80489523 0.62656422]. \t  -0.08340123641955693 \t -0.06502161591343952\n",
      "49     \t [1.05965468 1.12751865]. \t  \u001b[92m-0.005721493891098449\u001b[0m \t -0.005721493891098449\n",
      "50     \t [1.05935473 1.09429982]. \t  -0.08154609440103931 \t -0.005721493891098449\n",
      "51     \t [1.2028562  1.45826945]. \t  -0.05416127044640248 \t -0.005721493891098449\n",
      "52     \t [1.05246941 1.10453293]. \t  \u001b[92m-0.003750922429322165\u001b[0m \t -0.003750922429322165\n",
      "53     \t [0.80992688 0.61638729]. \t  -0.19289830800755395 \t -0.003750922429322165\n",
      "54     \t [1.12718742 1.23375325]. \t  -0.15158755765497411 \t -0.003750922429322165\n",
      "55     \t [1.11348249 1.21742902]. \t  -0.06311810945122164 \t -0.003750922429322165\n",
      "56     \t [1.0680548 1.1439297]. \t  -0.00564819811018818 \t -0.003750922429322165\n",
      "57     \t [0.81799419 0.64708526]. \t  -0.08165481752189904 \t -0.003750922429322165\n",
      "58     \t [ 2.048      -1.29817049]. \t  -3017.8259101800354 \t -0.003750922429322165\n",
      "59     \t [ 0.13998177 -0.00738035]. \t  -0.8123977217848732 \t -0.003750922429322165\n",
      "60     \t [0.78040765 0.58569219]. \t  -0.10271460312359984 \t -0.003750922429322165\n",
      "61     \t [0.09569697 0.01864653]. \t  -0.8267673675359998 \t -0.003750922429322165\n",
      "62     \t [-0.00301131 -0.04792802]. \t  -1.2358281627182084 \t -0.003750922429322165\n",
      "63     \t [0.85334518 0.75754536]. \t  -0.1076343808179056 \t -0.003750922429322165\n",
      "64     \t [1.23119677 1.49215426]. \t  -0.10957928879357076 \t -0.003750922429322165\n",
      "65     \t [1.44947074 2.048     ]. \t  -0.4825576391142698 \t -0.003750922429322165\n",
      "66     \t [1.09906821 1.20408023]. \t  -0.01131274780703906 \t -0.003750922429322165\n",
      "67     \t [0.88413242 0.7859762 ]. \t  -0.015262332012402673 \t -0.003750922429322165\n",
      "68     \t [-1.36284332  2.04799998]. \t  -9.218078107547138 \t -0.003750922429322165\n",
      "69     \t [1.25272663 1.51011543]. \t  -0.41443624784867883 \t -0.003750922429322165\n",
      "70     \t [1.22439863 1.46807176]. \t  -0.14695291330428978 \t -0.003750922429322165\n",
      "71     \t [1.25118788 1.50931188]. \t  -0.37848111913893684 \t -0.003750922429322165\n",
      "72     \t [0.94163906 0.89153198]. \t  -0.00575616991187285 \t -0.003750922429322165\n",
      "73     \t [0.98184963 0.96274122]. \t  \u001b[92m-0.0004951943332375657\u001b[0m \t -0.0004951943332375657\n",
      "74     \t [1.41854774 2.048     ]. \t  -0.3027905108455721 \t -0.0004951943332375657\n",
      "75     \t [1.15099143 1.32083128]. \t  -0.024358649156937508 \t -0.0004951943332375657\n",
      "76     \t [1.05065444 1.08100072]. \t  -0.0548880337896851 \t -0.0004951943332375657\n",
      "77     \t [1.21134485 1.44833151]. \t  -0.08086105648813482 \t -0.0004951943332375657\n",
      "78     \t [0.93780438 0.84955559]. \t  -0.09339768660197713 \t -0.0004951943332375657\n",
      "79     \t [0.81354673 0.65549409]. \t  -0.03881511708342446 \t -0.0004951943332375657\n",
      "80     \t [0.9730107  0.95734283]. \t  -0.011949594702506342 \t -0.0004951943332375657\n",
      "81     \t [0.84755066 0.67247165]. \t  -0.2336507160237995 \t -0.0004951943332375657\n",
      "82     \t [1.39824    2.04799868]. \t  -1.0220743099986738 \t -0.0004951943332375657\n",
      "83     \t [1.19514143 1.41915171]. \t  -0.0465650097085995 \t -0.0004951943332375657\n",
      "84     \t [1.01585716 1.0143459 ]. \t  -0.03129744072759065 \t -0.0004951943332375657\n",
      "85     \t [1.10259981 1.22178359]. \t  -0.014195752366977047 \t -0.0004951943332375657\n",
      "86     \t [1.42374343 2.048     ]. \t  -0.22346818895038031 \t -0.0004951943332375657\n",
      "87     \t [1.1636929  1.31158549]. \t  -0.20823445409100766 \t -0.0004951943332375657\n",
      "88     \t [0.81921586 0.63372156]. \t  -0.17250699950933626 \t -0.0004951943332375657\n",
      "89     \t [1.17789605 1.36495337]. \t  -0.0822078339116555 \t -0.0004951943332375657\n",
      "90     \t [1.0477549  1.09231015]. \t  -0.005283757296807988 \t -0.0004951943332375657\n",
      "91     \t [1.14656124 1.26269779]. \t  -0.29089186980310944 \t -0.0004951943332375657\n",
      "92     \t [0.87208241 0.75628229]. \t  -0.018165278156925667 \t -0.0004951943332375657\n",
      "93     \t [1.36217858 1.84764745]. \t  -0.13738753185721844 \t -0.0004951943332375657\n",
      "94     \t [0.97817863 0.96346351]. \t  -0.0048719716429606475 \t -0.0004951943332375657\n",
      "95     \t [1.03373069 1.05494877]. \t  -0.01977104500665873 \t -0.0004951943332375657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.92010578 0.85913067]. \t  -0.022098272797665637 \t -0.0004951943332375657\n",
      "97     \t [0.73289507 0.47650011]. \t  -0.4390061713547876 \t -0.0004951943332375657\n",
      "98     \t [1.15022837 1.28598466]. \t  -0.159769394613564 \t -0.0004951943332375657\n",
      "99     \t [0.74620719 0.50507761]. \t  -0.33219178114267806 \t -0.0004951943332375657\n",
      "100    \t [0.88229058 0.73970136]. \t  -0.16389787010238357 \t -0.0004951943332375657\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.9278091788796494\n",
      "2      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.9278091788796494\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.9278091788796494\n",
      "4      \t [-0.39642756  2.048     ]. \t  -359.47956228568347 \t -1.9278091788796494\n",
      "5      \t [-2.048       0.26265706]. \t  -1555.0750727407233 \t -1.9278091788796494\n",
      "6      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.9278091788796494\n",
      "7      \t [-0.01574563 -2.048     ]. \t  -420.56369541398857 \t -1.9278091788796494\n",
      "8      \t [-0.62481575  0.85988482]. \t  -24.682121549366762 \t -1.9278091788796494\n",
      "9      \t [ 2.048      -0.06144876]. \t  -1812.2414607482456 \t -1.9278091788796494\n",
      "10     \t [-0.50873309 -0.61436838]. \t  -78.52021135378169 \t -1.9278091788796494\n",
      "11     \t [0.53176441 1.0894878 ]. \t  -65.29805916554466 \t -1.9278091788796494\n",
      "12     \t [-1.22951591  2.048     ]. \t  -33.73150492876679 \t -1.9278091788796494\n",
      "13     \t [-0.22306792 -0.14910068]. \t  -5.450424075223325 \t -1.9278091788796494\n",
      "14     \t [-0.12754659 -1.13610969]. \t  -134.0688259441396 \t -1.9278091788796494\n",
      "15     \t [-0.98916645  1.47824219]. \t  -28.935980400915142 \t -1.9278091788796494\n",
      "16     \t [2.048      1.27040814]. \t  -856.0150030163055 \t -1.9278091788796494\n",
      "17     \t [0.90230365 0.62739124]. \t  -3.497498015731052 \t -1.9278091788796494\n",
      "18     \t [1.16995877 1.2767459 ]. \t  \u001b[92m-0.8763467055513338\u001b[0m \t -0.8763467055513338\n",
      "19     \t [1.35098316 2.048     ]. \t  -5.089156164335361 \t -0.8763467055513338\n",
      "20     \t [-0.65072158 -2.048     ]. \t  -613.5257420007507 \t -0.8763467055513338\n",
      "21     \t [-0.09547603 -0.50780565]. \t  -27.9208329920284 \t -0.8763467055513338\n",
      "22     \t [-1.02742974  0.43326101]. \t  -42.84252957773095 \t -0.8763467055513338\n",
      "23     \t [-1.09017971  1.02274493]. \t  -7.116053321062955 \t -0.8763467055513338\n",
      "24     \t [0.98675174 0.97886845]. \t  \u001b[92m-0.002868555651033312\u001b[0m \t -0.002868555651033312\n",
      "25     \t [ 0.57558343 -0.01101626]. \t  -11.897917433666672 \t -0.002868555651033312\n",
      "26     \t [-0.63972687  0.30693624]. \t  -3.7355243327446703 \t -0.002868555651033312\n",
      "27     \t [-2.048      -0.81531448]. \t  -2518.9180368392126 \t -0.002868555651033312\n",
      "28     \t [ 0.86709576 -2.048     ]. \t  -783.936496241441 \t -0.002868555651033312\n",
      "29     \t [ 1.10865238 -1.01776595]. \t  -504.85700215444274 \t -0.002868555651033312\n",
      "30     \t [-1.36579491  1.76563449]. \t  -6.592216489590334 \t -0.002868555651033312\n",
      "31     \t [ 0.53482245 -1.37779896]. \t  -277.05075318080947 \t -0.002868555651033312\n",
      "32     \t [-0.0987989   0.47610469]. \t  -22.95498170615789 \t -0.002868555651033312\n",
      "33     \t [1.32968168 1.80347388]. \t  -0.2341512696652523 \t -0.002868555651033312\n",
      "34     \t [-0.99503233 -1.36662804]. \t  -559.3918284558082 \t -0.002868555651033312\n",
      "35     \t [-0.84266923  0.643217  ]. \t  -3.842648917717181 \t -0.002868555651033312\n",
      "36     \t [-1.48266694  2.048     ]. \t  -8.422681868414184 \t -0.002868555651033312\n",
      "37     \t [ 2.048      -1.04408288]. \t  -2745.1680195945187 \t -0.002868555651033312\n",
      "38     \t [0.58202895 2.048     ]. \t  -292.32562602962525 \t -0.002868555651033312\n",
      "39     \t [ 0.14920416 -0.05815084]. \t  -1.370474186935117 \t -0.002868555651033312\n",
      "40     \t [-0.09330833  1.47938784]. \t  -217.48570030520762 \t -0.002868555651033312\n",
      "41     \t [-1.19519612  1.40426745]. \t  -4.877577505532856 \t -0.002868555651033312\n",
      "42     \t [0.71552896 0.58800003]. \t  -0.6588024216830873 \t -0.002868555651033312\n",
      "43     \t [ 1.26820343 -0.28949954]. \t  -360.25140248992517 \t -0.002868555651033312\n",
      "44     \t [1.49959441 2.048     ]. \t  -4.280991904747603 \t -0.002868555651033312\n",
      "45     \t [1.21360685 1.53607081]. \t  -0.44542120151825676 \t -0.002868555651033312\n",
      "46     \t [-1.28124255 -0.50613173]. \t  -466.471698382986 \t -0.002868555651033312\n",
      "47     \t [1.05221258 1.07490331]. \t  -0.1067195247352859 \t -0.002868555651033312\n",
      "48     \t [-0.03406641  0.02036865]. \t  -1.1061885662100077 \t -0.002868555651033312\n",
      "49     \t [1.15155333 1.35437935]. \t  -0.10308159222044022 \t -0.002868555651033312\n",
      "50     \t [1.44378374 2.04548465]. \t  -0.3492535180460097 \t -0.002868555651033312\n",
      "51     \t [1.08392886 1.1420495 ]. \t  -0.11497126011935671 \t -0.002868555651033312\n",
      "52     \t [1.29358915 1.71985797]. \t  -0.3022807652906616 \t -0.002868555651033312\n",
      "53     \t [-1.30984843 -2.048     ]. \t  -1421.8813490504822 \t -0.002868555651033312\n",
      "54     \t [0.84383081 0.6967716 ]. \t  -0.047733080289921875 \t -0.002868555651033312\n",
      "55     \t [1.41546715 2.01607176]. \t  -0.18829926932583313 \t -0.002868555651033312\n",
      "56     \t [1.02200209 1.0082046 ]. \t  -0.13213452872970988 \t -0.002868555651033312\n",
      "57     \t [0.95098029 0.87524269]. \t  -0.08720512790499585 \t -0.002868555651033312\n",
      "58     \t [1.35384446 1.8757338 ]. \t  -0.3087237367161976 \t -0.002868555651033312\n",
      "59     \t [1.14979923 1.32782475]. \t  -0.02578815393363464 \t -0.002868555651033312\n",
      "60     \t [1.03958741 1.05424737]. \t  -0.07176365422354838 \t -0.002868555651033312\n",
      "61     \t [1.40972393 2.048     ]. \t  -0.5360611343505858 \t -0.002868555651033312\n",
      "62     \t [0.6037121  0.36629915]. \t  -0.15737929389530567 \t -0.002868555651033312\n",
      "63     \t [1.41538715 2.048     ]. \t  -0.3721697687495362 \t -0.002868555651033312\n",
      "64     \t [0.9954914  0.94373973]. \t  -0.2234032807851235 \t -0.002868555651033312\n",
      "65     \t [0.61825411 0.40336427]. \t  -0.19036126020759359 \t -0.002868555651033312\n",
      "66     \t [1.16098852 1.39850508]. \t  -0.28206187066726773 \t -0.002868555651033312\n",
      "67     \t [0.71829829 0.51397798]. \t  -0.07974570158352601 \t -0.002868555651033312\n",
      "68     \t [1.2622095 1.6353785]. \t  -0.2468857342839469 \t -0.002868555651033312\n",
      "69     \t [1.00736066 0.97657727]. \t  -0.1459646816265707 \t -0.002868555651033312\n",
      "70     \t [1.31348001 1.73922222]. \t  -0.11784870090863252 \t -0.002868555651033312\n",
      "71     \t [0.72792354 0.55159938]. \t  -0.12123053679319458 \t -0.002868555651033312\n",
      "72     \t [1.39141898 1.97827599]. \t  -0.3315393666950835 \t -0.002868555651033312\n",
      "73     \t [1.12021085 1.2691827 ]. \t  -0.0349292706471006 \t -0.002868555651033312\n",
      "74     \t [0.85530417 0.71491701]. \t  -0.04858662717749265 \t -0.002868555651033312\n",
      "75     \t [1.10922063 1.19518532]. \t  -0.13572817268715692 \t -0.002868555651033312\n",
      "76     \t [0.79621201 0.64954745]. \t  -0.06584646369161788 \t -0.002868555651033312\n",
      "77     \t [1.10599648 1.20415258]. \t  -0.047623214781841026 \t -0.002868555651033312\n",
      "78     \t [1.43406651 2.04721328]. \t  -0.197125138089304 \t -0.002868555651033312\n",
      "79     \t [0.84755704 0.64408543]. \t  -0.5748051775963179 \t -0.002868555651033312\n",
      "80     \t [1.31311954 1.74455479]. \t  -0.13913865985880725 \t -0.002868555651033312\n",
      "81     \t [1.06650567 1.12614009]. \t  -0.01717903297991832 \t -0.002868555651033312\n",
      "82     \t [1.05925955 1.09234255]. \t  -0.09165083975915198 \t -0.002868555651033312\n",
      "83     \t [1.28853747 1.72302657]. \t  -0.47635455923701214 \t -0.002868555651033312\n",
      "84     \t [0.69593284 0.51303191]. \t  -0.17487979702717782 \t -0.002868555651033312\n",
      "85     \t [0.70322251 0.48486338]. \t  -0.09740556201918303 \t -0.002868555651033312\n",
      "86     \t [1.35049656 1.84850397]. \t  -0.1836742265472961 \t -0.002868555651033312\n",
      "87     \t [0.33542933 0.12821054]. \t  -0.4662959496475229 \t -0.002868555651033312\n",
      "88     \t [1.06143217 1.06113102]. \t  -0.4328936010962588 \t -0.002868555651033312\n",
      "89     \t [1.22188651 1.54313166]. \t  -0.30048541212098595 \t -0.002868555651033312\n",
      "90     \t [0.70331173 0.50695575]. \t  -0.10317350058526371 \t -0.002868555651033312\n",
      "91     \t [1.17437302 1.33201711]. \t  -0.25257571309110977 \t -0.002868555651033312\n",
      "92     \t [1.34162418 1.87380663]. \t  -0.6621068206626983 \t -0.002868555651033312\n",
      "93     \t [0.70248243 0.51140914]. \t  -0.12065648984333552 \t -0.002868555651033312\n",
      "94     \t [0.92587952 0.83306176]. \t  -0.06401487039409227 \t -0.002868555651033312\n",
      "95     \t [0.86410944 0.70297656]. \t  -0.20951010322411348 \t -0.002868555651033312\n",
      "96     \t [0.68952226 0.47399179]. \t  -0.09660643221952293 \t -0.002868555651033312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.03591102 1.13557972]. \t  -0.3915156897956238 \t -0.002868555651033312\n",
      "98     \t [0.96503396 0.92157713]. \t  -0.010657651615050178 \t -0.002868555651033312\n",
      "99     \t [0.58425331 0.31559392]. \t  -0.2391928278108737 \t -0.002868555651033312\n",
      "100    \t [0.72551944 0.50675545]. \t  -0.11384581870317573 \t -0.002868555651033312\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [0.47546261 1.56187371]. \t  -178.71371194373634 \t -3.0269049669752817\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.0269049669752817\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.0269049669752817\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.0269049669752817\n",
      "5      \t [-0.60190514  2.048     ]. \t  -286.7279869977491 \t -3.0269049669752817\n",
      "6      \t [2.048     0.7270468]. \t  -1203.2855504167007 \t -3.0269049669752817\n",
      "7      \t [0.23858608 0.50198406]. \t  -20.38765752260349 \t -3.0269049669752817\n",
      "8      \t [-0.07469316 -2.048     ]. \t  -422.8736639988958 \t -3.0269049669752817\n",
      "9      \t [-0.65378571 -0.45132224]. \t  -79.95656855839586 \t -3.0269049669752817\n",
      "10     \t [-0.28171395  0.06773844]. \t  \u001b[92m-1.6563030992441101\u001b[0m \t -1.6563030992441101\n",
      "11     \t [-0.27321732  1.13615517]. \t  -114.3008920318021 \t -1.6563030992441101\n",
      "12     \t [-0.0798089  -1.11861141]. \t  -127.72418424635836 \t -1.6563030992441101\n",
      "13     \t [1.07789894 2.048     ]. \t  -78.52939090481787 \t -1.6563030992441101\n",
      "14     \t [-1.26528744  1.32818804]. \t  -12.57156245289355 \t -1.6563030992441101\n",
      "15     \t [1.06830825 1.19309427]. \t  \u001b[92m-0.27311174119962867\u001b[0m \t -0.27311174119962867\n",
      "16     \t [0.87965255 0.50257857]. \t  -7.3699716190836 \t -0.27311174119962867\n",
      "17     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.27311174119962867\n",
      "18     \t [ 0.55538231 -1.48498082]. \t  -321.83691764474764 \t -0.27311174119962867\n",
      "19     \t [-2.048       1.17144107]. \t  -923.060333053139 \t -0.27311174119962867\n",
      "20     \t [-1.30025461  2.048     ]. \t  -18.06021141177815 \t -0.27311174119962867\n",
      "21     \t [ 0.56286027 -2.048     ]. \t  -559.4245208547303 \t -0.27311174119962867\n",
      "22     \t [-2.048     -0.5137302]. \t  -2225.848911082982 \t -0.27311174119962867\n",
      "23     \t [0.73700023 0.90461418]. \t  -13.133406554673105 \t -0.27311174119962867\n",
      "24     \t [-0.94161774  1.30270505]. \t  -21.080560890884577 \t -0.27311174119962867\n",
      "25     \t [ 0.80736341 -0.3780965 ]. \t  -106.11313679892383 \t -0.27311174119962867\n",
      "26     \t [1.33232648 1.73617217]. \t  \u001b[92m-0.26193052022949115\u001b[0m \t -0.26193052022949115\n",
      "27     \t [-0.78920119 -1.60401631]. \t  -499.0894811225644 \t -0.26193052022949115\n",
      "28     \t [-1.06572215  0.91466074]. \t  -9.155859826649017 \t -0.26193052022949115\n",
      "29     \t [0.52683138 0.13692359]. \t  -2.201503708139999 \t -0.26193052022949115\n",
      "30     \t [1.13575988 1.59822154]. \t  -9.521534708170691 \t -0.26193052022949115\n",
      "31     \t [1.4511119 2.048    ]. \t  -0.5367280141812323 \t -0.26193052022949115\n",
      "32     \t [-1.2756303   1.74115472]. \t  -6.476316696166909 \t -0.26193052022949115\n",
      "33     \t [-0.83843741  0.20645922]. \t  -28.032871857847795 \t -0.26193052022949115\n",
      "34     \t [-0.77401426 -2.048     ]. \t  -703.8599517105126 \t -0.26193052022949115\n",
      "35     \t [-1.51003916  2.048     ]. \t  -11.692829272529913 \t -0.26193052022949115\n",
      "36     \t [0.8694454  0.74994559]. \t  \u001b[92m-0.020632162005820322\u001b[0m \t -0.020632162005820322\n",
      "37     \t [ 2.048      -0.11265213]. \t  -1856.0854176038893 \t -0.020632162005820322\n",
      "38     \t [-0.25916411 -0.34940444]. \t  -18.938590404006504 \t -0.020632162005820322\n",
      "39     \t [0.10981846 0.02633336]. \t  -0.8127958055440617 \t -0.020632162005820322\n",
      "40     \t [1.39666148 2.048     ]. \t  -1.1047840634240491 \t -0.020632162005820322\n",
      "41     \t [0.67789367 0.44749597]. \t  -0.11825795834244547 \t -0.020632162005820322\n",
      "42     \t [-1.51019551 -1.19766156]. \t  -1216.1943819706219 \t -0.020632162005820322\n",
      "43     \t [1.17350082 1.39122486]. \t  -0.05004189212400805 \t -0.020632162005820322\n",
      "44     \t [ 2.048      -1.25662578]. \t  -2972.361847925116 \t -0.020632162005820322\n",
      "45     \t [1.35211746 1.90505697]. \t  -0.7143535090900377 \t -0.020632162005820322\n",
      "46     \t [0.7218421  0.54140277]. \t  -0.11877083026987478 \t -0.020632162005820322\n",
      "47     \t [-0.01900579  0.02930679]. \t  -1.1221573710086712 \t -0.020632162005820322\n",
      "48     \t [1.19818079 1.44275271]. \t  -0.04433865905152506 \t -0.020632162005820322\n",
      "49     \t [ 0.92749851 -1.07756734]. \t  -375.52021473936986 \t -0.020632162005820322\n",
      "50     \t [0.72624569 0.48056732]. \t  -0.294578819803752 \t -0.020632162005820322\n",
      "51     \t [1.1442823  1.33919125]. \t  -0.10967662144796705 \t -0.020632162005820322\n",
      "52     \t [-1.15154068  1.41206937]. \t  -5.369130213319609 \t -0.020632162005820322\n",
      "53     \t [-0.86403773  0.88618574]. \t  -5.42413797772204 \t -0.020632162005820322\n",
      "54     \t [1.33820199 1.80310941]. \t  -0.12957076775430404 \t -0.020632162005820322\n",
      "55     \t [1.03676333 1.10029789]. \t  -0.06596756077349691 \t -0.020632162005820322\n",
      "56     \t [0.70028651 0.47042429]. \t  -0.12973587880160808 \t -0.020632162005820322\n",
      "57     \t [1.22462794 1.52958273]. \t  -0.13967426823418017 \t -0.020632162005820322\n",
      "58     \t [0.7083225  0.43253245]. \t  -0.563778051547755 \t -0.020632162005820322\n",
      "59     \t [0.96206922 0.94297945]. \t  -0.031722630231556484 \t -0.020632162005820322\n",
      "60     \t [0.90421125 0.87835384]. \t  -0.3783029054237765 \t -0.020632162005820322\n",
      "61     \t [1.11122697 1.26303379]. \t  -0.091942875536211 \t -0.020632162005820322\n",
      "62     \t [0.66971291 0.41916073]. \t  -0.19525911345084196 \t -0.020632162005820322\n",
      "63     \t [1.28816371 1.64764937]. \t  -0.09676569284381095 \t -0.020632162005820322\n",
      "64     \t [1.15668781 1.33464311]. \t  -0.02562925484048015 \t -0.020632162005820322\n",
      "65     \t [0.16039313 2.048     ]. \t  -409.66417025669466 \t -0.020632162005820322\n",
      "66     \t [1.2121726  1.47672163]. \t  -0.05043301495345337 \t -0.020632162005820322\n",
      "67     \t [2.048      1.50541474]. \t  -724.1108512444623 \t -0.020632162005820322\n",
      "68     \t [0.99483177 0.99380104]. \t  \u001b[92m-0.0017165752928308298\u001b[0m \t -0.0017165752928308298\n",
      "69     \t [1.18205262 1.36251849]. \t  -0.1537597879517519 \t -0.0017165752928308298\n",
      "70     \t [1.38060658 1.9325808 ]. \t  -0.21511953728483213 \t -0.0017165752928308298\n",
      "71     \t [-0.04685951 -0.01647867]. \t  -1.1307884698745234 \t -0.0017165752928308298\n",
      "72     \t [1.12002416 1.30442905]. \t  -0.2641550960175771 \t -0.0017165752928308298\n",
      "73     \t [0.91633057 0.83257576]. \t  -0.012021656839472775 \t -0.0017165752928308298\n",
      "74     \t [1.09093648 1.17328011]. \t  -0.03670310690409695 \t -0.0017165752928308298\n",
      "75     \t [1.24496458 1.56730053]. \t  -0.09015753604097429 \t -0.0017165752928308298\n",
      "76     \t [0.61091398 0.3856546 ]. \t  -0.16686008980703565 \t -0.0017165752928308298\n",
      "77     \t [1.09016538 1.21671906]. \t  -0.08798411168700568 \t -0.0017165752928308298\n",
      "78     \t [1.12426535 1.26410086]. \t  -0.015443522657215242 \t -0.0017165752928308298\n",
      "79     \t [0.73643733 0.56130695]. \t  -0.10543998855794222 \t -0.0017165752928308298\n",
      "80     \t [0.78360852 0.61797612]. \t  -0.04837276242367713 \t -0.0017165752928308298\n",
      "81     \t [0.68106945 0.53850041]. \t  -0.6589015300692072 \t -0.0017165752928308298\n",
      "82     \t [1.09677207 1.19558727]. \t  -0.01472558434921736 \t -0.0017165752928308298\n",
      "83     \t [0.66975161 0.45602927]. \t  -0.11463222060463235 \t -0.0017165752928308298\n",
      "84     \t [0.85061931 0.72472308]. \t  -0.022451450323285087 \t -0.0017165752928308298\n",
      "85     \t [1.41924322 2.04799972]. \t  -0.28966029085193484 \t -0.0017165752928308298\n",
      "86     \t [0.8763174  0.77042807]. \t  -0.015920326692339105 \t -0.0017165752928308298\n",
      "87     \t [1.25012338 1.55721252]. \t  -0.06569316046564257 \t -0.0017165752928308298\n",
      "88     \t [1.0304445  1.05936354]. \t  \u001b[92m-0.0015282576420380743\u001b[0m \t -0.0015282576420380743\n",
      "89     \t [0.62204972 0.36654171]. \t  -0.18447931589427974 \t -0.0015282576420380743\n",
      "90     \t [0.80696205 0.62151814]. \t  -0.12529225145087888 \t -0.0015282576420380743\n",
      "91     \t [-0.09517888  0.05668415]. \t  -1.4262321003688772 \t -0.0015282576420380743\n",
      "92     \t [0.78088184 0.59749271]. \t  -0.0631018029422481 \t -0.0015282576420380743\n",
      "93     \t [0.81471955 0.67107257]. \t  -0.03966459812478109 \t -0.0015282576420380743\n",
      "94     \t [1.18306702 1.43030051]. \t  -0.12747382817569042 \t -0.0015282576420380743\n",
      "95     \t [1.1826468  1.39796265]. \t  -0.03340757458098964 \t -0.0015282576420380743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.75372332 0.55727322]. \t  -0.07237160707666836 \t -0.0015282576420380743\n",
      "97     \t [0.76320489 0.59180454]. \t  -0.06476346014172188 \t -0.0015282576420380743\n",
      "98     \t [0.98681533 0.97092696]. \t  \u001b[92m-0.0010018532059937995\u001b[0m \t -0.0010018532059937995\n",
      "99     \t [0.93339903 0.87486696]. \t  -0.005755719284405291 \t -0.0010018532059937995\n",
      "100    \t [1.08109389 1.14452147]. \t  -0.06534628006079968 \t -0.0010018532059937995\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.60810559  1.63311685]. \t  -162.18486816257092 \t -2.0077595729598063\n",
      "2      \t [ 0.12293374 -0.04820384]. \t  \u001b[92m-1.1701436763679447\u001b[0m \t -1.1701436763679447\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.1701436763679447\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.1701436763679447\n",
      "5      \t [-0.60693766 -2.048     ]. \t  -586.4682529886005 \t -1.1701436763679447\n",
      "6      \t [2.048      0.68233776]. \t  -1234.4889931365685 \t -1.1701436763679447\n",
      "7      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.1701436763679447\n",
      "8      \t [-0.36007373 -0.93168785]. \t  -114.49425909363379 \t -1.1701436763679447\n",
      "9      \t [0.52903571 2.048     ]. \t  -312.8470717405002 \t -1.1701436763679447\n",
      "10     \t [-0.09726439  0.78909069]. \t  -61.986333384664995 \t -1.1701436763679447\n",
      "11     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.1701436763679447\n",
      "12     \t [-0.63058053 -0.158338  ]. \t  -33.569036099028985 \t -1.1701436763679447\n",
      "13     \t [ 0.26547313 -2.048     ]. \t  -449.33357763700417 \t -1.1701436763679447\n",
      "14     \t [ 1.11283264 -0.36770386]. \t  -257.96856714151505 \t -1.1701436763679447\n",
      "15     \t [-1.37824584  1.37445443]. \t  -33.229806111549706 \t -1.1701436763679447\n",
      "16     \t [-1.20731892  2.048     ]. \t  -39.72723194658694 \t -1.1701436763679447\n",
      "17     \t [0.77960374 0.29835734]. \t  -9.622935853344948 \t -1.1701436763679447\n",
      "18     \t [1.23776441 2.048     ]. \t  -26.67586471894969 \t -1.1701436763679447\n",
      "19     \t [-0.9121486   1.13129969]. \t  -12.613440685521994 \t -1.1701436763679447\n",
      "20     \t [ 0.49468483 -0.91088992]. \t  -133.79717389850495 \t -1.1701436763679447\n",
      "21     \t [ 0.56820808 -0.22395317]. \t  -30.086955340122167 \t -1.1701436763679447\n",
      "22     \t [-0.45760273  0.36289985]. \t  -4.480818049484298 \t -1.1701436763679447\n",
      "23     \t [0.42587592 0.45753664]. \t  -7.956403324258487 \t -1.1701436763679447\n",
      "24     \t [-2.048       1.03425953]. \t  -1007.878406106228 \t -1.1701436763679447\n",
      "25     \t [0.99934353 1.6179653 ]. \t  -38.35050014586424 \t -1.1701436763679447\n",
      "26     \t [ 2.048      -0.48660191]. \t  -2192.186321189359 \t -1.1701436763679447\n",
      "27     \t [-0.07009082 -1.57694676]. \t  -251.37303574366652 \t -1.1701436763679447\n",
      "28     \t [-1.2280542   1.67077904]. \t  -7.61011581065907 \t -1.1701436763679447\n",
      "29     \t [1.21455345 1.29254189]. \t  -3.380243385359667 \t -1.1701436763679447\n",
      "30     \t [ 0.94412317 -1.66513313]. \t  -653.5732169592945 \t -1.1701436763679447\n",
      "31     \t [-1.50446953  2.048     ]. \t  -10.913314402464994 \t -1.1701436763679447\n",
      "32     \t [-0.81215861 -1.3488892 ]. \t  -406.68744927159423 \t -1.1701436763679447\n",
      "33     \t [1.47593736 2.048     ]. \t  -1.9266998156010982 \t -1.1701436763679447\n",
      "34     \t [-1.09516453  1.05039027]. \t  -6.609667593680205 \t -1.1701436763679447\n",
      "35     \t [0.39733632 0.15327943]. \t  \u001b[92m-0.36531650138014377\u001b[0m \t -0.36531650138014377\n",
      "36     \t [0.98424153 0.8161189 ]. \t  -2.3293055104497284 \t -0.36531650138014377\n",
      "37     \t [-1.42172994  1.83872594]. \t  -9.198689427155008 \t -0.36531650138014377\n",
      "38     \t [-2.048      -1.02307443]. \t  -2731.3940729845253 \t -0.36531650138014377\n",
      "39     \t [1.21281363 1.53563914]. \t  -0.46418660424125757 \t -0.36531650138014377\n",
      "40     \t [ 1.4751861  -1.17561028]. \t  -1123.6716040625154 \t -0.36531650138014377\n",
      "41     \t [-0.21265888  0.02912369]. \t  -1.4964629381956964 \t -0.36531650138014377\n",
      "42     \t [0.80959601 0.70134149]. \t  \u001b[92m-0.24689607890898532\u001b[0m \t -0.24689607890898532\n",
      "43     \t [0.16341299 0.08034736]. \t  -0.9876409277946407 \t -0.24689607890898532\n",
      "44     \t [-0.74312231  0.5692753 ]. \t  -3.0675270023779926 \t -0.24689607890898532\n",
      "45     \t [-1.25655904 -2.048     ]. \t  -1320.5618872634807 \t -0.24689607890898532\n",
      "46     \t [0.98222856 0.96281437]. \t  \u001b[92m-0.0006994252640097042\u001b[0m \t -0.0006994252640097042\n",
      "47     \t [-0.62379541  0.27832467]. \t  -3.864287855677215 \t -0.0006994252640097042\n",
      "48     \t [1.42748832 2.02499069]. \t  -0.19895723125093098 \t -0.0006994252640097042\n",
      "49     \t [1.36435916 1.89894994]. \t  -0.27318785323603484 \t -0.0006994252640097042\n",
      "50     \t [0.74176111 0.54281678]. \t  -0.07215261408652615 \t -0.0006994252640097042\n",
      "51     \t [0.64266486 0.40527007]. \t  -0.13369163556294933 \t -0.0006994252640097042\n",
      "52     \t [1.19639364 1.42929867]. \t  -0.038994435500620296 \t -0.0006994252640097042\n",
      "53     \t [0.59087665 0.32819459]. \t  -0.2112329206139803 \t -0.0006994252640097042\n",
      "54     \t [1.04539913 1.09182101]. \t  -0.0021688950723818697 \t -0.0006994252640097042\n",
      "55     \t [ 1.13805799 -2.048     ]. \t  -1117.7016361649617 \t -0.0006994252640097042\n",
      "56     \t [1.01596362 1.02546623]. \t  -0.004765091596772309 \t -0.0006994252640097042\n",
      "57     \t [2.048      1.43727542]. \t  -761.2189615805574 \t -0.0006994252640097042\n",
      "58     \t [0.80496816 0.6151848 ]. \t  -0.14554881441016712 \t -0.0006994252640097042\n",
      "59     \t [1.44026521 2.04799922]. \t  -0.2633429637214829 \t -0.0006994252640097042\n",
      "60     \t [0.76458262 0.55401609]. \t  -0.14887684405574553 \t -0.0006994252640097042\n",
      "61     \t [-0.08565864  0.07003564]. \t  -1.5717615934854723 \t -0.0006994252640097042\n",
      "62     \t [1.41766854 2.0479996 ]. \t  -0.3204894849044462 \t -0.0006994252640097042\n",
      "63     \t [1.38912334 1.9600057 ]. \t  -0.243481019678968 \t -0.0006994252640097042\n",
      "64     \t [1.25350302 1.60541936]. \t  -0.18088291294648667 \t -0.0006994252640097042\n",
      "65     \t [0.97108911 0.96030652]. \t  -0.030738761196914842 \t -0.0006994252640097042\n",
      "66     \t [1.18395285 1.41984251]. \t  -0.06659300071037855 \t -0.0006994252640097042\n",
      "67     \t [0.62172508 0.36654022]. \t  -0.18309932603681742 \t -0.0006994252640097042\n",
      "68     \t [1.21812565 1.49227307]. \t  -0.05470717987565025 \t -0.0006994252640097042\n",
      "69     \t [1.36854236 1.90890508]. \t  -0.2654010350806124 \t -0.0006994252640097042\n",
      "70     \t [1.20235267 1.43306457]. \t  -0.05679083497101771 \t -0.0006994252640097042\n",
      "71     \t [0.74626193 0.5569112 ]. \t  -0.06438300844835843 \t -0.0006994252640097042\n",
      "72     \t [0.82784892 0.68906771]. \t  -0.031030172514980442 \t -0.0006994252640097042\n",
      "73     \t [1.21203006 1.46359712]. \t  -0.047894108878318666 \t -0.0006994252640097042\n",
      "74     \t [1.29013623 1.66974978]. \t  -0.08698621549400924 \t -0.0006994252640097042\n",
      "75     \t [0.98980207 0.97437021]. \t  -0.002953353721040689 \t -0.0006994252640097042\n",
      "76     \t [1.2467403  1.55106604]. \t  -0.06196669612371922 \t -0.0006994252640097042\n",
      "77     \t [0.69367985 0.45666838]. \t  -0.15397150752434355 \t -0.0006994252640097042\n",
      "78     \t [1.2891381  1.70857375]. \t  -0.3016590784940034 \t -0.0006994252640097042\n",
      "79     \t [1.19472432 1.44832292]. \t  -0.08183590449355724 \t -0.0006994252640097042\n",
      "80     \t [0.70587209 0.46014561]. \t  -0.2317468799219714 \t -0.0006994252640097042\n",
      "81     \t [0.56644895 0.29305651]. \t  -0.2652944923581344 \t -0.0006994252640097042\n",
      "82     \t [1.00435175 1.01149104]. \t  -0.0007854480540481327 \t -0.0006994252640097042\n",
      "83     \t [0.98797364 0.98744838]. \t  -0.01304158161636947 \t -0.0006994252640097042\n",
      "84     \t [1.25698179 1.59456888]. \t  -0.08725546691512442 \t -0.0006994252640097042\n",
      "85     \t [1.12337218 1.26949225]. \t  -0.020886544176046698 \t -0.0006994252640097042\n",
      "86     \t [0.15422642 0.07170302]. \t  -0.944939015021414 \t -0.0006994252640097042\n",
      "87     \t [0.82478759 0.67290265]. \t  -0.036133912551145383 \t -0.0006994252640097042\n",
      "88     \t [0.52406299 0.17724965]. \t  -1.175043461417748 \t -0.0006994252640097042\n",
      "89     \t [1.09077395 1.21040492]. \t  -0.05074646304915739 \t -0.0006994252640097042\n",
      "90     \t [1.27476857 1.62866624]. \t  -0.07681643051955449 \t -0.0006994252640097042\n",
      "91     \t [0.61813169 0.35402239]. \t  -0.22458442920762428 \t -0.0006994252640097042\n",
      "92     \t [0.94722613 0.89677008]. \t  -0.0028069144127778637 \t -0.0006994252640097042\n",
      "93     \t [1.19600662 1.37528482]. \t  -0.3425379524305837 \t -0.0006994252640097042\n",
      "94     \t [0.48590467 0.22593647]. \t  -0.2746305487336253 \t -0.0006994252640097042\n",
      "95     \t [0.91390873 0.83490022]. \t  -0.007422527358232774 \t -0.0006994252640097042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.03279587 1.06768201]. \t  -0.0011785301063792703 \t -0.0006994252640097042\n",
      "97     \t [1.36082092 1.87749393]. \t  -0.19603713463914085 \t -0.0006994252640097042\n",
      "98     \t [1.38199398 1.92683554]. \t  -0.17457578039733668 \t -0.0006994252640097042\n",
      "99     \t [1.08973793 1.19314616]. \t  -0.01120840825311341 \t -0.0006994252640097042\n",
      "100    \t [1.41825018 2.048     ]. \t  -0.30864365038919234 \t -0.0006994252640097042\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.752975   0.78635495]. \t  \u001b[92m-4.873937703479205\u001b[0m \t -4.873937703479205\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -4.873937703479205\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -4.873937703479205\n",
      "4      \t [0.37555311 0.01947143]. \t  \u001b[92m-1.8678289049826764\u001b[0m \t -1.8678289049826764\n",
      "5      \t [-0.20387181  2.048     ]. \t  -404.0279639214682 \t -1.8678289049826764\n",
      "6      \t [-2.048       0.40145323]. \t  -1447.8620019270932 \t -1.8678289049826764\n",
      "7      \t [2.048      0.67678358]. \t  -1238.3932934208035 \t -1.8678289049826764\n",
      "8      \t [-0.38821676  0.97078945]. \t  -69.17980663448536 \t -1.8678289049826764\n",
      "9      \t [ 0.46167459 -2.048     ]. \t  -511.56675499160264 \t -1.8678289049826764\n",
      "10     \t [0.93645656 2.048     ]. \t  -137.13964017972378 \t -1.8678289049826764\n",
      "11     \t [-0.58861669 -0.58300867]. \t  -88.91668897944483 \t -1.8678289049826764\n",
      "12     \t [0.42410998 1.311986  ]. \t  -128.50047704614747 \t -1.8678289049826764\n",
      "13     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.8678289049826764\n",
      "14     \t [-0.26339362 -2.048     ]. \t  -449.92436139877077 \t -1.8678289049826764\n",
      "15     \t [-1.19280357  2.048     ]. \t  -43.898347618955114 \t -1.8678289049826764\n",
      "16     \t [-0.27168378  0.14877691]. \t  -2.179152316562637 \t -1.8678289049826764\n",
      "17     \t [-1.17488894  1.37098583]. \t  -4.7389369984868095 \t -1.8678289049826764\n",
      "18     \t [1.19167114 1.49026925]. \t  \u001b[92m-0.5293894819308692\u001b[0m \t -0.5293894819308692\n",
      "19     \t [-0.90929397  1.51719938]. \t  -51.308390388484405 \t -0.5293894819308692\n",
      "20     \t [-0.8760186   0.89331412]. \t  -5.104665803616547 \t -0.5293894819308692\n",
      "21     \t [-2.048      -0.83362572]. \t  -2537.2980354764295 \t -0.5293894819308692\n",
      "22     \t [-0.09029737 -1.29373563]. \t  -170.68031009798116 \t -0.5293894819308692\n",
      "23     \t [-0.11716322 -0.49459843]. \t  -27.087550771153623 \t -0.5293894819308692\n",
      "24     \t [1.41892193 2.048     ]. \t  \u001b[92m-0.2956310136869372\u001b[0m \t -0.2956310136869372\n",
      "25     \t [0.39675684 0.57478309]. \t  -17.783431925642518 \t -0.2956310136869372\n",
      "26     \t [1.22052185 1.77414973]. \t  -8.141297605519904 \t -0.2956310136869372\n",
      "27     \t [-2.048       1.36414628]. \t  -810.2695775250685 \t -0.2956310136869372\n",
      "28     \t [ 2.048      -0.29010676]. \t  -2012.0922906568023 \t -0.2956310136869372\n",
      "29     \t [-0.44344186 -0.10630251]. \t  -11.260982217257125 \t -0.2956310136869372\n",
      "30     \t [-1.33051843  1.7746976 ]. \t  -5.4332682923954065 \t -0.2956310136869372\n",
      "31     \t [-0.56979403  0.45645011]. \t  -4.200978494365882 \t -0.2956310136869372\n",
      "32     \t [1.0000583  1.18637502]. \t  -3.4692198574151347 \t -0.2956310136869372\n",
      "33     \t [1.08263931 0.83737392]. \t  -11.211511957809586 \t -0.2956310136869372\n",
      "34     \t [0.65770447 0.35512752]. \t  -0.7169800657558101 \t -0.2956310136869372\n",
      "35     \t [-1.46164367  2.048     ]. \t  -6.841184828544103 \t -0.2956310136869372\n",
      "36     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -0.2956310136869372\n",
      "37     \t [-0.73258012 -1.56090406]. \t  -442.9850504233421 \t -0.2956310136869372\n",
      "38     \t [ 0.95551464 -1.55107447]. \t  -607.1723329601031 \t -0.2956310136869372\n",
      "39     \t [-0.96180324 -2.048     ]. \t  -887.7605041559245 \t -0.2956310136869372\n",
      "40     \t [1.10338883 1.1467389 ]. \t  -0.5109344375199762 \t -0.2956310136869372\n",
      "41     \t [ 0.04941302 -0.00644523]. \t  -0.9115132726159888 \t -0.2956310136869372\n",
      "42     \t [-1.17251109  0.84959025]. \t  -32.302467676967005 \t -0.2956310136869372\n",
      "43     \t [1.36290168 1.85389951]. \t  \u001b[92m-0.13299468402763104\u001b[0m \t -0.13299468402763104\n",
      "44     \t [1.38016567 2.048     ]. \t  -2.1935102227319456 \t -0.13299468402763104\n",
      "45     \t [-1.30942551 -0.38975451]. \t  -448.16220540232166 \t -0.13299468402763104\n",
      "46     \t [0.84236219 0.70882393]. \t  \u001b[92m-0.02490594895000613\u001b[0m \t -0.02490594895000613\n",
      "47     \t [ 1.23559995 -2.048     ]. \t  -1277.9086893492572 \t -0.02490594895000613\n",
      "48     \t [2.048      1.44062548]. \t  -759.3728408696375 \t -0.02490594895000613\n",
      "49     \t [1.07010449 1.13916318]. \t  \u001b[92m-0.008467330109440875\u001b[0m \t -0.008467330109440875\n",
      "50     \t [0.89721032 0.74784868]. \t  -0.33703706379137827 \t -0.008467330109440875\n",
      "51     \t [0.96310717 0.87837688]. \t  -0.2434106684683182 \t -0.008467330109440875\n",
      "52     \t [1.17974226 1.41273916]. \t  -0.07618646653889644 \t -0.008467330109440875\n",
      "53     \t [0.95579356 0.90857662]. \t  \u001b[92m-0.0044190441739136955\u001b[0m \t -0.0044190441739136955\n",
      "54     \t [1.06679699 1.12669125]. \t  -0.017377202152773 \t -0.0044190441739136955\n",
      "55     \t [1.26608437 1.62017852]. \t  -0.10041544612818876 \t -0.0044190441739136955\n",
      "56     \t [0.90610587 0.80022154]. \t  -0.05210636287831466 \t -0.0044190441739136955\n",
      "57     \t [1.35137914 1.85244453]. \t  -0.1922106969178995 \t -0.0044190441739136955\n",
      "58     \t [0.64754284 0.43568797]. \t  -0.15104419414211215 \t -0.0044190441739136955\n",
      "59     \t [0.89389391 0.81594664]. \t  -0.039820613055831275 \t -0.0044190441739136955\n",
      "60     \t [0.55119593 0.33153906]. \t  -0.2782765600662199 \t -0.0044190441739136955\n",
      "61     \t [1.0835185  1.13335836]. \t  -0.17224991399901615 \t -0.0044190441739136955\n",
      "62     \t [-1.25465664 -1.08174873]. \t  -710.470334832833 \t -0.0044190441739136955\n",
      "63     \t [1.01776515 0.99545522]. \t  -0.16345631233532976 \t -0.0044190441739136955\n",
      "64     \t [0.18914333 0.03665158]. \t  -0.6575653518987861 \t -0.0044190441739136955\n",
      "65     \t [0.45555399 0.24622664]. \t  -0.44616880313891694 \t -0.0044190441739136955\n",
      "66     \t [0.49361622 0.2051953 ]. \t  -0.404354550940247 \t -0.0044190441739136955\n",
      "67     \t [0.87548739 0.7654707 ]. \t  -0.015604888645365125 \t -0.0044190441739136955\n",
      "68     \t [0.93930808 0.89542372]. \t  -0.02090761405811138 \t -0.0044190441739136955\n",
      "69     \t [1.03347707 1.06113935]. \t  -0.0059308322975303715 \t -0.0044190441739136955\n",
      "70     \t [ 0.12894813 -0.01733222]. \t  -0.8740584272081231 \t -0.0044190441739136955\n",
      "71     \t [1.36216546 1.85801544]. \t  -0.13179921309300588 \t -0.0044190441739136955\n",
      "72     \t [0.93817198 0.87424008]. \t  -0.007335146375918141 \t -0.0044190441739136955\n",
      "73     \t [1.03712128 1.0729572 ]. \t  \u001b[92m-0.002087330507328505\u001b[0m \t -0.002087330507328505\n",
      "74     \t [0.875777   0.74511705]. \t  -0.06325360912648126 \t -0.002087330507328505\n",
      "75     \t [1.08330066 1.14299145]. \t  -0.10026234802077713 \t -0.002087330507328505\n",
      "76     \t [0.90921754 0.79933929]. \t  -0.08297393072127714 \t -0.002087330507328505\n",
      "77     \t [0.72104453 0.50610082]. \t  -0.09687227779702162 \t -0.002087330507328505\n",
      "78     \t [0.77056869 0.60089763]. \t  -0.05771034120980806 \t -0.002087330507328505\n",
      "79     \t [0.14862122 0.05778413]. \t  -0.8522653118747054 \t -0.002087330507328505\n",
      "80     \t [0.96451375 0.91848143]. \t  -0.015195900703197346 \t -0.002087330507328505\n",
      "81     \t [1.31792467 1.73789654]. \t  -0.10117040049309 \t -0.002087330507328505\n",
      "82     \t [0.99284275 1.0056339 ]. \t  -0.03964099558735307 \t -0.002087330507328505\n",
      "83     \t [1.05115083 1.1103829 ]. \t  -0.005602842198996679 \t -0.002087330507328505\n",
      "84     \t [0.90857581 0.83818053]. \t  -0.024412620599469803 \t -0.002087330507328505\n",
      "85     \t [1.31620992 1.77033416]. \t  -0.24382381115972754 \t -0.002087330507328505\n",
      "86     \t [0.3166998  0.15907915]. \t  -0.8124124627118317 \t -0.002087330507328505\n",
      "87     \t [0.99293644 0.9804273 ]. \t  -0.003069909089702306 \t -0.002087330507328505\n",
      "88     \t [0.69758699 0.46129587]. \t  -0.15562332072227253 \t -0.002087330507328505\n",
      "89     \t [1.09557523 1.20028534]. \t  -0.009134624296447777 \t -0.002087330507328505\n",
      "90     \t [0.30214382 0.13770571]. \t  -0.7024368105313621 \t -0.002087330507328505\n",
      "91     \t [0.89081233 0.77798465]. \t  -0.03613940525578109 \t -0.002087330507328505\n",
      "92     \t [1.23183806 1.56194035]. \t  -0.25191037795890026 \t -0.002087330507328505\n",
      "93     \t [0.78207583 0.61807271]. \t  -0.051625580150817384 \t -0.002087330507328505\n",
      "94     \t [0.86338835 0.74420672]. \t  -0.018814704563076472 \t -0.002087330507328505\n",
      "95     \t [0.71920023 0.50554081]. \t  -0.09255659466601962 \t -0.002087330507328505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.13124357 1.31148336]. \t  -0.11816673671587703 \t -0.002087330507328505\n",
      "97     \t [0.76734999 0.61346598]. \t  -0.11483887427660605 \t -0.002087330507328505\n",
      "98     \t [0.84739661 0.66496656]. \t  -0.30540227712623935 \t -0.002087330507328505\n",
      "99     \t [0.95283385 0.89504386]. \t  -0.018732981682470017 \t -0.002087330507328505\n",
      "100    \t [1.23728991 1.5294751 ]. \t  -0.05650565384801041 \t -0.002087330507328505\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [-2.048  2.048]. \t  -469.9523900415999 \t -29.9831488845538\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -29.9831488845538\n",
      "3      \t [-0.01597409  2.048     ]. \t  -420.3580915775033 \t -29.9831488845538\n",
      "4      \t [-2.048      -0.00936751]. \t  -1776.3757203307102 \t -29.9831488845538\n",
      "5      \t [ 0.36605349 -1.50572582]. \t  -269.27037694551353 \t -29.9831488845538\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -29.9831488845538\n",
      "7      \t [-0.32604938  0.68294997]. \t  -35.0099806072706 \t -29.9831488845538\n",
      "8      \t [ 0.15871023 -2.048     ]. \t  -430.51900496185795 \t -29.9831488845538\n",
      "9      \t [-0.54515742 -0.51352802]. \t  -68.11495339920887 \t -29.9831488845538\n",
      "10     \t [0.78872733 1.21521478]. \t  -35.22424055451632 \t -29.9831488845538\n",
      "11     \t [2.048      0.95753788]. \t  -1048.763795231707 \t -29.9831488845538\n",
      "12     \t [1.04448349 2.048     ]. \t  -91.59725978763727 \t -29.9831488845538\n",
      "13     \t [-1.18082658  1.38878444]. \t  \u001b[92m-4.759103669984383\u001b[0m \t -4.759103669984383\n",
      "14     \t [ 1.287525   -0.67458832]. \t  -544.0491710717438 \t -4.759103669984383\n",
      "15     \t [-1.11463989  2.048     ]. \t  -69.36727896409279 \t -4.759103669984383\n",
      "16     \t [0.49244068 0.58727264]. \t  -12.14458433593183 \t -4.759103669984383\n",
      "17     \t [-0.60416056  1.39571045]. \t  -108.80767749262706 \t -4.759103669984383\n",
      "18     \t [-0.94552132  0.46022713]. \t  -22.601860053224495 \t -4.759103669984383\n",
      "19     \t [-0.31753029  0.01651664]. \t  \u001b[92m-2.446684186951239\u001b[0m \t -2.446684186951239\n",
      "20     \t [-2.048       1.10224655]. \t  -965.3722307774779 \t -2.446684186951239\n",
      "21     \t [-0.87523398  0.90727486]. \t  -5.511386069693153 \t -2.446684186951239\n",
      "22     \t [ 2.048      -0.10361485]. \t  -1848.3089510029422 \t -2.446684186951239\n",
      "23     \t [-0.63337821  0.20571126]. \t  -6.488256514728782 \t -2.446684186951239\n",
      "24     \t [1.3630216  1.50698238]. \t  -12.441041495095172 \t -2.446684186951239\n",
      "25     \t [-0.34473318 -1.26258044]. \t  -192.6408162873171 \t -2.446684186951239\n",
      "26     \t [1.09109606 0.92536721]. \t  -7.037340209692095 \t -2.446684186951239\n",
      "27     \t [1.45779865 2.048     ]. \t  \u001b[92m-0.8052070913980155\u001b[0m \t -0.8052070913980155\n",
      "28     \t [ 0.25570936 -0.7164202 ]. \t  -61.676260963833414 \t -0.8052070913980155\n",
      "29     \t [1.28922995 1.78166678]. \t  -1.5129437192719943 \t -0.8052070913980155\n",
      "30     \t [-1.48361438  2.048     ]. \t  -8.512657343022775 \t -0.8052070913980155\n",
      "31     \t [-1.36861069 -1.03920975]. \t  -853.7623439996376 \t -0.8052070913980155\n",
      "32     \t [-1.3279209   1.80383279]. \t  -5.58290773446152 \t -0.8052070913980155\n",
      "33     \t [0.18376148 0.08993373]. \t  -0.9817010503602208 \t -0.8052070913980155\n",
      "34     \t [ 0.70466652 -2.048     ]. \t  -647.5631886058395 \t -0.8052070913980155\n",
      "35     \t [ 0.74352608 -0.96863966]. \t  -231.5530843436416 \t -0.8052070913980155\n",
      "36     \t [ 0.04160915 -0.20778864]. \t  -5.308374370364185 \t -0.8052070913980155\n",
      "37     \t [0.79368458 0.45528066]. \t  -3.092987327131249 \t -0.8052070913980155\n",
      "38     \t [1.15997595 1.37996084]. \t  \u001b[92m-0.14404283603963047\u001b[0m \t -0.14404283603963047\n",
      "39     \t [-2.048      -0.94860513]. \t  -2654.24173143667 \t -0.14404283603963047\n",
      "40     \t [0.91943756 0.90223335]. \t  -0.32988646294063434 \t -0.14404283603963047\n",
      "41     \t [1.37944402 2.048     ]. \t  -2.2503710734908307 \t -0.14404283603963047\n",
      "42     \t [-1.11916364 -0.21670521]. \t  -220.3552573404179 \t -0.14404283603963047\n",
      "43     \t [-1.38230665  2.048     ]. \t  -7.558546530762205 \t -0.14404283603963047\n",
      "44     \t [ 2.048      -1.21625622]. \t  -2928.514497609347 \t -0.14404283603963047\n",
      "45     \t [-0.09221856  0.10407954]. \t  -2.1064047761666833 \t -0.14404283603963047\n",
      "46     \t [-1.10288654  1.00655391]. \t  -8.823937701209015 \t -0.14404283603963047\n",
      "47     \t [1.35442128 1.87916034]. \t  -0.32545312866997 \t -0.14404283603963047\n",
      "48     \t [-1.51000193  1.63968346]. \t  -47.314189306175955 \t -0.14404283603963047\n",
      "49     \t [1.24550669 1.56879794]. \t  \u001b[92m-0.09093710860115542\u001b[0m \t -0.09093710860115542\n",
      "50     \t [1.1075077  1.21579003]. \t  \u001b[92m-0.023185799781141634\u001b[0m \t -0.023185799781141634\n",
      "51     \t [1.27616065 1.63154705]. \t  -0.07714148414332536 \t -0.023185799781141634\n",
      "52     \t [0.97216771 0.93153001]. \t  \u001b[92m-0.019216418660793967\u001b[0m \t -0.019216418660793967\n",
      "53     \t [1.11598235 1.20889884]. \t  -0.1468066015464473 \t -0.019216418660793967\n",
      "54     \t [1.38698769 1.95210836]. \t  -0.23026499894195002 \t -0.019216418660793967\n",
      "55     \t [1.12612515 1.2493751 ]. \t  -0.0511867668346708 \t -0.019216418660793967\n",
      "56     \t [0.89834311 0.7849703 ]. \t  -0.05895456946701347 \t -0.019216418660793967\n",
      "57     \t [1.23561252 1.49441238]. \t  -0.16000976739212347 \t -0.019216418660793967\n",
      "58     \t [1.30185106 1.67181141]. \t  -0.14403595504270367 \t -0.019216418660793967\n",
      "59     \t [0.90344857 0.77988486]. \t  -0.14134143918445846 \t -0.019216418660793967\n",
      "60     \t [1.04444153 1.0512983 ]. \t  -0.15847284314154578 \t -0.019216418660793967\n",
      "61     \t [1.10379824 1.21246056]. \t  \u001b[92m-0.014266860811751419\u001b[0m \t -0.014266860811751419\n",
      "62     \t [0.93230248 0.87977955]. \t  -0.01580124920031731 \t -0.014266860811751419\n",
      "63     \t [1.03741245 1.07131074]. \t  \u001b[92m-0.0038142941401176174\u001b[0m \t -0.0038142941401176174\n",
      "64     \t [1.4346121 2.0479997]. \t  -0.19911328905999426 \t -0.0038142941401176174\n",
      "65     \t [1.35688428 1.83509655]. \t  -0.1310126226176541 \t -0.0038142941401176174\n",
      "66     \t [1.09380306 1.20556026]. \t  -0.017180626838107282 \t -0.0038142941401176174\n",
      "67     \t [1.414985  2.0425318]. \t  -0.3350186885333867 \t -0.0038142941401176174\n",
      "68     \t [0.80707765 0.64565181]. \t  -0.04049376294530818 \t -0.0038142941401176174\n",
      "69     \t [ 0.0313305  -0.01242652]. \t  -0.9562983627743064 \t -0.0038142941401176174\n",
      "70     \t [0.983032   0.93952382]. \t  -0.07226259227701555 \t -0.0038142941401176174\n",
      "71     \t [0.93045343 0.86066975]. \t  -0.007411115616067335 \t -0.0038142941401176174\n",
      "72     \t [0.91357964 0.76705471]. \t  -0.4640801130425572 \t -0.0038142941401176174\n",
      "73     \t [1.23773645 1.50235399]. \t  -0.1443570059367193 \t -0.0038142941401176174\n",
      "74     \t [0.85074583 0.68484647]. \t  -0.17376905586979718 \t -0.0038142941401176174\n",
      "75     \t [-0.00685117 -0.00724023]. \t  -1.0190595652839938 \t -0.0038142941401176174\n",
      "76     \t [0.85987867 0.71037728]. \t  -0.10381547960607226 \t -0.0038142941401176174\n",
      "77     \t [1.21579744 1.46733898]. \t  -0.058285403670080733 \t -0.0038142941401176174\n",
      "78     \t [1.3530357  1.84214976]. \t  -0.13773109777785636 \t -0.0038142941401176174\n",
      "79     \t [1.2859918  1.64422973]. \t  -0.09090236774646449 \t -0.0038142941401176174\n",
      "80     \t [0.94393342 0.89390118]. \t  -0.003979183538760232 \t -0.0038142941401176174\n",
      "81     \t [-0.69667737  0.49758572]. \t  -2.893662486151293 \t -0.0038142941401176174\n",
      "82     \t [1.4170839  2.01858831]. \t  -0.18490334041206455 \t -0.0038142941401176174\n",
      "83     \t [1.33341043 1.78313156]. \t  -0.11381290297665168 \t -0.0038142941401176174\n",
      "84     \t [0.9512599  0.89009026]. \t  -0.024294816157947468 \t -0.0038142941401176174\n",
      "85     \t [0.96492536 0.91288174]. \t  -0.034351323099097636 \t -0.0038142941401176174\n",
      "86     \t [1.0728121  1.12001131]. \t  -0.10087217933769532 \t -0.0038142941401176174\n",
      "87     \t [0.94123769 0.91195766]. \t  -0.07120526902292003 \t -0.0038142941401176174\n",
      "88     \t [0.8659603  0.70477058]. \t  -0.22151794259942095 \t -0.0038142941401176174\n",
      "89     \t [1.09018116 1.1989568 ]. \t  -0.019077636241983796 \t -0.0038142941401176174\n",
      "90     \t [-0.04696887  0.00477989]. \t  -1.0968062689937252 \t -0.0038142941401176174\n",
      "91     \t [1.3478552  1.82642195]. \t  -0.13042837576205718 \t -0.0038142941401176174\n",
      "92     \t [1.16634099 1.33773272]. \t  -0.07882938374927818 \t -0.0038142941401176174\n",
      "93     \t [0.9891076  0.96866693]. \t  -0.00946357710809476 \t -0.0038142941401176174\n",
      "94     \t [1.40922021 2.04799478]. \t  -0.5530175947791118 \t -0.0038142941401176174\n",
      "95     \t [1.0666485 1.1345443]. \t  -0.005462639484170794 \t -0.0038142941401176174\n",
      "96     \t [0.90749957 0.84391795]. \t  -0.050019389981883156 \t -0.0038142941401176174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.14498837 1.28642873]. \t  -0.08138833137338163 \t -0.0038142941401176174\n",
      "98     \t [1.08681255 1.14225135]. \t  -0.15893663121499896 \t -0.0038142941401176174\n",
      "99     \t [1.36246271 1.84170477]. \t  -0.15269482018227526 \t -0.0038142941401176174\n",
      "100    \t [1.03966417 1.05722436]. \t  -0.05763436514377697 \t -0.0038142941401176174\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
      "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
      "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
      "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
      "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
      "1      \t [0.35154688 2.048     ]. \t  -370.75771980518005 \t -8.580376531587937\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -8.580376531587937\n",
      "3      \t [-0.58045515 -2.048     ]. \t  -571.2860797431229 \t -8.580376531587937\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -8.580376531587937\n",
      "5      \t [-0.11329274  0.62779615]. \t  -39.05711221853108 \t -8.580376531587937\n",
      "6      \t [-0.74996258  2.048     ]. \t  -223.75006979455117 \t -8.580376531587937\n",
      "7      \t [-0.48987451  1.16733152]. \t  -88.2183601070642 \t -8.580376531587937\n",
      "8      \t [-0.01724095 -0.50532837]. \t  -26.600505967629637 \t -8.580376531587937\n",
      "9      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -8.580376531587937\n",
      "10     \t [-2.048       0.88343453]. \t  -1105.4759669199962 \t -8.580376531587937\n",
      "11     \t [-0.68354547  0.15386295]. \t  -12.654492490629995 \t -8.580376531587937\n",
      "12     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -8.580376531587937\n",
      "13     \t [ 0.74442793 -0.48067872]. \t  -107.15711202680131 \t -8.580376531587937\n",
      "14     \t [1.11814676 1.26907055]. \t  \u001b[92m-0.04937176587518481\u001b[0m \t -0.04937176587518481\n",
      "15     \t [1.22844677 2.048     ]. \t  -29.095507326794834 \t -0.04937176587518481\n",
      "16     \t [-0.03512161 -1.30987109]. \t  -172.97100760261137 \t -0.04937176587518481\n",
      "17     \t [ 0.0637623 -2.048    ]. \t  -421.9738764128737 \t -0.04937176587518481\n",
      "18     \t [2.048      1.19127954]. \t  -902.9138970935334 \t -0.04937176587518481\n",
      "19     \t [0.85061906 1.50980031]. \t  -61.84083075025761 \t -0.04937176587518481\n",
      "20     \t [0.74994259 0.62610667]. \t  -0.4682057740752472 \t -0.04937176587518481\n",
      "21     \t [-0.87135077  0.78022551]. \t  -3.545941816162709 \t -0.04937176587518481\n",
      "22     \t [-0.75523241 -0.84375495]. \t  -203.05747146250255 \t -0.04937176587518481\n",
      "23     \t [-1.37423278  2.048     ]. \t  -8.180504310952472 \t -0.04937176587518481\n",
      "24     \t [0.84703205 0.98424273]. \t  -7.140525804533387 \t -0.04937176587518481\n",
      "25     \t [-0.3193249   0.10942658]. \t  -1.746180650062494 \t -0.04937176587518481\n",
      "26     \t [0.34188681 0.16639069]. \t  -0.6781785350092318 \t -0.04937176587518481\n",
      "27     \t [1.26863562 1.67833439]. \t  -0.5468593532971947 \t -0.04937176587518481\n",
      "28     \t [-0.47762102 -0.36115388]. \t  -36.90795161891874 \t -0.04937176587518481\n",
      "29     \t [-0.60709586  0.58710763]. \t  -7.358828442948799 \t -0.04937176587518481\n",
      "30     \t [ 0.48536734 -0.93369391]. \t  -136.98533544679498 \t -0.04937176587518481\n",
      "31     \t [-1.217707    1.80188676]. \t  -15.099200935834263 \t -0.04937176587518481\n",
      "32     \t [-1.49429971  1.56371909]. \t  -51.0060729408569 \t -0.04937176587518481\n",
      "33     \t [1.4886635 2.048    ]. \t  -3.0651926068501627 \t -0.04937176587518481\n",
      "34     \t [1.02307022 0.80176537]. \t  -5.9984912740022915 \t -0.04937176587518481\n",
      "35     \t [ 1.00923098 -2.048     ]. \t  -940.3712448190217 \t -0.04937176587518481\n",
      "36     \t [-2.048      -1.19139009]. \t  -2909.8603849789824 \t -0.04937176587518481\n",
      "37     \t [1.41306053 2.04799999]. \t  -0.433377148010202 \t -0.04937176587518481\n",
      "38     \t [ 2.048      -0.86837476]. \t  -2564.1699300669065 \t -0.04937176587518481\n",
      "39     \t [0.02913257 0.03487845]. \t  -1.0583858888667221 \t -0.04937176587518481\n",
      "40     \t [0.76142922 0.3304391 ]. \t  -6.273728447538661 \t -0.04937176587518481\n",
      "41     \t [-1.20854317 -1.72599078]. \t  -1020.2988207023952 \t -0.04937176587518481\n",
      "42     \t [-0.57882854 -1.45287634]. \t  -322.15806969843607 \t -0.04937176587518481\n",
      "43     \t [-1.42802752  1.90100933]. \t  -7.806714093927352 \t -0.04937176587518481\n",
      "44     \t [0.55125458 0.35819677]. \t  -0.49638613357093353 \t -0.04937176587518481\n",
      "45     \t [1.36504251 1.8920253 ]. \t  -0.2155345852339372 \t -0.04937176587518481\n",
      "46     \t [-1.40378519  0.4798489 ]. \t  -228.01589860548586 \t -0.04937176587518481\n",
      "47     \t [1.42173303 2.04761935]. \t  -0.24699909944623943 \t -0.04937176587518481\n",
      "48     \t [1.0036204  1.02620966]. \t  \u001b[92m-0.035945177002061665\u001b[0m \t -0.035945177002061665\n",
      "49     \t [0.99621325 0.9759489 ]. \t  \u001b[92m-0.027212720838110355\u001b[0m \t -0.027212720838110355\n",
      "50     \t [-1.15409855  1.11636316]. \t  -9.287627421635888 \t -0.027212720838110355\n",
      "51     \t [0.77148732 0.58393711]. \t  -0.06488684571752519 \t -0.027212720838110355\n",
      "52     \t [0.98534379 0.94428485]. \t  -0.07106408836747799 \t -0.027212720838110355\n",
      "53     \t [0.98112895 0.99023036]. \t  -0.07662233994124867 \t -0.027212720838110355\n",
      "54     \t [-1.27708356  1.59073447]. \t  -5.346777335478966 \t -0.027212720838110355\n",
      "55     \t [0.6416585  0.40567576]. \t  -0.13206872080515883 \t -0.027212720838110355\n",
      "56     \t [1.13383445 1.36552738]. \t  -0.6570612464667637 \t -0.027212720838110355\n",
      "57     \t [0.84753907 0.69907552]. \t  -0.060288823879941944 \t -0.027212720838110355\n",
      "58     \t [1.32080675 1.82027308]. \t  -0.6766113316116504 \t -0.027212720838110355\n",
      "59     \t [1.06733363 1.19228691]. \t  -0.28634432403568827 \t -0.027212720838110355\n",
      "60     \t [0.66135685 0.42751884]. \t  -0.12442884849354584 \t -0.027212720838110355\n",
      "61     \t [-0.06411902  0.06495482]. \t  -1.5025432699282713 \t -0.027212720838110355\n",
      "62     \t [0.61894886 0.40219658]. \t  -0.18167673334873674 \t -0.027212720838110355\n",
      "63     \t [0.866379   0.72318285]. \t  -0.0930935173886313 \t -0.027212720838110355\n",
      "64     \t [0.91388574 0.87655893]. \t  -0.17857812384788313 \t -0.027212720838110355\n",
      "65     \t [0.91787163 0.834915  ]. \t  \u001b[92m-0.01248060630920594\u001b[0m \t -0.01248060630920594\n",
      "66     \t [0.79659683 0.61462693]. \t  -0.0811315398219761 \t -0.01248060630920594\n",
      "67     \t [1.05181532 1.14569006]. \t  -0.15772071932923748 \t -0.01248060630920594\n",
      "68     \t [1.03168634 1.14801274]. \t  -0.7005027360110544 \t -0.01248060630920594\n",
      "69     \t [1.05006142 1.19362609]. \t  -0.8305534699619068 \t -0.01248060630920594\n",
      "70     \t [1.12341793 1.33396563]. \t  -0.5321611359151446 \t -0.01248060630920594\n",
      "71     \t [0.68049416 0.43810234]. \t  -0.16443386047906478 \t -0.01248060630920594\n",
      "72     \t [0.656551   0.43085038]. \t  -0.1179615740429513 \t -0.01248060630920594\n",
      "73     \t [1.37157785 1.90910253]. \t  -0.21578128400988184 \t -0.01248060630920594\n",
      "74     \t [0.78604834 0.58406832]. \t  -0.16004412518064265 \t -0.01248060630920594\n",
      "75     \t [1.03228909 1.04830382]. \t  -0.03103027942357156 \t -0.01248060630920594\n",
      "76     \t [0.67938713 0.45462808]. \t  -0.10760729643096352 \t -0.01248060630920594\n",
      "77     \t [1.03019749 1.06277831]. \t  \u001b[92m-0.0011284028602427027\u001b[0m \t -0.0011284028602427027\n",
      "78     \t [0.8640729  0.72294115]. \t  -0.07455431383301213 \t -0.0011284028602427027\n",
      "79     \t [0.95613718 0.97255166]. \t  -0.3424352823596317 \t -0.0011284028602427027\n",
      "80     \t [0.81744983 0.63383604]. \t  -0.15157935458386976 \t -0.0011284028602427027\n",
      "81     \t [0.59590602 0.32792707]. \t  -0.2371504419933906 \t -0.0011284028602427027\n",
      "82     \t [1.01320442 1.0250001 ]. \t  \u001b[92m-0.00042497435016257694\u001b[0m \t -0.00042497435016257694\n",
      "83     \t [1.14310411 1.35827587]. \t  -0.2866198776417168 \t -0.00042497435016257694\n",
      "84     \t [0.93293703 0.90391272]. \t  -0.11699879053022397 \t -0.00042497435016257694\n",
      "85     \t [1.42439345 2.04799974]. \t  -0.2166023766275147 \t -0.00042497435016257694\n",
      "86     \t [0.95414032 0.91224794]. \t  -0.002450628528569203 \t -0.00042497435016257694\n",
      "87     \t [0.94173384 0.90664859]. \t  -0.04254340689019411 \t -0.00042497435016257694\n",
      "88     \t [0.92719819 0.8732343 ]. \t  -0.023627374740972824 \t -0.00042497435016257694\n",
      "89     \t [1.02681759 1.07633057]. \t  -0.04901456554364348 \t -0.00042497435016257694\n",
      "90     \t [1.01082264 1.0467674 ]. \t  -0.06264205804972182 \t -0.00042497435016257694\n",
      "91     \t [1.12441941 1.3521237 ]. \t  -0.7864466543267405 \t -0.00042497435016257694\n",
      "92     \t [0.78803714 0.60522857]. \t  -0.06981003996827864 \t -0.00042497435016257694\n",
      "93     \t [0.94406383 0.87110763]. \t  -0.043726641734691235 \t -0.00042497435016257694\n",
      "94     \t [2.048      0.52199873]. \t  -1349.680904115345 \t -0.00042497435016257694\n",
      "95     \t [0.10771828 0.13694858]. \t  -2.367312257617443 \t -0.00042497435016257694\n",
      "96     \t [0.78324182 0.59158853]. \t  -0.09485415443619927 \t -0.00042497435016257694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.29348254 1.75517232]. \t  -0.7597664741253847 \t -0.00042497435016257694\n",
      "98     \t [0.72698521 0.50967692]. \t  -0.10999614247570602 \t -0.00042497435016257694\n",
      "99     \t [1.15938435 1.30920269]. \t  -0.14768913417819024 \t -0.00042497435016257694\n",
      "100    \t [1.385286   1.92222234]. \t  -0.14947253253587578 \t -0.00042497435016257694\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.69370909  1.36934462]. \t  -81.74299988633194 \t -45.00854175284071\n",
      "2      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -45.00854175284071\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -45.00854175284071\n",
      "4      \t [0.06733099 0.53703121]. \t  \u001b[92m-29.225256372675968\u001b[0m \t -29.225256372675968\n",
      "5      \t [0.52359139 2.048     ]. \t  -314.8820758501786 \t -29.225256372675968\n",
      "6      \t [-0.19955722 -0.65816033]. \t  -50.1570229368472 \t -29.225256372675968\n",
      "7      \t [-2.048       0.85806891]. \t  -1122.336764065748 \t -29.225256372675968\n",
      "8      \t [2.048      0.05986112]. \t  -1710.4600940423961 \t -29.225256372675968\n",
      "9      \t [2.048 2.048]. \t  -461.7603900415999 \t -29.225256372675968\n",
      "10     \t [-0.74605436  2.048     ]. \t  -225.4769638571157 \t -29.225256372675968\n",
      "11     \t [-0.31388385  0.03480739]. \t  \u001b[92m-2.13225961614023\u001b[0m \t -2.13225961614023\n",
      "12     \t [ 0.12299147 -2.048     ]. \t  -426.41840550734963 \t -2.13225961614023\n",
      "13     \t [ 0.38550659 -1.06019132]. \t  -146.49895470639464 \t -2.13225961614023\n",
      "14     \t [0.91836807 1.14107148]. \t  -8.867500497971639 \t -2.13225961614023\n",
      "15     \t [0.18683341 1.34754332]. \t  -172.96272312728885 \t -2.13225961614023\n",
      "16     \t [-0.78550375  0.60408526]. \t  -3.2047443988501954 \t -2.13225961614023\n",
      "17     \t [0.75101209 0.53325721]. \t  \u001b[92m-0.15662473706674898\u001b[0m \t -0.15662473706674898\n",
      "18     \t [ 0.20353838 -0.3086024 ]. \t  -12.886470075992785 \t -0.15662473706674898\n",
      "19     \t [1.31239261 2.048     ]. \t  -10.700795486423582 \t -0.15662473706674898\n",
      "20     \t [-1.35812288  2.048     ]. \t  -9.70206027058766 \t -0.15662473706674898\n",
      "21     \t [1.11158466 1.70831314]. \t  -22.356287673845287 \t -0.15662473706674898\n",
      "22     \t [-1.25082512  1.59112707]. \t  -5.136776105048792 \t -0.15662473706674898\n",
      "23     \t [0.58336504 0.76447231]. \t  -18.164546203775807 \t -0.15662473706674898\n",
      "24     \t [-0.25278867 -1.52769231]. \t  -254.88675841364557 \t -0.15662473706674898\n",
      "25     \t [-0.61753533  0.27410316]. \t  -3.7666063535538847 \t -0.15662473706674898\n",
      "26     \t [0.45609369 0.14744358]. \t  -0.6628019248155945 \t -0.15662473706674898\n",
      "27     \t [-1.07024479  1.16214836]. \t  -4.313884195631819 \t -0.15662473706674898\n",
      "28     \t [-1.02049814 -0.9872535 ]. \t  -415.6325950252469 \t -0.15662473706674898\n",
      "29     \t [-2.048      -1.37336174]. \t  -3109.1804879487 \t -0.15662473706674898\n",
      "30     \t [-0.58858813 -2.048     ]. \t  -575.8559817946393 \t -0.15662473706674898\n",
      "31     \t [ 0.9163856 -2.048    ]. \t  -833.9242554717141 \t -0.15662473706674898\n",
      "32     \t [ 2.048      -0.96556462]. \t  -2663.5227233619207 \t -0.15662473706674898\n",
      "33     \t [1.39541923 1.67199554]. \t  -7.72982201296176 \t -0.15662473706674898\n",
      "34     \t [-0.60083223 -0.29808592]. \t  -46.00200582822898 \t -0.15662473706674898\n",
      "35     \t [1.07991831 1.05742222]. \t  -1.1901600214861001 \t -0.15662473706674898\n",
      "36     \t [-1.16237824  1.70160759]. \t  -16.959813310868576 \t -0.15662473706674898\n",
      "37     \t [1.19852931 1.49658926]. \t  -0.40081642819725705 \t -0.15662473706674898\n",
      "38     \t [1.31609831 1.75297429]. \t  \u001b[92m-0.1434301037165151\u001b[0m \t -0.1434301037165151\n",
      "39     \t [0.91923553 0.89240896]. \t  -0.23134121085150647 \t -0.1434301037165151\n",
      "40     \t [ 1.14813561 -1.23986968]. \t  -654.4018604606681 \t -0.1434301037165151\n",
      "41     \t [0.07952237 0.02717481]. \t  -0.8907554891501229 \t -0.1434301037165151\n",
      "42     \t [1.44918575 2.048     ]. \t  -0.4736189789060036 \t -0.1434301037165151\n",
      "43     \t [-0.74385649  0.78101159]. \t  -8.22526880536689 \t -0.1434301037165151\n",
      "44     \t [-2.048       0.10901152]. \t  -1678.251768729204 \t -0.1434301037165151\n",
      "45     \t [1.34734666 1.8199788 ]. \t  \u001b[92m-0.12279874166864851\u001b[0m \t -0.12279874166864851\n",
      "46     \t [0.86703644 0.78655786]. \t  -0.1388227305032919 \t -0.12279874166864851\n",
      "47     \t [0.58221956 0.38095579]. \t  -0.35074043720413167 \t -0.12279874166864851\n",
      "48     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -0.12279874166864851\n",
      "49     \t [ 1.2997153  -0.18222392]. \t  -350.33498461480195 \t -0.12279874166864851\n",
      "50     \t [1.12113284 1.29172565]. \t  -0.13568538399773677 \t -0.12279874166864851\n",
      "51     \t [-0.06400861 -0.05682834]. \t  -1.5033053113638681 \t -0.12279874166864851\n",
      "52     \t [0.75493879 0.53827308]. \t  -0.16028737462459092 \t -0.12279874166864851\n",
      "53     \t [1.26877243 1.64499311]. \t  -0.1962103708070641 \t -0.12279874166864851\n",
      "54     \t [1.19791677 1.4182528 ]. \t  \u001b[92m-0.06723331163686302\u001b[0m \t -0.06723331163686302\n",
      "55     \t [1.31622497 1.75260917]. \t  -0.14064484453935927 \t -0.06723331163686302\n",
      "56     \t [1.14117055 1.32861483]. \t  -0.08933299142659375 \t -0.06723331163686302\n",
      "57     \t [1.28742753 1.66356628]. \t  -0.08633149269262731 \t -0.06723331163686302\n",
      "58     \t [0.66662532 0.4163954 ]. \t  -0.1895046073782882 \t -0.06723331163686302\n",
      "59     \t [ 0.53913438 -1.66898042]. \t  -384.2337613205364 \t -0.06723331163686302\n",
      "60     \t [-2.048      1.5852871]. \t  -689.9872235093956 \t -0.06723331163686302\n",
      "61     \t [-1.45919184  1.95150501]. \t  -9.206626874004817 \t -0.06723331163686302\n",
      "62     \t [0.95808929 0.92646657]. \t  \u001b[92m-0.00903512625473456\u001b[0m \t -0.00903512625473456\n",
      "63     \t [1.33986634 1.78785383]. \t  -0.1209673351188758 \t -0.00903512625473456\n",
      "64     \t [1.4314973  2.04799979]. \t  -0.18633028417705005 \t -0.00903512625473456\n",
      "65     \t [1.38530645 1.88614241]. \t  -0.2569098273430356 \t -0.00903512625473456\n",
      "66     \t [1.05390879 1.09653258]. \t  -0.023045075684409888 \t -0.00903512625473456\n",
      "67     \t [1.0330052  1.07222591]. \t  \u001b[92m-0.0037170952482864627\u001b[0m \t -0.0037170952482864627\n",
      "68     \t [0.62583088 0.4059586 ]. \t  -0.16043524949920074 \t -0.0037170952482864627\n",
      "69     \t [0.71076535 0.51937349]. \t  -0.1037812565803721 \t -0.0037170952482864627\n",
      "70     \t [0.69071806 0.50139921]. \t  -0.15474205665998247 \t -0.0037170952482864627\n",
      "71     \t [0.94489974 0.88027943]. \t  -0.018801560659508023 \t -0.0037170952482864627\n",
      "72     \t [1.23528053 1.53957237]. \t  -0.07400113586258704 \t -0.0037170952482864627\n",
      "73     \t [1.34024364 1.80186912]. \t  -0.1189197901705071 \t -0.0037170952482864627\n",
      "74     \t [1.36251629 1.86609399]. \t  -0.14071748013848978 \t -0.0037170952482864627\n",
      "75     \t [1.13027469 1.28507003]. \t  -0.022670470621979785 \t -0.0037170952482864627\n",
      "76     \t [0.28003287 0.15117997]. \t  -1.0477771573417507 \t -0.0037170952482864627\n",
      "77     \t [1.22142344 1.49529455]. \t  -0.05019751913222554 \t -0.0037170952482864627\n",
      "78     \t [1.07149297 1.18618939]. \t  -0.1502128522916678 \t -0.0037170952482864627\n",
      "79     \t [0.6930968  0.49409132]. \t  -0.11298092687323709 \t -0.0037170952482864627\n",
      "80     \t [0.65556053 0.42531222]. \t  -0.12061647000850834 \t -0.0037170952482864627\n",
      "81     \t [1.06478171 1.14294812]. \t  -0.01263863859183257 \t -0.0037170952482864627\n",
      "82     \t [0.67016075 0.49069022]. \t  -0.28164029146963626 \t -0.0037170952482864627\n",
      "83     \t [0.91294711 0.83617066]. \t  -0.008306252969051365 \t -0.0037170952482864627\n",
      "84     \t [1.06775113 1.18569521]. \t  -0.21255126437932262 \t -0.0037170952482864627\n",
      "85     \t [0.80539559 0.65404579]. \t  -0.04076933419044103 \t -0.0037170952482864627\n",
      "86     \t [0.82719028 0.68252921]. \t  -0.030157166887452944 \t -0.0037170952482864627\n",
      "87     \t [1.11149931 1.27603732]. \t  -0.1773217057351339 \t -0.0037170952482864627\n",
      "88     \t [0.78089749 0.6327485 ]. \t  -0.10066514971814303 \t -0.0037170952482864627\n",
      "89     \t [0.82727438 0.70591745]. \t  -0.07620786314515911 \t -0.0037170952482864627\n",
      "90     \t [1.12315582 1.2629814 ]. \t  -0.01539307634745033 \t -0.0037170952482864627\n",
      "91     \t [1.17842662 1.40209684]. \t  -0.049812289123935635 \t -0.0037170952482864627\n",
      "92     \t [1.20792693 1.5186274 ]. \t  -0.39773413724297085 \t -0.0037170952482864627\n",
      "93     \t [0.88135996 0.7890169 ]. \t  -0.029012036950891096 \t -0.0037170952482864627\n",
      "94     \t [1.02685032 1.08965121]. \t  -0.12483360360990317 \t -0.0037170952482864627\n",
      "95     \t [0.60599402 0.38259866]. \t  -0.178864104120597 \t -0.0037170952482864627\n",
      "96     \t [1.40379981 2.048     ]. \t  -0.7612961090021982 \t -0.0037170952482864627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.76692274 0.59747828]. \t  -0.06298851137653683 \t -0.0037170952482864627\n",
      "98     \t [1.35506155 1.84720976]. \t  -0.13820822082749118 \t -0.0037170952482864627\n",
      "99     \t [1.2107871  1.46701096]. \t  -0.04453231719838447 \t -0.0037170952482864627\n",
      "100    \t [1.42560611 2.048     ]. \t  -0.20562413909006877 \t -0.0037170952482864627\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-2.048  2.048]. \t  -469.9523900415999 \t -19.52113145175031\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -19.52113145175031\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -19.52113145175031\n",
      "4      \t [ 0.08523732 -0.47314183]. \t  -23.91590136845557 \t -19.52113145175031\n",
      "5      \t [1.40059071 0.19849191]. \t  -311.0346467783002 \t -19.52113145175031\n",
      "6      \t [0.44331993 2.048     ]. \t  -343.1030616229277 \t -19.52113145175031\n",
      "7      \t [ 0.05346915 -2.048     ]. \t  -421.49816413986076 \t -19.52113145175031\n",
      "8      \t [-2.048      0.7752506]. \t  -1178.282917865764 \t -19.52113145175031\n",
      "9      \t [-0.75591477  2.048     ]. \t  -221.11588426740943 \t -19.52113145175031\n",
      "10     \t [ 2.048      -0.06677989]. \t  -1816.7818932458404 \t -19.52113145175031\n",
      "11     \t [0.63723851 0.08371351]. \t  \u001b[92m-10.523154838523093\u001b[0m \t -10.523154838523093\n",
      "12     \t [1.01599786 1.09360534]. \t  \u001b[92m-0.3766833994106193\u001b[0m \t -0.3766833994106193\n",
      "13     \t [ 0.38919805 -1.09499393]. \t  -155.74158859794065 \t -0.3766833994106193\n",
      "14     \t [-0.63809664  0.22790823]. \t  -5.896742758101327 \t -0.3766833994106193\n",
      "15     \t [-1.01296574 -0.57623873]. \t  -260.8008394637738 \t -0.3766833994106193\n",
      "16     \t [1.47758031 2.048     ]. \t  -2.057165691100198 \t -0.3766833994106193\n",
      "17     \t [ 0.78546131 -0.40537166]. \t  -104.56007486571879 \t -0.3766833994106193\n",
      "18     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.3766833994106193\n",
      "19     \t [-0.38060746 -1.21292066]. \t  -186.26346284219886 \t -0.3766833994106193\n",
      "20     \t [-2.048      -0.34819663]. \t  -2072.7214978604884 \t -0.3766833994106193\n",
      "21     \t [-0.48475626 -0.49601529]. \t  -55.64117447913431 \t -0.3766833994106193\n",
      "22     \t [1.15167443 0.75972738]. \t  -32.12957650409091 \t -0.3766833994106193\n",
      "23     \t [-1.38245294  2.048     ]. \t  -7.5481588849088945 \t -0.3766833994106193\n",
      "24     \t [1.18679389 1.78826743]. \t  -14.458760888274051 \t -0.3766833994106193\n",
      "25     \t [0.1730896  0.56436303]. \t  -29.242439049383155 \t -0.3766833994106193\n",
      "26     \t [-0.71585196 -2.048     ]. \t  -658.5315123649108 \t -0.3766833994106193\n",
      "27     \t [-1.33893378  1.63674391]. \t  -7.90420354354639 \t -0.3766833994106193\n",
      "28     \t [ 0.87516794 -2.048     ]. \t  -791.8295527526648 \t -0.3766833994106193\n",
      "29     \t [1.22626017 2.048     ]. \t  -29.675916962447246 \t -0.3766833994106193\n",
      "30     \t [-0.95971914  0.79941006]. \t  -5.320389816699562 \t -0.3766833994106193\n",
      "31     \t [0.02394788 0.00290754]. \t  -0.9532225113799934 \t -0.3766833994106193\n",
      "32     \t [0.78821614 0.69121701]. \t  -0.5339055347086273 \t -0.3766833994106193\n",
      "33     \t [-1.22363171  1.78182782]. \t  -13.04159441598388 \t -0.3766833994106193\n",
      "34     \t [1.34111755 1.65041634]. \t  -2.3120910233960243 \t -0.3766833994106193\n",
      "35     \t [-1.16553275  1.27985246]. \t  -5.307550138062126 \t -0.3766833994106193\n",
      "36     \t [0.29135302 0.01786293]. \t  -0.9513975906053977 \t -0.3766833994106193\n",
      "37     \t [-1.16731584 -1.45507752]. \t  -798.6427250389893 \t -0.3766833994106193\n",
      "38     \t [ 2.048      -0.99713938]. \t  -2696.206738569432 \t -0.3766833994106193\n",
      "39     \t [1.19741058 1.43460485]. \t  \u001b[92m-0.03903699314808363\u001b[0m \t -0.03903699314808363\n",
      "40     \t [-2.048      -1.21262621]. \t  -2932.7797294632037 \t -0.03903699314808363\n",
      "41     \t [0.83086591 0.65735959]. \t  -0.1373650086144254 \t -0.03903699314808363\n",
      "42     \t [1.35776704 1.84245838]. \t  -0.1281123778514543 \t -0.03903699314808363\n",
      "43     \t [ 0.080644   -0.10809271]. \t  -2.1584435780149933 \t -0.03903699314808363\n",
      "44     \t [1.42049242 2.048     ]. \t  -0.2680256954498447 \t -0.03903699314808363\n",
      "45     \t [0.67806201 0.42752592]. \t  -0.2075998607664994 \t -0.03903699314808363\n",
      "46     \t [1.18417227 1.4384929 ]. \t  -0.16517304706396385 \t -0.03903699314808363\n",
      "47     \t [-2.048       1.52795405]. \t  -720.2325085509425 \t -0.03903699314808363\n",
      "48     \t [-0.79425317  0.4832571 ]. \t  -5.397359429082815 \t -0.03903699314808363\n",
      "49     \t [2.048     0.7551794]. \t  -1183.8561078068315 \t -0.03903699314808363\n",
      "50     \t [1.38144549 1.92490153]. \t  -0.17275831237636943 \t -0.03903699314808363\n",
      "51     \t [0.85234672 0.72202859]. \t  \u001b[92m-0.023796306809060305\u001b[0m \t -0.023796306809060305\n",
      "52     \t [0.80260645 0.55229808]. \t  -0.8831399252692222 \t -0.023796306809060305\n",
      "53     \t [1.36319283 1.89761395]. \t  -0.2865095406263952 \t -0.023796306809060305\n",
      "54     \t [1.25908139 1.59805564]. \t  -0.08342968070317933 \t -0.023796306809060305\n",
      "55     \t [1.26903651 1.63024124]. \t  -0.11153542572555172 \t -0.023796306809060305\n",
      "56     \t [1.09544927 1.24792138]. \t  -0.23866922434840943 \t -0.023796306809060305\n",
      "57     \t [1.40815505 1.98570036]. \t  -0.1673743851750949 \t -0.023796306809060305\n",
      "58     \t [0.60780306 0.38928455]. \t  -0.19326033709503196 \t -0.023796306809060305\n",
      "59     \t [1.21189236 1.47548762]. \t  -0.049528519906897855 \t -0.023796306809060305\n",
      "60     \t [1.14086348 1.34001671]. \t  -0.16766152891231445 \t -0.023796306809060305\n",
      "61     \t [1.17391843 1.40292666]. \t  -0.09196100876175262 \t -0.023796306809060305\n",
      "62     \t [1.24975045 1.57285189]. \t  -0.07442186733278632 \t -0.023796306809060305\n",
      "63     \t [1.17831613 1.42011017]. \t  -0.132166934598134 \t -0.023796306809060305\n",
      "64     \t [0.92449018 0.89220985]. \t  -0.1465350650934671 \t -0.023796306809060305\n",
      "65     \t [0.80363999 0.59430235]. \t  -0.3041417050244662 \t -0.023796306809060305\n",
      "66     \t [-1.5148404  2.048    ]. \t  -12.412556150917007 \t -0.023796306809060305\n",
      "67     \t [0.20355209 0.08308669]. \t  -0.8078284621768703 \t -0.023796306809060305\n",
      "68     \t [1.39840854 2.00425904]. \t  -0.3960210455107696 \t -0.023796306809060305\n",
      "69     \t [0.79604882 0.60945163]. \t  -0.10036398686135577 \t -0.023796306809060305\n",
      "70     \t [1.29567808 1.67552607]. \t  -0.08848542406583756 \t -0.023796306809060305\n",
      "71     \t [1.21152416 1.50595109]. \t  -0.19036320741260837 \t -0.023796306809060305\n",
      "72     \t [0.79144079 0.60709712]. \t  -0.08067419709631068 \t -0.023796306809060305\n",
      "73     \t [0.21813025 0.01304911]. \t  -0.7305641343540847 \t -0.023796306809060305\n",
      "74     \t [1.29200691 1.66624963]. \t  -0.0861874706700897 \t -0.023796306809060305\n",
      "75     \t [1.11232047 1.2520306 ]. \t  -0.034442341678544895 \t -0.023796306809060305\n",
      "76     \t [1.40643885 2.02843117]. \t  -0.4188149171507366 \t -0.023796306809060305\n",
      "77     \t [0.84541996 0.68829059]. \t  -0.09382517539428598 \t -0.023796306809060305\n",
      "78     \t [0.62204547 0.38429157]. \t  -0.1435513429158885 \t -0.023796306809060305\n",
      "79     \t [0.76137231 0.53469481]. \t  -0.25938003401043375 \t -0.023796306809060305\n",
      "80     \t [0.41997053 0.21233286]. \t  -0.4657291739251196 \t -0.023796306809060305\n",
      "81     \t [1.26681055 1.61397675]. \t  -0.0795926864388149 \t -0.023796306809060305\n",
      "82     \t [0.64516364 0.42974475]. \t  -0.1441571300304464 \t -0.023796306809060305\n",
      "83     \t [0.73172183 0.5141069 ]. \t  -0.11738453814473468 \t -0.023796306809060305\n",
      "84     \t [0.72320845 0.49100589]. \t  -0.1791709061999888 \t -0.023796306809060305\n",
      "85     \t [1.19495444 1.4252173 ]. \t  -0.038735596786829536 \t -0.023796306809060305\n",
      "86     \t [0.63822119 0.43651843]. \t  -0.2161020396128786 \t -0.023796306809060305\n",
      "87     \t [1.04664678 1.11842284]. \t  -0.05486152687766916 \t -0.023796306809060305\n",
      "88     \t [0.71096225 0.46575342]. \t  -0.24126229316063155 \t -0.023796306809060305\n",
      "89     \t [0.23911882 0.0347125 ]. \t  -0.6294091629416673 \t -0.023796306809060305\n",
      "90     \t [1.02914726 1.06957439]. \t  \u001b[92m-0.011728676968830749\u001b[0m \t -0.011728676968830749\n",
      "91     \t [0.86406711 0.76141641]. \t  -0.04039490489223939 \t -0.011728676968830749\n",
      "92     \t [1.4247635 2.048    ]. \t  -0.213000587162543 \t -0.011728676968830749\n",
      "93     \t [1.13767551 1.32818343]. \t  -0.13372552832958123 \t -0.011728676968830749\n",
      "94     \t [1.14824718 1.33658928]. \t  -0.054802331800073864 \t -0.011728676968830749\n",
      "95     \t [1.38918894 1.95261974]. \t  -0.2033328123713055 \t -0.011728676968830749\n",
      "96     \t [0.22566356 0.08011163]. \t  -0.6847884232761061 \t -0.011728676968830749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.15326846 1.3571479 ]. \t  -0.09703933835179178 \t -0.011728676968830749\n",
      "98     \t [1.28873249 1.66118043]. \t  -0.08337863127791323 \t -0.011728676968830749\n",
      "99     \t [ 0.1666491  -1.64636867]. \t  -280.9691458226784 \t -0.011728676968830749\n",
      "100    \t [0.51272624 0.27490294]. \t  -0.2518711088872295 \t -0.011728676968830749\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.21315746 1.00825346]. \t  -93.32084653105952 \t -62.0309701572776\n",
      "2      \t [1.88743872 2.048     ]. \t  -230.1358273308555 \t -62.0309701572776\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -62.0309701572776\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -62.0309701572776\n",
      "5      \t [-0.29029598 -2.048     ]. \t  -456.3231476056351 \t -62.0309701572776\n",
      "6      \t [-2.048       0.49994627]. \t  -1374.1182047121965 \t -62.0309701572776\n",
      "7      \t [0.38632543 2.048     ]. \t  -360.9027648712583 \t -62.0309701572776\n",
      "8      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -62.0309701572776\n",
      "9      \t [-0.2840875  -0.73560675]. \t  -68.28548272327981 \t -62.0309701572776\n",
      "10     \t [-0.8397123  2.048    ]. \t  -183.7180844313476 \t -62.0309701572776\n",
      "11     \t [2.048      1.13457052]. \t  -937.2952033489494 \t -62.0309701572776\n",
      "12     \t [-0.80214677  0.1839042 ]. \t  \u001b[92m-24.36499675923683\u001b[0m \t -24.36499675923683\n",
      "13     \t [-0.89356806  1.16668605]. \t  \u001b[92m-17.144356511947187\u001b[0m \t -17.144356511947187\n",
      "14     \t [1.10206596 1.52427676]. \t  \u001b[92m-9.603522203716366\u001b[0m \t -9.603522203716366\n",
      "15     \t [ 0.38943796 -2.048     ]. \t  -484.22404528394645 \t -9.603522203716366\n",
      "16     \t [1.23994534 2.048     ]. \t  -26.122229637797624 \t -9.603522203716366\n",
      "17     \t [0.76296296 0.65254132]. \t  \u001b[92m-0.5522088094081294\u001b[0m \t -0.5522088094081294\n",
      "18     \t [ 0.07163732 -1.30149633]. \t  -171.58959077926502 \t -0.5522088094081294\n",
      "19     \t [-0.37724809  0.51528263]. \t  -15.807214362773447 \t -0.5522088094081294\n",
      "20     \t [-0.54378713  1.2820921 ]. \t  -99.67933872075422 \t -0.5522088094081294\n",
      "21     \t [-1.31671017  1.7486446 ]. \t  -5.389403447807329 \t -0.5522088094081294\n",
      "22     \t [-1.3741997  2.048    ]. \t  -8.183247925628926 \t -0.5522088094081294\n",
      "23     \t [0.92730609 1.05654666]. \t  -3.8724098421494215 \t -0.5522088094081294\n",
      "24     \t [-0.76066923 -0.40909263]. \t  -100.65712054321756 \t -0.5522088094081294\n",
      "25     \t [ 0.089278   -0.26988905]. \t  -8.550010684408935 \t -0.5522088094081294\n",
      "26     \t [-2.048      -0.54402783]. \t  -2254.469153061878 \t -0.5522088094081294\n",
      "27     \t [0.80902625 1.55087478]. \t  -80.38103888610752 \t -0.5522088094081294\n",
      "28     \t [ 0.28717204 -0.69150587]. \t  -60.411645270475276 \t -0.5522088094081294\n",
      "29     \t [-0.70197022 -1.37624368]. \t  -352.2150009275687 \t -0.5522088094081294\n",
      "30     \t [-0.91854305  0.72479165]. \t  -5.095234216752889 \t -0.5522088094081294\n",
      "31     \t [-0.40108787  0.03562981]. \t  -3.5315946928375013 \t -0.5522088094081294\n",
      "32     \t [0.28686828 0.15474438]. \t  -1.0334712352676663 \t -0.5522088094081294\n",
      "33     \t [-2.048       1.37638956]. \t  -803.3544843646616 \t -0.5522088094081294\n",
      "34     \t [ 2.048     -1.0515345]. \t  -2752.980461941393 \t -0.5522088094081294\n",
      "35     \t [1.46985079 2.048     ]. \t  -1.4855153153537306 \t -0.5522088094081294\n",
      "36     \t [-0.69554625  0.48555545]. \t  -2.875190693087531 \t -0.5522088094081294\n",
      "37     \t [1.36049877 1.8684362 ]. \t  \u001b[92m-0.16051194491332155\u001b[0m \t -0.16051194491332155\n",
      "38     \t [-0.05528802  0.06090389]. \t  -1.4482618105785814 \t -0.16051194491332155\n",
      "39     \t [-1.200151    1.32823164]. \t  -6.097995753486861 \t -0.16051194491332155\n",
      "40     \t [0.72696467 0.31543749]. \t  -4.61315821228945 \t -0.16051194491332155\n",
      "41     \t [-1.10322214 -2.048     ]. \t  -1070.5107572534775 \t -0.16051194491332155\n",
      "42     \t [ 1.13463932 -2.048     ]. \t  -1112.511698452529 \t -0.16051194491332155\n",
      "43     \t [0.97531398 0.88305254]. \t  -0.46552628460052675 \t -0.16051194491332155\n",
      "44     \t [-1.1646415   1.62913572]. \t  -12.124705311536488 \t -0.16051194491332155\n",
      "45     \t [1.35531473 1.9377503 ]. \t  -1.1437699611772438 \t -0.16051194491332155\n",
      "46     \t [2.048      0.28893997]. \t  -1526.2851266188425 \t -0.16051194491332155\n",
      "47     \t [1.18641052 1.30382005]. \t  -1.1111526654247579 \t -0.16051194491332155\n",
      "48     \t [1.06803407 1.13729721]. \t  \u001b[92m-0.0057843383187265885\u001b[0m \t -0.0057843383187265885\n",
      "49     \t [1.08962214 1.20648047]. \t  -0.04491174500083923 \t -0.0057843383187265885\n",
      "50     \t [0.92529484 0.84384716]. \t  -0.020767429422872657 \t -0.0057843383187265885\n",
      "51     \t [1.41306028 2.04799979]. \t  -0.43338198693749624 \t -0.0057843383187265885\n",
      "52     \t [1.28928825 1.66310763]. \t  -0.08375883043418042 \t -0.0057843383187265885\n",
      "53     \t [0.95983124 0.89016309]. \t  -0.09841490237777949 \t -0.0057843383187265885\n",
      "54     \t [1.07408063 1.15946756]. \t  -0.008873261197951034 \t -0.0057843383187265885\n",
      "55     \t [0.88589945 0.76589331]. \t  -0.04883274558770732 \t -0.0057843383187265885\n",
      "56     \t [1.25086018 1.56580651]. \t  -0.06306430703424289 \t -0.0057843383187265885\n",
      "57     \t [1.32121693 1.76280609]. \t  -0.13273649115030675 \t -0.0057843383187265885\n",
      "58     \t [1.17788774 1.42878739]. \t  -0.20277409670845678 \t -0.0057843383187265885\n",
      "59     \t [0.9118812  0.82178104]. \t  -0.0172639202404706 \t -0.0057843383187265885\n",
      "60     \t [1.026676   1.06362017]. \t  -0.009844385038550807 \t -0.0057843383187265885\n",
      "61     \t [1.10034989 1.20964999]. \t  -0.01019551311059706 \t -0.0057843383187265885\n",
      "62     \t [1.18212574 1.40233105]. \t  -0.03558037783278659 \t -0.0057843383187265885\n",
      "63     \t [0.96238417 0.93918237]. \t  -0.018312564324216435 \t -0.0057843383187265885\n",
      "64     \t [1.18600312 1.37615164]. \t  -0.1273281731305396 \t -0.0057843383187265885\n",
      "65     \t [1.00574189 0.9945065 ]. \t  -0.02896781394242152 \t -0.0057843383187265885\n",
      "66     \t [1.08206108 1.14815336]. \t  -0.05827583501747021 \t -0.0057843383187265885\n",
      "67     \t [1.43443295 2.04780343]. \t  -0.19832514390677705 \t -0.0057843383187265885\n",
      "68     \t [1.09951244 1.21299817]. \t  -0.01155967999123647 \t -0.0057843383187265885\n",
      "69     \t [1.03384946 1.06219294]. \t  \u001b[92m-0.005570400246611965\u001b[0m \t -0.005570400246611965\n",
      "70     \t [1.03555957 1.07899061]. \t  -0.005629698347100377 \t -0.005570400246611965\n",
      "71     \t [1.33949495 1.80561602]. \t  -0.12818290010349095 \t -0.005570400246611965\n",
      "72     \t [1.00051147 0.97984243]. \t  -0.04486275412078571 \t -0.005570400246611965\n",
      "73     \t [1.15726671 1.34428853]. \t  -0.027255153798585598 \t -0.005570400246611965\n",
      "74     \t [1.09269588 1.20467343]. \t  -0.020018304192889858 \t -0.005570400246611965\n",
      "75     \t [1.24503113 1.58996961]. \t  -0.21897874870438575 \t -0.005570400246611965\n",
      "76     \t [1.17841565 1.38329805]. \t  -0.03471088248102084 \t -0.005570400246611965\n",
      "77     \t [0.95945875 0.92152028]. \t  \u001b[92m-0.0017355959018070994\u001b[0m \t -0.0017355959018070994\n",
      "78     \t [0.80294487 0.59617602]. \t  -0.2744869937572622 \t -0.0017355959018070994\n",
      "79     \t [0.96568528 0.9156064 ]. \t  -0.029879518392464367 \t -0.0017355959018070994\n",
      "80     \t [1.23040638 1.53301244]. \t  -0.0896161450821609 \t -0.0017355959018070994\n",
      "81     \t [0.86744104 0.73139032]. \t  -0.06193954717732741 \t -0.0017355959018070994\n",
      "82     \t [1.099743   1.21029986]. \t  -0.010023522509316448 \t -0.0017355959018070994\n",
      "83     \t [0.8954471  0.74990752]. \t  -0.2804791408313572 \t -0.0017355959018070994\n",
      "84     \t [1.38292169 1.94735646]. \t  -0.268318740743312 \t -0.0017355959018070994\n",
      "85     \t [1.0300414  1.08205594]. \t  -0.045299700183425745 \t -0.0017355959018070994\n",
      "86     \t [-0.00400937  0.00344865]. \t  -1.0092130761010514 \t -0.0017355959018070994\n",
      "87     \t [1.07284175 1.12495018]. \t  -0.0731101932624818 \t -0.0017355959018070994\n",
      "88     \t [1.05765091 1.08124045]. \t  -0.143087363316123 \t -0.0017355959018070994\n",
      "89     \t [0.96898594 0.92493294]. \t  -0.020564140164153047 \t -0.0017355959018070994\n",
      "90     \t [0.90442299 0.82706784]. \t  -0.017392143187383573 \t -0.0017355959018070994\n",
      "91     \t [0.95249747 0.91276658]. \t  -0.005298176060497539 \t -0.0017355959018070994\n",
      "92     \t [1.06460299 1.12424108]. \t  -0.01252467654191422 \t -0.0017355959018070994\n",
      "93     \t [1.25003523 1.59211689]. \t  -0.14971271208782072 \t -0.0017355959018070994\n",
      "94     \t [1.0005806  0.99891143]. \t  \u001b[92m-0.0005066365316785102\u001b[0m \t -0.0005066365316785102\n",
      "95     \t [1.11511024 1.24311577]. \t  -0.013262973719465365 \t -0.0005066365316785102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.08834396 1.18717739]. \t  -0.008525475006365982 \t -0.0005066365316785102\n",
      "97     \t [1.01299041 0.99753468]. \t  -0.08204993722640865 \t -0.0005066365316785102\n",
      "98     \t [0.90912651 0.77472447]. \t  -0.2764426892900103 \t -0.0005066365316785102\n",
      "99     \t [1.02752374 1.04326831]. \t  -0.016474511888485928 \t -0.0005066365316785102\n",
      "100    \t [1.41247491 2.01900794]. \t  -0.22736446030052537 \t -0.0005066365316785102\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [2.048    0.565773]. \t  -1317.7220223338327 \t -4.306489127802793\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -4.306489127802793\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -4.306489127802793\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -4.306489127802793\n",
      "5      \t [1.63517772 2.048     ]. \t  -39.566786693352206 \t -4.306489127802793\n",
      "6      \t [-0.07181142 -1.16097849]. \t  -137.13594882281868 \t -4.306489127802793\n",
      "7      \t [-2.048      0.6922705]. \t  -1235.7141697878608 \t -4.306489127802793\n",
      "8      \t [-0.39898277  0.42234308]. \t  -8.88225177988669 \t -4.306489127802793\n",
      "9      \t [ 0.1989311 -2.048    ]. \t  -436.43805755979236 \t -4.306489127802793\n",
      "10     \t [0.89340167 2.048     ]. \t  -156.21973186049007 \t -4.306489127802793\n",
      "11     \t [-1.00001956  2.048     ]. \t  -113.82228072011867 \t -4.306489127802793\n",
      "12     \t [-0.17075744 -0.29165284]. \t  -11.662638888594342 \t -4.306489127802793\n",
      "13     \t [0.25562649 0.61853339]. \t  -31.155841939748786 \t -4.306489127802793\n",
      "14     \t [ 0.3952177  -0.75917482]. \t  -84.15632377742907 \t -4.306489127802793\n",
      "15     \t [2.048 2.048]. \t  -461.7603900415999 \t -4.306489127802793\n",
      "16     \t [-0.92932532  1.33615694]. \t  -26.048998271599302 \t -4.306489127802793\n",
      "17     \t [ 1.48966153 -0.42091241]. \t  -697.2018238170776 \t -4.306489127802793\n",
      "18     \t [1.27308004 1.60157448]. \t  \u001b[92m-0.11127676243754442\u001b[0m \t -0.11127676243754442\n",
      "19     \t [-0.69481958 -0.46981732]. \t  -93.61548360968239 \t -0.11127676243754442\n",
      "20     \t [-0.57100194 -2.048     ]. \t  -566.0761650397893 \t -0.11127676243754442\n",
      "21     \t [-2.048    -0.583374]. \t  -2291.91100861333 \t -0.11127676243754442\n",
      "22     \t [-0.81364809  0.1549215 ]. \t  -29.004533566750062 \t -0.11127676243754442\n",
      "23     \t [-0.61707651  1.47877373]. \t  -123.17320828410755 \t -0.11127676243754442\n",
      "24     \t [-1.35571155  1.72741846]. \t  -6.771182982953851 \t -0.11127676243754442\n",
      "25     \t [0.91182436 1.43074472]. \t  -35.92634812784421 \t -0.11127676243754442\n",
      "26     \t [-1.4440345  2.048    ]. \t  -6.111953926466317 \t -0.11127676243754442\n",
      "27     \t [1.38945779 2.048     ]. \t  -1.5301191352483996 \t -0.11127676243754442\n",
      "28     \t [0.10811789 0.10648978]. \t  -1.694163358523208 \t -0.11127676243754442\n",
      "29     \t [-0.42691795 -0.0046783 ]. \t  -5.53064778408685 \t -0.11127676243754442\n",
      "30     \t [1.15551298 1.21696996]. \t  -1.4222610256606791 \t -0.11127676243754442\n",
      "31     \t [ 2.048      -0.97864382]. \t  -2677.037214359236 \t -0.11127676243754442\n",
      "32     \t [1.02487363 0.48898339]. \t  -31.5156565077181 \t -0.11127676243754442\n",
      "33     \t [ 0.9132722 -2.048    ]. \t  -830.6380329891758 \t -0.11127676243754442\n",
      "34     \t [-1.26297552  1.83056353]. \t  -10.665028135230294 \t -0.11127676243754442\n",
      "35     \t [-0.97639135 -1.42835292]. \t  -571.1522678319125 \t -0.11127676243754442\n",
      "36     \t [-1.17529393  0.81910362]. \t  -36.340160480495946 \t -0.11127676243754442\n",
      "37     \t [ 0.44711779 -1.53593603]. \t  -301.6233228437397 \t -0.11127676243754442\n",
      "38     \t [1.07713568 1.24300813]. \t  -0.691316482633068 \t -0.11127676243754442\n",
      "39     \t [-0.13892307  0.03861561]. \t  -1.3344565170949987 \t -0.11127676243754442\n",
      "40     \t [0.62801272 0.46576277]. \t  -0.6476393742838373 \t -0.11127676243754442\n",
      "41     \t [-0.85850869  0.71401493]. \t  -3.5070569072521707 \t -0.11127676243754442\n",
      "42     \t [0.904313   0.78868402]. \t  \u001b[92m-0.09382527727890301\u001b[0m \t -0.09382527727890301\n",
      "43     \t [2.048      1.41797185]. \t  -771.900327429127 \t -0.09382527727890301\n",
      "44     \t [-2.048      1.4230505]. \t  -777.2748976237838 \t -0.09382527727890301\n",
      "45     \t [1.38233846 1.90080668]. \t  -0.15628884276562297 \t -0.09382527727890301\n",
      "46     \t [-1.23952492 -2.048     ]. \t  -1289.8235984105715 \t -0.09382527727890301\n",
      "47     \t [0.76877424 0.5448371 ]. \t  -0.2666944310204857 \t -0.09382527727890301\n",
      "48     \t [1.2503025  1.53964767]. \t  -0.11838827879667353 \t -0.09382527727890301\n",
      "49     \t [1.05183606 1.15078821]. \t  -0.20008162405612198 \t -0.09382527727890301\n",
      "50     \t [0.80184768 0.6375144 ]. \t  \u001b[92m-0.04222948176962696\u001b[0m \t -0.04222948176962696\n",
      "51     \t [0.80112286 0.6196632 ]. \t  -0.08854636946890082 \t -0.04222948176962696\n",
      "52     \t [1.10732066 1.2409399 ]. \t  \u001b[92m-0.033365110395308144\u001b[0m \t -0.033365110395308144\n",
      "53     \t [1.41396424 1.97461412]. \t  -0.23228040932314112 \t -0.033365110395308144\n",
      "54     \t [1.43752006 2.04799848]. \t  -0.22552103770364337 \t -0.033365110395308144\n",
      "55     \t [1.19272477 1.42495248]. \t  -0.03769984449128316 \t -0.033365110395308144\n",
      "56     \t [0.72361877 0.51935685]. \t  -0.07820755117446297 \t -0.033365110395308144\n",
      "57     \t [1.10401922 1.21394298]. \t  \u001b[92m-0.013236181896704897\u001b[0m \t -0.013236181896704897\n",
      "58     \t [0.67765326 0.42809723]. \t  -0.20073244650057712 \t -0.013236181896704897\n",
      "59     \t [1.17600287 1.40783447]. \t  -0.09273781941692005 \t -0.013236181896704897\n",
      "60     \t [0.7793897  0.56113433]. \t  -0.2631673766928083 \t -0.013236181896704897\n",
      "61     \t [1.44052584 2.04080339]. \t  -0.3117896819593975 \t -0.013236181896704897\n",
      "62     \t [0.97704411 0.96587373]. \t  \u001b[92m-0.013202422076619404\u001b[0m \t -0.013202422076619404\n",
      "63     \t [1.40898435 1.97859139]. \t  -0.17168448179724258 \t -0.013202422076619404\n",
      "64     \t [-1.60511206 -1.15869865]. \t  -1401.871383833137 \t -0.013202422076619404\n",
      "65     \t [-1.15416973  1.27587155]. \t  -4.956698474241467 \t -0.013202422076619404\n",
      "66     \t [-0.2168147   0.04206525]. \t  -1.483081698077286 \t -0.013202422076619404\n",
      "67     \t [0.77399884 0.56912651]. \t  -0.14076295783156192 \t -0.013202422076619404\n",
      "68     \t [1.03996266 1.14191288]. \t  -0.36629887097388586 \t -0.013202422076619404\n",
      "69     \t [0.78843792 0.62473414]. \t  -0.045719380333716204 \t -0.013202422076619404\n",
      "70     \t [1.0586854  1.17070369]. \t  -0.2523343654334973 \t -0.013202422076619404\n",
      "71     \t [1.40110357 1.98725089]. \t  -0.2192530649190036 \t -0.013202422076619404\n",
      "72     \t [1.43543342 2.04799987]. \t  -0.2051504469623611 \t -0.013202422076619404\n",
      "73     \t [1.28830482 1.68699392]. \t  -0.15745550837657923 \t -0.013202422076619404\n",
      "74     \t [1.05253757 1.0987825 ]. \t  \u001b[92m-0.01095559731997343\u001b[0m \t -0.01095559731997343\n",
      "75     \t [1.07928752 1.18056025]. \t  -0.030931443039341013 \t -0.01095559731997343\n",
      "76     \t [1.35889945 1.76282049]. \t  -0.8308386083380694 \t -0.01095559731997343\n",
      "77     \t [1.07188306 1.21191112]. \t  -0.4017878211395285 \t -0.01095559731997343\n",
      "78     \t [0.77240592 0.59831718]. \t  -0.052090206417226136 \t -0.01095559731997343\n",
      "79     \t [0.6953077  0.42563257]. \t  -0.42715531068065765 \t -0.01095559731997343\n",
      "80     \t [0.96206622 0.93309932]. \t  \u001b[92m-0.0071059237416912675\u001b[0m \t -0.0071059237416912675\n",
      "81     \t [1.01153555 1.03767918]. \t  -0.021085693467764278 \t -0.0071059237416912675\n",
      "82     \t [1.16653678 1.39737878]. \t  -0.1614763305287739 \t -0.0071059237416912675\n",
      "83     \t [0.96552068 0.97102336]. \t  -0.15167981913613632 \t -0.0071059237416912675\n",
      "84     \t [1.43835    2.04799413]. \t  -0.23565051304066634 \t -0.0071059237416912675\n",
      "85     \t [0.75432161 0.55270783]. \t  -0.08690488996352472 \t -0.0071059237416912675\n",
      "86     \t [1.33983706 1.81342847]. \t  -0.14885069434085285 \t -0.0071059237416912675\n",
      "87     \t [0.64152888 0.38583213]. \t  -0.19469026511600418 \t -0.0071059237416912675\n",
      "88     \t [0.91629958 0.84373095]. \t  -0.008708168913064977 \t -0.0071059237416912675\n",
      "89     \t [0.82469705 0.66583953]. \t  -0.051139229040293006 \t -0.0071059237416912675\n",
      "90     \t [1.03999063 1.09091825]. \t  -0.01031859075411449 \t -0.0071059237416912675\n",
      "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
      "92     \t [1.3101173  1.71843983]. \t  -0.09658584164634189 \t -0.00598628680283637\n",
      "93     \t [0.88017043 0.77401868]. \t  -0.01440554385536713 \t -0.00598628680283637\n",
      "94     \t [1.11276056 1.29958148]. \t  -0.38904113994973744 \t -0.00598628680283637\n",
      "95     \t [0.77188092 0.57846788]. \t  -0.08207906604482199 \t -0.00598628680283637\n",
      "96     \t [0.78212674 0.62325806]. \t  -0.060776294277356195 \t -0.00598628680283637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.09545872 1.22879729]. \t  -0.09186916478316538 \t -0.00598628680283637\n",
      "98     \t [1.02019131 1.03830413]. \t  \u001b[92m-0.001025797814883375\u001b[0m \t -0.001025797814883375\n",
      "99     \t [0.80099164 0.61376263]. \t  -0.11702727462244467 \t -0.001025797814883375\n",
      "100    \t [1.10033177 1.19763652]. \t  -0.0272103705220275 \t -0.001025797814883375\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-2.048       0.74271395]. \t  -1200.637688037653 \t -6.867717811955245\n",
      "2      \t [-0.33643925  0.24944437]. \t  \u001b[92m-3.6425578530696523\u001b[0m \t -3.6425578530696523\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.6425578530696523\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.6425578530696523\n",
      "5      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.6425578530696523\n",
      "6      \t [2.048      0.42261237]. \t  -1423.6640809525513 \t -3.6425578530696523\n",
      "7      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.6425578530696523\n",
      "8      \t [ 0.08820464 -2.048     ]. \t  -423.4545358582294 \t -3.6425578530696523\n",
      "9      \t [0.93232195 2.048     ]. \t  -138.95581257195738 \t -3.6425578530696523\n",
      "10     \t [0.7254169  0.61214058]. \t  \u001b[92m-0.8134642373239745\u001b[0m \t -0.8134642373239745\n",
      "11     \t [ 0.48629697 -0.12237796]. \t  -13.14213512600684 \t -0.8134642373239745\n",
      "12     \t [-1.21316098 -0.22242657]. \t  -291.92474421832696 \t -0.8134642373239745\n",
      "13     \t [-1.20330678  2.048     ]. \t  -40.86089720396113 \t -0.8134642373239745\n",
      "14     \t [1.07021861 1.38371322]. \t  -5.685780567593577 \t -0.8134642373239745\n",
      "15     \t [-0.11636834 -0.46262191]. \t  -23.9194464125653 \t -0.8134642373239745\n",
      "16     \t [-0.85988752  0.32729233]. \t  -20.442995293760546 \t -0.8134642373239745\n",
      "17     \t [0.44780345 0.92968771]. \t  -53.472320330454274 \t -0.8134642373239745\n",
      "18     \t [-2.048      -0.41867246]. \t  -2137.24548326374 \t -0.8134642373239745\n",
      "19     \t [-1.33214976  1.52961622]. \t  -11.441753427759533 \t -0.8134642373239745\n",
      "20     \t [ 1.29747726 -0.52713023]. \t  -488.75376145957676 \t -0.8134642373239745\n",
      "21     \t [-0.66497918 -0.42953717]. \t  -78.7642548080835 \t -0.8134642373239745\n",
      "22     \t [-0.08879084  2.048     ]. \t  -417.3928710410198 \t -0.8134642373239745\n",
      "23     \t [-0.64056717 -2.048     ]. \t  -607.0282799330683 \t -0.8134642373239745\n",
      "24     \t [1.45445016 2.048     ]. \t  \u001b[92m-0.6611414702766669\u001b[0m \t -0.6611414702766669\n",
      "25     \t [1.37323301 1.72663676]. \t  -2.6716068529157235 \t -0.6611414702766669\n",
      "26     \t [-0.55508695  0.79906606]. \t  -26.520948974665615 \t -0.6611414702766669\n",
      "27     \t [ 2.048      -0.72211545]. \t  -2418.216329330784 \t -0.6611414702766669\n",
      "28     \t [1.29316555 1.10349781]. \t  -32.436939386527534 \t -0.6611414702766669\n",
      "29     \t [1.02002902 0.19102436]. \t  -72.15435664819222 \t -0.6611414702766669\n",
      "30     \t [ 0.91760928 -2.048     ]. \t  -835.2207166179213 \t -0.6611414702766669\n",
      "31     \t [-1.08828684  1.33523601]. \t  -6.63705019822471 \t -0.6611414702766669\n",
      "32     \t [0.17976423 0.30207238]. \t  -7.949681588465023 \t -0.6611414702766669\n",
      "33     \t [0.99418668 0.93951012]. \t  \u001b[92m-0.23912580256486368\u001b[0m \t -0.23912580256486368\n",
      "34     \t [-1.14677248 -1.2759748 ]. \t  -675.9688136472477 \t -0.23912580256486368\n",
      "35     \t [1.35705171 2.048     ]. \t  -4.388022097354003 \t -0.23912580256486368\n",
      "36     \t [-1.25789566  1.75430944]. \t  -8.056766225875315 \t -0.23912580256486368\n",
      "37     \t [-1.50096145  2.048     ]. \t  -10.452606137662885 \t -0.23912580256486368\n",
      "38     \t [1.25477144 1.42354999]. \t  -2.3420309436286293 \t -0.23912580256486368\n",
      "39     \t [ 0.98425577 -1.36351748]. \t  -543.9518065444447 \t -0.23912580256486368\n",
      "40     \t [-0.04180928 -0.03984927]. \t  -1.2584000218182885 \t -0.23912580256486368\n",
      "41     \t [2.048      1.38097946]. \t  -792.5777986978602 \t -0.23912580256486368\n",
      "42     \t [-2.048      -1.22567154]. \t  -2946.90379102875 \t -0.23912580256486368\n",
      "43     \t [1.26381314 1.72286966]. \t  -1.6482895684933883 \t -0.23912580256486368\n",
      "44     \t [1.12555567 1.25561732]. \t  \u001b[92m-0.028439049588921565\u001b[0m \t -0.028439049588921565\n",
      "45     \t [-0.72316924  0.51420602]. \t  -2.9769995635570052 \t -0.028439049588921565\n",
      "46     \t [0.87945431 0.81891167]. \t  -0.22129958039750622 \t -0.028439049588921565\n",
      "47     \t [0.89191225 0.75064374]. \t  -0.2129582953464293 \t -0.028439049588921565\n",
      "48     \t [1.3590212  1.87256939]. \t  -0.19458981266053207 \t -0.028439049588921565\n",
      "49     \t [1.27740944 1.65313658]. \t  -0.12258823100101499 \t -0.028439049588921565\n",
      "50     \t [1.04734176 1.08157023]. \t  \u001b[92m-0.02581740205057448\u001b[0m \t -0.02581740205057448\n",
      "51     \t [0.85762585 0.72109078]. \t  -0.04109671641652615 \t -0.02581740205057448\n",
      "52     \t [ 0.40061906 -1.67013082]. \t  -335.4785796589838 \t -0.02581740205057448\n",
      "53     \t [0.94485611 0.86219756]. \t  -0.0964047787407096 \t -0.02581740205057448\n",
      "54     \t [1.38375955 1.92189958]. \t  -0.15232530158504404 \t -0.02581740205057448\n",
      "55     \t [1.06523051 1.15401641]. \t  -0.04150542470265062 \t -0.02581740205057448\n",
      "56     \t [0.74135306 0.51255285]. \t  -0.20417965908224134 \t -0.02581740205057448\n",
      "57     \t [1.07149053 1.14124543]. \t  \u001b[92m-0.009798399168977203\u001b[0m \t -0.009798399168977203\n",
      "58     \t [0.95246631 0.91919694]. \t  -0.01667115218375357 \t -0.009798399168977203\n",
      "59     \t [1.30574045 1.68552337]. \t  -0.13124825488244046 \t -0.009798399168977203\n",
      "60     \t [1.08547455 1.21262348]. \t  -0.1254252123456839 \t -0.009798399168977203\n",
      "61     \t [1.22150258 1.48674523]. \t  -0.051897160699688236 \t -0.009798399168977203\n",
      "62     \t [1.08209406 1.15742345]. \t  -0.024975503466480198 \t -0.009798399168977203\n",
      "63     \t [0.91777832 0.79128408]. \t  -0.267196779682559 \t -0.009798399168977203\n",
      "64     \t [1.19452841 1.44601415]. \t  -0.07438358922033664 \t -0.009798399168977203\n",
      "65     \t [1.1247659  1.28656382]. \t  -0.06164326333451819 \t -0.009798399168977203\n",
      "66     \t [0.68320998 0.47494266]. \t  -0.10702555738314529 \t -0.009798399168977203\n",
      "67     \t [1.29494786 1.69136865]. \t  -0.10795752135934306 \t -0.009798399168977203\n",
      "68     \t [1.01348857 1.02512557]. \t  \u001b[92m-0.0005954557996102115\u001b[0m \t -0.0005954557996102115\n",
      "69     \t [0.78327913 0.60559696]. \t  -0.053255213377600315 \t -0.0005954557996102115\n",
      "70     \t [1.18367946 1.3662576 ]. \t  -0.1551169984916745 \t -0.0005954557996102115\n",
      "71     \t [0.80753664 0.66707574]. \t  -0.05942327938162725 \t -0.0005954557996102115\n",
      "72     \t [0.81020142 0.65038639]. \t  -0.03967160592631083 \t -0.0005954557996102115\n",
      "73     \t [1.22992587 1.47550505]. \t  -0.19134361625029594 \t -0.0005954557996102115\n",
      "74     \t [1.41104097 1.99016335]. \t  -0.16903093855242962 \t -0.0005954557996102115\n",
      "75     \t [0.8836055  0.82794883]. \t  -0.23623864018736446 \t -0.0005954557996102115\n",
      "76     \t [1.09876569 1.18745598]. \t  -0.04907775103243363 \t -0.0005954557996102115\n",
      "77     \t [1.43405151 2.03584196]. \t  -0.2310916528389635 \t -0.0005954557996102115\n",
      "78     \t [1.08292288 1.15665847]. \t  -0.032679772610505094 \t -0.0005954557996102115\n",
      "79     \t [1.09120982 1.18665701]. \t  -0.00998538395808353 \t -0.0005954557996102115\n",
      "80     \t [1.00174888 0.97537704]. \t  -0.0790977139331041 \t -0.0005954557996102115\n",
      "81     \t [1.31361046 1.70470809]. \t  -0.1418836365009325 \t -0.0005954557996102115\n",
      "82     \t [1.23039473 1.51901563]. \t  -0.055728259920683185 \t -0.0005954557996102115\n",
      "83     \t [-2.048       1.53105527]. \t  -718.5796844432666 \t -0.0005954557996102115\n",
      "84     \t [1.11615528 1.25337459]. \t  -0.01922552861814839 \t -0.0005954557996102115\n",
      "85     \t [1.08302317 1.10066317]. \t  -0.5292752382212822 \t -0.0005954557996102115\n",
      "86     \t [1.3991455  1.96973449]. \t  -0.17402200859273953 \t -0.0005954557996102115\n",
      "87     \t [1.43026136 2.04799857]. \t  -0.18567756473889435 \t -0.0005954557996102115\n",
      "88     \t [0.83385682 0.71001805]. \t  -0.049215068837831076 \t -0.0005954557996102115\n",
      "89     \t [-1.31508887 -2.048     ]. \t  -1432.2790947925057 \t -0.0005954557996102115\n",
      "90     \t [0.19180002 0.05030662]. \t  -0.6714645583264132 \t -0.0005954557996102115\n",
      "91     \t [0.63904208 0.36249323]. \t  -0.3408022904463338 \t -0.0005954557996102115\n",
      "92     \t [1.1832349  1.38873834]. \t  -0.046358708247612866 \t -0.0005954557996102115\n",
      "93     \t [0.76023295 0.56299857]. \t  -0.07985514872330877 \t -0.0005954557996102115\n",
      "94     \t [0.65225679 0.35188804]. \t  -0.6618983841946648 \t -0.0005954557996102115\n",
      "95     \t [0.77021791 0.58120135]. \t  -0.06728218937937198 \t -0.0005954557996102115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.02630762 0.02835015]. \t  -1.0245736595290822 \t -0.0005954557996102115\n",
      "97     \t [1.00305402 0.98151861]. \t  -0.06051921143573193 \t -0.0005954557996102115\n",
      "98     \t [0.90962851 0.80433661]. \t  -0.061469861721548634 \t -0.0005954557996102115\n",
      "99     \t [1.11061702 1.22205424]. \t  -0.02526845250903746 \t -0.0005954557996102115\n",
      "100    \t [0.99436496 0.97587777]. \t  -0.016631252944379303 \t -0.0005954557996102115\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
      "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
      "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
      "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
      "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
      "1      \t [-0.13823078 -1.26759214]. \t  -166.85523047624642 \t -21.690996320546372\n",
      "2      \t [0.12874914 1.59550592]. \t  -250.06094104670106 \t -21.690996320546372\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -21.690996320546372\n",
      "4      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -21.690996320546372\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -21.690996320546372\n",
      "6      \t [ 0.75139951 -2.048     ]. \t  -682.6303187679813 \t -21.690996320546372\n",
      "7      \t [-0.05824138 -0.36410103]. \t  \u001b[92m-14.624991836700517\u001b[0m \t -14.624991836700517\n",
      "8      \t [-2.048       0.58060565]. \t  -1315.1718788631658 \t -14.624991836700517\n",
      "9      \t [-0.88733509  2.048     ]. \t  -162.4824567159021 \t -14.624991836700517\n",
      "10     \t [-0.5101775   0.82052957]. \t  -33.66847261382172 \t -14.624991836700517\n",
      "11     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -14.624991836700517\n",
      "12     \t [1.01040374 2.048     ]. \t  -105.49032170159957 \t -14.624991836700517\n",
      "13     \t [2.048      0.39230711]. \t  -1446.616340456736 \t -14.624991836700517\n",
      "14     \t [0.90768928 1.12768208]. \t  \u001b[92m-9.236886952515928\u001b[0m \t -9.236886952515928\n",
      "15     \t [-0.03137743 -2.048     ]. \t  -420.8975051785738 \t -9.236886952515928\n",
      "16     \t [-0.88616321 -0.50140414]. \t  -169.11456563984754 \t -9.236886952515928\n",
      "17     \t [-0.57390861  0.12701158]. \t  \u001b[92m-6.572125542491383\u001b[0m \t -6.572125542491383\n",
      "18     \t [-1.07562171  1.31923893]. \t  -6.941583898533809 \t -6.572125542491383\n",
      "19     \t [1.33348896 1.51794123]. \t  -6.884302954205285 \t -6.572125542491383\n",
      "20     \t [ 0.34109515 -0.88408614]. \t  -100.5205818534488 \t -6.572125542491383\n",
      "21     \t [-0.8884021   0.88698527]. \t  \u001b[92m-4.5211186781411525\u001b[0m \t -4.5211186781411525\n",
      "22     \t [-0.74485779  1.4185681 ]. \t  -77.65179501335899 \t -4.5211186781411525\n",
      "23     \t [0.34619142 0.38979889]. \t  -7.714786828343683 \t -4.5211186781411525\n",
      "24     \t [-2.048      -0.57247517]. \t  -2281.508665195052 \t -4.5211186781411525\n",
      "25     \t [-1.41645411  2.048     ]. \t  -6.01278738347314 \t -4.5211186781411525\n",
      "26     \t [2.048      1.41035007]. \t  -776.1382530011011 \t -4.5211186781411525\n",
      "27     \t [0.9118881  0.62095129]. \t  \u001b[92m-4.4425203671407605\u001b[0m \t -4.4425203671407605\n",
      "28     \t [1.45453499 2.048     ]. \t  \u001b[92m-0.6645524369261966\u001b[0m \t -0.6645524369261966\n",
      "29     \t [-0.87099295 -2.048     ]. \t  -791.2170960688982 \t -0.6645524369261966\n",
      "30     \t [1.1460913  1.10141545]. \t  -4.520399579918828 \t -0.6645524369261966\n",
      "31     \t [-1.26972423  1.79591319]. \t  -8.526715555421646 \t -0.6645524369261966\n",
      "32     \t [-0.39801775 -0.52617167]. \t  -48.82077294408288 \t -0.6645524369261966\n",
      "33     \t [1.28919959 1.81280935]. \t  -2.3569088955247093 \t -0.6645524369261966\n",
      "34     \t [-0.8407137  -1.32329488]. \t  -415.5165595583515 \t -0.6645524369261966\n",
      "35     \t [-0.08493388  0.06122932]. \t  -1.4688495118566331 \t -0.6645524369261966\n",
      "36     \t [-1.29867243  2.048     ]. \t  -18.34849946325905 \t -0.6645524369261966\n",
      "37     \t [ 0.48453615 -1.54577357]. \t  -317.30112121940846 \t -0.6645524369261966\n",
      "38     \t [-1.86285568  1.45140997]. \t  -415.75989537887983 \t -0.6645524369261966\n",
      "39     \t [ 0.20064262 -0.00248168]. \t  -0.821635611730396 \t -0.6645524369261966\n",
      "40     \t [1.13648348 1.35285113]. \t  \u001b[92m-0.3938627870339364\u001b[0m \t -0.3938627870339364\n",
      "41     \t [1.38254653 2.048     ]. \t  -2.0113439554622015 \t -0.3938627870339364\n",
      "42     \t [ 2.048     -1.2142197]. \t  -2926.311166020998 \t -0.3938627870339364\n",
      "43     \t [-0.71090265  0.44174729]. \t  -3.332132967158874 \t -0.3938627870339364\n",
      "44     \t [1.25344083 1.58447903]. \t  \u001b[92m-0.08209486751045508\u001b[0m \t -0.08209486751045508\n",
      "45     \t [ 0.07742491 -0.02442018]. \t  -0.9436507771702458 \t -0.08209486751045508\n",
      "46     \t [1.36437593 1.83966013]. \t  -0.18056254377247644 \t -0.08209486751045508\n",
      "47     \t [-1.2695076  0.9813447]. \t  -44.87908460943761 \t -0.08209486751045508\n",
      "48     \t [0.90911526 0.83635763]. \t  \u001b[92m-0.017995970219906943\u001b[0m \t -0.017995970219906943\n",
      "49     \t [1.35298404 1.81578647]. \t  -0.14644066628793592 \t -0.017995970219906943\n",
      "50     \t [-2.048      -1.33960282]. \t  -3071.702773595408 \t -0.017995970219906943\n",
      "51     \t [1.30769243 1.75807126]. \t  -0.32518750987866607 \t -0.017995970219906943\n",
      "52     \t [1.42261795 2.048     ]. \t  -0.236967665954422 \t -0.017995970219906943\n",
      "53     \t [1.19822118 1.43874699]. \t  -0.0401994482718281 \t -0.017995970219906943\n",
      "54     \t [1.00460009 0.98435586]. \t  -0.06185037205587684 \t -0.017995970219906943\n",
      "55     \t [1.12626849 1.25507626]. \t  -0.033911634558545956 \t -0.017995970219906943\n",
      "56     \t [ 0.04506434 -0.00781415]. \t  -0.9215944074532564 \t -0.017995970219906943\n",
      "57     \t [1.13993588 1.27430228]. \t  -0.08284202774605495 \t -0.017995970219906943\n",
      "58     \t [1.14669778 1.32753093]. \t  -0.037434401518996116 \t -0.017995970219906943\n",
      "59     \t [1.20356837 1.43056424]. \t  -0.07388534585594567 \t -0.017995970219906943\n",
      "60     \t [0.87131164 0.75869075]. \t  \u001b[92m-0.01658502069093218\u001b[0m \t -0.01658502069093218\n",
      "61     \t [1.23140488 1.48885865]. \t  -0.12916947315043598 \t -0.01658502069093218\n",
      "62     \t [1.36122049 1.8731095 ]. \t  -0.17123690317088297 \t -0.01658502069093218\n",
      "63     \t [0.58830489 0.34378445]. \t  -0.17003026768286897 \t -0.01658502069093218\n",
      "64     \t [0.83828017 0.67834968]. \t  -0.08551354649332034 \t -0.01658502069093218\n",
      "65     \t [1.04294563 1.04326719]. \t  -0.19958819076860354 \t -0.01658502069093218\n",
      "66     \t [0.66595717 0.51295255]. \t  -0.5939648801365197 \t -0.01658502069093218\n",
      "67     \t [1.07142379 1.16368196]. \t  -0.029854168165562703 \t -0.01658502069093218\n",
      "68     \t [ 0.10821962 -0.02862332]. \t  -0.9579619353690085 \t -0.01658502069093218\n",
      "69     \t [1.23404143 1.44765575]. \t  -0.6203169271777981 \t -0.01658502069093218\n",
      "70     \t [1.30275895 1.74562117]. \t  -0.3263090460553237 \t -0.01658502069093218\n",
      "71     \t [1.39925872 1.99022507]. \t  -0.2637372196502358 \t -0.01658502069093218\n",
      "72     \t [1.1596305  1.39228705]. \t  -0.25152650108614777 \t -0.01658502069093218\n",
      "73     \t [1.19665488 1.40070166]. \t  -0.13652482716080472 \t -0.01658502069093218\n",
      "74     \t [1.17001987 1.3447501 ]. \t  -0.08745329084247008 \t -0.01658502069093218\n",
      "75     \t [0.70693167 0.4856955 ]. \t  -0.10564866097076443 \t -0.01658502069093218\n",
      "76     \t [1.11817116 1.17651108]. \t  -0.5585444161673739 \t -0.01658502069093218\n",
      "77     \t [1.02899782 1.02662366]. \t  -0.10460759600790373 \t -0.01658502069093218\n",
      "78     \t [0.57716626 0.36447815]. \t  -0.2771160892930471 \t -0.01658502069093218\n",
      "79     \t [0.88470993 0.80010107]. \t  -0.04353093929301088 \t -0.01658502069093218\n",
      "80     \t [1.22279478 1.47324009]. \t  -0.0979802267768495 \t -0.01658502069093218\n",
      "81     \t [0.17375133 0.00642836]. \t  -0.7391461684808115 \t -0.01658502069093218\n",
      "82     \t [0.10635671 0.03798211]. \t  -0.8697291402744215 \t -0.01658502069093218\n",
      "83     \t [1.09678511 1.2017407 ]. \t  \u001b[92m-0.009510606008283583\u001b[0m \t -0.009510606008283583\n",
      "84     \t [0.83429259 0.71816222]. \t  -0.07637991571337804 \t -0.009510606008283583\n",
      "85     \t [1.02765221 0.99470513]. \t  -0.37731797197628997 \t -0.009510606008283583\n",
      "86     \t [1.39932479 1.91379037]. \t  -0.35588203440409116 \t -0.009510606008283583\n",
      "87     \t [0.99893513 1.00609003]. \t  \u001b[92m-0.0067557300461\u001b[0m \t -0.0067557300461\n",
      "88     \t [0.66758424 0.46166906]. \t  -0.1361013192954895 \t -0.0067557300461\n",
      "89     \t [1.17876844 1.37845072]. \t  -0.044155848896645665 \t -0.0067557300461\n",
      "90     \t [0.7777922  0.64186433]. \t  -0.1855639881022369 \t -0.0067557300461\n",
      "91     \t [1.13973745 1.29405301]. \t  -0.02197525614359102 \t -0.0067557300461\n",
      "92     \t [0.82226613 0.70348283]. \t  -0.1064531087681587 \t -0.0067557300461\n",
      "93     \t [0.88858942 0.840318  ]. \t  -0.2697336897390456 \t -0.0067557300461\n",
      "94     \t [1.20785632 1.44487514]. \t  -0.0629213455824753 \t -0.0067557300461\n",
      "95     \t [0.81339114 0.63748723]. \t  -0.09299023428736897 \t -0.0067557300461\n",
      "96     \t [1.09048691 1.21994895]. \t  -0.10297336564719023 \t -0.0067557300461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.80212501 0.6482129 ]. \t  -0.04146654641593419 \t -0.0067557300461\n",
      "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.0067557300461\n",
      "99     \t [0.77260723 0.66983368]. \t  -0.5833197154283127 \t -0.0067557300461\n",
      "100    \t [0.71105061 0.47815746]. \t  -0.15876244762650446 \t -0.0067557300461\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.73817989 0.64177085]. \t  -566.7462179445403 \t -31.22188590191926\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -31.22188590191926\n",
      "3      \t [ 1.44705232 -2.048     ]. \t  -1715.7834707085842 \t -31.22188590191926\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -31.22188590191926\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -31.22188590191926\n",
      "6      \t [-0.06878508 -0.14396613]. \t  \u001b[92m-3.353396608570467\u001b[0m \t -3.353396608570467\n",
      "7      \t [-2.048       0.73510696]. \t  -1205.8947222374227 \t -3.353396608570467\n",
      "8      \t [ 2.048      -0.64732106]. \t  -2345.2316291265665 \t -3.353396608570467\n",
      "9      \t [ 0.00732218 -2.048     ]. \t  -420.4377699684907 \t -3.353396608570467\n",
      "10     \t [-0.55461101  0.68492062]. \t  -16.654400393808952 \t -3.353396608570467\n",
      "11     \t [-1.00387996  2.048     ]. \t  -112.22234380900892 \t -3.353396608570467\n",
      "12     \t [-0.61265241 -0.23026248]. \t  -39.276444190406494 \t -3.353396608570467\n",
      "13     \t [1.04028085 2.048     ]. \t  -93.28162794716282 \t -3.353396608570467\n",
      "14     \t [ 0.41782178 -1.1614092 ]. \t  -178.82432040607554 \t -3.353396608570467\n",
      "15     \t [-2.048     -0.4563904]. \t  -2172.1861487610095 \t -3.353396608570467\n",
      "16     \t [-0.78167995  0.31915959]. \t  -11.69284061102163 \t -3.353396608570467\n",
      "17     \t [-1.05147189  1.31085182]. \t  -8.421649859191653 \t -3.353396608570467\n",
      "18     \t [ 0.6739847  -0.16971213]. \t  -39.039830242862756 \t -3.353396608570467\n",
      "19     \t [ 0.18086469 -0.57047763]. \t  -37.05475966603192 \t -3.353396608570467\n",
      "20     \t [1.40314281 1.44802932]. \t  -27.2837504272121 \t -3.353396608570467\n",
      "21     \t [0.44938554 0.19973826]. \t  \u001b[92m-0.3036642988777614\u001b[0m \t -0.3036642988777614\n",
      "22     \t [1.06539467 0.84968111]. \t  -8.148718830224956 \t -0.3036642988777614\n",
      "23     \t [-0.740268    1.44060013]. \t  -82.7026180475072 \t -0.3036642988777614\n",
      "24     \t [-0.71983218 -2.048     ]. \t  -661.4746973937695 \t -0.3036642988777614\n",
      "25     \t [-1.36577189  1.80805911]. \t  -5.924904828840703 \t -0.3036642988777614\n",
      "26     \t [-1.42160363  2.048     ]. \t  -5.937297221964496 \t -0.3036642988777614\n",
      "27     \t [2.048      1.34775088]. \t  -811.3847733231346 \t -0.3036642988777614\n",
      "28     \t [1.4401344 2.048    ]. \t  \u001b[92m-0.2612511174913814\u001b[0m \t -0.2612511174913814\n",
      "29     \t [1.29030109 1.75218011]. \t  -0.8464599836589731 \t -0.2612511174913814\n",
      "30     \t [1.143385   1.26013397]. \t  \u001b[92m-0.2432986566259653\u001b[0m \t -0.2432986566259653\n",
      "31     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.2432986566259653\n",
      "32     \t [ 0.66450648 -2.048     ]. \t  -619.9078687612463 \t -0.2432986566259653\n",
      "33     \t [-1.06770998 -1.13786273]. \t  -523.1433843862255 \t -0.2432986566259653\n",
      "34     \t [ 1.05869967 -1.10953867]. \t  -497.46457788832146 \t -0.2432986566259653\n",
      "35     \t [ 0.25550095 -0.05779372]. \t  -2.0690110328958102 \t -0.2432986566259653\n",
      "36     \t [-0.09193451  0.29533421]. \t  -9.422463649161095 \t -0.2432986566259653\n",
      "37     \t [-0.98920564  0.77122758]. \t  -8.254277525452816 \t -0.2432986566259653\n",
      "38     \t [-1.26495669  1.84788783]. \t  -11.269144766727255 \t -0.2432986566259653\n",
      "39     \t [0.7987671  0.43287397]. \t  -4.249348610717894 \t -0.2432986566259653\n",
      "40     \t [-0.0588141   0.99744003]. \t  -99.92089599646118 \t -0.2432986566259653\n",
      "41     \t [-2.048       1.47504621]. \t  -748.7265994928202 \t -0.2432986566259653\n",
      "42     \t [1.24796225 1.54773873]. \t  \u001b[92m-0.0708382155301621\u001b[0m \t -0.0708382155301621\n",
      "43     \t [0.88018577 0.78474951]. \t  \u001b[92m-0.024400547770304676\u001b[0m \t -0.024400547770304676\n",
      "44     \t [-0.9095905   0.92166184]. \t  -4.535916305636117 \t -0.024400547770304676\n",
      "45     \t [-0.4943235  -1.60584772]. \t  -344.55828174135735 \t -0.024400547770304676\n",
      "46     \t [-2.048      -1.29640371]. \t  -3024.07742410115 \t -0.024400547770304676\n",
      "47     \t [ 1.40127237 -0.18170226]. \t  -460.3778570232799 \t -0.024400547770304676\n",
      "48     \t [1.35443585 1.78295057]. \t  -0.3913225996623251 \t -0.024400547770304676\n",
      "49     \t [1.40458644 2.048     ]. \t  -0.7282460741324028 \t -0.024400547770304676\n",
      "50     \t [0.17202959 0.00483217]. \t  -0.7468507291577072 \t -0.024400547770304676\n",
      "51     \t [1.12116454 1.25352473]. \t  \u001b[92m-0.015895499551223238\u001b[0m \t -0.015895499551223238\n",
      "52     \t [1.30669555 1.69323253]. \t  -0.11428503887673735 \t -0.015895499551223238\n",
      "53     \t [0.73348041 0.51871551]. \t  -0.10819682746314384 \t -0.015895499551223238\n",
      "54     \t [0.95052144 0.88625807]. \t  -0.032145574403569015 \t -0.015895499551223238\n",
      "55     \t [1.40588468 1.97336383]. \t  -0.1657332896346479 \t -0.015895499551223238\n",
      "56     \t [1.21837424 1.45361745]. \t  -0.14266430930968854 \t -0.015895499551223238\n",
      "57     \t [0.17735697 0.03571505]. \t  -0.678555945312548 \t -0.015895499551223238\n",
      "58     \t [0.79272177 0.60346502]. \t  -0.10517854178754119 \t -0.015895499551223238\n",
      "59     \t [1.07284145 1.16405562]. \t  -0.022380097071222124 \t -0.015895499551223238\n",
      "60     \t [1.18476541 1.42736758]. \t  -0.09030019452486242 \t -0.015895499551223238\n",
      "61     \t [1.16016239 1.33453143]. \t  -0.038751576768427866 \t -0.015895499551223238\n",
      "62     \t [0.79407537 0.62499124]. \t  -0.04550126729736489 \t -0.015895499551223238\n",
      "63     \t [0.39426885 0.21267649]. \t  -0.6944211040267874 \t -0.015895499551223238\n",
      "64     \t [1.28344057 1.63002428]. \t  -0.10990680240430001 \t -0.015895499551223238\n",
      "65     \t [1.12621597 1.23905101]. \t  -0.10184634793668347 \t -0.015895499551223238\n",
      "66     \t [1.04083569 1.09140472]. \t  \u001b[92m-0.00817324769449453\u001b[0m \t -0.00817324769449453\n",
      "67     \t [1.43419755 2.048     ]. \t  -0.19648880695824378 \t -0.00817324769449453\n",
      "68     \t [0.88321337 0.75867755]. \t  -0.059385126205887546 \t -0.00817324769449453\n",
      "69     \t [1.41028795 2.048     ]. \t  -0.5174742598936973 \t -0.00817324769449453\n",
      "70     \t [0.87317558 0.76007076]. \t  -0.016643680206856894 \t -0.00817324769449453\n",
      "71     \t [1.38095053 1.87999942]. \t  -0.21815812747744523 \t -0.00817324769449453\n",
      "72     \t [0.90284785 0.78043328]. \t  -0.12985413408860766 \t -0.00817324769449453\n",
      "73     \t [1.19168415 1.40339199]. \t  -0.06469569233782727 \t -0.00817324769449453\n",
      "74     \t [1.35278365 1.75320307]. \t  -0.7145957777358598 \t -0.00817324769449453\n",
      "75     \t [0.78294207 0.58107184]. \t  -0.14904392336735062 \t -0.00817324769449453\n",
      "76     \t [1.38749294 1.93002063]. \t  -0.15253609403502008 \t -0.00817324769449453\n",
      "77     \t [1.13891079 1.28004979]. \t  -0.04842785883870651 \t -0.00817324769449453\n",
      "78     \t [0.16133525 0.03608641]. \t  -0.7134735750540968 \t -0.00817324769449453\n",
      "79     \t [1.17178257 1.36111415]. \t  -0.04381399571743078 \t -0.00817324769449453\n",
      "80     \t [1.37573771 1.87876019]. \t  -0.16048332244982316 \t -0.00817324769449453\n",
      "81     \t [0.11518229 0.0175198 ]. \t  -0.7847110463544797 \t -0.00817324769449453\n",
      "82     \t [1.2157671 1.4528108]. \t  -0.11045749277621027 \t -0.00817324769449453\n",
      "83     \t [1.13612053 1.28802979]. \t  -0.019279603739198473 \t -0.00817324769449453\n",
      "84     \t [1.03800346 1.092703  ]. \t  -0.024706060723517755 \t -0.00817324769449453\n",
      "85     \t [0.8504676  0.66668304]. \t  -0.34285296793012043 \t -0.00817324769449453\n",
      "86     \t [1.3629697 1.7558672]. \t  -1.16846191686989 \t -0.00817324769449453\n",
      "87     \t [1.01939695 1.07068557]. \t  -0.09969846428570008 \t -0.00817324769449453\n",
      "88     \t [-0.32376244  0.07912714]. \t  -1.8183701895662707 \t -0.00817324769449453\n",
      "89     \t [0.81151693 0.71512554]. \t  -0.35549500028789444 \t -0.00817324769449453\n",
      "90     \t [0.18459217 0.04265312]. \t  -0.6722495912732361 \t -0.00817324769449453\n",
      "91     \t [0.86603414 0.70099759]. \t  -0.2582187992419383 \t -0.00817324769449453\n",
      "92     \t [0.57863273 0.34727335]. \t  -0.1930693637551321 \t -0.00817324769449453\n",
      "93     \t [1.3866298  1.86841751]. \t  -0.44459976419322134 \t -0.00817324769449453\n",
      "94     \t [0.84508445 0.74732108]. \t  -0.1339132568038867 \t -0.00817324769449453\n",
      "95     \t [1.23858829 1.51162788]. \t  -0.1074281933194568 \t -0.00817324769449453\n",
      "96     \t [1.16826438 1.33648239]. \t  -0.10873779998524807 \t -0.00817324769449453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.2371045  1.47126538]. \t  -0.40623459266498746 \t -0.00817324769449453\n",
      "98     \t [0.64001144 0.42809533]. \t  -0.1637453048836222 \t -0.00817324769449453\n",
      "99     \t [1.09841604 1.20797332]. \t  -0.009897573875014786 \t -0.00817324769449453\n",
      "100    \t [1.09371988 1.19584553]. \t  -0.008797678840244554 \t -0.00817324769449453\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-1.06159133  0.96230717]. \t  -6.96174586054512 \t -1.7663579664225912\n",
      "2      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.7663579664225912\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.7663579664225912\n",
      "4      \t [0.01451047 2.048     ]. \t  -420.3153512017588 \t -1.7663579664225912\n",
      "5      \t [-2.048    -0.077764]. \t  -1834.3468052157702 \t -1.7663579664225912\n",
      "6      \t [2.048      1.17653782]. \t  -911.7895777624595 \t -1.7663579664225912\n",
      "7      \t [-0.25724617  0.77888257]. \t  -52.37579146374304 \t -1.7663579664225912\n",
      "8      \t [ 0.21276036 -1.27794566]. \t  -175.70891264012576 \t -1.7663579664225912\n",
      "9      \t [-1.0600691  2.048    ]. \t  -89.66833959015769 \t -1.7663579664225912\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
      "11     \t [0.85418004 1.13452186]. \t  -16.41552864311162 \t -1.7663579664225912\n",
      "12     \t [1.08336781 2.048     ]. \t  -76.44948022567648 \t -1.7663579664225912\n",
      "13     \t [-0.82761238  1.41312936]. \t  -56.365812709731784 \t -1.7663579664225912\n",
      "14     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.7663579664225912\n",
      "15     \t [-0.22652223 -2.048     ]. \t  -442.215577902652 \t -1.7663579664225912\n",
      "16     \t [-0.42102978 -1.2554196 ]. \t  -207.27814995045833 \t -1.7663579664225912\n",
      "17     \t [ 0.75299699 -0.56744576]. \t  -128.7587426687183 \t -1.7663579664225912\n",
      "18     \t [-2.048       1.14671597]. \t  -938.0695866978414 \t -1.7663579664225912\n",
      "19     \t [-0.69208363  0.42496682]. \t  -3.1548865626605416 \t -1.7663579664225912\n",
      "20     \t [ 0.14747314 -0.67249493]. \t  -48.92417171303219 \t -1.7663579664225912\n",
      "21     \t [ 2.048      -0.54176045]. \t  -2244.1289539569316 \t -1.7663579664225912\n",
      "22     \t [0.8356224  0.42070966]. \t  -7.73070534023852 \t -1.7663579664225912\n",
      "23     \t [1.23022132 1.56507011]. \t  \u001b[92m-0.3195221685383271\u001b[0m \t -0.3195221685383271\n",
      "24     \t [ 0.71928673 -2.048     ]. \t  -658.1928706941662 \t -0.3195221685383271\n",
      "25     \t [1.17592377 0.93185575]. \t  -20.365724139544056 \t -0.3195221685383271\n",
      "26     \t [-1.11036526 -0.75825358]. \t  -400.9272819774755 \t -0.3195221685383271\n",
      "27     \t [-1.3652662  1.6147079]. \t  -11.806736022151995 \t -0.3195221685383271\n",
      "28     \t [1.41924631 2.048     ]. \t  \u001b[92m-0.2896056213940912\u001b[0m \t -0.2896056213940912\n",
      "29     \t [-1.130532    1.34379976]. \t  -4.970778219568483 \t -0.2896056213940912\n",
      "30     \t [-1.44852622  2.048     ]. \t  -6.247568044174827 \t -0.2896056213940912\n",
      "31     \t [-0.96050497  0.0345315 ]. \t  -82.7047825666644 \t -0.2896056213940912\n",
      "32     \t [ 0.46639216 -0.1718879 ]. \t  -15.448716722895348 \t -0.2896056213940912\n",
      "33     \t [1.33619381 1.85887666]. \t  -0.6527040877947967 \t -0.2896056213940912\n",
      "34     \t [0.42443851 0.44015177]. \t  -7.091464962206214 \t -0.2896056213940912\n",
      "35     \t [-1.30352191  1.82767316]. \t  -6.957535798843077 \t -0.2896056213940912\n",
      "36     \t [0.91253904 0.85566979]. \t  \u001b[92m-0.06028430682060292\u001b[0m \t -0.06028430682060292\n",
      "37     \t [-0.43412357  0.062195  ]. \t  -3.651078242079361 \t -0.06028430682060292\n",
      "38     \t [-2.048      -1.07209844]. \t  -2782.7897680564092 \t -0.06028430682060292\n",
      "39     \t [ 0.9157148  -1.43049184]. \t  -514.8547429290264 \t -0.06028430682060292\n",
      "40     \t [-0.86576829  0.879431  ]. \t  -5.167875665489585 \t -0.06028430682060292\n",
      "41     \t [1.29952684 1.55645594]. \t  -1.8404179468284154 \t -0.06028430682060292\n",
      "42     \t [0.28517065 0.10473167]. \t  -0.5657808641144525 \t -0.06028430682060292\n",
      "43     \t [2.048      0.31866061]. \t  -1503.159474637337 \t -0.06028430682060292\n",
      "44     \t [1.34947386 1.7774536 ]. \t  -0.3124557042630047 \t -0.06028430682060292\n",
      "45     \t [1.18423177 1.35731137]. \t  -0.23728386613109434 \t -0.06028430682060292\n",
      "46     \t [1.09059446 1.13807512]. \t  -0.2715933930879733 \t -0.06028430682060292\n",
      "47     \t [0.96715794 0.90306969]. \t  -0.1055677387386528 \t -0.06028430682060292\n",
      "48     \t [1.34293955 1.77185955]. \t  -0.21763472143893342 \t -0.06028430682060292\n",
      "49     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.06028430682060292\n",
      "50     \t [0.97561449 0.96840903]. \t  \u001b[92m-0.028102179301308167\u001b[0m \t -0.028102179301308167\n",
      "51     \t [-1.09868041 -1.44191973]. \t  -706.1342897629502 \t -0.028102179301308167\n",
      "52     \t [0.30921347 0.13922708]. \t  -0.6674050397678447 \t -0.028102179301308167\n",
      "53     \t [1.36035313 1.821956  ]. \t  -0.21167684681255416 \t -0.028102179301308167\n",
      "54     \t [0.05016673 0.0647841 ]. \t  -1.2899061342491756 \t -0.028102179301308167\n",
      "55     \t [0.89021888 0.7594219 ]. \t  -0.12139953383479797 \t -0.028102179301308167\n",
      "56     \t [0.17868171 0.02730814]. \t  -0.6766972565757533 \t -0.028102179301308167\n",
      "57     \t [1.42903003 2.04799984]. \t  -0.1875160032684397 \t -0.028102179301308167\n",
      "58     \t [1.00714032 0.99768999]. \t  \u001b[92m-0.027745380817298968\u001b[0m \t -0.027745380817298968\n",
      "59     \t [0.19044644 0.00882106]. \t  -0.7307205549281539 \t -0.027745380817298968\n",
      "60     \t [1.39802696 1.94767668]. \t  -0.16305312924042237 \t -0.027745380817298968\n",
      "61     \t [1.18368085 1.37192312]. \t  -0.11886985069909625 \t -0.027745380817298968\n",
      "62     \t [0.96893566 0.9446466 ]. \t  \u001b[92m-0.004340940055392839\u001b[0m \t -0.004340940055392839\n",
      "63     \t [0.9771434  0.92679334]. \t  -0.07901143747941478 \t -0.004340940055392839\n",
      "64     \t [1.08652676 1.2203998 ]. \t  -0.1663640888900133 \t -0.004340940055392839\n",
      "65     \t [1.04955548 1.0600081 ]. \t  -0.1751674039182813 \t -0.004340940055392839\n",
      "66     \t [0.98525968 0.96503363]. \t  \u001b[92m-0.003469710241063105\u001b[0m \t -0.003469710241063105\n",
      "67     \t [ 1.66413556 -1.25248749]. \t  -1617.9564713299676 \t -0.003469710241063105\n",
      "68     \t [1.34208154 1.76289245]. \t  -0.2636352733477233 \t -0.003469710241063105\n",
      "69     \t [0.30839803 0.06863603]. \t  -0.5483969260169099 \t -0.003469710241063105\n",
      "70     \t [1.04478492 1.04757154]. \t  -0.19564086054227547 \t -0.003469710241063105\n",
      "71     \t [1.38248881 1.87069401]. \t  -0.3109818491335265 \t -0.003469710241063105\n",
      "72     \t [1.11014546 1.22796215]. \t  -0.01412188488140422 \t -0.003469710241063105\n",
      "73     \t [ 1.3533201 -2.048    ]. \t  -1505.1576829850474 \t -0.003469710241063105\n",
      "74     \t [1.34824454 1.7862884 ]. \t  -0.22034138522757155 \t -0.003469710241063105\n",
      "75     \t [1.05020174 1.11178622]. \t  -0.010374635590558526 \t -0.003469710241063105\n",
      "76     \t [0.24300948 0.04024232]. \t  -0.6084211090806629 \t -0.003469710241063105\n",
      "77     \t [1.0214542 1.0135335]. \t  -0.08947406694006076 \t -0.003469710241063105\n",
      "78     \t [0.1791063  0.01587382]. \t  -0.7001274591214469 \t -0.003469710241063105\n",
      "79     \t [0.94058308 0.86078732]. \t  -0.06069544688455131 \t -0.003469710241063105\n",
      "80     \t [0.92533046 0.86928635]. \t  -0.022605483887965457 \t -0.003469710241063105\n",
      "81     \t [0.99374365 0.98019811]. \t  -0.005409579618637759 \t -0.003469710241063105\n",
      "82     \t [1.05681221 1.10630799]. \t  -0.0143453667818342 \t -0.003469710241063105\n",
      "83     \t [0.92113979 0.88280601]. \t  -0.12391936680495996 \t -0.003469710241063105\n",
      "84     \t [1.0670588  1.14796359]. \t  -0.013237459439823868 \t -0.003469710241063105\n",
      "85     \t [1.14429454 1.27412807]. \t  -0.14530228570024836 \t -0.003469710241063105\n",
      "86     \t [ 0.20998121 -0.01441194]. \t  -0.9664020521246761 \t -0.003469710241063105\n",
      "87     \t [1.01687194 1.02417848]. \t  -0.009987036657374925 \t -0.003469710241063105\n",
      "88     \t [1.11114494 1.23755194]. \t  -0.013199341003649421 \t -0.003469710241063105\n",
      "89     \t [1.04779559 1.08256261]. \t  -0.025733207977007115 \t -0.003469710241063105\n",
      "90     \t [0.99864949 1.0047711 ]. \t  -0.005582352951844997 \t -0.003469710241063105\n",
      "91     \t [1.12023973 1.27597345]. \t  -0.05871057705432174 \t -0.003469710241063105\n",
      "92     \t [1.28061873 1.64889098]. \t  -0.08667968684048195 \t -0.003469710241063105\n",
      "93     \t [0.94546228 0.91499998]. \t  -0.04749979690725458 \t -0.003469710241063105\n",
      "94     \t [0.83466946 0.65299331]. \t  -0.21812659108505525 \t -0.003469710241063105\n",
      "95     \t [1.41573379 2.048     ]. \t  -0.3637847357520435 \t -0.003469710241063105\n",
      "96     \t [0.20872643 0.02530279]. \t  -0.6594709871541551 \t -0.003469710241063105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.03110902 1.07090098]. \t  -0.0069201721416625875 \t -0.003469710241063105\n",
      "98     \t [0.92837511 0.80752415]. \t  -0.3005897976390527 \t -0.003469710241063105\n",
      "99     \t [1.03103833 1.05763369]. \t  -0.003886243538881619 \t -0.003469710241063105\n",
      "100    \t [0.21520556 0.02056289]. \t  -0.6822113851315443 \t -0.003469710241063105\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
      "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
      "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
      "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
      "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
      "1      \t [ 0.55777892 -0.64921412]. \t  -92.41920807265583 \t -4.219752052396591\n",
      "2      \t [-2.048      1.7410763]. \t  -611.1229179942204 \t -4.219752052396591\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -4.219752052396591\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -4.219752052396591\n",
      "5      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -4.219752052396591\n",
      "6      \t [2.048      0.40364038]. \t  -1438.0113717369084 \t -4.219752052396591\n",
      "7      \t [-2.048      0.3465583]. \t  -1489.805000845631 \t -4.219752052396591\n",
      "8      \t [0.6859272 2.048    ]. \t  -248.95048897241927 \t -4.219752052396591\n",
      "9      \t [ 1.87982448e-04 -2.04800000e+00]. \t  -420.43003854464126 \t -4.219752052396591\n",
      "10     \t [-1.07776829  2.048     ]. \t  -82.89036670653961 \t -4.219752052396591\n",
      "11     \t [-0.21729783 -1.0962444 ]. \t  -132.23251895288564 \t -4.219752052396591\n",
      "12     \t [0.70671119 0.23733615]. \t  -6.955898486479032 \t -4.219752052396591\n",
      "13     \t [1.167731   1.28230229]. \t  \u001b[92m-0.6889953683780363\u001b[0m \t -0.6889953683780363\n",
      "14     \t [-0.76381311  0.49028004]. \t  -3.978364184243369 \t -0.6889953683780363\n",
      "15     \t [-0.26816916  2.048     ]. \t  -392.09956440558017 \t -0.6889953683780363\n",
      "16     \t [0.82807515 0.93136921]. \t  -6.064478925177963 \t -0.6889953683780363\n",
      "17     \t [ 0.2556457  -0.19829461]. \t  -7.5051604635988545 \t -0.6889953683780363\n",
      "18     \t [1.37008995 2.048     ]. \t  -3.0560595310359417 \t -0.6889953683780363\n",
      "19     \t [1.09982981 1.62932789]. \t  -17.624966027581767 \t -0.6889953683780363\n",
      "20     \t [ 0.44092893 -1.55163482]. \t  -305.182718546565 \t -0.6889953683780363\n",
      "21     \t [0.23518484 0.44730532]. \t  -15.950825400327307 \t -0.6889953683780363\n",
      "22     \t [-1.21638508  1.62173745]. \t  -6.932877024097681 \t -0.6889953683780363\n",
      "23     \t [ 2.048      -0.73587182]. \t  -2431.761664599168 \t -0.6889953683780363\n",
      "24     \t [-1.5867588  2.048    ]. \t  -28.762852384031994 \t -0.6889953683780363\n",
      "25     \t [-2.048      -0.77884895]. \t  -2482.5153342764875 \t -0.6889953683780363\n",
      "26     \t [1.42388603 1.66655496]. \t  -13.204305898071482 \t -0.6889953683780363\n",
      "27     \t [ 0.13572811 -0.86599217]. \t  -78.96582940473675 \t -0.6889953683780363\n",
      "28     \t [0.48236513 0.14794591]. \t  -0.9858667413693276 \t -0.6889953683780363\n",
      "29     \t [-0.8222387 -2.048    ]. \t  -745.3798208633226 \t -0.6889953683780363\n",
      "30     \t [ 0.81513489 -2.048     ]. \t  -735.7698998197741 \t -0.6889953683780363\n",
      "31     \t [-0.60534142  0.69523166]. \t  -13.387632515810648 \t -0.6889953683780363\n",
      "32     \t [-1.01305358 -1.19159335]. \t  -495.9475247467044 \t -0.6889953683780363\n",
      "33     \t [0.91719393 0.71505131]. \t  -1.5993342707531601 \t -0.6889953683780363\n",
      "34     \t [1.29635181 1.7233288 ]. \t  \u001b[92m-0.27101504225348194\u001b[0m \t -0.27101504225348194\n",
      "35     \t [-0.37783981  0.19026914]. \t  -2.1241266305786026 \t -0.27101504225348194\n",
      "36     \t [2.048      1.32238426]. \t  -825.8906046193907 \t -0.27101504225348194\n",
      "37     \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.27101504225348194\n",
      "38     \t [-1.40516192  2.048     ]. \t  -6.3253226808566 \t -0.27101504225348194\n",
      "39     \t [ 1.2310687  -1.34923153]. \t  -820.7393384391203 \t -0.27101504225348194\n",
      "40     \t [ 0.00033277 -0.00258925]. \t  -1.0000050472380373 \t -0.27101504225348194\n",
      "41     \t [0.6797163  0.40488729]. \t  -0.42893061710449704 \t -0.27101504225348194\n",
      "42     \t [-1.40766217  1.04589   ]. \t  -93.33583851437999 \t -0.27101504225348194\n",
      "43     \t [0.26658661 0.10222227]. \t  -0.63495143986112 \t -0.27101504225348194\n",
      "44     \t [1.4471665 2.048    ]. \t  -0.41424243342874434 \t -0.27101504225348194\n",
      "45     \t [-1.28212519  1.43776806]. \t  -9.454865768380749 \t -0.27101504225348194\n",
      "46     \t [1.16360133 1.4881267 ]. \t  -1.8266196651846176 \t -0.27101504225348194\n",
      "47     \t [1.37550702 1.8790076 ]. \t  \u001b[92m-0.15793663931008753\u001b[0m \t -0.15793663931008753\n",
      "48     \t [1.08573381 1.20856769]. \t  \u001b[92m-0.09585522343538208\u001b[0m \t -0.09585522343538208\n",
      "49     \t [-1.40178204  1.87765604]. \t  -6.531329308146922 \t -0.09585522343538208\n",
      "50     \t [0.74300816 0.49821486]. \t  -0.35598692543990595 \t -0.09585522343538208\n",
      "51     \t [0.98222681 0.98833188]. \t  \u001b[92m-0.05583441063403874\u001b[0m \t -0.05583441063403874\n",
      "52     \t [1.30385858 1.75783614]. \t  -0.4262862455807078 \t -0.05583441063403874\n",
      "53     \t [1.16974342 1.38273188]. \t  \u001b[92m-0.049641675678956665\u001b[0m \t -0.049641675678956665\n",
      "54     \t [0.26995751 0.05297039]. \t  -0.5725895924315585 \t -0.049641675678956665\n",
      "55     \t [0.73062496 0.53485252]. \t  -0.07267100908182662 \t -0.049641675678956665\n",
      "56     \t [1.33517181 1.79109525]. \t  -0.11941544961368673 \t -0.049641675678956665\n",
      "57     \t [1.0107756  1.03877757]. \t  \u001b[92m-0.029392175961909926\u001b[0m \t -0.029392175961909926\n",
      "58     \t [1.10628997 1.21562296]. \t  \u001b[92m-0.018111289393614297\u001b[0m \t -0.018111289393614297\n",
      "59     \t [1.18749388 1.41754923]. \t  -0.040641062221131154 \t -0.018111289393614297\n",
      "60     \t [1.4199054  2.03499426]. \t  -0.2119014890051892 \t -0.018111289393614297\n",
      "61     \t [1.34257212 1.80174545]. \t  -0.11741257102273009 \t -0.018111289393614297\n",
      "62     \t [1.25494899 1.55718366]. \t  -0.09637508149398595 \t -0.018111289393614297\n",
      "63     \t [1.1453553  1.33684103]. \t  -0.08363951537733134 \t -0.018111289393614297\n",
      "64     \t [1.15343103 1.35659059]. \t  -0.09211928440511866 \t -0.018111289393614297\n",
      "65     \t [0.73195914 0.517394  ]. \t  -0.10559223020066967 \t -0.018111289393614297\n",
      "66     \t [1.31873449 1.76089511]. \t  -0.14926607349617096 \t -0.018111289393614297\n",
      "67     \t [1.28583575 1.65457951]. \t  -0.08184750424165917 \t -0.018111289393614297\n",
      "68     \t [1.17443747 1.38313662]. \t  -0.031897816596806926 \t -0.018111289393614297\n",
      "69     \t [0.848729   0.69350936]. \t  -0.09487616486289106 \t -0.018111289393614297\n",
      "70     \t [0.7284284  0.49569926]. \t  -0.19561273230144782 \t -0.018111289393614297\n",
      "71     \t [1.20456308 1.49484366]. \t  -0.2343164702767194 \t -0.018111289393614297\n",
      "72     \t [0.97955475 0.94901259]. \t  \u001b[92m-0.011474348161723396\u001b[0m \t -0.011474348161723396\n",
      "73     \t [1.23670764 1.53498957]. \t  -0.05910386582939608 \t -0.011474348161723396\n",
      "74     \t [1.13084637 1.29294362]. \t  -0.03708677712082101 \t -0.011474348161723396\n",
      "75     \t [0.73791785 0.54743066]. \t  -0.06953264422734338 \t -0.011474348161723396\n",
      "76     \t [0.57634932 0.31257715]. \t  -0.21790135834678698 \t -0.011474348161723396\n",
      "77     \t [0.89835093 0.79957502]. \t  -0.015896748045800414 \t -0.011474348161723396\n",
      "78     \t [1.05194689 1.1473068 ]. \t  -0.16846590804486164 \t -0.011474348161723396\n",
      "79     \t [1.05873458 1.17706184]. \t  -0.3186526169276534 \t -0.011474348161723396\n",
      "80     \t [0.79455101 0.62054689]. \t  -0.05379657612913584 \t -0.011474348161723396\n",
      "81     \t [0.77569672 0.56063308]. \t  -0.2190055114662125 \t -0.011474348161723396\n",
      "82     \t [0.62431764 0.3766679 ]. \t  -0.1583103247320686 \t -0.011474348161723396\n",
      "83     \t [1.29494904 1.67276479]. \t  -0.08869915598555525 \t -0.011474348161723396\n",
      "84     \t [1.15971811 1.3459108 ]. \t  -0.025602939899820345 \t -0.011474348161723396\n",
      "85     \t [1.12407182 1.29383605]. \t  -0.10719428138259958 \t -0.011474348161723396\n",
      "86     \t [0.26231476 0.0737927 ]. \t  -0.5466632026641928 \t -0.011474348161723396\n",
      "87     \t [0.87499096 0.74409058]. \t  -0.0619322451656779 \t -0.011474348161723396\n",
      "88     \t [1.28950816 1.65392344]. \t  -0.09174995833229087 \t -0.011474348161723396\n",
      "89     \t [0.23477136 0.09610098]. \t  -0.7535386607422676 \t -0.011474348161723396\n",
      "90     \t [0.26178775 0.08620701]. \t  -0.576194992648963 \t -0.011474348161723396\n",
      "91     \t [0.72896395 0.54696825]. \t  -0.09773360590918789 \t -0.011474348161723396\n",
      "92     \t [1.23408871 1.53049355]. \t  -0.06045045780297801 \t -0.011474348161723396\n",
      "93     \t [0.13926746 0.00032572]. \t  -0.7772258630919305 \t -0.011474348161723396\n",
      "94     \t [ 1.34901578 -0.41084906]. \t  -497.72077037803615 \t -0.011474348161723396\n",
      "95     \t [1.20888243 1.45113061]. \t  -0.0541711681473207 \t -0.011474348161723396\n",
      "96     \t [1.05875851 1.13250571]. \t  -0.01676076629462157 \t -0.011474348161723396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.26553688 1.56267662]. \t  -0.2218851256146382 \t -0.011474348161723396\n",
      "98     \t [0.7535567  0.55008279]. \t  -0.092293468505454 \t -0.011474348161723396\n",
      "99     \t [1.43169611 2.048     ]. \t  -0.1866690927393541 \t -0.011474348161723396\n",
      "100    \t [1.40414726 1.97842573]. \t  -0.16795383564947297 \t -0.011474348161723396\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.777577453542735\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.777577453542735\n",
      "3      \t [-0.49141028  2.048     ]. \t  -328.57428934260986 \t -3.777577453542735\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.777577453542735\n",
      "5      \t [2.048      0.46329361]. \t  -1393.1421580488152 \t -3.777577453542735\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.777577453542735\n",
      "7      \t [-0.12563239 -2.048     ]. \t  -427.18728097759015 \t -3.777577453542735\n",
      "8      \t [0.74132347 2.048     ]. \t  -224.59901152197614 \t -3.777577453542735\n",
      "9      \t [0.06575504 1.13433903]. \t  -128.56627243314702 \t -3.777577453542735\n",
      "10     \t [-0.73884094 -0.63405803]. \t  -142.25034389375267 \t -3.777577453542735\n",
      "11     \t [-1.10964751  1.24231781]. \t  -4.46271308440803 \t -3.777577453542735\n",
      "12     \t [ 1.06317267 -0.51021587]. \t  -269.1450779057454 \t -3.777577453542735\n",
      "13     \t [-0.59737031  0.86750089]. \t  -28.627893161653 \t -3.777577453542735\n",
      "14     \t [1.10792163 1.25834947]. \t  \u001b[92m-0.10687571544549501\u001b[0m \t -0.10687571544549501\n",
      "15     \t [-1.27955252  2.048     ]. \t  -22.067534560919498 \t -0.10687571544549501\n",
      "16     \t [-0.38043862 -1.34913973]. \t  -225.07134637738693 \t -0.10687571544549501\n",
      "17     \t [ 0.58782134 -1.32347613]. \t  -278.72934644855934 \t -0.10687571544549501\n",
      "18     \t [ 0.50426801 -0.5237264 ]. \t  -60.77611568884244 \t -0.10687571544549501\n",
      "19     \t [1.36333064 2.048     ]. \t  -3.7165775008917823 \t -0.10687571544549501\n",
      "20     \t [1.14729642 1.65995649]. \t  -11.83242610899533 \t -0.10687571544549501\n",
      "21     \t [-2.048       1.22393961]. \t  -891.5967642688776 \t -0.10687571544549501\n",
      "22     \t [0.72453188 0.97602834]. \t  -20.42336998737157 \t -0.10687571544549501\n",
      "23     \t [-1.01963569  1.61200054]. \t  -36.836649410257586 \t -0.10687571544549501\n",
      "24     \t [-2.048      -0.69749971]. \t  -2402.2646560823077 \t -0.10687571544549501\n",
      "25     \t [-0.11746203 -0.00533164]. \t  -1.2853131311091908 \t -0.10687571544549501\n",
      "26     \t [ 2.048      -0.76623936]. \t  -2461.7973500302087 \t -0.10687571544549501\n",
      "27     \t [0.97083942 0.32257971]. \t  -38.434584879049645 \t -0.10687571544549501\n",
      "28     \t [ 0.64036299 -2.048     ]. \t  -604.3375742143866 \t -0.10687571544549501\n",
      "29     \t [1.47169386 1.61429729]. \t  -30.64715432095631 \t -0.10687571544549501\n",
      "30     \t [-0.98963509  0.7269022 ]. \t  -10.333031022953987 \t -0.10687571544549501\n",
      "31     \t [-0.82472072 -2.048     ]. \t  -747.6176281419557 \t -0.10687571544549501\n",
      "32     \t [-0.19975235  0.40196354]. \t  -14.548333952760265 \t -0.10687571544549501\n",
      "33     \t [0.72185381 0.5368627 ]. \t  \u001b[92m-0.1022970220840386\u001b[0m \t -0.1022970220840386\n",
      "34     \t [-1.40477749  1.76891436]. \t  -9.964383711151061 \t -0.1022970220840386\n",
      "35     \t [0.62936456 0.1429094 ]. \t  -6.547906101924662 \t -0.1022970220840386\n",
      "36     \t [1.27434783 1.59962156]. \t  -0.1345143172668743 \t -0.1022970220840386\n",
      "37     \t [-1.49289867  2.048     ]. \t  -9.48147074496088 \t -0.1022970220840386\n",
      "38     \t [1.52027433 2.048     ]. \t  -7.199900968095417 \t -0.1022970220840386\n",
      "39     \t [-0.74351377  0.58362465]. \t  -3.1347777069016907 \t -0.1022970220840386\n",
      "40     \t [-1.19088712 -0.13121327]. \t  -244.8718941401432 \t -0.1022970220840386\n",
      "41     \t [ 1.30442478 -1.50913076]. \t  -1030.923081344002 \t -0.1022970220840386\n",
      "42     \t [0.95358468 0.87697976]. \t  -0.10676764526368056 \t -0.1022970220840386\n",
      "43     \t [ 0.10887158 -0.01583425]. \t  -0.8707683520946806 \t -0.1022970220840386\n",
      "44     \t [1.36183748 1.86739726]. \t  -0.1472999472486181 \t -0.1022970220840386\n",
      "45     \t [1.24470587 1.56652859]. \t  \u001b[92m-0.08958855138342464\u001b[0m \t -0.08958855138342464\n",
      "46     \t [0.99085856 1.05291506]. \t  -0.5058088612086309 \t -0.08958855138342464\n",
      "47     \t [0.78030723 0.56902694]. \t  -0.20708650062071815 \t -0.08958855138342464\n",
      "48     \t [0.74159908 0.53070409]. \t  -0.10388545336371828 \t -0.08958855138342464\n",
      "49     \t [1.30028115 1.68996506]. \t  -0.09022744165524908 \t -0.08958855138342464\n",
      "50     \t [-1.33470723  1.86802542]. \t  -6.200502526307622 \t -0.08958855138342464\n",
      "51     \t [1.19134639 1.41015727]. \t  \u001b[92m-0.04498378365344856\u001b[0m \t -0.04498378365344856\n",
      "52     \t [1.4070328  1.96561523]. \t  -0.18563031521400258 \t -0.04498378365344856\n",
      "53     \t [0.73658869 0.51165424]. \t  -0.16492002604542402 \t -0.04498378365344856\n",
      "54     \t [1.03056479 1.09335567]. \t  -0.09885236696914732 \t -0.04498378365344856\n",
      "55     \t [1.25886967 1.59229426]. \t  -0.07270079765243244 \t -0.04498378365344856\n",
      "56     \t [0.87455866 0.77278224]. \t  \u001b[92m-0.022023055662431804\u001b[0m \t -0.022023055662431804\n",
      "57     \t [0.69607727 0.44671314]. \t  -0.23533187632035324 \t -0.022023055662431804\n",
      "58     \t [1.41017188 1.97776983]. \t  -0.17993714671014258 \t -0.022023055662431804\n",
      "59     \t [1.24722182 1.55794251]. \t  -0.06168518500379787 \t -0.022023055662431804\n",
      "60     \t [1.17397494 1.40563929]. \t  -0.10546463932844857 \t -0.022023055662431804\n",
      "61     \t [1.03771163 1.090337  ]. \t  \u001b[92m-0.019624425639081764\u001b[0m \t -0.019624425639081764\n",
      "62     \t [-0.73174588  0.39659807]. \t  -4.92698599506355 \t -0.019624425639081764\n",
      "63     \t [0.8109387  0.69435157]. \t  -0.17065339833066273 \t -0.019624425639081764\n",
      "64     \t [1.17814563 1.40216257]. \t  -0.05171693977828352 \t -0.019624425639081764\n",
      "65     \t [0.75161947 0.53238372]. \t  -0.16763082238404275 \t -0.019624425639081764\n",
      "66     \t [1.07102583 1.14227638]. \t  \u001b[92m-0.0073678672885846555\u001b[0m \t -0.0073678672885846555\n",
      "67     \t [1.17871599 1.40787125]. \t  -0.06616388717160579 \t -0.0073678672885846555\n",
      "68     \t [1.07933893 1.17557753]. \t  -0.01754126093029469 \t -0.0073678672885846555\n",
      "69     \t [1.44461202 2.048     ]. \t  -0.34903112118088614 \t -0.0073678672885846555\n",
      "70     \t [1.35005742 1.79989217]. \t  -0.17435495679381896 \t -0.0073678672885846555\n",
      "71     \t [1.27197906 1.60919212]. \t  -0.08160894722426373 \t -0.0073678672885846555\n",
      "72     \t [0.75668241 0.54073483]. \t  -0.16054022085441072 \t -0.0073678672885846555\n",
      "73     \t [1.14615881 1.35442465]. \t  -0.18737489220688358 \t -0.0073678672885846555\n",
      "74     \t [1.09253888 1.20403353]. \t  -0.019363492099804456 \t -0.0073678672885846555\n",
      "75     \t [1.215303   1.45659021]. \t  -0.08785382736299563 \t -0.0073678672885846555\n",
      "76     \t [1.40957276 2.04799996]. \t  -0.5411269548147771 \t -0.0073678672885846555\n",
      "77     \t [0.76675122 0.58497959]. \t  -0.055262219989357035 \t -0.0073678672885846555\n",
      "78     \t [ 0.02147833 -0.02953327]. \t  -1.0474721930627326 \t -0.0073678672885846555\n",
      "79     \t [0.78726071 0.6089738 ]. \t  -0.05693417332292169 \t -0.0073678672885846555\n",
      "80     \t [1.08723655 1.20790692]. \t  -0.07429605994089172 \t -0.0073678672885846555\n",
      "81     \t [0.94159171 0.87789667]. \t  -0.010977516465638146 \t -0.0073678672885846555\n",
      "82     \t [0.80531027 0.63374995]. \t  -0.05973319803048751 \t -0.0073678672885846555\n",
      "83     \t [1.07460349 1.19175476]. \t  -0.14233329265875716 \t -0.0073678672885846555\n",
      "84     \t [0.66120468 0.45603165]. \t  -0.1502768800814033 \t -0.0073678672885846555\n",
      "85     \t [1.08327869 1.19873712]. \t  -0.07066328126271695 \t -0.0073678672885846555\n",
      "86     \t [1.13670618 1.31270205]. \t  -0.061129110348498814 \t -0.0073678672885846555\n",
      "87     \t [1.17319568 1.39488785]. \t  -0.06422076591302235 \t -0.0073678672885846555\n",
      "88     \t [1.13817857 1.32841821]. \t  -0.1277805644477598 \t -0.0073678672885846555\n",
      "89     \t [0.94831781 0.94989871]. \t  -0.25862657529395705 \t -0.0073678672885846555\n",
      "90     \t [0.99617259 1.02826299]. \t  -0.12891827876907466 \t -0.0073678672885846555\n",
      "91     \t [ 1.275626 -2.048   ]. \t  -1350.801424081381 \t -0.0073678672885846555\n",
      "92     \t [1.17648345 1.40644653]. \t  -0.0810237468682157 \t -0.0073678672885846555\n",
      "93     \t [1.16868041 1.39940252]. \t  -0.141272631083396 \t -0.0073678672885846555\n",
      "94     \t [0.69509719 0.4380506 ]. \t  -0.29645242169841995 \t -0.0073678672885846555\n",
      "95     \t [1.41220492 2.02436214]. \t  -0.26014940703822453 \t -0.0073678672885846555\n",
      "96     \t [0.86086815 0.73837552]. \t  -0.020096669464505515 \t -0.0073678672885846555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.71805351 0.47217066]. \t  -0.2681118787026915 \t -0.0073678672885846555\n",
      "98     \t [1.25514242 1.57017525]. \t  -0.06780919837119254 \t -0.0073678672885846555\n",
      "99     \t [0.87765521 0.8259362 ]. \t  -0.32474442445436835 \t -0.0073678672885846555\n",
      "100    \t [1.1986307  1.45917217]. \t  -0.08988415257212753 \t -0.0073678672885846555\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.7382985562695135, -4.914216210786352)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.578961856157758, -7.931186533597945)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.423724656799146, -4.713481561202813)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.805857922469597, -7.610560280040693)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.163924777679048, -5.853946633366838)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.49470853304788, -6.905903788055976)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.1406456930929565, -7.265251611582902)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.519425086485754, -6.171869298629812)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.629380121121366, -5.568999653614086)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.174907200404692, -7.7634817434197885)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.192581773904122, -5.594812762951899)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.92524445582635, -4.445718413059077)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.556308931050801, -7.587716711511194)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.118283958431365, -6.88228461316991)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.127312051267847, -7.426183395919943)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.755273764345566, -4.997364238487369)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3190113124632847, -4.806888934472219)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.836586915668434, -5.66368419252642)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.166481028093252, -4.467641329727569)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.263532134808551, -4.910626992055536)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.263532134808551, -4.910626992055536)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
