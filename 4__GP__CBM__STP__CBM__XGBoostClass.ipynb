{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Classification - 'real-world' example: UCI Skin Segmentation dataset\n",
    "\n",
    "GP CBM versus STP nu = 3 CBM (winner)\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some default Python modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential, matern32, matern52\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    #Data in format [B G R Label] from\n",
    "    data = np.genfromtxt('/home/ulsterconorc/Downloads/Skin_NonSkin.txt', dtype=np.int32)\n",
    "\n",
    "    labels = data[:,3]\n",
    "    data = data[:,0:3]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = ReadData()\n",
    "\n",
    "X = data\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayesian Optimization - inputs:\n",
    "\n",
    "obj_func = 'XGBoost'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'CBMinimized'\n",
    "util_winner = 'tCBMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "test_perc = 0.15\n",
    "train_perc = 1 - test_perc\n",
    "n_est = 2\n",
    "\n",
    "obj_classifier = 'binary:logistic'\n",
    "cov_func = squaredExponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'XGBoost':\n",
    "    \n",
    "    # Constraints:\n",
    "    param_lb_alpha = 0\n",
    "    param_ub_alpha = 10\n",
    "    \n",
    "    param_lb_gamma = 0\n",
    "    param_ub_gamma = 10\n",
    "    \n",
    "    param_lb_max_depth = 5\n",
    "    param_ub_max_depth = 15\n",
    "    \n",
    "    param_lb_min_child_weight = 1\n",
    "    param_ub_min_child_weight = 20\n",
    "    \n",
    "    param_lb_subsample = .5\n",
    "    param_ub_subsample = 1\n",
    "    \n",
    "    param_lb_colsample = .1\n",
    "    param_ub_colsample = 1\n",
    "    \n",
    "    # 6-D inputs' parameter bounds:\n",
    "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
    "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
    "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
    "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
    "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
    "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
    "        }\n",
    "       \n",
    "    # True y bounds:\n",
    "    y_global_orig = 1\n",
    "    dim = 6\n",
    "    \n",
    "    max_iter = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 44444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 7777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 888\n",
    "run_num_15 = 1557\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - CBM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'CBMinimized': self.CBMinimized,\n",
    "            'tCBMinimized': self.tCBMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def CBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    \n",
    "    def tCBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.22421779 6.94445632 6.5712523  0.88103706 5.42874773 0.76880439]. \t  0.9779930741227703 \t 0.9846806025668978\n",
      "2      \t [3.76679194 6.9790262  7.00193195 0.92064317 4.86068331 0.89106459]. \t  0.9821985819156532 \t 0.9846806025668978\n",
      "3      \t [4.40051285 6.6489461  6.85876242 0.94985398 4.70171366 0.99030592]. \t  0.9779498664490864 \t 0.9846806025668978\n",
      "4      \t [3.83137218 6.63142611 6.33385058 0.99952837 4.76580902 0.99667067]. \t  0.9780218792155114 \t 0.9846806025668978\n",
      "5      \t [3.992105   6.55576469 6.72852903 0.94358975 4.78460433 0.33456891]. \t  0.8878721872887638 \t 0.9846806025668978\n",
      "6      \t [3.96199674 6.59913835 6.68703132 0.5        4.84889614 1.        ]. \t  \u001b[92m0.9887564933542389\u001b[0m \t 0.9887564933542389\n",
      "7      \t [4.46681765 7.62029267 6.19140212 0.79483841 5.09552989 0.71929483]. \t  0.9771433347079063 \t 0.9887564933542389\n",
      "8      \t [4.82484865 7.39150477 5.87131689 1.         5.50846295 1.        ]. \t  0.9842293241173986 \t 0.9887564933542389\n",
      "9      \t [4.17844127 7.72845891 5.86335506 1.         5.67104987 1.        ]. \t  0.9842293241173986 \t 0.9887564933542389\n",
      "10     \t [4.38783618 7.36469789 5.55532988 0.62279237 5.55674987 0.60942322]. \t  0.8856253846617331 \t 0.9887564933542389\n",
      "11     \t [4.57862115 7.67872203 6.18682504 0.5        5.73067038 1.        ]. \t  0.9886508750496213 \t 0.9887564933542389\n",
      "12     \t [4.76206708 8.13591738 5.57498344 0.69578698 5.54729393 1.        ]. \t  0.9841285059816572 \t 0.9887564933542389\n",
      "13     \t [ 5.15456083  2.40373428 10.58200293  0.81154094 14.20720268  0.76828586]. \t  0.9846373948240704 \t 0.9887564933542389\n",
      "14     \t [ 4.87890077  2.845944   10.47930624  0.82734195 13.73884708  0.77546142]. \t  0.9844741655042449 \t 0.9887564933542389\n",
      "15     \t [ 4.59868818  2.77173783 10.89848672  0.80508429 14.275676    0.68643152]. \t  0.9846421956152408 \t 0.9887564933542389\n",
      "16     \t [ 5.06746783  2.93464579 10.55207311  0.96147683 14.22797575  0.24003747]. \t  0.8887747490278275 \t 0.9887564933542389\n",
      "17     \t [ 4.83502523  2.37021167 10.95161368  0.95961739 13.74700033  0.42353801]. \t  0.8887651473071991 \t 0.9887564933542389\n",
      "18     \t [ 4.95142569  2.6678583  10.72380217  0.5        13.9645264   0.37021097]. \t  0.8876993571471793 \t 0.9887564933542389\n",
      "19     \t [3.71223147 6.36302284 7.01032035 0.92850055 4.23487857 1.        ]. \t  \u001b[92m0.9928851900787028\u001b[0m \t 0.9928851900787028\n",
      "20     \t [ 5.06968546  3.17087711 10.48509116  0.8175513  14.47121408  1.        ]. \t  \u001b[92m0.9947383072940582\u001b[0m \t 0.9947383072940582\n",
      "21     \t [4.80915684 8.24196033 6.30269973 1.         5.56993303 1.        ]. \t  0.9893277890246779 \t 0.9947383072940582\n",
      "22     \t [ 4.77731473  3.49269359 10.91419689  1.         14.03844837  1.        ]. \t  \u001b[92m0.9949303393557365\u001b[0m \t 0.9949303393557365\n",
      "23     \t [4.29898797 8.52778116 6.0593659  0.63207386 5.60956423 0.78682479]. \t  0.977671420008054 \t 0.9949303393557365\n",
      "24     \t [4.55374738 8.59943822 5.87520276 0.95939218 6.14924219 1.        ]. \t  0.9841765154145242 \t 0.9949303393557365\n",
      "25     \t [4.18779982 8.33355407 6.56382146 0.97493727 6.07530625 1.        ]. \t  0.9891405583073204 \t 0.9949303393557365\n",
      "26     \t [4.66510624 8.71675581 6.46896455 0.5        6.20013147 1.        ]. \t  0.9887036843056457 \t 0.9949303393557365\n",
      "27     \t [4.52579117 8.95355495 6.56514068 1.         6.14965077 0.57542989]. \t  0.8984099747975832 \t 0.9949303393557365\n",
      "28     \t [4.09650883 8.62585867 6.25859689 0.50371532 6.51858653 0.68444046]. \t  0.9759575222794504 \t 0.9949303393557365\n",
      "29     \t [4.55678896 8.48147755 6.54469758 0.88692399 6.85147578 0.88749486]. \t  0.9779402653507523 \t 0.9949303393557365\n",
      "30     \t [4.14478081 9.10997877 6.57376735 0.82481275 6.81117491 1.        ]. \t  0.9890493416847762 \t 0.9949303393557365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06038052756033965"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, \n",
    "                       min_child_weight=min_child_weight,colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_1 = loser_1.getResult()[0]\n",
    "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
    "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
    "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
    "\n",
    "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
    "rmse_loser_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 2.64876414  7.92084049  6.37658483  0.88984319 12.25127731  0.72213678]. \t  0.9906384035610044 \t 0.9907392220424587\n",
      "2      \t [ 3.18102662  8.25027287  6.45248494  0.90696894 11.935455    0.7867412 ]. \t  \u001b[92m0.990873645301542\u001b[0m \t 0.990873645301542\n",
      "3      \t [ 3.05946758  8.38997801  6.17075734  0.90277152 12.60037909  0.77850887]. \t  \u001b[92m0.9908784454704284\u001b[0m \t 0.9908784454704284\n",
      "4      \t [ 2.64615133  8.63078557  6.67058987  0.93231129 12.26764888  0.73417894]. \t  0.9907344211821445 \t 0.9908784454704284\n",
      "5      \t [ 2.91752315  8.35978756  6.38379221  0.65684174 12.24401664  0.1747138 ]. \t  0.8960096061634673 \t 0.9908784454704284\n",
      "6      \t [ 2.77735539  8.39776261  6.34397944  0.5        12.20424053  1.        ]. \t  0.9885452595782814 \t 0.9908784454704284\n",
      "7      \t [ 6.26454771  9.72764931  9.40010488  0.75457162 14.96533849  0.90947694]. \t  0.9905087880765141 \t 0.9908784454704284\n",
      "8      \t [ 6.29854602  9.47865248  9.70877522  0.68903242 14.38371904  0.88346825]. \t  0.990672015667769 \t 0.9908784454704284\n",
      "9      \t [ 5.73130114  9.47515021  9.83009105  0.70220045 14.84849223  0.82682865]. \t  0.9904175753259699 \t 0.9908784454704284\n",
      "10     \t [ 6.25593672  8.98304882  9.59436972  0.75075798 14.95587962  0.9024605 ]. \t  0.9905087880765141 \t 0.9908784454704284\n",
      "11     \t [ 6.2159251   9.39829568  9.62990631  0.7774984  14.82075045  0.25097051]. \t  0.8904406383452875 \t 0.9908784454704284\n",
      "12     \t [ 8.06446025  4.47437107 12.11828865  0.83519143 15.49060931  0.77005458]. \t  \u001b[92m0.9909984821896959\u001b[0m \t 0.9909984821896959\n",
      "13     \t [ 7.84939419  4.41798442 11.58817709  0.80539793 15.89809964  0.71060094]. \t  0.9909264643067025 \t 0.9909984821896959\n",
      "14     \t [ 8.19472327  5.03135048 11.82379742  0.84007347 15.866831    0.76103617]. \t  \u001b[92m0.9910848947713538\u001b[0m \t 0.9910848947713538\n",
      "15     \t [ 8.42041045  4.54706323 11.48046793  0.81082024 15.50487527  0.99906243]. \t  0.9909696714963877 \t 0.9910848947713538\n",
      "16     \t [ 8.37262111  4.57215945 11.69315871  0.50053968 15.63858859  0.32364998]. \t  0.892860225479675 \t 0.9910848947713538\n",
      "17     \t [ 8.01901054  4.64006085 11.93539237  0.5        15.88972851  1.        ]. \t  \u001b[92m0.9929284016234465\u001b[0m \t 0.9929284016234465\n",
      "18     \t [ 6.08989366  9.14873957 10.15112872  1.         14.62348661  1.        ]. \t  \u001b[92m0.9948487230702795\u001b[0m \t 0.9948487230702795\n",
      "19     \t [ 5.82491843  9.24556612  9.28929408  1.         14.53179234  1.        ]. \t  0.9939269664636649 \t 0.9948487230702795\n",
      "20     \t [ 5.95736661  9.38071905  9.73358317  1.         15.44279865  1.        ]. \t  0.9939509711800923 \t 0.9948487230702795\n",
      "21     \t [ 6.00113434 10.          9.87856299  1.         14.59597269  1.        ]. \t  0.9939461699049215 \t 0.9948487230702795\n",
      "22     \t [ 6.78377609  9.36078261  9.55577448  1.         14.70237002  1.        ]. \t  0.9939365684608643 \t 0.9948487230702795\n",
      "23     \t [ 3.12354795  8.17460219  6.95032169  0.76940074 12.59303585  0.85902638]. \t  0.9904607800265147 \t 0.9948487230702795\n",
      "24     \t [ 8.42375423  4.70134284 11.26811513  0.70095114 16.26250757  0.94796902]. \t  0.9908064426605626 \t 0.9948487230702795\n",
      "25     \t [ 3.41329105  8.82294038  6.79264176  0.66427185 12.44870808  0.83863479]. \t  0.9895342229744427 \t 0.9948487230702795\n",
      "26     \t [ 2.9807872   8.74460735  6.85167926  0.5        13.00601565  0.71519492]. \t  0.9878011377072881 \t 0.9948487230702795\n",
      "27     \t [ 8.01259513  5.00997581 11.05804875  0.5        15.79534204  0.94230484]. \t  0.9893037869342146 \t 0.9948487230702795\n",
      "28     \t [ 3.33902986  8.72496543  7.01254629  1.         12.92599718  0.37694487]. \t  0.8870752445301892 \t 0.9948487230702795\n",
      "29     \t [ 8.29216882  4.9144933  10.86681938  1.         15.86794631  0.55328093]. \t  0.8869744264635915 \t 0.9948487230702795\n",
      "30     \t [ 3.05916627  8.76474299  7.40225584  0.5        12.51841071  0.52278719]. \t  0.892519391086485 \t 0.9948487230702795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06308217274420128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_2 = loser_2.getResult()[0]\n",
    "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
    "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
    "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
    "\n",
    "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
    "rmse_loser_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [5.79248939 4.41050568 9.55581296 0.70735615 9.12056036 0.69497557]. \t  0.9848342349370162 \t 0.9850406749037081\n",
      "2      \t [6.26998599 4.00288138 9.52711102 0.69768476 8.81448361 0.80964412]. \t  0.9843109490451631 \t 0.9850406749037081\n",
      "3      \t [6.24832619 4.04399497 9.73347939 0.69494946 9.53011674 0.81447131]. \t  \u001b[92m0.9853863342880081\u001b[0m \t 0.9853863342880081\n",
      "4      \t [6.49706306 4.49087231 9.39238248 0.69229871 9.22508958 0.46742808]. \t  0.8851981309112206 \t 0.9853863342880081\n",
      "5      \t [6.05073294 3.97646683 9.89578742 0.51509648 9.09664974 0.26562887]. \t  0.8854285670896607 \t 0.9853863342880081\n",
      "6      \t [ 5.86595275  3.92597276 10.0435522   1.          9.05338351  0.87036788]. \t  0.9669175867324311 \t 0.9853863342880081\n",
      "7      \t [6.59150938 4.02747484 9.95477172 1.         9.10810575 0.49991298]. \t  0.7998492531797377 \t 0.9853863342880081\n",
      "8      \t [5.56409233 3.83088216 9.83270982 0.5        9.10049045 1.        ]. \t  \u001b[92m0.9941334068460081\u001b[0m \t 0.9941334068460081\n",
      "9      \t [ 6.22156034  3.69490925 10.28598272  0.5         9.01295428  1.        ]. \t  0.9939893831108967 \t 0.9941334068460081\n",
      "10     \t [ 5.7127627   4.15520285 10.44023213  0.5         9.4685275   1.        ]. \t  0.9940373910226005 \t 0.9941334068460081\n",
      "11     \t [ 5.59478588  4.16370673 10.42584583  0.5         8.6664581   1.        ]. \t  0.9941190041267781 \t 0.9941334068460081\n",
      "12     \t [ 5.4885539   3.63509669 10.78820016  0.5         9.02986525  0.79908834]. \t  0.9850406773928843 \t 0.9941334068460081\n",
      "13     \t [ 5.94112994  4.05366978 11.14857737  0.5         8.95205194  1.        ]. \t  0.9940277893019722 \t 0.9941334068460081\n",
      "14     \t [ 5.61587134  4.28808421 11.01767838  0.5         9.00630673  0.42461554]. \t  0.8861342918963926 \t 0.9941334068460081\n",
      "15     \t [6.10452186 4.21788513 8.90631266 0.5        9.31205718 1.        ]. \t  0.9937733475082293 \t 0.9941334068460081\n",
      "16     \t [ 6.04968955  3.78001452 10.91154553  0.5         8.44750307  0.57579897]. \t  0.8860094711185313 \t 0.9941334068460081\n",
      "17     \t [6.02102777 3.89470926 8.85332506 1.         9.3551473  0.73705968]. \t  0.9614398593226912 \t 0.9941334068460081\n",
      "18     \t [6.36800562 4.53712233 8.91871245 1.         9.21539787 1.        ]. \t  0.9939749765887586 \t 0.9941334068460081\n",
      "19     \t [5.93947207 4.45194135 8.94977087 1.         9.86062236 1.        ]. \t  0.9939845787933933 \t 0.9941334068460081\n",
      "20     \t [6.60960978 4.15443216 8.69892703 1.         9.85200363 1.        ]. \t  0.993965374937274 \t 0.9941334068460081\n",
      "21     \t [6.28152289 4.53249187 8.34042043 0.99199326 9.79877012 0.60104095]. \t  0.8878817849990529 \t 0.9941334068460081\n",
      "22     \t [6.56681942 4.82729504 8.9459282  0.7097069  9.99295656 1.        ]. \t  0.9935141047850286 \t 0.9941334068460081\n",
      "23     \t [ 6.28861586  4.26420024  8.80830718  0.5        10.31546389  0.73745115]. \t  0.9840997152016792 \t 0.9941334068460081\n",
      "24     \t [ 6.57104109  4.53305355  9.02331961  1.         10.4160301   0.50855091]. \t  0.7999212650472934 \t 0.9941334068460081\n",
      "25     \t [ 6.17717249  3.75883139 11.06408114  0.5         9.38487839  0.53761309]. \t  0.8860526788613589 \t 0.9941334068460081\n",
      "26     \t [ 6.44087285  4.6314293   8.33726111  0.66878168 10.57116059  1.        ]. \t  \u001b[92m0.9941622120078932\u001b[0m \t 0.9941622120078932\n",
      "27     \t [ 7.0422861   4.53223464  8.35098078  0.5        10.29424973  0.87402351]. \t  0.9833555926394132 \t 0.9941622120078932\n",
      "28     \t [ 6.96494326  4.35197958  8.77077993  0.5        10.83406318  1.        ]. \t  0.9937301401802645 \t 0.9941622120078932\n",
      "29     \t [ 6.79877687  3.91008455  8.18093317  0.5        10.743741    1.        ]. \t  0.9937301401802645 \t 0.9941622120078932\n",
      "30     \t [ 6.92764998  4.29061213  8.18605086  0.5        11.08322159  0.51633396]. \t  0.8870464446387981 \t 0.9941622120078932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06732742476664018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_3 = loser_3.getResult()[0]\n",
    "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
    "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
    "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
    "\n",
    "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
    "rmse_loser_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [2.61739452 4.2914533  9.99999875 0.78987893 1.00000001 1.        ]. \t  \u001b[92m0.9952375940002973\u001b[0m \t 0.9952375940002973\n",
      "2      \t [ 2.72780037  3.92005396 10.56863183  0.73933291  1.00008404  0.99270193]. \t  0.9678729543349865 \t 0.9952375940002973\n",
      "3      \t [2.97206514 3.82867644 9.98464076 0.6940346  1.00004085 0.58699276]. \t  0.7978329500383543 \t 0.9952375940002973\n",
      "4      \t [ 2.47920798  4.06186514 10.25970799  0.93991792  1.60723203  1.        ]. \t  \u001b[92m0.9962937722065685\u001b[0m \t 0.9962937722065685\n",
      "5      \t [ 2.19492601  4.2465553  10.42351721  0.86754979  1.12511831  0.5595824 ]. \t  0.7985386629525507 \t 0.9962937722065685\n",
      "6      \t [ 2.34151111  4.02017632 10.08623904  0.5         1.46427001  1.        ]. \t  0.9959817191901444 \t 0.9962937722065685\n",
      "7      \t [1.59923401 8.53101997 6.37323643 0.73787627 8.49092788 0.82950827]. \t  0.9546227066757546 \t 0.9962937722065685\n",
      "8      \t [2.20397334 8.81731541 6.2495523  0.69463214 8.36162631 0.97135365]. \t  0.9545458937404413 \t 0.9962937722065685\n",
      "9      \t [1.99255686 8.49606037 6.70238815 0.69281824 7.95529064 0.77607025]. \t  0.9546707161086098 \t 0.9962937722065685\n",
      "10     \t [1.65849777 9.13848418 6.4570096  0.73384094 8.03308783 0.77308466]. \t  0.9547235247423521 \t 0.9962937722065685\n",
      "11     \t [1.99140962 8.75766163 6.33217254 0.99517108 8.25166365 0.29245303]. \t  0.7932674057240773 \t 0.9962937722065685\n",
      "12     \t [1.78801841 8.76182367 6.27219451 0.5        8.20888178 0.42784211]. \t  0.7948228667653142 \t 0.9962937722065685\n",
      "13     \t [1.285364   8.55615449 6.62440221 1.         7.78053433 1.        ]. \t  0.9892509802364701 \t 0.9962937722065685\n",
      "14     \t [2.01931303 8.88820185 6.40421314 1.         7.44766923 1.        ]. \t  0.9890445431046753 \t 0.9962937722065685\n",
      "15     \t [1.85474015 8.93665106 7.04543871 1.         8.20923151 1.        ]. \t  0.9927555795024133 \t 0.9962937722065685\n",
      "16     \t [ 2.99179637  4.42068863 10.41488441  0.5         1.55591501  1.        ]. \t  0.9954920427775166 \t 0.9962937722065685\n",
      "17     \t [ 3.08510247  3.70684254 10.24958627  0.52542025  1.7391284   1.        ]. \t  0.9958568964071194 \t 0.9962937722065685\n",
      "18     \t [3.08519128 4.20969512 9.69562149 0.67485134 1.80169896 1.        ]. \t  0.9950071533966452 \t 0.9962937722065685\n",
      "19     \t [ 3.50829076  4.15806698 10.21731808  1.          1.57479588  1.        ]. \t  0.9962361618827983 \t 0.9962937722065685\n",
      "20     \t [ 3.35390959  4.21963874 10.25461618  0.8698683   2.30133457  1.        ]. \t  0.9957704903248835 \t 0.9962937722065685\n",
      "21     \t [ 3.30265705  4.22194129 10.20086924  0.80580838  2.03573387  0.41424332]. \t  0.7985818628131004 \t 0.9962937722065685\n",
      "22     \t [ 3.88249293  4.24748994 10.06527808  0.5         2.08552995  1.        ]. \t  0.9952423981794652 \t 0.9962937722065685\n",
      "23     \t [3.81039426 3.77005568 9.79007447 0.950715   2.25959989 1.        ]. \t  0.9951703832004691 \t 0.9962937722065685\n",
      "24     \t [3.94851896 4.50823245 9.65013052 1.         2.31976113 1.        ]. \t  0.9952135899752976 \t 0.9962937722065685\n",
      "25     \t [3.78089439 4.1734519  9.55270649 0.57587258 2.80387757 1.        ]. \t  0.9947479070779656 \t 0.9962937722065685\n",
      "26     \t [4.27212034 4.17452892 9.9871054  0.93912296 2.89954241 1.        ]. \t  0.9951703833387547 \t 0.9962937722065685\n",
      "27     \t [4.32244125 4.14676753 9.56220131 0.78550354 2.73914958 0.47253899]. \t  0.798284217909042 \t 0.9962937722065685\n",
      "28     \t [3.99985632 4.81094739 9.98459267 0.59668732 2.96360276 1.        ]. \t  0.9948727299992762 \t 0.9962937722065685\n",
      "29     \t [3.7311203  4.54623257 9.77655568 1.         3.32842714 0.86641139]. \t  0.9668503857463397 \t 0.9962937722065685\n",
      "30     \t [4.31783968 4.76684517 9.4199241  0.92530386 3.29637221 1.        ]. \t  0.9950935702651559 \t 0.9962937722065685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05584258369345021"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_4 = loser_4.getResult()[0]\n",
    "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
    "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
    "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
    "\n",
    "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
    "rmse_loser_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 8.62535726  7.92131555 11.47494959  0.8114241  16.65069652  0.7124129 ]. \t  \u001b[92m0.9876571089956068\u001b[0m \t 0.9876571089956068\n",
      "2      \t [ 8.73617278  7.39641924 11.65099826  0.78736258 17.06842456  0.62466786]. \t  0.8875025212520042 \t 0.9876571089956068\n",
      "3      \t [ 9.07155923  8.13939792 11.45481541  0.79979268 17.15972408  0.71798817]. \t  0.9875658939633695 \t 0.9876571089956068\n",
      "4      \t [ 8.35202484  8.07972204 11.4145939   0.8106939  17.31595134  0.82629738]. \t  \u001b[92m0.987661909786777\u001b[0m \t 0.987661909786777\n",
      "5      \t [ 8.5797362   8.1239144  11.54161849  0.68977992 17.10716071  0.14349245]. \t  0.8875793339107304 \t 0.987661909786777\n",
      "6      \t [4.93732611 9.83457489 5.         0.87542632 5.41513892 0.84156567]. \t  0.979034848572495 \t 0.987661909786777\n",
      "7      \t [5.0372325  9.59246655 5.         0.95382266 5.66002673 0.23578808]. \t  0.8874977192853896 \t 0.987661909786777\n",
      "8      \t [5.57106186 9.99195696 5.18268334 0.99945813 5.50866432 0.68500448]. \t  0.9788764192141146 \t 0.987661909786777\n",
      "9      \t [5.48575127 9.40807889 5.00435162 0.60613071 5.48608849 0.84426222]. \t  0.9782907249039288 \t 0.987661909786777\n",
      "10     \t [5.20752023 9.49076891 5.5825836  1.         5.63319365 0.87576733]. \t  0.9788764192141146 \t 0.987661909786777\n",
      "11     \t [5.19507001 9.91735185 5.50383094 0.5        5.60858382 0.59705319]. \t  0.8874977192853896 \t 0.987661909786777\n",
      "12     \t [ 8.62651213  8.4940649  11.56113344  0.5        16.94042961  1.        ]. \t  \u001b[92m0.9924531180436428\u001b[0m \t 0.9924531180436428\n",
      "13     \t [ 8.57921361  8.7218451  11.5674539   1.         16.94915293  0.73020351]. \t  0.9876907125977735 \t 0.9924531180436428\n",
      "14     \t [ 8.62250431  8.56133269 10.85039991  0.80837452 16.9463901   0.79663857]. \t  0.9876571089956068 \t 0.9924531180436428\n",
      "15     \t [ 8.68017125  8.87642558 11.14362988  0.63214153 17.51102124  0.69291257]. \t  0.9874218691911012 \t 0.9924531180436428\n",
      "16     \t [ 8.09215576  8.89655541 11.07993901  0.5        17.09052107  0.55586058]. \t  0.8874977192853896 \t 0.9924531180436428\n",
      "17     \t [ 8.93940717  9.00237566 11.10915538  0.5        16.88627928  0.39774182]. \t  0.8874977192853896 \t 0.9924531180436428\n",
      "18     \t [5.8188574  9.3488921  5.30849657 1.         5.97834409 0.58237724]. \t  0.9000230635865686 \t 0.9924531180436428\n",
      "19     \t [5.10462971 9.00884522 5.         1.         5.81078018 1.        ]. \t  0.9841189074416429 \t 0.9924531180436428\n",
      "20     \t [4.68132801 9.16660088 5.18644904 0.5        5.7766488  0.8929481 ]. \t  0.9797597669329227 \t 0.9924531180436428\n",
      "21     \t [5.18457139 8.86090425 5.376534   0.52537737 6.18670882 0.69026955]. \t  0.9789004238614044 \t 0.9924531180436428\n",
      "22     \t [5.06281265 8.64243085 5.40460085 0.69024172 5.5294958  0.52813589]. \t  0.8875793339107304 \t 0.9924531180436428\n",
      "23     \t [4.65806261 8.90317162 5.44932416 1.         6.19809761 0.56336318]. \t  0.8993749505556262 \t 0.9924531180436428\n",
      "24     \t [ 8.61019133  9.38025026 10.92936419  0.7055855  17.10669934  1.        ]. \t  \u001b[92m0.9935765117513419\u001b[0m \t 0.9935765117513419\n",
      "25     \t [ 8.54084754  9.42463744 10.68983389  1.         17.2725959   0.46498817]. \t  0.8950925775547415 \t 0.9935765117513419\n",
      "26     \t [ 9.23770801  9.17236067 10.92385678  1.         17.24852727  1.        ]. \t  \u001b[92m0.9940613907606816\u001b[0m \t 0.9940613907606816\n",
      "27     \t [ 9.066007    9.32512691 10.35998113  0.56173305 17.3873448   0.92119653]. \t  0.9868121691964687 \t 0.9940613907606816\n",
      "28     \t [ 9.24012146  9.84942616 10.79546807  0.65100241 17.56092583  0.8118417 ]. \t  0.9874650762424908 \t 0.9940613907606816\n",
      "29     \t [ 8.96154425  9.61235635 10.51841513  1.         17.93124088  1.        ]. \t  \u001b[92m0.9941958142271843\u001b[0m \t 0.9941958142271843\n",
      "30     \t [ 9.20016095  9.9931876  10.25528369  1.         17.32644478  1.        ]. \t  0.994099797020901 \t 0.9941958142271843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06615881657543449"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_5 = loser_5.getResult()[0]\n",
    "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
    "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
    "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
    "\n",
    "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
    "rmse_loser_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.52079167  6.31724052 13.99999996  0.76006641 12.58921441  0.71818225]. \t  \u001b[92m0.9894526268810538\u001b[0m \t 0.9894526268810538\n",
      "2      \t [ 5.86780997  6.82789198 14.14921807  0.5521767  12.54178256  0.97421261]. \t  0.98928939832181 \t 0.9894526268810538\n",
      "3      \t [ 6.26756594  6.31817221 13.92945666  0.77841309 12.550722    0.77870644]. \t  0.9890157482467457 \t 0.9894526268810538\n",
      "4      \t [ 5.92977065  6.15831237 14.50337138  0.53559594 12.42102298  0.99972997]. \t  0.9889485379309418 \t 0.9894526268810538\n",
      "5      \t [ 5.9549182   6.57253065 14.49360802  1.         12.40174434  0.51867207]. \t  0.7962246411184712 \t 0.9894526268810538\n",
      "6      \t [ 5.83115657  5.9533357  13.57074845  0.5        12.41817253  1.        ]. \t  \u001b[92m0.9936821298485282\u001b[0m \t 0.9936821298485282\n",
      "7      \t [ 5.94367214  6.23631645 13.97067706  0.5        12.3876315   0.33649086]. \t  0.8562729139836217 \t 0.9936821298485282\n",
      "8      \t [ 5.8596312   6.12824841 13.97628705  0.5        13.21382061  1.        ]. \t  0.9935189015658595 \t 0.9936821298485282\n",
      "9      \t [ 4.90831509  5.88295637 13.98617998  0.81772561 14.35348457  0.56816661]. \t  0.8605840332358877 \t 0.9936821298485282\n",
      "10     \t [ 5.78793331  6.57820987 13.33861616  0.5        12.85642669  1.        ]. \t  0.9935429055217115 \t 0.9936821298485282\n",
      "11     \t [ 5.66881509  6.08891766 13.40618661  1.         13.23543963  1.        ]. \t  \u001b[92m0.9950359593889485\u001b[0m \t 0.9950359593889485\n",
      "12     \t [ 5.13197232  6.06990477 13.37965298  0.5        13.37632633  1.        ]. \t  0.9934804952364965 \t 0.9950359593889485\n",
      "13     \t [ 5.24135733  6.63212297 13.56434488  0.82054548 13.71206649  1.        ]. \t  0.9949879518921075 \t 0.9950359593889485\n",
      "14     \t [ 4.71445408  6.15003374 13.24497382  0.97628291 14.38912193  0.67784683]. \t  0.9893422057109523 \t 0.9950359593889485\n",
      "15     \t [ 5.39829424  6.30480535 13.19332987  0.62223349 13.91781491  0.45713414]. \t  0.8548374582708487 \t 0.9950359593889485\n",
      "16     \t [ 4.32310942  6.38715773 13.65488153  0.58625668 14.50583768  0.56877677]. \t  0.8558888410098612 \t 0.9950359593889485\n",
      "17     \t [ 4.98340628  6.10631035 13.44113564  0.61460446 14.70647763  1.        ]. \t  0.9948679321128479 \t 0.9950359593889485\n",
      "18     \t [ 4.90489663  6.44350546 13.66457833  1.         15.0419608   0.56969955]. \t  0.7963206569418789 \t 0.9950359593889485\n",
      "19     \t [ 4.27984904  5.79540933 13.66434158  1.         14.85262803  1.        ]. \t  0.9949927524067027 \t 0.9950359593889485\n",
      "20     \t [ 4.45236162  5.54697146 13.29386625  0.57841684 14.96346095  0.62190163]. \t  0.8560088654908982 \t 0.9950359593889485\n",
      "21     \t [ 5.12088267  5.51377072 13.29120944  1.         14.34495416  1.        ]. \t  0.9949591468685101 \t 0.9950359593889485\n",
      "22     \t [ 4.92554176  5.43206867 13.72594206  1.         15.18075437  1.        ]. \t  0.9949543460081959 \t 0.9950359593889485\n",
      "23     \t [ 4.54664157  5.03076247 13.85441027  1.         14.71012813  1.        ]. \t  0.9950167564316984 \t 0.9950359593889485\n",
      "24     \t [ 4.47187464  5.40187826 14.1851598   0.53680422 15.10912517  1.        ]. \t  0.9943110374330444 \t 0.9950359593889485\n",
      "25     \t [ 4.20426847  5.04233999 14.08792646  1.         15.47298688  0.92353186]. \t  0.9912385146277775 \t 0.9950359593889485\n",
      "26     \t [ 4.56692043  5.1509464  14.66072568  1.         15.22586058  1.        ]. \t  0.9949543460081959 \t 0.9950359593889485\n",
      "27     \t [ 4.73316156  4.73435078 14.33740777  0.80585869 15.35738131  0.58088746]. \t  0.8625475470061579 \t 0.9950359593889485\n",
      "28     \t [ 3.92250977  5.02457621 14.5748689   0.90536296 14.95707764  0.75617421]. \t  0.9892845942117378 \t 0.9950359593889485\n",
      "29     \t [ 5.21078508  6.26693692 12.69120427  0.84155495 13.93283874  1.        ]. \t  0.9947191059271151 \t 0.9950359593889485\n",
      "30     \t [ 5.74715231  6.2540828  13.11260097  1.         14.3483373   1.        ]. \t  0.9949543460773397 \t 0.9950359593889485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.060471588000536855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_6 = loser_6.getResult()[0]\n",
    "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
    "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
    "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
    "\n",
    "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
    "rmse_loser_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 2.97034638  9.39401637 13.55371715  0.67162186  1.14876129  0.81843788]. \t  \u001b[92m0.9887948815679307\u001b[0m \t 0.9887948815679307\n",
      "2      \t [ 2.561995    9.92132523 13.45565291  0.6413595   1.36375273  0.81462283]. \t  \u001b[92m0.9888332872058561\u001b[0m \t 0.9888332872058561\n",
      "3      \t [ 3.1746942  10.         13.8700162   0.68675658  1.36317129  0.78948907]. \t  \u001b[92m0.9889293037207015\u001b[0m \t 0.9889293037207015\n",
      "4      \t [ 3.04760536 10.         13.42625534  0.5         1.          1.        ]. \t  \u001b[92m0.9950311546565827\u001b[0m \t 0.9950311546565827\n",
      "5      \t [ 2.91468204  9.88605078 13.59413142  0.5         1.          0.26603868]. \t  0.8263644026205758 \t 0.9950311546565827\n",
      "6      \t [ 2.85712242  9.98106972 13.5792532   1.          1.          0.93957499]. \t  0.9926211486386451 \t 0.9950311546565827\n",
      "7      \t [ 1.71050883  1.932979   11.6790889   0.73838889  5.40845167  0.83535114]. \t  0.9891741455224107 \t 0.9950311546565827\n",
      "8      \t [ 1.71007792  1.2632774  11.60571584  0.73092073  5.19244923  0.81583166]. \t  0.98878528254391 \t 0.9950311546565827\n",
      "9      \t [ 1.18609029  1.47343205 11.79628007  0.69627478  5.632943    0.89245834]. \t  0.9886316559818695 \t 0.9950311546565827\n",
      "10     \t [ 1.17485441  1.69926934 11.2800672   0.6689932   5.11737663  0.8004662 ]. \t  0.9887852805387402 \t 0.9950311546565827\n",
      "11     \t [ 1.34909739  1.60524032 11.70165234  0.97244975  5.30228762  0.27565417]. \t  0.8290288739411423 \t 0.9950311546565827\n",
      "12     \t [ 5.41291537  8.44805346  6.          0.77663441 14.85137668  0.84370287]. \t  0.9816608736677357 \t 0.9950311546565827\n",
      "13     \t [ 5.79503996  8.90342148  6.21485179  0.80171255 15.16922851  0.86074823]. \t  0.9816752764561096 \t 0.9950311546565827\n",
      "14     \t [ 5.54286706  8.98222352  6.28808042  1.         14.50996459  0.77799639]. \t  0.9908976551357785 \t 0.9950311546565827\n",
      "15     \t [ 5.24842628  8.70614468  6.68532679  0.86828328 15.00117237  0.78112927]. \t  0.9808783324685133 \t 0.9950311546565827\n",
      "16     \t [ 5.40453334  8.83885263  6.17662691  0.93629845 14.99214145  0.216557  ]. \t  0.8289856749104315 \t 0.9950311546565827\n",
      "17     \t [ 5.56602727  8.86487514  6.48622032  0.5        14.61549726  0.77242224]. \t  0.9807583150401422 \t 0.9950311546565827\n",
      "18     \t [ 1.32900536  1.59114568 11.54149953  1.          5.19850026  1.        ]. \t  \u001b[92m0.9968554663643342\u001b[0m \t 0.9968554663643342\n",
      "19     \t [ 1.56590263  1.4851972  11.09151825  0.5         5.72458502  1.        ]. \t  0.9954056211387936 \t 0.9968554663643342\n",
      "20     \t [ 2.69986762  9.53433169 13.98346131  1.          1.67561151  1.        ]. \t  0.9967354472765124 \t 0.9968554663643342\n",
      "21     \t [ 3.16460451  9.77908718 13.28144702  0.97000935  1.75530148  1.        ]. \t  0.9966106269135139 \t 0.9968554663643342\n",
      "22     \t [ 3.00512135  9.49360826 13.67281801  0.5         2.04804651  1.        ]. \t  0.9950407558932044 \t 0.9968554663643342\n",
      "23     \t [ 2.93075972  9.48728225 13.64177957  0.93694896  2.04933773  0.44317332]. \t  0.8285487892234574 \t 0.9968554663643342\n",
      "24     \t [ 5.90042188  8.41439857  6.65734062  1.         14.69506716  1.        ]. \t  0.988727690404955 \t 0.9968554663643342\n",
      "25     \t [ 2.80309663 10.         13.72163917  0.85602453  2.28806955  1.        ]. \t  0.9965050082631771 \t 0.9968554663643342\n",
      "26     \t [ 3.40302939  9.74522109 13.95514546  1.          2.28966378  1.        ]. \t  0.9966010242248724 \t 0.9968554663643342\n",
      "27     \t [ 1.89360485  1.75124968 11.11890802  0.5         4.96188033  1.        ]. \t  0.9953144034790927 \t 0.9968554663643342\n",
      "28     \t [ 3.54122209 10.         13.49979657  0.5         2.40503164  1.        ]. \t  0.9947623086915902 \t 0.9968554663643342\n",
      "29     \t [ 3.35971824  9.72388542 13.39760474  0.9371108   2.9050227   1.        ]. \t  0.996596223433702 \t 0.9968554663643342\n",
      "30     \t [ 3.32454617  9.87659206 13.96099476  0.54125868  3.09537172  1.        ]. \t  0.9953816134491774 \t 0.9968554663643342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.055229244455986544"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_7 = loser_7.getResult()[0]\n",
    "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
    "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
    "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
    "\n",
    "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
    "rmse_loser_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [ 1.70399382  0.51636912  6.92944568  0.6336952  17.14110862  0.49680383]. \t  0.8892355979298845 \t 0.8894852354753713\n",
      "2      \t [ 1.42959644  0.68746428  7.55125917  0.65037548 17.09985285  0.59051313]. \t  \u001b[92m0.8896148549701431\u001b[0m \t 0.8896148549701431\n",
      "3      \t [ 1.25144298  1.01916723  6.98082406  0.70142931 17.37664213  0.68730172]. \t  \u001b[92m0.9800814012009155\u001b[0m \t 0.9800814012009155\n",
      "4      \t [ 1.39901836  1.06712237  6.97629806  0.79822065 16.71053352  0.75141703]. \t  \u001b[92m0.9802638283614309\u001b[0m \t 0.9802638283614309\n",
      "5      \t [ 1.31384316  1.14196202  7.03638984  0.53267252 16.95856017  0.12309188]. \t  0.889629257136229 \t 0.9802638283614309\n",
      "6      \t [ 1.30510333  0.83821347  7.07723267  1.         17.13959643  0.23744134]. \t  0.895673510263797 \t 0.9802638283614309\n",
      "7      \t [ 0.7631835   1.26012988  7.28461794  0.6363733  16.88568872  0.79159488]. \t  \u001b[92m0.9834323718305207\u001b[0m \t 0.9834323718305207\n",
      "8      \t [ 1.30074866  1.6901182   7.43481245  0.74263967 17.03843944  0.81544136]. \t  \u001b[92m0.9836916099903513\u001b[0m \t 0.9836916099903513\n",
      "9      \t [ 0.91625893  1.80309749  6.8898639   0.98700038 16.98097149  0.70643414]. \t  0.9798701467528231 \t 0.9836916099903513\n",
      "10     \t [ 1.11074279  1.857535    6.81435387  0.5        16.84018034  1.        ]. \t  \u001b[92m0.9886076744960762\u001b[0m \t 0.9886076744960762\n",
      "11     \t [ 0.98492432  1.98179362  7.2044327   0.64523345 16.38180345  0.64052331]. \t  0.8894708297138605 \t 0.9886076744960762\n",
      "12     \t [ 0.74892155  2.10815906  7.17378477  0.5        17.20567059  0.57427351]. \t  0.8886307069542818 \t 0.9886076744960762\n",
      "13     \t [ 1.56981878  2.06326459  6.8098512   0.65113967 17.00224651  0.50299512]. \t  0.8894228224935746 \t 0.9886076744960762\n",
      "14     \t [ 6.08851876  2.94094698 13.39992335  0.62322314 14.99872343  0.53354737]. \t  0.8890819712295445 \t 0.9886076744960762\n",
      "15     \t [ 6.74545227  2.89885143 13.59029821  0.73388908 15.00079363  0.67501424]. \t  0.9850550397995897 \t 0.9886076744960762\n",
      "16     \t [ 6.61483569  3.02185723 13.58181815  0.57554337 15.56669378  0.39606726]. \t  0.8902773510146641 \t 0.9886076744960762\n",
      "17     \t [ 6.7104854   3.35334221 13.47178882  0.51414926 14.84828558  0.25714577]. \t  0.8903589632891503 \t 0.9886076744960762\n",
      "18     \t [ 6.51353221  3.46853144 13.51034165  0.83400173 15.18425607  0.8990091 ]. \t  0.9853910892353066 \t 0.9886076744960762\n",
      "19     \t [ 6.48120469  3.22706917 13.56866393  1.         15.13481619  0.22422249]. \t  0.8967824983483806 \t 0.9886076744960762\n",
      "20     \t [ 7.08468941  3.47542629 13.98542647  0.7298594  15.22141802  0.76204035]. \t  0.9852902706155667 \t 0.9886076744960762\n",
      "21     \t [ 7.31641105  3.4398169  13.3162596   0.85253231 15.32096219  0.78797445]. \t  0.9849782227157092 \t 0.9886076744960762\n",
      "22     \t [ 7.2437404   3.5871065  13.58114646  1.         14.67870616  0.97510268]. \t  \u001b[92m0.9912625217629829\u001b[0m \t 0.9912625217629829\n",
      "23     \t [ 7.30729818  3.96031182 13.46458588  0.5        14.97563591  1.        ]. \t  \u001b[92m0.9920642621165952\u001b[0m \t 0.9920642621165952\n",
      "24     \t [ 7.67793071  3.31160163 13.57814745  0.5        14.81901574  1.        ]. \t  \u001b[92m0.9920978681387943\u001b[0m \t 0.9920978681387943\n",
      "25     \t [ 7.81263303  3.81031407 13.60821391  0.69065144 14.81507606  0.51208315]. \t  0.8890915726736016 \t 0.9920978681387943\n",
      "26     \t [ 7.35284629  3.50220177 12.92117717  0.5        14.6146127   0.96526215]. \t  0.9843157088323492 \t 0.9920978681387943\n",
      "27     \t [ 8.00439037  3.80585202 12.97837284  0.78731908 14.74624362  1.        ]. \t  \u001b[92m0.994027796699393\u001b[0m \t 0.994027796699393\n",
      "28     \t [ 7.87341178  3.91350835 13.22152526  0.5        14.1440113   1.        ]. \t  0.9921074694445658 \t 0.994027796699393\n",
      "29     \t [ 7.55066036  4.266215   12.83449805  0.86832267 14.35076806  0.86083388]. \t  0.9852950732735936 \t 0.994027796699393\n",
      "30     \t [ 7.86078006  3.64582188 12.84798244  1.         14.08452598  0.64914489]. \t  0.8971521532531544 \t 0.994027796699393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06606111457272737"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_8 = loser_8.getResult()[0]\n",
    "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
    "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
    "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
    "\n",
    "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
    "rmse_loser_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 6.79854917  5.13542879 13.59476154  0.87262215  5.41307251  0.82755322]. \t  0.9683194243228429 \t 0.968425042143454\n",
      "2      \t [ 7.06551336  4.51982545 13.53714515  0.88698654  5.19585651  0.7981785 ]. \t  0.9684058386330537 \t 0.968425042143454\n",
      "3      \t [ 7.32655326  5.13110484 13.66250236  0.88740967  4.89405502  0.88456156]. \t  0.9683482280327085 \t 0.968425042143454\n",
      "4      \t [ 7.31860976  4.88368592 14.09885788  0.89262705  5.47607956  0.84030526]. \t  \u001b[92m0.9684874526361004\u001b[0m \t 0.9684874526361004\n",
      "5      \t [ 7.13063072  4.96290865 13.80345087  0.76571003  5.17409667  0.2219694 ]. \t  0.7909485703084864 \t 0.9684874526361004\n",
      "6      \t [ 7.11317602  4.8614589  13.78413175  0.5         5.23534106  1.        ]. \t  \u001b[92m0.9940085842704084\u001b[0m \t 0.9940085842704084\n",
      "7      \t [7.6308514  7.20592132 7.         0.7418031  9.57300597 1.        ]. \t  0.9917282003055092 \t 0.9940085842704084\n",
      "8      \t [7.37722748 7.82814689 6.87350923 0.73678994 9.53904515 0.98189141]. \t  0.9545170707457857 \t 0.9940085842704084\n",
      "9      \t [7.29509015 7.48027003 7.52721391 0.71645672 9.3683205  1.        ]. \t  0.9920258471454719 \t 0.9940085842704084\n",
      "10     \t [7.91371535 7.71047844 7.40020285 0.72507443 9.57455841 0.74386407]. \t  0.9565046195866103 \t 0.9940085842704084\n",
      "11     \t [7.35672585 7.41354943 7.14603226 0.87810658 9.41662026 0.37135197]. \t  0.7891530744107625 \t 0.9940085842704084\n",
      "12     \t [ 7.50188322  7.62722786  7.44554411  1.         10.00151391  1.        ]. \t  0.9918194123645638 \t 0.9940085842704084\n",
      "13     \t [7.92158979 7.20356014 7.64589338 1.         9.32622733 1.        ]. \t  0.991713793506796 \t 0.9940085842704084\n",
      "14     \t [7.6368515  8.05743512 7.49807697 1.         9.18992754 1.        ]. \t  0.9917858064806522 \t 0.9940085842704084\n",
      "15     \t [8.14053348 7.7123752  6.86172963 1.         9.53686435 1.        ]. \t  0.9883244210426038 \t 0.9940085842704084\n",
      "16     \t [ 7.56989994  4.96610098 13.36448737  1.          5.59627831  1.        ]. \t  \u001b[92m0.9960009218713851\u001b[0m \t 0.9960009218713851\n",
      "17     \t [ 7.92738356  4.5604252  13.67081805  1.          5.20540973  1.        ]. \t  0.9959241077606396 \t 0.9960009218713851\n",
      "18     \t [ 8.10881202  4.89434213 13.52763326  0.5         5.43679564  0.86819685]. \t  0.9639938829910873 \t 0.9960009218713851\n",
      "19     \t [ 7.88981747  4.32426246 13.54911358  0.6247144   5.82970277  1.        ]. \t  0.9944502564357899 \t 0.9960009218713851\n",
      "20     \t [ 7.96197177  4.37939058 12.98355491  0.56153642  5.34454079  1.        ]. \t  0.994181412545111 \t 0.9960009218713851\n",
      "21     \t [ 8.14264838  4.37653192 13.18937278  0.87701834  5.62765938  0.47086724]. \t  0.7911118019100568 \t 0.9960009218713851\n",
      "22     \t [ 7.96938639  3.96270288 13.61994019  0.5         5.12828721  1.        ]. \t  0.9938549595061064 \t 0.9960009218713851\n",
      "23     \t [ 7.92421149  4.45197467 13.36659331  0.5         4.56970211  1.        ]. \t  0.9940037840323882 \t 0.9960009218713851\n",
      "24     \t [ 8.64968098  4.21327218 13.31331304  0.5         4.9660374   1.        ]. \t  0.9938549595061064 \t 0.9960009218713851\n",
      "25     \t [ 8.28336161  3.84243987 13.048294    0.77600487  4.60029965  1.        ]. \t  0.9951751830934713 \t 0.9960009218713851\n",
      "26     \t [ 8.35694247  4.0268936  13.19896571  0.5         4.54789432  0.42372263]. \t  0.7925376319093087 \t 0.9960009218713851\n",
      "27     \t [ 8.42710241  4.59876116 12.85241349  0.93198631  4.59693984  1.        ]. \t  0.9957128717045546 \t 0.9960009218713851\n",
      "28     \t [ 8.5793178   4.20636993 12.51784851  0.5         4.37318618  1.        ]. \t  0.9938645610193033 \t 0.9960009218713851\n",
      "29     \t [ 8.80989569  4.2421794  13.02380427  0.71923923  3.97558107  1.        ]. \t  0.9946518907021028 \t 0.9960009218713851\n",
      "30     \t [ 9.12790034  4.02787061 12.62111955  1.          4.39553382  1.        ]. \t  0.9958088878736812 \t 0.9960009218713851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06239982607615677"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_9 = loser_9.getResult()[0]\n",
    "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
    "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
    "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
    "\n",
    "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
    "rmse_loser_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.99999999  3.09635666  8.56845776  0.84006023 19.30335837  0.83274398]. \t  0.9854775611438326 \t 0.9854871661142187\n",
      "2      \t [ 9.99988658  3.33626313  8.42614802  0.85534044 18.65663031  0.75917938]. \t  \u001b[92m0.9857464098054324\u001b[0m \t 0.9857464098054324\n",
      "3      \t [ 9.61442392  2.83933558  8.64808811  0.6590893  18.77907588  0.99894711]. \t  0.9843829750180507 \t 0.9857464098054324\n",
      "4      \t [ 9.93926125  2.8630636   8.88474026  0.95377487 18.80944642  0.41223294]. \t  0.8904118482721805 \t 0.9857464098054324\n",
      "5      \t [ 9.36931806  3.30841749  8.46990685  0.80943612 19.04531851  0.56436473]. \t  0.8884915288308438 \t 0.9857464098054324\n",
      "6      \t [10.          3.23737993  8.64021335  0.5        18.97294705  0.45026044]. \t  0.8927594537541764 \t 0.9857464098054324\n",
      "7      \t [ 6.66763747  1.08347011  6.4395676   0.94147342 13.56048824  0.13494633]. \t  0.8924857762290301 \t 0.9857464098054324\n",
      "8      \t [ 7.00681205  0.7608248   5.91869621  0.99692497 13.65582717  0.13666788]. \t  0.891924107309269 \t 0.9857464098054324\n",
      "9      \t [ 7.11936075  0.99816689  6.41262727  0.9481996  14.14707176  0.11758274]. \t  0.8928554377023 \t 0.9857464098054324\n",
      "10     \t [ 6.57527914  0.47620645  6.36274363  1.         14.01386993  0.10463336]. \t  0.8873680894882475 \t 0.9857464098054324\n",
      "11     \t [ 6.79296293  0.84269654  6.25662642  0.95496    13.91441364  0.75464222]. \t  0.9827219077034695 \t 0.9857464098054324\n",
      "12     \t [ 6.81487938  0.73763134  6.21692455  0.5        13.88445915  0.39879754]. \t  0.8883859151588597 \t 0.9857464098054324\n",
      "13     \t [ 7.33109129  0.72597702  8.5630607   0.98303614 11.45889843  0.16684538]. \t  0.892495386661775 \t 0.9857464098054324\n",
      "14     \t [ 7.03784353  1.1714792   8.66949822  0.87812669 11.02045407  0.1       ]. \t  0.8911511641660543 \t 0.9857464098054324\n",
      "15     \t [ 7.51976524  1.26311419  9.02106275  0.84630393 11.45909155  0.1       ]. \t  0.8904694578353688 \t 0.9857464098054324\n",
      "16     \t [ 7.77999112  1.13934972  8.47535752  0.93541353 11.0054724   0.1       ]. \t  0.8926250078850879 \t 0.9857464098054324\n",
      "17     \t [ 7.45699797  1.09308443  8.7557772   0.93373775 11.14512945  0.74195864]. \t  \u001b[92m0.9859912507082722\u001b[0m \t 0.9859912507082722\n",
      "18     \t [ 7.51327046  0.84014054  8.66333508  0.5        11.12593521  0.46328466]. \t  0.891535242187311 \t 0.9859912507082722\n",
      "19     \t [ 0.68637963  5.72001352 12.27609939  0.79940652 18.55219006  0.13461355]. \t  0.8930666844756714 \t 0.9859912507082722\n",
      "20     \t [ 1.08156386  5.99401514 12.38050017  0.76945069 18.10243107  0.37026768]. \t  0.8925337903636744 \t 0.9859912507082722\n",
      "21     \t [ 0.9440272   5.66327888 11.82352899  0.8144214  18.33573349  0.61797135]. \t  0.8937483989653234 \t 0.9859912507082722\n",
      "22     \t [ 1.06917863  5.27581512 12.20187664  0.80598469 18.06032206  0.16427748]. \t  0.8933643356716926 \t 0.9859912507082722\n",
      "23     \t [ 0.56723722  5.48787866 12.42997498  0.74290017 18.07530277  0.67897047]. \t  \u001b[92m0.9872250563208061\u001b[0m \t 0.9872250563208061\n",
      "24     \t [ 1.05485847  5.35505192 12.51350975  0.5        18.4804579   0.74515085]. \t  0.9865337378979246 \t 0.9872250563208061\n",
      "25     \t [ 0.48135792  5.05784009 12.04592868  0.5        18.39343948  0.505389  ]. \t  0.8928986621087792 \t 0.9872250563208061\n",
      "26     \t [ 0.84250982  5.0540504  12.47326657  1.         18.58960951  0.81527215]. \t  0.9848534295970124 \t 0.9872250563208061\n",
      "27     \t [ 0.68988506  4.94593118 12.97257773  0.67317559 18.46726612  0.47293371]. \t  0.891813677357907 \t 0.9872250563208061\n",
      "28     \t [ 0.95750865  4.7220813  12.54601014  0.65335077 18.10275196  1.        ]. \t  \u001b[92m0.994291831640899\u001b[0m \t 0.994291831640899\n",
      "29     \t [ 1.28131249  5.15400153 12.95463267  1.         18.07761593  0.95785262]. \t  0.9847958191349547 \t 0.994291831640899\n",
      "30     \t [ 3.53580588  2.7317511  13.8263569   0.94028354 18.86834634  0.37527641]. \t  0.8913960038243062 \t 0.994291831640899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06055749165482397"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_10 = loser_10.getResult()[0]\n",
    "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
    "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
    "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
    "\n",
    "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
    "rmse_loser_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [8.28412335 5.52827156 5.19181002 0.54394859 1.         0.73138706]. \t  \u001b[92m0.9803838730348305\u001b[0m \t 0.9803838730348305\n",
      "2      \t [8.67920994 5.92469468 5.62094656 0.56642778 1.00000001 0.78199338]. \t  \u001b[92m0.9808351481654284\u001b[0m \t 0.9808351481654284\n",
      "3      \t [8.56482233 5.82027525 5.23143556 0.60224909 1.61144461 0.84445656]. \t  0.9806431162420379 \t 0.9808351481654284\n",
      "4      \t [8.38760949 6.24088258 5.         0.55159795 1.04415938 0.91820946]. \t  \u001b[92m0.9810847893062884\u001b[0m \t 0.9810847893062884\n",
      "5      \t [8.58798317 6.00298081 5.11986539 0.50014684 1.19131878 0.22387974]. \t  0.8133783707838496 \t 0.9810847893062884\n",
      "6      \t [8.56602181 5.81799311 5.16647562 1.         1.04359779 1.        ]. \t  \u001b[92m0.9838788597241571\u001b[0m \t 0.9838788597241571\n",
      "7      \t [7.94565465 6.05172839 5.60920093 0.61408387 1.31607257 1.        ]. \t  \u001b[92m0.9838884613756417\u001b[0m \t 0.9838884613756417\n",
      "8      \t [8.41314048 6.52492214 5.60359573 0.89725825 1.48261118 1.        ]. \t  \u001b[92m0.9839748762390027\u001b[0m \t 0.9839748762390027\n",
      "9      \t [7.90675176 6.3265516  5.3094897  1.         1.51205564 0.61367293]. \t  0.7770405763699446 \t 0.9839748762390027\n",
      "10     \t [9.16345138 6.40070743 5.23418308 0.6807247  1.34882539 1.        ]. \t  0.9838500546314161 \t 0.9839748762390027\n",
      "11     \t [8.8912091  6.4348265  5.87970459 0.5        1.75137864 1.        ]. \t  0.9838548567363183 \t 0.9839748762390027\n",
      "12     \t [9.20469034 6.09777522 5.82525877 1.         1.66095142 1.        ]. \t  0.983874058932987 \t 0.9839748762390027\n",
      "13     \t [8.40889122 5.9385652  6.18717307 0.94989055 1.75233818 1.        ]. \t  \u001b[92m0.9888621061273538\u001b[0m \t 0.9888621061273538\n",
      "14     \t [8.85469218 6.46792397 6.42731886 0.98098484 1.4038246  1.        ]. \t  0.9888477037538427 \t 0.9888621061273538\n",
      "15     \t [8.82488663 6.42986152 6.21534619 1.         1.76406747 0.44003991]. \t  0.7860757151361332 \t 0.9888621061273538\n",
      "16     \t [8.19002166 6.3855887  6.57460545 0.51399725 1.47311795 1.        ]. \t  0.9884540337612312 \t 0.9888621061273538\n",
      "17     \t [8.850483   5.91864892 6.74255287 0.5        1.582397   1.        ]. \t  0.9885596546241687 \t 0.9888621061273538\n",
      "18     \t [8.66948492 6.38476117 6.97727534 0.72195259 2.03953994 1.        ]. \t  0.9887804939911892 \t 0.9888621061273538\n",
      "19     \t [8.39962751 6.05717904 7.25993917 0.96736397 1.54196669 1.        ]. \t  \u001b[92m0.9914785504525324\u001b[0m \t 0.9914785504525324\n",
      "20     \t [8.71746305 6.50120384 7.45426374 0.5        1.43280756 1.        ]. \t  0.9896206306482682 \t 0.9914785504525324\n",
      "21     \t [8.42307487 6.19791988 7.39807052 0.5        1.67318488 0.46055186]. \t  0.8213332033441268 \t 0.9914785504525324\n",
      "22     \t [9.22292641 6.1786836  7.43958688 0.9988317  1.64760983 1.        ]. \t  0.9912049032123632 \t 0.9914785504525324\n",
      "23     \t [8.88567502 5.93381509 7.84726022 0.55476911 1.85465086 1.        ]. \t  0.9898894737783653 \t 0.9914785504525324\n",
      "24     \t [8.76277667 6.47556729 8.04830865 1.         1.87994013 1.        ]. \t  \u001b[92m0.9936149124109148\u001b[0m \t 0.9936149124109148\n",
      "25     \t [8.89257816 6.09291602 8.2282527  0.95016565 1.28049329 1.        ]. \t  \u001b[92m0.9937829401018782\u001b[0m \t 0.9937829401018782\n",
      "26     \t [9.35270638 6.45733608 8.35361324 0.62055573 1.59390221 1.        ]. \t  0.992669155858911 \t 0.9937829401018782\n",
      "27     \t [9.19346439 6.24503928 8.28606889 0.88665316 1.63313584 0.39678114]. \t  0.8240312686558743 \t 0.9937829401018782\n",
      "28     \t [8.65048983 6.43184956 8.65150863 0.5        1.5343697  1.        ]. \t  0.9920546458078413 \t 0.9937829401018782\n",
      "29     \t [9.05096916 6.06858383 8.91423618 0.82978427 1.84221812 1.        ]. \t  0.9935909117048206 \t 0.9937829401018782\n",
      "30     \t [9.04063954 6.6681638  9.0630489  1.         1.45470547 1.        ]. \t  \u001b[92m0.9942630217772367\u001b[0m \t 0.9942630217772367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06482273109697756"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_11 = loser_11.getResult()[0]\n",
    "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
    "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
    "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
    "\n",
    "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
    "rmse_loser_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.12096564  6.48685337  8.70533664  0.87185013 15.55776316  0.76342941]. \t  0.9901823525305562 \t 0.9904271937791528\n",
      "2      \t [ 2.28116778  6.3161366   9.36806802  0.89471839 15.57212946  0.67377731]. \t  0.9902639642518999 \t 0.9904271937791528\n",
      "3      \t [ 2.14082276  5.7764869   8.88323863  0.81674575 15.51858915  0.63691114]. \t  0.88937006143007 \t 0.9904271937791528\n",
      "4      \t [ 1.6714967   6.41767157  9.13370301  0.67734625 15.58362456  0.40411407]. \t  0.8896821192864891 \t 0.9904271937791528\n",
      "5      \t [ 2.37335028  6.3647471   8.92336828  0.82150531 16.03459316  0.31323926]. \t  0.8898885543440002 \t 0.9904271937791528\n",
      "6      \t [ 2.36324168  6.35508059  9.05390043  0.5        15.78395095  1.        ]. \t  \u001b[92m0.9932932618907571\u001b[0m \t 0.9932932618907571\n",
      "7      \t [ 2.74402081  6.16778368  8.88585305  1.         15.86533883  1.        ]. \t  \u001b[92m0.9937397333262691\u001b[0m \t 0.9937397333262691\n",
      "8      \t [ 2.44251475  6.89020834  9.13976748  1.         16.02030451  1.        ]. \t  \u001b[92m0.9939653738993206\u001b[0m \t 0.9939653738993206\n",
      "9      \t [ 1.75605698  6.98360516  9.34386528  0.90935725 15.6606646   1.        ]. \t  \u001b[92m0.9943398375466863\u001b[0m \t 0.9943398375466863\n",
      "10     \t [ 1.61915065  6.3604542   9.24464285  1.         15.19190431  1.        ]. \t  0.9942150150402324 \t 0.9943398375466863\n",
      "11     \t [ 2.24619497  6.98182667  9.25478753  1.         15.0798213   1.        ]. \t  0.9939221660182075 \t 0.9943398375466863\n",
      "12     \t [ 1.70428533  6.85962434  9.36127259  0.5        14.89461284  1.        ]. \t  0.9933700737197856 \t 0.9943398375466863\n",
      "13     \t [ 1.49232013  7.10217268  9.24194866  1.         14.80443467  0.68502669]. \t  0.9884204302958345 \t 0.9943398375466863\n",
      "14     \t [ 1.49730545  7.1895995   8.75817157  0.73772511 14.96780386  1.        ]. \t  0.9939605699967237 \t 0.9943398375466863\n",
      "15     \t [ 1.73804361  6.7458087   8.83337069  0.88124803 14.37438663  1.        ]. \t  0.9940997943926924 \t 0.9943398375466863\n",
      "16     \t [ 2.67266017  6.12843585  9.3376533   0.94253809 16.53794341  1.        ]. \t  0.993984578032005 \t 0.9943398375466863\n",
      "17     \t [ 1.97230785  7.08477365  8.78669934  0.55065859 14.59802289  0.48378484]. \t  0.8901574025215937 \t 0.9943398375466863\n",
      "18     \t [ 2.66566849  6.30267659  8.67395026  0.84833962 16.70702935  1.        ]. \t  0.9939509718715204 \t 0.9943398375466863\n",
      "19     \t [ 3.24588779  6.48335572  9.03003959  0.68243318 16.547753    0.87888228]. \t  0.9900095197615336 \t 0.9943398375466863\n",
      "20     \t [ 3.01045435  5.71365477  8.85788913  0.50187098 16.53381313  0.80458291]. \t  0.9895630391991737 \t 0.9943398375466863\n",
      "21     \t [ 3.24470794  5.94845747  8.82767573  1.         16.79729368  0.50884467]. \t  0.8976274581990945 \t 0.9943398375466863\n",
      "22     \t [ 2.78111314  6.26956336  9.11740515  0.5        17.0230748   0.55304576]. \t  0.8891252175540475 \t 0.9943398375466863\n",
      "23     \t [ 1.24453647  6.53073911  8.69013315  0.5761372  14.62737458  0.67304754]. \t  0.9900815336342452 \t 0.9943398375466863\n",
      "24     \t [ 1.77661316  6.26719948  9.15398873  0.80534934 14.42441747  0.42010185]. \t  0.8895764984235793 \t 0.9943398375466863\n",
      "25     \t [ 1.48846895  6.68806484  8.37556628  1.         14.61029986  0.39400173]. \t  0.8980739187100459 \t 0.9943398375466863\n",
      "26     \t [ 2.91518919  7.73640391  7.34066737  0.80972729 13.30352947  0.26352021]. \t  0.8897781345567743 \t 0.9943398375466863\n",
      "27     \t [ 3.46566804  8.15141235  7.49861766  0.83587771 13.39359068  0.37556856]. \t  0.8899461646677702 \t 0.9943398375466863\n",
      "28     \t [ 3.5467305   7.57505842  7.54209583  0.65519147 13.06148925  0.1       ]. \t  0.8881506429102713 \t 0.9943398375466863\n",
      "29     \t [ 3.10468739  8.1516515   7.68475496  0.5        12.90389203  0.1       ]. \t  0.8865231512637682 \t 0.9943398375466863\n",
      "30     \t [ 3.29241858  7.85128534  7.37599727  0.5        12.90437882  0.73559324]. \t  0.9884876502225105 \t 0.9943398375466863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06466275933578129"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_12 = loser_12.getResult()[0]\n",
    "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
    "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
    "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
    "\n",
    "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
    "rmse_loser_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [8.74935883 2.68284269 9.39335823 0.91539117 5.73982429 0.87921172]. \t  0.9878923398114204 \t 0.9879595510260936\n",
      "2      \t [8.83515564 2.91569475 8.99859085 0.94161865 6.27075578 0.92127399]. \t  0.9878059251554908 \t 0.9879595510260936\n",
      "3      \t [8.73298042 2.47643881 9.55427457 0.99476133 6.4279536  0.79260446]. \t  0.9872346298307707 \t 0.9879595510260936\n",
      "4      \t [9.39542368 2.56436126 9.36087112 0.87314731 6.13673537 0.89123442]. \t  \u001b[92m0.9882524072390221\u001b[0m \t 0.9882524072390221\n",
      "5      \t [8.94284728 2.59780392 9.1886864  0.69069037 6.12297554 0.30066946]. \t  0.8194800918677053 \t 0.9882524072390221\n",
      "6      \t [8.80353165 2.50972973 9.34781315 0.5        6.24723852 1.        ]. \t  \u001b[92m0.9937685427758632\u001b[0m \t 0.9937685427758632\n",
      "7      \t [ 6.15547589  8.87582304  7.99990669  0.7723454  10.39147208  0.79720606]. \t  0.987047398560263 \t 0.9937685427758632\n",
      "8      \t [ 6.10000199  9.0796824   8.56818516  0.799401   10.04094212  0.69394798]. \t  0.9870618024549374 \t 0.9937685427758632\n",
      "9      \t [5.66143055 9.25367789 8.01070735 0.70468508 9.98564971 0.79736985]. \t  0.9861688530846449 \t 0.9937685427758632\n",
      "10     \t [ 6.02498719  9.56179252  8.21007711  0.82546686 10.50243617  0.58490084]. \t  0.8193024756625761 \t 0.9937685427758632\n",
      "11     \t [6.14217277 8.984139   8.00250414 0.50247105 9.87014317 0.30093602]. \t  0.8173101339129724 \t 0.9937685427758632\n",
      "12     \t [6.0050481  8.71032279 8.06550079 1.         9.6627594  1.        ]. \t  \u001b[92m0.9939365739240159\u001b[0m \t 0.9939365739240159\n",
      "13     \t [5.97381908 8.67140201 8.34860643 0.5        9.89884343 1.        ]. \t  0.9933892804808342 \t 0.9939365739240159\n",
      "14     \t [9.04622853 3.14030591 9.78961012 0.8719802  6.28818571 1.        ]. \t  \u001b[92m0.994325440290562\u001b[0m \t 0.994325440290562\n",
      "15     \t [ 5.49071787  8.69823903  8.42051378  1.         10.23127117  1.        ]. \t  0.9939509765741023 \t 0.994325440290562\n",
      "16     \t [5.48999885 9.02388522 8.6277316  1.         9.55283103 1.        ]. \t  0.9939605782255868 \t 0.994325440290562\n",
      "17     \t [5.36032793 8.53172159 8.45710573 0.91319039 9.6840858  0.54456764]. \t  0.8220917505442021 \t 0.994325440290562\n",
      "18     \t [ 9.10338384  2.4881737  10.13523333  0.86018596  6.00581545  1.        ]. \t  \u001b[92m0.9944550613755873\u001b[0m \t 0.9944550613755873\n",
      "19     \t [9.00440651 1.82245179 9.68383754 0.92736713 5.84705863 1.        ]. \t  0.9944358584874808 \t 0.9944550613755873\n",
      "20     \t [6.05265415 9.36139325 8.36203828 0.65935646 9.24627602 1.        ]. \t  0.9937109356327074 \t 0.9944550613755873\n",
      "21     \t [5.9308509  9.84215002 8.46034358 1.         9.63277013 1.        ]. \t  0.9939461756446443 \t 0.9944550613755873\n",
      "22     \t [5.69233057 9.66420121 8.79426104 0.5        9.78072438 1.        ]. \t  0.9933892804808342 \t 0.9944550613755873\n",
      "23     \t [6.14503995 9.62434998 9.10066665 0.8343259  9.3692757  1.        ]. \t  0.9942726291676554 \t 0.9944550613755873\n",
      "24     \t [5.76196859 9.81273372 8.91361869 0.74510089 9.21234692 0.51823204]. \t  0.8204018611966526 \t 0.9944550613755873\n",
      "25     \t [ 5.54016106  8.30067638  7.69058941  0.79051761 10.25524475  1.        ]. \t  0.9910944944189989 \t 0.9944550613755873\n",
      "26     \t [6.55611283 9.98282552 8.71171096 0.5        9.70325571 0.99190949]. \t  0.9857463791747363 \t 0.9944550613755873\n",
      "27     \t [ 5.91584702  7.92231854  8.05218911  1.         10.38685633  1.        ]. \t  0.9939221715505048 \t 0.9944550613755873\n",
      "28     \t [ 5.58799064  8.0011683   8.02462558  0.62209817 10.77866046  0.65628717]. \t  0.8201282261257893 \t 0.9944550613755873\n",
      "29     \t [ 6.27410832  8.01290074  7.50643868  0.61690086 10.27607456  1.        ]. \t  0.9917810066574949 \t 0.9944550613755873\n",
      "30     \t [ 6.03660918  7.92518198  7.46380506  1.         10.2630478   0.4558348 ]. \t  0.7992539565266293 \t 0.9944550613755873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06377192739669198"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_13 = loser_13.getResult()[0]\n",
    "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
    "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
    "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
    "\n",
    "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
    "rmse_loser_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  0.8869120213105831 \t 0.991771393943005\n",
      "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  0.9848582277607189 \t 0.991771393943005\n",
      "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  0.991771393943005 \t 0.991771393943005\n",
      "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  0.8867439936196196 \t 0.991771393943005\n",
      "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  0.8866719807149072 \t 0.991771393943005\n",
      "1      \t [1.80801576 4.32095266 9.62299503 0.96586358 3.00000001 0.99999953]. \t  0.9916321718287892 \t 0.991771393943005\n",
      "2      \t [2.28667898 4.05129353 9.37851695 0.94701873 2.6272893  0.96888011]. \t  0.9916993820754492 \t 0.991771393943005\n",
      "3      \t [2.267207   3.93468457 9.38293378 0.93951966 3.3616168  0.91574876]. \t  0.9914977488462929 \t 0.991771393943005\n",
      "4      \t [1.78628736 3.76406243 9.3557714  0.92489768 2.91645228 0.55086523]. \t  0.8868256070695161 \t 0.991771393943005\n",
      "5      \t [2.3085193  4.45797026 9.45171096 0.73318526 3.03512342 0.51172581]. \t  0.8868256070695161 \t 0.991771393943005\n",
      "6      \t [1.97625767 3.98567529 9.33383196 0.5        2.99890309 1.        ]. \t  \u001b[92m0.9948823334492022\u001b[0m \t 0.9948823334492022\n",
      "7      \t [ 0.48132533  8.88496039  5.00000001  0.50653276 17.49318038  0.62434877]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "8      \t [ 0.61123779  8.64885549  5.00000719  0.69477138 16.79307719  0.81897847]. \t  0.9833267582297078 \t 0.9948823334492022\n",
      "9      \t [ 0.71499631  8.96211527  5.0001729   0.96883078 16.93214176  0.26151077]. \t  0.8869120213105831 \t 0.9948823334492022\n",
      "10     \t [ 0.07261962  8.97627013  5.00024769  0.99449895 16.96981594  0.81815074]. \t  0.9840948869604267 \t 0.9948823334492022\n",
      "11     \t [ 0.07887985  8.88755813  5.          0.5        16.71525475  0.39798631]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "12     \t [ 0.40851784  8.94391136  5.63173341  0.77322726 16.96540174  0.75279291]. \t  0.9839124569650958 \t 0.9948823334492022\n",
      "13     \t [ 0.42393143  9.15093644  5.29440455  1.         16.31602703  0.87534805]. \t  0.9894094039957376 \t 0.9948823334492022\n",
      "14     \t [ 8.46595261  1.60070279 11.60295028  0.90114014 14.8492624   0.50116976]. \t  0.8869120213105831 \t 0.9948823334492022\n",
      "15     \t [ 8.52289933  1.46315928 11.43813693  0.84786797 15.51490995  0.42085142]. \t  0.8869120213105831 \t 0.9948823334492022\n",
      "16     \t [ 8.48267199  0.96397207 11.26210015  0.87305397 14.99280507  0.38789344]. \t  0.8869120213105831 \t 0.9948823334492022\n",
      "17     \t [ 9.08306162  1.29471242 11.43205181  0.95127219 15.0917607   0.66667729]. \t  0.991060872355442 \t 0.9948823334492022\n",
      "18     \t [ 9.04499767  1.39344089 11.43399083  0.56691114 15.0168356   0.10062226]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "19     \t [ 8.7426769   1.30116927 11.36278604  0.5        15.08286275  0.94477312]. \t  0.9881467801531438 \t 0.9948823334492022\n",
      "20     \t [ 9.13685429  1.05075429 10.94811334  0.5        15.44005912  0.65029726]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "21     \t [ 9.00385588  0.8636517  11.82350845  0.5        15.33151765  0.6623437 ]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "22     \t [ 9.23576286  1.73716403 11.56379985  0.5        15.4254054   0.80459826]. \t  0.9881755849001662 \t 0.9948823334492022\n",
      "23     \t [ 0.          9.18999718  5.4614197   0.5        16.50815093  1.        ]. \t  0.9841717156605106 \t 0.9948823334492022\n",
      "24     \t [ 0.02654895  9.66245915  5.50201618  0.93289694 16.68052553  0.65366195]. \t  0.8869120213105831 \t 0.9948823334492022\n",
      "25     \t [ 9.60356371  1.37774311 11.47954089  0.5        14.92428185  0.95061092]. \t  0.9881755851075976 \t 0.9948823334492022\n",
      "26     \t [ 9.28030931  1.65427578 12.10203478  0.5        14.81481699  0.91555888]. \t  0.988161182526655 \t 0.9948823334492022\n",
      "27     \t [ 9.28838104  2.07840099 11.47859696  0.5        14.61514933  0.90410956]. \t  0.988161182526655 \t 0.9948823334492022\n",
      "28     \t [ 9.12532241  1.38452554 11.58855528  0.5        14.21133286  0.77365003]. \t  0.988161182526655 \t 0.9948823334492022\n",
      "29     \t [ 9.70500216  1.80386927 11.83804253  0.5        14.37088421  0.48762553]. \t  0.8868304078606867 \t 0.9948823334492022\n",
      "30     \t [ 9.43779112  1.75309751 11.76478093  1.         14.2415752   1.        ]. \t  0.9942198224699507 \t 0.9948823334492022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06328410674912122"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_14 = loser_14.getResult()[0]\n",
    "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
    "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
    "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
    "\n",
    "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
    "rmse_loser_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.84881371  3.83864986 13.58542333  0.50377376 15.58542333  0.85347034]. \t  \u001b[92m0.9871674273263862\u001b[0m \t 0.9871674273263862\n",
      "2      \t [ 7.24798163  3.42029425 13.97554967  0.5        15.47410969  0.90284265]. \t  0.9870186027309306 \t 0.9871674273263862\n",
      "3      \t [ 7.26418472  3.41673309 13.44748917  0.50153923 15.99138542  0.99984365]. \t  0.9871578250526155 \t 0.9871674273263862\n",
      "4      \t [ 6.73495758  3.09098419 13.57587828  0.54996279 15.64895717  0.73546963]. \t  0.9870474139773808 \t 0.9871674273263862\n",
      "5      \t [ 7.14130941  3.51070248 13.70460817  0.91064843 15.82833381  0.42149212]. \t  0.7998108531838432 \t 0.9871674273263862\n",
      "6      \t [ 6.80113195  3.39043788 13.70959377  1.         15.589053    1.        ]. \t  \u001b[92m0.9945702789800519\u001b[0m \t 0.9945702789800519\n",
      "7      \t [ 7.14990791  3.25351037 13.19559542  0.55499629 15.1859369   1.        ]. \t  0.992664351817027 \t 0.9945702789800519\n",
      "8      \t [ 6.57723784  3.33105277 13.75435746  0.5        14.96105208  1.        ]. \t  0.9926499540760796 \t 0.9945702789800519\n",
      "9      \t [ 6.3715769   3.37551873 13.05923004  0.5        15.29143297  1.        ]. \t  0.9926259501893654 \t 0.9945702789800519\n",
      "10     \t [ 6.55618442  3.4787151  13.14297833  0.98955195 14.76987662  1.        ]. \t  \u001b[92m0.9945702799480509\u001b[0m \t 0.9945702799480509\n",
      "11     \t [ 6.54250786  3.38609026 13.11484414  0.54197441 14.72517711  0.46978976]. \t  0.7997868469462744 \t 0.9945702799480509\n",
      "12     \t [ 6.44383875  2.79410003 13.1658821   0.90886933 14.87435198  1.        ]. \t  \u001b[92m0.9947239073398189\u001b[0m \t 0.9947239073398189\n",
      "13     \t [ 5.86929062  3.2257855  13.42666297  1.         15.00447646  1.        ]. \t  0.9946374919924493 \t 0.9947239073398189\n",
      "14     \t [ 5.85170141  3.10955585 13.05661348  0.77039511 14.38684938  1.        ]. \t  0.9946470945427911 \t 0.9947239073398189\n",
      "15     \t [ 5.78734719  3.06432884 12.56491738  1.         14.81197859  1.        ]. \t  0.9946710971849351 \t 0.9947239073398189\n",
      "16     \t [ 5.55747999  3.7229133  12.81398445  1.         14.64765056  1.        ]. \t  0.9946951010716493 \t 0.9947239073398189\n",
      "17     \t [ 5.18454961  3.27530635 12.84615423  0.59758147 14.84437726  1.        ]. \t  0.9940325876031458 \t 0.9947239073398189\n",
      "18     \t [ 5.28544311  3.25793338 12.85530301  1.         14.66563709  0.4415634 ]. \t  0.8876897435875565 \t 0.9947239073398189\n",
      "19     \t [ 4.92276668  3.23042964 12.7101292   1.         14.34418597  1.        ]. \t  \u001b[92m0.9949255389095898\u001b[0m \t 0.9949255389095898\n",
      "20     \t [ 5.19137147  3.45263122 12.25159382  0.5987034  14.30558536  0.94416964]. \t  0.9880603847173955 \t 0.9949255389095898\n",
      "21     \t [ 4.82900607  3.53171938 12.1537794   1.         14.80811579  1.        ]. \t  0.9948919338553898 \t 0.9949255389095898\n",
      "22     \t [ 4.55243484  3.89589216 12.49832529  0.67999787 14.46005908  0.91155705]. \t  0.9890013234692883 \t 0.9949255389095898\n",
      "23     \t [ 4.36388198  3.29156601 12.19947177  0.53608737 14.54334323  0.69240447]. \t  0.9879451700161352 \t 0.9949255389095898\n",
      "24     \t [ 4.53818369  3.6795338  12.0544005   1.         14.2422403   0.43261179]. \t  0.8876849427272423 \t 0.9949255389095898\n",
      "25     \t [ 4.75774423  3.81684848 12.12808261  0.5        14.8812978   0.42143283]. \t  0.7996620317689559 \t 0.9949255389095898\n",
      "26     \t [ 4.30236823  3.69583204 11.74556134  0.55522148 14.28674197  1.        ]. \t  0.9939701761424904 \t 0.9949255389095898\n",
      "27     \t [ 3.84851854  3.54493992 12.09762629  1.         14.38869442  1.        ]. \t  \u001b[92m0.9949255390478754\u001b[0m \t 0.9949255390478754\n",
      "28     \t [ 4.05992344  3.41353801 12.19898025  0.67112032 13.76271467  1.        ]. \t  0.9945558808242478 \t 0.9949255390478754\n",
      "29     \t [ 4.23100409  3.0097258  11.73423061  1.         14.05816103  1.        ]. \t  0.9947575108728776 \t 0.9949255390478754\n",
      "30     \t [ 3.57444004  3.10860407 11.68015958  0.55829128 14.0335063   1.        ]. \t  0.9944022549536213 \t 0.9949255390478754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05551408455709058"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_15 = loser_15.getResult()[0]\n",
    "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
    "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
    "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
    "\n",
    "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
    "rmse_loser_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 3.88274357  9.59273792 13.87645556  0.92434907 16.75250015  0.73022484]. \t  \u001b[92m0.9861400470911503\u001b[0m \t 0.9861400470911503\n",
      "2      \t [ 3.5512344   9.69360327 13.32802227  0.92298755 17.02879293  0.79478045]. \t  \u001b[92m0.9861640516692923\u001b[0m \t 0.9861640516692923\n",
      "3      \t [ 3.39650974  9.81622703 13.60730013  0.99971008 16.40037017  0.98990122]. \t  \u001b[92m0.986476099845721\u001b[0m \t 0.986476099845721\n",
      "4      \t [ 3.35090675  9.13829632 13.68627573  0.9688143  16.75088361  0.98505322]. \t  0.9864040978655689 \t 0.986476099845721\n",
      "5      \t [ 3.29176785  9.50435779 13.63428492  0.73802671 16.58950958  0.35352928]. \t  0.8857838424222196 \t 0.986476099845721\n",
      "6      \t [ 3.75711865  9.27958786 13.81730768  1.         17.43398836  1.        ]. \t  \u001b[92m0.994752712086847\u001b[0m \t 0.994752712086847\n",
      "7      \t [ 3.58592069  9.543063   13.72893993  0.5        16.91177439  1.        ]. \t  0.9936101181183498 \t 0.994752712086847\n",
      "8      \t [ 3.58451108  9.08947765 13.80808672  0.71818046 17.42521973  0.44348988]. \t  0.8867679926506504 \t 0.994752712086847\n",
      "9      \t [ 4.05808691  8.96908679 13.3504688   0.83458251 17.07228211  0.95093861]. \t  0.9864184928408047 \t 0.994752712086847\n",
      "10     \t [ 3.88960313  9.31419003 13.13265542  0.85286102 16.3031534   1.        ]. \t  \u001b[92m0.9947671159123597\u001b[0m \t 0.9947671159123597\n",
      "11     \t [ 3.59908085  8.97264834 12.91212353  0.5        16.64890971  0.75521768]. \t  0.9845558055028607 \t 0.9947671159123597\n",
      "12     \t [ 3.87977879  8.89347683 13.00484607  1.         16.54888926  0.3569269 ]. \t  0.7895659197430193 \t 0.9947671159123597\n",
      "13     \t [ 3.15272161  9.43927879 12.81175383  0.72315698 16.21946333  1.        ]. \t  \u001b[92m0.9948439302305289\u001b[0m \t 0.9948439302305289\n",
      "14     \t [ 3.30769393  9.12978753 13.22194307  0.5        15.83189058  1.        ]. \t  0.9939461780637705 \t 0.9948439302305289\n",
      "15     \t [ 3.50089387  9.70310809 12.89395901  0.5        15.76640121  0.69738868]. \t  0.9842005414247555 \t 0.9948439302305289\n",
      "16     \t [ 3.9865876   8.70515319 13.62992031  0.5        16.49937266  0.79161923]. \t  0.9837684656559071 \t 0.9948439302305289\n",
      "17     \t [ 4.07639955  8.7728817  14.2790964   0.77434707 17.16447319  0.95166662]. \t  0.9854151281082714 \t 0.9948439302305289\n",
      "18     \t [ 3.55499199  8.98694586 12.49670925  0.5        15.6972679   0.79424518]. \t  0.9845221990658048 \t 0.9948439302305289\n",
      "19     \t [ 3.25647884  9.29668791 12.87002064  1.         15.53383099  0.61167442]. \t  0.7884953278929597 \t 0.9948439302305289\n",
      "20     \t [ 3.84971973  8.25434979 13.78288885  0.59413144 17.28624298  1.        ]. \t  0.9937733462627469 \t 0.9948439302305289\n",
      "21     \t [ 4.25171744  8.20404073 13.91679576  1.         17.24792253  0.61777232]. \t  0.7893834743288078 \t 0.9948439302305289\n",
      "22     \t [ 4.36037371  8.6350534  13.88964507  0.5        17.68789237  1.        ]. \t  0.9935093000517523 \t 0.9948439302305289\n",
      "23     \t [ 3.73978698  9.57166118 12.21153097  0.5        16.20089218  0.79279315]. \t  0.9842629542682667 \t 0.9948439302305289\n",
      "24     \t [ 4.22801983  9.28945044 12.60038046  0.5        15.96622538  0.44677839]. \t  0.8844011902266008 \t 0.9948439302305289\n",
      "25     \t [ 3.75780838  8.49840911 14.28485459  0.5        17.92922068  1.        ]. \t  0.9933076640568429 \t 0.9948439302305289\n",
      "26     \t [ 3.32219431  9.40797995 12.32148782  0.5        16.07978937  0.2380656 ]. \t  0.8850972946441757 \t 0.9948439302305289\n",
      "27     \t [ 3.85645334 10.         12.82877392  0.5        16.53764794  0.47552134]. \t  0.8851068963648041 \t 0.9948439302305289\n",
      "28     \t [ 3.89041886  8.39282104 13.68535358  0.86150361 18.14211091  1.        ]. \t  0.9947239069249619 \t 0.9948439302305289\n",
      "29     \t [ 3.58315963  8.82000161 13.48581826  0.5        18.09977384  1.        ]. \t  0.9938453604120298 \t 0.9948439302305289\n",
      "30     \t [ 3.26505002  8.13804804 13.71087288  0.5        18.20627702  1.        ]. \t  0.9938357584148302 \t 0.9948439302305289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06537008677772904"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_16 = loser_16.getResult()[0]\n",
    "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
    "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
    "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
    "\n",
    "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
    "rmse_loser_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.90053797  0.80267336  6.          0.95614895 13.51015284  0.72846266]. \t  0.9911184981661268 \t 0.991204913651782\n",
      "2      \t [ 3.22107512  0.62812751  5.56929504  0.99346679 13.90633713  0.57125152]. \t  0.8932058195240776 \t 0.991204913651782\n",
      "3      \t [ 2.74645858  0.1592134   5.78079585  0.9853602  13.43315495  0.60789534]. \t  0.8932106202461063 \t 0.991204913651782\n",
      "4      \t [ 3.33703227  0.33828318  6.24259144  0.84321818 13.64685764  0.85284436]. \t  0.9908304518021286 \t 0.991204913651782\n",
      "5      \t [ 2.99670367  0.50478492  5.74996273  0.5        13.63475524  1.        ]. \t  0.9840180962871153 \t 0.991204913651782\n",
      "6      \t [ 2.94500359  0.50458446  6.17906757  0.5        13.81658537  0.37076759]. \t  0.8930089979414687 \t 0.991204913651782\n",
      "7      \t [ 9.9999997   5.55504333 10.18495177  0.79842991 14.51954541  0.99999997]. \t  \u001b[92m0.9938741654356291\u001b[0m \t 0.9938741654356291\n",
      "8      \t [ 9.35258898  5.69137018 10.1469173   0.79101177 14.51488292  0.75622019]. \t  0.9913633417655784 \t 0.9938741654356291\n",
      "9      \t [ 9.73437108  6.10438632 10.56420796  0.74664227 14.34472626  0.94595771]. \t  0.9910800944641819 \t 0.9938741654356291\n",
      "10     \t [ 9.76965674  6.19533053  9.87321481  0.94569314 14.5980029   1.        ]. \t  \u001b[92m0.9938981705669135\u001b[0m \t 0.9938981705669135\n",
      "11     \t [ 9.93308979  6.00151159 10.05254091  0.51798183 14.42765496  0.44589108]. \t  0.8904886697655491 \t 0.9938981705669135\n",
      "12     \t [ 9.84439458  5.78632405 10.36266695  1.         14.21558557  0.47543049]. \t  0.8873777037082623 \t 0.9938981705669135\n",
      "13     \t [ 7.08360687  7.49104393 12.84393033  0.50000395 17.84392852  0.74890306]. \t  0.9896014351571266 \t 0.9938981705669135\n",
      "14     \t [ 6.78424868  7.00058473 12.4365776   0.53359011 17.85896489  0.68268549]. \t  0.9899663041364283 \t 0.9938981705669135\n",
      "15     \t [ 6.65526182  7.24055897 12.83250923  0.50003288 17.2901397   0.69027001]. \t  0.9896062360174408 \t 0.9938981705669135\n",
      "16     \t [ 6.93216925  6.93983895 13.01985588  0.95318014 17.69521421  0.83512279]. \t  0.9914497500603835 \t 0.9938981705669135\n",
      "17     \t [ 6.9461835   6.98813865 13.01902762  0.5        17.7300659   0.19717492]. \t  0.8888995739387133 \t 0.9938981705669135\n",
      "18     \t [ 6.68172148  7.47856961 12.62880257  1.         17.69317261  0.45299579]. \t  0.8873777037082623 \t 0.9938981705669135\n",
      "19     \t [ 3.29549228  0.96989814  6.21393624  0.64437168 14.0576379   1.        ]. \t  0.9888333095377048 \t 0.9938981705669135\n",
      "20     \t [ 2.79161425  0.6416192   6.62906841  0.53560755 13.63757371  1.        ]. \t  0.9887756996287914 \t 0.9938981705669135\n",
      "21     \t [ 9.23135339  6.35063349  9.97896501  0.5        14.28398849  1.        ]. \t  0.9930916281084167 \t 0.9938981705669135\n",
      "22     \t [ 9.46005668  6.1596624  10.18958422  0.5        15.04187311  1.        ]. \t  0.9930676243599882 \t 0.9938981705669135\n",
      "23     \t [ 9.46563767  5.82495373  9.488442    0.5        14.71341635  1.        ]. \t  0.9930820266643595 \t 0.9938981705669135\n",
      "24     \t [ 6.52262247  6.55732044 13.00610131  0.5        17.64909517  1.        ]. \t  0.9931156326865587 \t 0.9938981705669135\n",
      "25     \t [ 6.14331393  6.86213705 12.66377515  0.91870284 17.57387195  1.        ]. \t  \u001b[92m0.9945990833122235\u001b[0m \t 0.9945990833122235\n",
      "26     \t [ 6.17839794  6.52463191 12.86530631  0.84517944 17.54168239  0.42235586]. \t  0.8927257571394943 \t 0.9945990833122235\n",
      "27     \t [ 6.63290944  6.49045282 12.4861319   0.84945658 17.17750259  0.99098206]. \t  0.9910032710191649 \t 0.9945990833122235\n",
      "28     \t [ 6.32051294  6.60997485 13.10941287  0.9955426  16.97771811  1.        ]. \t  0.994531872927252 \t 0.9945990833122235\n",
      "29     \t [ 5.99913439  6.50666893 12.68623438  0.5        16.88961395  1.        ]. \t  0.993144437433587 \t 0.9945990833122235\n",
      "30     \t [ 6.07132711  6.80935316 12.47185079  1.         16.64929993  0.7645949 ]. \t  0.9880939788470352 \t 0.9945990833122235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.062471077391935824"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_17 = loser_17.getResult()[0]\n",
    "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
    "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
    "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
    "\n",
    "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
    "rmse_loser_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8596968089427542 \t 0.9883196147183771\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.9878107251845023 \t 0.9883196147183771\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8627837722196116 \t 0.9883196147183771\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9856023554376728 \t 0.9883196147183771\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9883196147183771 \t 0.9883196147183771\n",
      "1      \t [ 3.25797746  2.33049336 10.71380175  0.76198781 13.91856439  0.99018838]. \t  \u001b[92m0.9886076715920792\u001b[0m \t 0.9886076715920792\n",
      "2      \t [ 2.58251757  2.22283183 10.68924156  0.78258852 13.7384221   0.9997988 ]. \t  \u001b[92m0.9886268753099069\u001b[0m \t 0.9886268753099069\n",
      "3      \t [ 2.89399994  2.43438462 11.33264403  0.78912436 13.74847419  0.99599768]. \t  0.9882476022285213 \t 0.9886268753099069\n",
      "4      \t [ 3.09778367  2.03316328 10.94089439  0.74231028 13.26867772  0.92971171]. \t  0.9881371875578617 \t 0.9886268753099069\n",
      "5      \t [ 2.92687886  2.21140662 10.95671951  0.51741923 13.74693878  0.40187436]. \t  0.862956593925788 \t 0.9886268753099069\n",
      "6      \t [ 6.17155849  7.92380305  8.13902181  0.73006487 12.44411003  0.99912929]. \t  0.9877435207458106 \t 0.9886268753099069\n",
      "7      \t [ 5.66246116  7.47890044  7.9358414   0.74709836 12.49967095  0.99953686]. \t  0.9873162365721427 \t 0.9886268753099069\n",
      "8      \t [ 6.15578766  7.27862853  8.37957886  0.6992949  12.53222898  0.72939635]. \t  0.9879067505496346 \t 0.9886268753099069\n",
      "9      \t [ 5.7525316   7.81576143  8.26338501  0.82243071 12.6779381   0.42785843]. \t  0.862831775982758 \t 0.9886268753099069\n",
      "10     \t [ 6.27237457  7.45000025  7.71042066  0.99986366 12.43395151  0.67922985]. \t  0.9881708011857676 \t 0.9886268753099069\n",
      "11     \t [ 6.13817753  7.57436293  7.69201069  0.5        12.53327834  0.55699086]. \t  0.863743933911027 \t 0.9886268753099069\n",
      "12     \t [10.          3.70448859 13.99999909  0.52520372  6.55038956  0.68900211]. \t  0.9859384166276638 \t 0.9886268753099069\n",
      "13     \t [10.          3.9597862  14.00000069  0.5         7.21006709  0.66969572]. \t  0.9845653835766557 \t 0.9886268753099069\n",
      "14     \t [ 9.42788026  3.92145564 13.74906256  0.6660174   6.84679569  0.58991534]. \t  0.8604361400482805 \t 0.9886268753099069\n",
      "15     \t [ 9.90642407  3.68464477 14.41261215  0.86067277  6.93897885  0.35425117]. \t  0.8593895569249302 \t 0.9886268753099069\n",
      "16     \t [10.          3.65125643 13.83355482  1.          6.97969334  1.        ]. \t  \u001b[92m0.9945414783815908\u001b[0m \t 0.9945414783815908\n",
      "17     \t [10.          3.52561797 13.52484609  0.74413995  6.99817455  0.36167927]. \t  0.8596391972361284 \t 0.9945414783815908\n",
      "18     \t [ 3.02378775  2.11340126 10.9237243   1.         13.68972459  0.56844894]. \t  0.7919999203829097 \t 0.9945414783815908\n",
      "19     \t [ 2.92738243  2.78959366 10.80436677  0.5        13.27851964  1.        ]. \t  0.9943782487160524 \t 0.9945414783815908\n",
      "20     \t [ 2.41989597  2.13480411 11.26353669  0.5        13.24821617  1.        ]. \t  0.9943782480246246 \t 0.9945414783815908\n",
      "21     \t [ 9.71574214  3.87683802 14.53199635  0.520526    6.88268883  1.        ]. \t  0.9932020419493629 \t 0.9945414783815908\n",
      "22     \t [ 6.12824447  6.84093585  7.89918842  1.         12.79236349  1.        ]. \t  0.9919970525613371 \t 0.9945414783815908\n",
      "23     \t [ 6.81438555  7.23524332  8.10043968  1.         12.67125074  1.        ]. \t  0.9934996981928385 \t 0.9945414783815908\n",
      "24     \t [ 6.48717439  6.9444472   7.86370652  0.73719148 12.11989551  1.        ]. \t  0.9917666026925515 \t 0.9945414783815908\n",
      "25     \t [ 6.69229381  6.95598525  7.60624471  0.51284331 12.78380686  1.        ]. \t  0.9916465795252702 \t 0.9945414783815908\n",
      "26     \t [ 3.57223181  2.15248695 11.33628202  0.5        13.68412957  1.        ]. \t  0.9942486256258555 \t 0.9945414783815908\n",
      "27     \t [ 2.94018623  1.65309445 11.6674849   0.5        13.5443625   1.        ]. \t  0.9942486256258555 \t 0.9945414783815908\n",
      "28     \t [ 3.10717387  2.34379271 11.67190373  0.5        13.02082127  1.        ]. \t  0.9943494453518799 \t 0.9945414783815908\n",
      "29     \t [ 6.79720523  6.60682098  7.84299438  0.86146807 12.59693652  0.58238534]. \t  0.8623372922103938 \t 0.9945414783815908\n",
      "30     \t [ 6.76054991  6.91052637  7.22411209  1.         12.49819765  1.        ]. \t  0.9919154402868511 \t 0.9945414783815908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06090072368145899"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_18 = loser_18.getResult()[0]\n",
    "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
    "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
    "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
    "\n",
    "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
    "rmse_loser_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.92265467  0.22716054 10.51172532  0.73846531  4.25587412  0.66803149]. \t  \u001b[92m0.9875803138994016\u001b[0m \t 0.9875803138994016\n",
      "2      \t [ 1.11079465  0.7499545  10.1215565   0.71269639  4.06078887  0.62456066]. \t  0.8920825244291999 \t 0.9875803138994016\n",
      "3      \t [ 0.39992307  0.65308564 10.4993183   0.84802064  4.10864442  0.80574644]. \t  0.9873546705605967 \t 0.9875803138994016\n",
      "4      \t [ 0.72611357  0.36562652 10.56813282  0.58912055  3.56790884  0.72291994]. \t  0.9868505866579809 \t 0.9875803138994016\n",
      "5      \t [ 0.50106024  0.41816002 10.3826022   0.5798657   3.99162911  0.14984105]. \t  0.8909591365987222 \t 0.9875803138994016\n",
      "6      \t [ 0.87339035  0.30450213 10.43513491  1.          3.67964696  0.40493425]. \t  0.8890723908202941 \t 0.9875803138994016\n",
      "7      \t [6.96969672 1.8434589  5.53400911 0.83583667 9.81829224 0.82696534]. \t  0.9715648155062065 \t 0.9875803138994016\n",
      "8      \t [7.05273669 1.94407261 5.01605139 0.83921317 9.35592154 0.87759873]. \t  0.9716176242782245 \t 0.9875803138994016\n",
      "9      \t [6.39762526 1.69949318 5.21678818 0.80072979 9.50749146 0.92531859]. \t  0.9716704334651053 \t 0.9875803138994016\n",
      "10     \t [6.63165165 2.39270795 5.38427076 0.64862871 9.51981606 1.        ]. \t  0.9839748821853681 \t 0.9875803138994016\n",
      "11     \t [6.61215866 2.17296094 5.21417359 0.988562   9.59211534 0.36615524]. \t  0.8936283960571468 \t 0.9875803138994016\n",
      "12     \t [6.78886963 1.85181849 5.04642666 0.5        9.74901479 0.54934743]. \t  0.8925098195276947 \t 0.9875803138994016\n",
      "13     \t [ 0.1991157   0.         10.62530674  0.5         4.03533458  0.94912288]. \t  0.986869788716362 \t 0.9875803138994016\n",
      "14     \t [6.7175384  2.02116973 5.72512072 1.         9.04721424 1.        ]. \t  0.9841573164676135 \t 0.9875803138994016\n",
      "15     \t [6.26977675 2.13320064 5.89896094 1.         9.69024812 1.        ]. \t  0.9841621171204964 \t 0.9875803138994016\n",
      "16     \t [ 0.39906117  0.4105258  10.24138976  0.5         4.61297033  1.        ]. \t  \u001b[92m0.9953960209393286\u001b[0m \t 0.9953960209393286\n",
      "17     \t [6.38597779 1.95138843 6.02648891 0.54453739 9.27156881 0.67803717]. \t  0.9776282208390557 \t 0.9953960209393286\n",
      "18     \t [5.93895156 2.24037204 5.79326475 0.9101635  9.00965221 0.80131946]. \t  0.9720641033194294 \t 0.9953960209393286\n",
      "19     \t [6.38588031 2.54974617 6.21886355 1.         9.17404845 0.56395767]. \t  0.8889187428928246 \t 0.9953960209393286\n",
      "20     \t [6.0672022  1.72780114 6.11298611 1.         9.15297329 0.39954597]. \t  0.8887315115531731 \t 0.9953960209393286\n",
      "21     \t [6.01428879 2.03367631 6.51588105 0.92423544 8.97502019 1.        ]. \t  0.9889725313769387 \t 0.9953960209393286\n",
      "22     \t [ 0.          0.55991298 10.07807142  0.5         3.88847228  1.        ]. \t  \u001b[92m0.995928911386705\u001b[0m \t 0.995928911386705\n",
      "23     \t [5.56750188 2.25909485 6.52632577 0.6784002  9.30320266 0.70186062]. \t  0.9785547777527107 \t 0.995928911386705\n",
      "24     \t [5.7222547  2.23889243 6.59803932 0.53681379 8.62530869 0.51821492]. \t  0.8890436171188304 \t 0.995928911386705\n",
      "25     \t [5.4523902  1.72485511 6.28837809 0.5        9.01237852 1.        ]. \t  0.9888717147623606 \t 0.995928911386705\n",
      "26     \t [5.09700331 1.97076842 6.53042362 1.         8.80612817 0.85412751]. \t  0.9837204381790242 \t 0.995928911386705\n",
      "27     \t [5.27728737 1.58741444 7.0006194  0.70089887 8.99446386 0.70903694]. \t  0.9834948165513674 \t 0.995928911386705\n",
      "28     \t [5.01259707 2.09247856 7.05412745 0.5        8.80006364 1.        ]. \t  0.9909312695935194 \t 0.995928911386705\n",
      "29     \t [4.66171369 1.8421753  6.71939327 0.5        8.94425266 0.504505  ]. \t  0.8898069429149218 \t 0.995928911386705\n",
      "30     \t [4.81385248 1.77226766 6.88420794 0.77271533 9.44495296 1.        ]. \t  0.9887804988312542 \t 0.995928911386705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05301747881113507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_19 = loser_19.getResult()[0]\n",
    "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
    "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
    "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
    "\n",
    "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
    "rmse_loser_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.38908336  5.71876821 11.78020937  0.52347406 16.65937155  0.7316268 ]. \t  0.9911328909287936 \t 0.9913825280594238\n",
      "2      \t [ 5.8677012   5.90731614 11.78636909  0.78955791 16.224081    0.78048858]. \t  \u001b[92m0.9915985669118469\u001b[0m \t 0.9915985669118469\n",
      "3      \t [ 5.81226721  5.44734045 12.23171097  0.50142878 16.35011664  0.56507664]. \t  0.8879777943766932 \t 0.9915985669118469\n",
      "4      \t [ 5.40928366  5.91961256 12.26371237  0.99722354 16.4565032   0.93379885]. \t  \u001b[92m0.9916225755002706\u001b[0m \t 0.9916225755002706\n",
      "5      \t [ 5.52022407  6.29604424 12.15991761  0.50000734 16.38881781  0.51231807]. \t  0.887228880633948 \t 0.9916225755002706\n",
      "6      \t [ 5.34557307  5.72504057 11.86490935  1.         16.28560947  0.30395124]. \t  0.8981267111643761 \t 0.9916225755002706\n",
      "7      \t [0.7425564  6.42152097 6.17146931 0.56525303 7.82853066 0.98274098]. \t  0.9911472937863074 \t 0.9916225755002706\n",
      "8      \t [0.7175067  5.98308921 6.68480222 0.5793355  7.92704586 0.79479403]. \t  0.9911616900061134 \t 0.9916225755002706\n",
      "9      \t [0.84065125 6.20318343 6.35514578 0.52620533 8.50911602 1.        ]. \t  0.9889629232244825 \t 0.9916225755002706\n",
      "10     \t [1.35488492 6.00260429 6.31102571 0.55758993 7.97260784 1.        ]. \t  0.9889149171104812 \t 0.9916225755002706\n",
      "11     \t [1.02547531 6.23180236 6.22212218 0.70561252 8.14216849 0.36639149]. \t  0.888899552227878 \t 0.9916225755002706\n",
      "12     \t [ 5.69909413  5.72122893 11.3107628   0.63472502 18.24257186  0.64963205]. \t  0.8865663702077273 \t 0.9916225755002706\n",
      "13     \t [0.88105699 6.05990069 6.36604363 1.         8.10288764 1.        ]. \t  0.9895582281057269 \t 0.9916225755002706\n",
      "14     \t [ 5.17450704  6.24032647 11.59658576  0.95380115 16.28184932  1.        ]. \t  \u001b[92m0.9949159299981116\u001b[0m \t 0.9949159299981116\n",
      "15     \t [ 5.59733198  6.26499802 11.5801766   1.         17.06949101  1.        ]. \t  \u001b[92m0.9949351352370807\u001b[0m \t 0.9949351352370807\n",
      "16     \t [ 5.52638083  6.1757945  10.89949571  0.78495325 18.51983172  0.90294115]. \t  0.9916753777728765 \t 0.9949351352370807\n",
      "17     \t [ 6.08192095  6.20345344 11.22679378  0.75053105 18.73111708  0.97572806]. \t  0.9916321744551874 \t 0.9949351352370807\n",
      "18     \t [ 5.93568106  6.26612533 10.99745772  1.         18.62544706  0.36031509]. \t  0.8978530618498972 \t 0.9949351352370807\n",
      "19     \t [ 5.49154303  6.15375803 11.50423004  1.         18.86439212  0.81679087]. \t  0.991334532455126 \t 0.9949351352370807\n",
      "20     \t [ 5.58838023  6.53671751 11.41421826  0.5        18.6099625   0.64545705]. \t  0.887228880633948 \t 0.9949351352370807\n",
      "21     \t [ 5.68213579  6.37244866 10.95972289  0.85198576 19.38809228  0.88462158]. \t  0.9915505620424158 \t 0.9949351352370807\n",
      "22     \t [ 5.43205647  5.36605709 11.57257796  1.         16.36890264  1.        ]. \t  \u001b[92m0.9949687411209943\u001b[0m \t 0.9949687411209943\n",
      "23     \t [ 5.48231327  5.87154336 10.94884434  1.         16.86530367  1.        ]. \t  \u001b[92m0.9949975457297371\u001b[0m \t 0.9949975457297371\n",
      "24     \t [ 4.92478711  5.86804794 11.35564998  1.         17.20486534  1.        ]. \t  0.9949351352370807 \t 0.9949975457297371\n",
      "25     \t [ 5.17590947  6.29626913 11.11742409  1.         17.19796519  0.47569726]. \t  0.8983859576213412 \t 0.9949975457297371\n",
      "26     \t [ 6.04806705  6.17263246 11.51462045  0.72913394 19.51375286  0.56854864]. \t  0.8865423632787305 \t 0.9949975457297371\n",
      "27     \t [ 5.16479879  6.33296368 11.07795581  0.5        17.1297881   1.        ]. \t  0.9931252248654824 \t 0.9949975457297371\n",
      "28     \t [ 6.01006322  5.70354858 11.03794851  1.         19.40635925  1.        ]. \t  0.9945798721269746 \t 0.9949975457297371\n",
      "29     \t [ 5.46089969  5.82558424 11.38295059  0.65132918 19.61778514  1.        ]. \t  0.9938357456925573 \t 0.9949975457297371\n",
      "30     \t [ 4.80083241  5.74671581 10.97608534  0.69625959 16.45135513  1.        ]. \t  0.9943590379456603 \t 0.9949975457297371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06408935476080316"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_20 = loser_20.getResult()[0]\n",
    "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
    "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
    "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
    "\n",
    "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
    "rmse_loser_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.22421775 6.94445625 6.57125228 0.88103707 5.42874771 0.76880439]. \t  0.9779930741227703 \t 0.9846806025668978\n",
      "2      \t [4.03733537 6.89983276 7.15869848 0.82026985 4.84130141 0.83465477]. \t  0.9822657921623131 \t 0.9846806025668978\n",
      "3      \t [4.50894    6.483207   7.19162277 0.99279668 5.19356152 0.96118914]. \t  0.9822609920625808 \t 0.9846806025668978\n",
      "4      \t [3.87880622 6.75026185 7.34034623 0.99999713 5.52389673 0.98063776]. \t  0.9822417891053306 \t 0.9846806025668978\n",
      "5      \t [4.05581398 6.5066273  7.1860131  0.98426354 5.25906357 0.31759802]. \t  0.8876465468539979 \t 0.9846806025668978\n",
      "6      \t [4.1065093  6.46667445 7.14892667 0.5        5.38686014 0.95493125]. \t  0.9819825451375417 \t 0.9846806025668978\n",
      "7      \t [4.66236562 7.53361376 6.92011655 0.73621929 5.71487614 0.6802024 ]. \t  0.9776618179417067 \t 0.9846806025668978\n",
      "8      \t [4.50008935 7.91326273 6.47500285 1.         5.55443836 1.        ]. \t  \u001b[92m0.9893277890246779\u001b[0m \t 0.9893277890246779\n",
      "9      \t [4.0746246  7.92145406 6.53704727 0.61556374 5.82253798 0.64129593]. \t  0.887334490933577 \t 0.9893277890246779\n",
      "10     \t [4.50163317 7.62040521 6.51282736 1.         6.24979119 1.        ]. \t  \u001b[92m0.9893277890246779\u001b[0m \t 0.9893277890246779\n",
      "11     \t [4.7347794  7.57886737 6.2775647  0.5        5.83846087 1.        ]. \t  0.9886220698877363 \t 0.9893277890246779\n",
      "12     \t [4.83308085 8.28295083 6.52132356 0.65198248 6.16045675 1.        ]. \t  0.9887996993684718 \t 0.9893277890246779\n",
      "13     \t [ 5.06471931  2.79992029 10.40127347  0.82839015 14.01297839  0.75039022]. \t  0.9844741655733887 \t 0.9893277890246779\n",
      "14     \t [ 5.42404863  2.62887233 10.62741737  0.80574371 14.552454    0.72129912]. \t  0.9846661999168117 \t 0.9893277890246779\n",
      "15     \t [ 4.86008639  2.29876287 10.8337256   0.81497918 14.26845002  0.64604883]. \t  0.8884242862248932 \t 0.9893277890246779\n",
      "16     \t [ 5.30033329  2.78899875 10.65475556  0.74455085 14.17403903  0.114177  ]. \t  0.8878961915903346 \t 0.9893277890246779\n",
      "17     \t [ 5.58837967  2.32577766 10.5599145   0.67775684 13.90559864  0.73239048]. \t  0.9833315735410727 \t 0.9893277890246779\n",
      "18     \t [ 5.71556824  3.05203101 10.48314512  0.5        14.07650025  0.86225511]. \t  0.9823666078780223 \t 0.9893277890246779\n",
      "19     \t [ 5.91105896  2.90596227 10.45772863  1.         13.95636452  0.66654287]. \t  0.8980835227957353 \t 0.9893277890246779\n",
      "20     \t [5.32469228 7.81146802 6.43577921 1.         6.00916476 1.        ]. \t  \u001b[92m0.9893277890246779\u001b[0m \t 0.9893277890246779\n",
      "21     \t [3.94049766 6.27219015 7.76080617 0.83551378 4.91378821 1.        ]. \t  \u001b[92m0.9929716040431945\u001b[0m \t 0.9929716040431945\n",
      "22     \t [5.209913   7.75393007 6.55162548 0.56090258 6.5568199  0.75380086]. \t  0.9768600851939584 \t 0.9929716040431945\n",
      "23     \t [5.20687172 8.01814811 5.99220218 0.85035536 6.6580076  1.        ]. \t  0.9843349429060226 \t 0.9929716040431945\n",
      "24     \t [5.25222687 8.29635104 6.49478512 1.         6.69419974 0.56427653]. \t  0.8981123257450193 \t 0.9929716040431945\n",
      "25     \t [5.66317831 8.25634042 6.12880197 0.5        6.34323065 0.79826525]. \t  0.9768792882894962 \t 0.9929716040431945\n",
      "26     \t [4.01862861 5.76547875 7.5716196  1.         5.40403921 0.99908111]. \t  0.9857415823247616 \t 0.9929716040431945\n",
      "27     \t [4.2466973  5.65493063 7.45368645 0.7027027  4.80018926 0.76722029]. \t  0.982083361959551 \t 0.9929716040431945\n",
      "28     \t [3.58194844 5.67775086 7.6168871  0.56047407 5.08940728 0.62623132]. \t  0.8876897545968254 \t 0.9929716040431945\n",
      "29     \t [4.36608174 5.8555029  7.97145111 0.61878399 5.28213081 0.57102777]. \t  0.8876897545968254 \t 0.9929716040431945\n",
      "30     \t [3.98586908 5.66218015 7.87339585 1.         4.77222125 0.39548634]. \t  0.8978626819766955 \t 0.9929716040431945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07154817640984364"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_1 = winner_1.getResult()[0]\n",
    "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
    "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
    "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
    "\n",
    "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
    "rmse_winner_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 3.2476424   8.20977996  6.18582198  0.93284731 12.02216765  0.71996595]. \t  0.9907344211821445 \t 0.9907392220424587\n",
      "2      \t [ 2.93073108  8.7025569   6.48324781  0.88787235 12.282161    0.73897393]. \t  \u001b[92m0.99081123342603\u001b[0m \t 0.99081123342603\n",
      "3      \t [ 2.86212502  8.17011586  6.25766121  0.93935773 12.53831178  0.35340924]. \t  0.8939356746105634 \t 0.99081123342603\n",
      "4      \t [ 3.04774804  8.49020681  5.89674985  0.66741272 12.47645352  1.        ]. \t  0.9840661026776879 \t 0.99081123342603\n",
      "5      \t [ 3.17262775  8.80165204  5.83733004  0.98456634 12.20196833  0.4424552 ]. \t  0.8969217806860054 \t 0.99081123342603\n",
      "6      \t [ 2.99368457  8.4864826   6.21957361  0.5        11.99753222  0.43172658]. \t  0.8910215178284694 \t 0.99081123342603\n",
      "7      \t [ 6.76863558  9.16634027 10.18829946  0.733105   15.23775036  0.84332077]. \t  0.9904271735894589 \t 0.99081123342603\n",
      "8      \t [ 6.17366627  9.00203374 10.23566877  0.73732814 14.8937964   0.82865612]. \t  \u001b[92m0.9911377000862424\u001b[0m \t 0.9911377000862424\n",
      "9      \t [ 6.55111159  9.01080068  9.59381882  0.74878171 14.876513    0.81127942]. \t  0.9898702736547298 \t 0.9911377000862424\n",
      "10     \t [ 6.7728312   9.20791933 10.1692896   0.74661837 14.47340286  0.92027292]. \t  0.9908064416925636 \t 0.9911377000862424\n",
      "11     \t [ 6.59659364  9.21260735 10.07521571  0.7135918  14.79206007  0.23063437]. \t  0.8908295185404388 \t 0.9911377000862424\n",
      "12     \t [ 8.28924121  4.77923418 11.85605185  0.87595909 16.43183616  0.82594131]. \t  0.9909264594667073 \t 0.9911377000862424\n",
      "13     \t [ 8.18664314  4.9338742  11.76294742  0.82076368 15.7576344   0.80752002]. \t  0.9908496533073873 \t 0.9911377000862424\n",
      "14     \t [ 7.70630422  4.57777722 12.05151495  0.90770512 16.07592334  0.72936795]. \t  0.9908400452256224 \t 0.9911377000862424\n",
      "15     \t [ 8.00619728  4.4164829  11.36872092  0.99899448 16.0790107   0.76985877]. \t  0.9908256430595367 \t 0.9911377000862424\n",
      "16     \t [ 8.14537245  4.62505684 11.74983783  0.7850172  16.08704744  0.16606447]. \t  0.8908343211984655 \t 0.9911377000862424\n",
      "17     \t [ 7.93751728  4.49340967 11.651532    0.5        16.14459052  1.        ]. \t  \u001b[92m0.9928515878584198\u001b[0m \t 0.9928515878584198\n",
      "18     \t [ 6.73169164  8.56346573 10.09740278  0.5        14.77388517  1.        ]. \t  0.9928419865526482 \t 0.9928515878584198\n",
      "19     \t [ 2.71668242  7.99613323  6.521126    0.67087576 12.28439247  1.        ]. \t  0.9887949027934769 \t 0.9928515878584198\n",
      "20     \t [ 6.76663611  8.51662715 10.14712222  1.         14.75786633  0.75165538]. \t  0.9917089984533466 \t 0.9928515878584198\n",
      "21     \t [ 2.37592262  8.39883092  6.23687894  1.         12.71820287  1.        ]. \t  0.9887132876149938 \t 0.9928515878584198\n",
      "22     \t [ 2.70820595  7.75086689  5.88763697  1.         12.55408831  1.        ]. \t  0.9839556839967466 \t 0.9928515878584198\n",
      "23     \t [ 3.10256691  8.07188412  6.44379271  1.         12.92033841  1.        ]. \t  0.9886460764003089 \t 0.9928515878584198\n",
      "24     \t [ 2.59594466  7.90538876  6.20062676  0.5002157  13.07232645  1.        ]. \t  0.9884444413733985 \t 0.9928515878584198\n",
      "25     \t [ 2.37855754  7.57322181  6.54323946  1.         13.07968212  1.        ]. \t  0.9886988850340512 \t 0.9928515878584198\n",
      "26     \t [ 3.17677359  8.9010621   6.07430693  0.90698429 11.61284385  1.        ]. \t  0.9887660974933064 \t 0.9928515878584198\n",
      "27     \t [ 2.97830538  7.22774066  6.40558104  0.70445482 12.95202702  1.        ]. \t  0.988862116220731 \t 0.9928515878584198\n",
      "28     \t [ 2.81363573  7.41598998  6.03993939  1.         13.51985111  1.        ]. \t  0.9886796828373647 \t 0.9928515878584198\n",
      "29     \t [ 2.57091461  7.16044674  6.13594468  0.74641988 13.29220223  0.47482711]. \t  0.8965953065583719 \t 0.9928515878584198\n",
      "30     \t [ 2.90602372  7.41358421  6.68255077  0.71818017 13.67543496  0.75167922]. \t  0.9899999079460583 \t 0.9928515878584198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06744705753275529"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_2 = winner_2.getResult()[0]\n",
    "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
    "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
    "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
    "\n",
    "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
    "rmse_winner_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [5.78447071 4.42074168 9.5623012  0.70777186 9.12206735 0.69494315]. \t  0.9848342349370162 \t 0.9850406749037081\n",
      "2      \t [6.25625875 4.00681191 9.52092856 0.69893582 8.81652898 0.80827564]. \t  0.9843493553745262 \t 0.9850406749037081\n",
      "3      \t [6.23911713 4.04747474 9.72957782 0.69565842 9.53161842 0.8131828 ]. \t  \u001b[92m0.9850982826691581\u001b[0m \t 0.9850982826691581\n",
      "4      \t [6.48359898 4.48861184 9.40313541 0.80426826 9.22224457 0.46191915]. \t  0.8847228487824444 \t 0.9850982826691581\n",
      "5      \t [6.03312265 3.96706589 9.92673687 0.77699572 9.08956428 0.26359811]. \t  0.8858654508406089 \t 0.9850982826691581\n",
      "6      \t [5.87870793 4.02946844 9.96561523 1.         9.07220065 1.        ]. \t  \u001b[92m0.9944502577495217\u001b[0m \t 0.9944502577495217\n",
      "7      \t [6.23261453 4.60081778 9.49477193 1.         9.23548642 1.        ]. \t  \u001b[92m0.994479062496544\u001b[0m \t 0.994479062496544\n",
      "8      \t [6.74753803 4.0161104  9.96535227 1.         9.10891854 0.99311148]. \t  0.9654677419217532 \t 0.994479062496544\n",
      "9      \t [ 5.99915403  4.56976046 10.069497    1.          9.62571403  0.64405567]. \t  0.7999788745413382 \t 0.994479062496544\n",
      "10     \t [5.959395   3.84685568 9.15984291 1.         9.32916085 0.81561968]. \t  0.9656117669705967 \t 0.994479062496544\n",
      "11     \t [6.06206318 4.5097913  9.7196254  1.         8.51208414 0.62190361]. \t  0.7999788745413382 \t 0.994479062496544\n",
      "12     \t [6.38233909 3.34599398 9.63863688 1.         9.21176094 1.        ]. \t  0.9944742616362299 \t 0.994479062496544\n",
      "13     \t [6.80476202 3.71649706 9.16121165 1.         9.3480948  1.        ]. \t  0.9944646599847453 \t 0.994479062496544\n",
      "14     \t [6.76269327 3.40351755 9.4172325  1.         9.29573016 0.40184575]. \t  0.7998492531797377 \t 0.994479062496544\n",
      "15     \t [ 0.84657177  4.48154073 10.5223815   0.66853684 14.9257901   0.62108012]. \t  0.8864415358934941 \t 0.994479062496544\n",
      "16     \t [ 0.8110757   4.20682287 11.05193128  0.67989504 15.3019923   0.56893641]. \t  0.886614364375628 \t 0.994479062496544\n",
      "17     \t [ 0.42693936  4.14579732 10.96818153  0.60453371 14.69812114  0.73360347]. \t  0.9877147231222848 \t 0.994479062496544\n",
      "18     \t [ 0.17172314  4.26923505 10.66871073  0.70271735 15.20771145  0.91364885]. \t  0.9883436312599513 \t 0.994479062496544\n",
      "19     \t [ 0.16367984  4.23916711 10.72615912  0.81974218 15.00477546  0.22230307]. \t  0.8869888262943494 \t 0.994479062496544\n",
      "20     \t [ 0.54726106  4.31809219 10.95838837  1.         14.87563422  1.        ]. \t  \u001b[92m0.9954824346272453\u001b[0m \t 0.9954824346272453\n",
      "21     \t [ 0.42297105  3.64878234 10.5586339   1.         14.97588199  0.8333527 ]. \t  0.9678633499222379 \t 0.9954824346272453\n",
      "22     \t [ 0.07940463  4.17577958 10.35605087  1.         14.53103881  0.93258709]. \t  0.9681369952263813 \t 0.9954824346272453\n",
      "23     \t [ 0.          3.74788767 11.06342871  1.         14.80928879  1.        ]. \t  0.9954536294653602 \t 0.9954824346272453\n",
      "24     \t [ 0.          3.5626172  10.72384285  1.         14.26617492  0.6115389 ]. \t  0.7998492531797377 \t 0.9954824346272453\n",
      "25     \t [ 5.85663008  0.96848247  7.14846772  0.8917817  12.47105485  0.38198562]. \t  0.8862254974559317 \t 0.9954824346272453\n",
      "26     \t [ 5.64241624  0.51673246  6.66459908  0.89545844 12.40839549  0.26692869]. \t  0.8883906719745838 \t 0.9954824346272453\n",
      "27     \t [ 6.33738456  0.66009593  6.71564059  0.89369592 12.62212688  0.23614175]. \t  0.8888083408064072 \t 0.9954824346272453\n",
      "28     \t [ 5.82294743  1.15260544  6.46029671  0.84618598 12.73888144  0.26891604]. \t  0.8878337931287067 \t 0.9954824346272453\n",
      "29     \t [ 5.93958603  0.73674718  6.66410126  0.64656571 12.67298833  0.85698184]. \t  0.9857799963290845 \t 0.9954824346272453\n",
      "30     \t [ 5.91309407  0.77196906  6.81471851  0.5        12.59825758  0.13698359]. \t  0.8846748330574931 \t 0.9954824346272453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0587782698845408"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_3 = winner_3.getResult()[0]\n",
    "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
    "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
    "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
    "\n",
    "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
    "rmse_winner_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [ 2.40931713  4.0277959  10.55087137  0.84400819  1.05677146  0.99556753]. \t  0.9677577353468735 \t 0.9680361805433857\n",
      "2      \t [ 2.97853234  3.92523825 10.15415884  0.92672901  1.          0.99964453]. \t  \u001b[92m0.9680889922194108\u001b[0m \t 0.9680889922194108\n",
      "3      \t [ 2.63257211  3.82888955 10.19927821  0.81607051  1.64147012  0.96615418]. \t  0.9680361805433857 \t 0.9680889922194108\n",
      "4      \t [ 2.63516256  3.8888517  10.26401795  0.90894985  1.1978451   0.35048561]. \t  0.7980393891061471 \t 0.9680889922194108\n",
      "5      \t [2.57272673 4.33938094 9.9764716  0.5        1.21777193 1.        ]. \t  \u001b[92m0.9949015368205881\u001b[0m \t 0.9949015368205881\n",
      "6      \t [2.36781298 4.40086286 9.92086024 1.         1.33975235 1.        ]. \t  \u001b[92m0.9954584313621799\u001b[0m \t 0.9954584313621799\n",
      "7      \t [2.22948638 8.4124416  6.00167472 0.77238561 7.59236678 0.77597772]. \t  0.9549347632876035 \t 0.9954584313621799\n",
      "8      \t [1.76972172 8.4282456  6.51170664 0.73100606 7.53511301 0.93283363]. \t  0.9549971701848348 \t 0.9954584313621799\n",
      "9      \t [2.23857469 8.76674638 6.51754228 0.76660978 7.99353015 0.83305177]. \t  0.9549827698164615 \t 0.9954584313621799\n",
      "10     \t [2.05343674 8.09579422 6.47110556 0.75199793 8.00316074 0.50859489]. \t  0.7948036969274525 \t 0.9954584313621799\n",
      "11     \t [1.87394193 8.92157148 6.26091735 0.67754209 7.56226822 0.40216256]. \t  0.7961911061468582 \t 0.9954584313621799\n",
      "12     \t [1.96619924 9.02348698 6.06948958 1.         7.58823009 1.        ]. \t  0.9892029712184717 \t 0.9954584313621799\n",
      "13     \t [2.18906534 9.02770058 5.94071544 0.5        7.73123289 1.        ]. \t  0.9838260556516977 \t 0.9954584313621799\n",
      "14     \t [2.807241   4.00760401 9.40390361 0.88119546 1.40703183 1.        ]. \t  0.995261598025297 \t 0.9954584313621799\n",
      "15     \t [3.14945383 4.47875898 9.83342366 0.97091497 1.63824831 1.        ]. \t  0.9952808016048392 \t 0.9954584313621799\n",
      "16     \t [3.05146014 4.62751872 9.46649794 1.         1.         1.        ]. \t  0.9953048043852689 \t 0.9954584313621799\n",
      "17     \t [3.01222846 4.60810403 9.38263843 0.85925195 1.40756835 0.44557383]. \t  0.7981786088004063 \t 0.9954584313621799\n",
      "18     \t [3.53063553 4.3249242  9.45856299 0.56460967 1.22324168 1.        ]. \t  0.994834321180765 \t 0.9954584313621799\n",
      "19     \t [3.17034198 4.79466699 9.16486978 0.5        1.51249826 1.        ]. \t  0.9947287062641349 \t 0.9954584313621799\n",
      "20     \t [3.63765864 4.62574175 8.93196183 1.         1.50478691 1.        ]. \t  0.9945510767833655 \t 0.9954584313621799\n",
      "21     \t [3.84230278 5.14792086 9.31883749 0.7421549  1.335711   1.        ]. \t  0.9950455605557297 \t 0.9954584313621799\n",
      "22     \t [3.75823831 4.95407393 8.7124782  0.52680428 1.         1.        ]. \t  0.9940517903529745 \t 0.9954584313621799\n",
      "23     \t [4.17027371 4.8918735  8.79819633 0.5        1.65631691 1.        ]. \t  0.9938837614865488 \t 0.9954584313621799\n",
      "24     \t [3.79387889 5.45708465 8.52817503 0.64495957 1.59696939 1.        ]. \t  0.9943158384999672 \t 0.9954584313621799\n",
      "25     \t [3.99246873 5.21668002 8.67448    0.5        1.45762808 0.41232885]. \t  0.7991819688312302 \t 0.9954584313621799\n",
      "26     \t [4.3543287  5.30354337 8.52213745 0.98578284 1.28368051 1.        ]. \t  0.9944694643705937 \t 0.9954584313621799\n",
      "27     \t [4.10193365 4.99711118 7.95686998 0.75200557 1.48745244 1.        ]. \t  0.9930292167860469 \t 0.9954584313621799\n",
      "28     \t [4.46136281 5.52767806 7.99924931 0.5        1.24322653 1.        ]. \t  0.9920018445713744 \t 0.9954584313621799\n",
      "29     \t [4.61073947 5.4682818  8.01045088 0.70554597 1.92669668 1.        ]. \t  0.9941766160399967 \t 0.9954584313621799\n",
      "30     \t [4.28899743 5.78653454 7.64237172 1.         1.58304569 1.        ]. \t  0.9929956190609822 \t 0.9954584313621799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0579840047531662"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_4 = winner_4.getResult()[0]\n",
    "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
    "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
    "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
    "\n",
    "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
    "rmse_winner_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 8.62535625  7.92131476 11.47494853  0.811424   16.65069748  0.71241283]. \t  \u001b[92m0.9876571089956068\u001b[0m \t 0.9876571089956068\n",
      "2      \t [ 8.42331565  7.40574226 11.85634965  0.79786974 16.79829222  0.54659997]. \t  0.8875025212520042 \t 0.9876571089956068\n",
      "3      \t [ 8.77559582  7.95158984 11.57428208  0.83756592 17.07639133  0.17289143]. \t  0.8875793339107304 \t 0.9876571089956068\n",
      "4      \t [ 8.74349997  7.69080311 11.46486143  0.79619483 17.27282976  0.94943209]. \t  \u001b[92m0.9876619093027706\u001b[0m \t 0.9876619093027706\n",
      "5      \t [ 8.11651419  8.02667509 11.52823646  0.80272178 17.18754009  0.73837442]. \t  0.987623503457414 \t 0.9876619093027706\n",
      "6      \t [4.9584602  9.81353364 5.         0.79987014 5.40479372 0.82130251]. \t  0.9790300477813249 \t 0.9876619093027706\n",
      "7      \t [4.47772436 9.82775171 5.         0.86868769 5.91622143 0.75742348]. \t  0.9790492509460064 \t 0.9876619093027706\n",
      "8      \t [4.8265774  9.3241908  5.39106931 0.82183875 5.7667879  0.69962542]. \t  0.9789724375266985 \t 0.9876619093027706\n",
      "9      \t [4.89110272 9.99608348 5.44462767 0.94475969 5.74921104 0.35529258]. \t  0.8875793339107304 \t 0.9876619093027706\n",
      "10     \t [4.8111998  9.90391248 5.48120693 0.5        5.84162108 1.        ]. \t  0.9839652825390534 \t 0.9876619093027706\n",
      "11     \t [4.8284743  9.61320706 5.         0.5        5.81508879 0.3580095 ]. \t  0.8874977192853896 \t 0.9876619093027706\n",
      "12     \t [ 8.69418804  8.45580479 11.39631981  1.         17.23018531  0.94706531]. \t  \u001b[92m0.9876907125977735\u001b[0m \t 0.9876907125977735\n",
      "13     \t [ 8.68555841  8.36688263 11.37069085  0.5        17.21330175  0.78409522]. \t  0.9862648763755811 \t 0.9876907125977735\n",
      "14     \t [ 8.50093907  8.09566821 10.78296635  0.92463879 17.21514514  0.72670681]. \t  0.987661908542189 \t 0.9876907125977735\n",
      "15     \t [ 8.53642907  8.20465293 11.15485851  0.93000927 17.86434437  0.63159273]. \t  0.8874977192853896 \t 0.9876907125977735\n",
      "16     \t [4.40539476 9.76839898 5.60638903 1.         5.40133598 1.        ]. \t  0.984133309815154 \t 0.9876907125977735\n",
      "17     \t [4.22018172 9.44477073 5.26313058 0.5        5.35020922 1.        ]. \t  0.9839172740741992 \t 0.9876907125977735\n",
      "18     \t [4.1618319  9.77624667 5.65356462 0.5        5.30672475 0.51158205]. \t  0.8874977192853896 \t 0.9876907125977735\n",
      "19     \t [4.17714589 9.38147799 5.15167522 1.         5.11613433 0.57269852]. \t  0.8993749505556262 \t 0.9876907125977735\n",
      "20     \t [4.65288837 9.38811581 5.65611169 0.6008646  4.89930763 0.89508525]. \t  0.9788764214267157 \t 0.9876907125977735\n",
      "21     \t [4.28238386 9.89307483 5.32342821 0.60817293 4.6853383  1.        ]. \t  0.9838932701183473 \t 0.9876907125977735\n",
      "22     \t [3.89949941 9.40791475 5.70888219 0.72014745 4.50756604 1.        ]. \t  0.9839892860108987 \t 0.9876907125977735\n",
      "23     \t [4.19913034 9.88617248 6.12462798 0.5        4.47574862 1.        ]. \t  \u001b[92m0.9887900991690067\u001b[0m \t 0.9887900991690067\n",
      "24     \t [4.27620589 9.80739018 5.9072221  1.         4.2345265  0.62232848]. \t  0.8993749505556262 \t 0.9887900991690067\n",
      "25     \t [4.29453273 9.57114644 5.81254387 0.5        3.8588913  1.        ]. \t  0.9838788674682609 \t 0.9887900991690067\n",
      "26     \t [3.72872678 9.92234691 5.78034561 0.5        3.93105034 0.70320747]. \t  0.9797837708887748 \t 0.9887900991690067\n",
      "27     \t [4.07694212 9.36771019 5.77877096 0.5        4.10758292 0.33180163]. \t  0.8874977192853896 \t 0.9887900991690067\n",
      "28     \t [3.78132865 9.55857809 6.41824855 0.5        3.7651823  0.86488339]. \t  0.9832163519946636 \t 0.9887900991690067\n",
      "29     \t [ 4.18138931 10.          6.39371334  0.5         3.52096317  0.5615866 ]. \t  0.8874977192853896 \t 0.9887900991690067\n",
      "30     \t [3.74656958 9.54966988 5.99491042 0.85769554 3.31426772 0.72811055]. \t  0.9790588524592033 \t 0.9887900991690067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08355944828198339"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_5 = winner_5.getResult()[0]\n",
    "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
    "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
    "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
    "\n",
    "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
    "rmse_winner_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.52079206  6.31724056 14.00000035  0.7600664  12.58921452  0.71818252]. \t  \u001b[92m0.9894526268810538\u001b[0m \t 0.9894526268810538\n",
      "2      \t [ 5.24282073  6.00356462 14.00000026  0.71892709 12.00867229  0.72783809]. \t  0.9891261674116775 \t 0.9894526268810538\n",
      "3      \t [ 5.14610809  6.68314314 13.72917485  0.70906444 12.12261155  0.72660041]. \t  0.9891165662441993 \t 0.9894526268810538\n",
      "4      \t [ 5.5793742   6.23213171 13.47601639  0.8696256  12.14189914  0.39185185]. \t  0.8631332519644864 \t 0.9894526268810538\n",
      "5      \t [ 5.34396674  6.5360838  14.30843052  0.75546597 12.0966844   0.28413915]. \t  0.8640454014571342 \t 0.9894526268810538\n",
      "6      \t [ 5.21292511  6.51204288 14.279971    1.         12.13876756  1.        ]. \t  \u001b[92m0.9950839673697963\u001b[0m \t 0.9950839673697963\n",
      "7      \t [ 5.11685146  6.49725799 14.41498792  0.5        12.33152857  1.        ]. \t  0.9936869306396986 \t 0.9950839673697963\n",
      "8      \t [ 4.68152461  6.23621196 13.98577185  0.77954021 12.69233261  0.90826719]. \t  0.9890781586702481 \t 0.9950839673697963\n",
      "9      \t [ 4.52759328  5.95442468 14.02165492  0.88063761 14.11782318  0.52248261]. \t  0.8632100646232127 \t 0.9950839673697963\n",
      "10     \t [ 5.15386765  6.09082046 13.43287106  0.5        12.54810205  1.        ]. \t  0.9936869306396986 \t 0.9950839673697963\n",
      "11     \t [ 4.79786431  6.05119657 13.58168028  0.5        12.90824332  0.36782032]. \t  0.8566473790829571 \t 0.9950839673697963\n",
      "12     \t [ 4.89747934  6.59641042 13.69675542  0.5        13.18671678  1.        ]. \t  0.9935141005672578 \t 0.9950839673697963\n",
      "13     \t [ 4.86560055  6.23411676 13.30658595  1.         13.34010048  0.94536165]. \t  0.9911905048491917 \t 0.9950839673697963\n",
      "14     \t [ 4.67685579  6.10302427 13.33722013  0.75390066 14.44984451  0.71470075]. \t  0.9895150365439748 \t 0.9950839673697963\n",
      "15     \t [ 4.9519961   5.76607119 13.51006375  0.5        13.82061928  1.        ]. \t  0.9935141005672578 \t 0.9950839673697963\n",
      "16     \t [ 4.2524274   6.00429649 13.30464394  0.51580327 13.78422646  1.        ]. \t  0.9941574121847357 \t 0.9950839673697963\n",
      "17     \t [ 4.39836547  5.45522095 13.41999773  1.         14.4421182   0.87994507]. \t  0.9912625183070544 \t 0.9950839673697963\n",
      "18     \t [ 4.41336004  5.49360131 13.36100022  0.5        14.43257747  0.32950736]. \t  0.8565705705728579 \t 0.9950839673697963\n",
      "19     \t [ 4.33374556  5.74860349 13.78849942  0.5        14.76474002  1.        ]. \t  0.9938213512021624 \t 0.9950839673697963\n",
      "20     \t [ 3.85004888  5.94859121 13.53704349  0.9368623  14.78559768  0.72132091]. \t  0.9892365869223282 \t 0.9950839673697963\n",
      "21     \t [ 4.24239077  5.65729787 13.31027468  0.86539885 15.34224686  0.79995522]. \t  0.9894718287320035 \t 0.9950839673697963\n",
      "22     \t [ 3.83384988  5.23636241 13.75725415  0.892314   15.21106064  0.74632824]. \t  0.9895198372660013 \t 0.9950839673697963\n",
      "23     \t [ 3.98995278  5.78582988 13.98802661  1.         15.50810637  1.        ]. \t  0.9949495450095941 \t 0.9950839673697963\n",
      "24     \t [ 3.77776608  5.76901735 13.81419282  0.5        15.60692215  0.56714251]. \t  0.8565753714331721 \t 0.9950839673697963\n",
      "25     \t [ 4.90204657  6.10554221 12.75950718  0.5        13.69229705  1.        ]. \t  0.9935141005672578 \t 0.9950839673697963\n",
      "26     \t [ 4.31304681  5.68123067 12.74882915  0.5        14.49532389  1.        ]. \t  0.9938213512021624 \t 0.9950839673697963\n",
      "27     \t [ 4.63589305  5.42195714 12.70676539  0.69977877 13.61423648  1.        ]. \t  0.9950359607026805 \t 0.9950839673697963\n",
      "28     \t [ 4.35183239  5.85800717 12.49442984  1.         13.98890491  0.6686663 ]. \t  0.9912577168244461 \t 0.9950839673697963\n",
      "29     \t [ 4.97383535  5.61120163 12.46532409  1.         14.24396256  1.        ]. \t  0.9950503619007472 \t 0.9950839673697963\n",
      "30     \t [ 5.09857708  5.60123698 12.60159061  0.84138878 13.80691334  0.4003064 ]. \t  0.8606800435969366 \t 0.9950839673697963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.059992957524201744"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None, eval_metric = 'rmse')\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_6 = winner_6.getResult()[0]\n",
    "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
    "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
    "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
    "\n",
    "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
    "rmse_winner_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 2.97034611  9.3940163  13.55371707  0.67162138  1.14876098  0.81843745]. \t  \u001b[92m0.9887948815679307\u001b[0m \t 0.9887948815679307\n",
      "2      \t [ 2.28695058  9.39138007 13.44893224  0.69628642  1.00000043  0.84705301]. \t  \u001b[92m0.9892221530192518\u001b[0m \t 0.9892221530192518\n",
      "3      \t [ 2.5204369   8.9824398  13.92989344  0.50000023  1.223558    0.72913095]. \t  0.9882139832779941 \t 0.9892221530192518\n",
      "4      \t [ 2.47024169  9.46162389 13.76692764  0.89903093  1.63812076  0.78898301]. \t  \u001b[92m0.9894573945523519\u001b[0m \t 0.9894573945523519\n",
      "5      \t [ 2.62049586  9.08402703 13.9209376   1.          1.          1.        ]. \t  \u001b[92m0.9967402480676828\u001b[0m \t 0.9967402480676828\n",
      "6      \t [ 1.49385417  2.00172726 11.7456191   0.70293396  5.50716869  0.89858259]. \t  0.9887804807155828 \t 0.9967402480676828\n",
      "7      \t [ 2.57605158  9.29658147 13.75483495  1.          1.08308581  0.26676796]. \t  0.789229831063024 \t 0.9967402480676828\n",
      "8      \t [ 2.03671965  1.94193592 11.7603377   0.70262125  5.05693041  0.88569628]. \t  0.9889053003871436 \t 0.9967402480676828\n",
      "9      \t [ 1.60840896  1.52258203 11.3294526   0.64325262  5.13790423  0.9119967 ]. \t  0.9878875313452902 \t 0.9967402480676828\n",
      "10     \t [ 1.44535166  2.17755261 11.4252806   0.84701967  4.85488707  0.99676057]. \t  0.9889197063561315 \t 0.9967402480676828\n",
      "11     \t [ 1.58264581  1.9087399  11.5428708   0.90783999  5.07931601  0.31925971]. \t  0.8288944533786786 \t 0.9967402480676828\n",
      "12     \t [ 1.76506538  1.50016947 11.77860633  1.          5.32862062  1.        ]. \t  0.9965818213367661 \t 0.9967402480676828\n",
      "13     \t [ 5.70859164  9.07406883  6.21223754  0.79018153 15.21223751  0.87755316]. \t  0.9816656739057558 \t 0.9967402480676828\n",
      "14     \t [ 5.64690322  8.95715941  6.14862801  0.7952373  14.51948988  0.88370574]. \t  0.9816368697118838 \t 0.9967402480676828\n",
      "15     \t [ 5.4286017   8.49002789  6.4376821   0.73692862 14.95165205  0.75555172]. \t  0.9814544396474091 \t 0.9967402480676828\n",
      "16     \t [ 5.28248315  9.14119899  6.70698688  0.91140074 14.83285706  0.92906643]. \t  0.9808351243108232 \t 0.9967402480676828\n",
      "17     \t [ 5.41266355  9.10592487  6.38165912  0.58247346 14.84916275  0.30674155]. \t  0.8275454350009962 \t 0.9967402480676828\n",
      "18     \t [ 5.64345856  8.93552166  6.65614532  0.5        14.83210454  1.        ]. \t  0.9887612950442785 \t 0.9967402480676828\n",
      "19     \t [ 1.9091046   9.34261896 14.14783552  0.70578021  1.29868617  1.        ]. \t  0.9965770201998767 \t 0.9967402480676828\n",
      "20     \t [ 2.0081479   8.82756769 13.61528609  0.85112664  1.55731569  1.        ]. \t  \u001b[92m0.9967978585988843\u001b[0m \t 0.9967978585988843\n",
      "21     \t [ 1.84828377  9.39275397 13.54438394  0.5         1.77828731  1.        ]. \t  0.9960633312577309 \t 0.9967978585988843\n",
      "22     \t [ 2.55223241  9.0161707  13.17538146  0.5         1.73822101  1.        ]. \t  0.9956216534225594 \t 0.9967978585988843\n",
      "23     \t [ 2.25862275  8.96842607 13.96395236  0.5         2.07039882  1.        ]. \t  0.9956840643992123 \t 0.9967978585988843\n",
      "24     \t [ 1.78092497  1.55652019 11.56688063  1.          4.4771636   1.        ]. \t  0.996495407164843 \t 0.9967978585988843\n",
      "25     \t [ 0.97308409  1.57962337 11.47073096  1.          5.0647872   1.        ]. \t  \u001b[92m0.9970667038724387\u001b[0m \t 0.9970667038724387\n",
      "26     \t [ 3.00809602  9.3251848  14.14601437  0.5         1.65690386  1.        ]. \t  0.9950455568226624 \t 0.9970667038724387\n",
      "27     \t [ 3.03613083  8.67881108 13.7154095   0.80083317  1.79166363  1.        ]. \t  0.996485805651646 \t 0.9970667038724387\n",
      "28     \t [ 3.11809337  9.17184738 13.54485155  0.5         2.26246627  1.        ]. \t  0.9949975487035272 \t 0.9970667038724387\n",
      "29     \t [ 2.99359474  8.86511136 13.60664104  0.5         2.13899716  0.44182567]. \t  0.8265708374706277 \t 0.9970667038724387\n",
      "30     \t [ 4.96961301  8.71404882  6.45538411  0.5        14.42339394  1.        ]. \t  0.9888285071578206 \t 0.9970667038724387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05347983388736371"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_7 = winner_7.getResult()[0]\n",
    "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
    "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
    "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
    "\n",
    "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
    "rmse_winner_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [ 1.70297801  0.51632551  6.92769887  0.6338198  17.14459738  0.49689011]. \t  0.8892355979298845 \t 0.8894852354753713\n",
      "2      \t [ 1.41412054  0.19950232  7.42341499  0.58588353 17.40826262  0.47212937]. \t  0.8894132246449423 \t 0.8894852354753713\n",
      "3      \t [ 1.58001805  0.          7.2322642   0.51584959 16.7412291   0.42382563]. \t  0.8888995473878829 \t 0.8894852354753713\n",
      "4      \t [ 1.50153414  0.66582604  7.56972782  0.51045632 16.83354324  0.5694693 ]. \t  \u001b[92m0.8898260867394013\u001b[0m \t 0.8898260867394013\n",
      "5      \t [ 1.51419734  0.25796845  7.31872274  1.         16.95266765  0.93599609]. \t  \u001b[92m0.9925923455500221\u001b[0m \t 0.9925923455500221\n",
      "6      \t [ 1.47706957  0.3853818   7.40265693  1.         16.90196343  0.18413315]. \t  0.8956831119844254 \t 0.9925923455500221\n",
      "7      \t [ 1.04011567  0.04225489  7.77542728  0.90233395 16.76825728  0.79227415]. \t  0.9833987694037466 \t 0.9925923455500221\n",
      "8      \t [ 1.6896731   0.          8.06956269  0.97904791 16.85576804  0.80235209]. \t  0.9849206259439255 \t 0.9925923455500221\n",
      "9      \t [ 1.53334643  0.12302718  7.84613622  1.         16.19699461  0.93151875]. \t  0.992544338329736 \t 0.9925923455500221\n",
      "10     \t [ 1.33118848  0.52715118  8.31152656  1.         16.53851834  1.        ]. \t  \u001b[92m0.9938357604891138\u001b[0m \t 0.9938357604891138\n",
      "11     \t [ 1.34690138  0.18742495  8.43164174  1.         16.26454713  0.42984536]. \t  0.896835294674707 \t 0.9938357604891138\n",
      "12     \t [ 1.39836709  0.04520126  8.38829372  0.5        16.36768783  1.        ]. \t  0.9931060335933563 \t 0.9938357604891138\n",
      "13     \t [ 1.99933459  0.43516143  8.43817812  0.71634534 16.22633322  0.9662875 ]. \t  0.9847669986904434 \t 0.9938357604891138\n",
      "14     \t [ 1.72944705  0.          8.86765628  1.         16.12272987  1.        ]. \t  \u001b[92m0.9938885699525697\u001b[0m \t 0.9938885699525697\n",
      "15     \t [ 1.49716448  0.44814733  8.72048221  0.78040114 15.64560273  1.        ]. \t  0.9935909218679431 \t 0.9938885699525697\n",
      "16     \t [ 1.58921974  0.57260215  9.22837675  0.60448669 16.14994776  1.        ]. \t  0.9935861186567742 \t 0.9938885699525697\n",
      "17     \t [ 1.92750429  0.1976504   9.17482995  0.5        15.69779532  0.70526002]. \t  0.9847478034080356 \t 0.9938885699525697\n",
      "18     \t [ 2.00841549  0.71133783  9.26630169  1.         15.72591282  0.72865832]. \t  0.9924579303115021 \t 0.9938885699525697\n",
      "19     \t [ 1.46874998  0.35745615  9.61486302  0.94959436 15.57699539  0.69474671]. \t  0.9854967293199114 \t 0.9938885699525697\n",
      "20     \t [ 1.90797117  0.58091507  9.7186555   0.65644956 15.29712342  1.        ]. \t  0.9936149231272312 \t 0.9938885699525697\n",
      "21     \t [ 2.05118922  0.07559953  9.45475168  1.         15.17737438  1.        ]. \t  \u001b[92m0.9940806033971373\u001b[0m \t 0.9940806033971373\n",
      "22     \t [ 2.13420654  0.2078006   9.9474903   0.98233835 15.78029511  1.        ]. \t  0.9939317785251106 \t 0.9940806033971373\n",
      "23     \t [ 2.21858873  0.25524963  9.98226845  0.98706123 15.24168212  0.45355053]. \t  0.8891107745937169 \t 0.9940806033971373\n",
      "24     \t [ 2.69753665  0.35162391  9.49272254  0.78453649 15.53732944  1.        ]. \t  0.9937877510562587 \t 0.9940806033971373\n",
      "25     \t [ 2.50778729  0.6127582   9.63237755  0.5        16.16296365  1.        ]. \t  0.9929476152978358 \t 0.9940806033971373\n",
      "26     \t [ 2.63073284  0.20546815  9.42790049  0.91423111 16.31942334  0.66637846]. \t  0.8888371334380872 \t 0.9940806033971373\n",
      "27     \t [ 2.19615386  0.62237299 10.05163886  0.52867855 16.01771518  0.45486307]. \t  0.8896148549701431 \t 0.9940806033971373\n",
      "28     \t [ 2.71449219  0.         10.02265942  0.5        15.75141109  0.86333914]. \t  0.9845893640931077 \t 0.9940806033971373\n",
      "29     \t [ 3.05313504  0.56472241 10.19024362  0.74676917 15.90906166  0.78659639]. \t  0.9861064208792572 \t 0.9940806033971373\n",
      "30     \t [ 3.14452319  0.49884706  9.767045    0.5        15.83849356  0.32532851]. \t  0.8895092377026584 \t 0.9940806033971373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06068696707556021"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_8 = winner_8.getResult()[0]\n",
    "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
    "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
    "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
    "\n",
    "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
    "rmse_winner_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 6.82922525  5.21528193 13.62488713  0.8856258   5.37497208  0.80306631]. \t  \u001b[92m0.9684346432417884\u001b[0m \t 0.9684346432417884\n",
      "2      \t [ 6.67110493  4.63245409 13.35277719  0.83086651  5.13096216  0.82346002]. \t  0.9681177861844782 \t 0.9684346432417884\n",
      "3      \t [ 6.42323354  4.68861753 13.83895114  0.86521578  5.63235465  0.76886526]. \t  0.9684250417285915 \t 0.9684346432417884\n",
      "4      \t [ 6.11543571  5.10121355 13.59967517  0.8719216   5.11309424  0.78634662]. \t  \u001b[92m0.9686314763020683\u001b[0m \t 0.9686314763020683\n",
      "5      \t [ 6.5392679   4.89576523 13.67265446  0.79767422  5.21323105  0.17353318]. \t  0.7910877926992773 \t 0.9686314763020683\n",
      "6      \t [ 6.47182788  4.99008727 13.72202421  0.5         5.32348325  1.        ]. \t  \u001b[92m0.9943446405512045\u001b[0m \t 0.9943446405512045\n",
      "7      \t [7.63084201 7.20588874 7.         0.74180239 9.573075   0.99923299]. \t  0.9545794810310007 \t 0.9943446405512045\n",
      "8      \t [7.22481903 7.68070706 7.27753088 0.71164644 9.38502782 1.        ]. \t  0.9919730389266043 \t 0.9943446405512045\n",
      "9      \t [7.89069006 7.6876967  7.5272374  0.71755159 9.46720105 0.99113655]. \t  0.9565190209921085 \t 0.9943446405512045\n",
      "10     \t [7.69471508 7.97074781 6.83384382 0.7716568  9.57305264 1.        ]. \t  0.988535657651839 \t 0.9943446405512045\n",
      "11     \t [7.58512097 7.68186957 7.13403746 0.92092267 9.49688309 0.38939556]. \t  0.7897915775621102 \t 0.9943446405512045\n",
      "12     \t [7.23768138 7.6299939  6.52396421 0.5        9.35151831 1.        ]. \t  0.9884684463680222 \t 0.9943446405512045\n",
      "13     \t [7.5587483  8.239711   7.12290779 0.5        9.03765504 1.        ]. \t  0.9911809100429415 \t 0.9943446405512045\n",
      "14     \t [8.12197531 7.66038448 6.83774677 0.5        9.43594938 0.77922035]. \t  0.9531872282209903 \t 0.9943446405512045\n",
      "15     \t [ 7.47886545  7.80251986  7.17818247  0.5        10.03550628  1.        ]. \t  0.9911425035752909 \t 0.9943446405512045\n",
      "16     \t [ 6.27344182  4.98553008 13.1429001   1.          5.70449079  1.        ]. \t  \u001b[92m0.996111340068304\u001b[0m \t 0.996111340068304\n",
      "17     \t [ 5.89352898  4.42927561 13.33080449  1.          5.40116108  1.        ]. \t  \u001b[92m0.996274566968097\u001b[0m \t 0.996274566968097\n",
      "18     \t [ 5.88621591  4.63379737 13.04679835  0.5         5.65596115  0.78108138]. \t  0.9659334149315163 \t 0.996274566968097\n",
      "19     \t [ 5.53364534  4.86630274 13.45522277  0.80306938  5.8907456   1.        ]. \t  0.9956696634777206 \t 0.996274566968097\n",
      "20     \t [ 5.36320429  4.93996067 12.87508505  0.94349675  5.50459885  1.        ]. \t  0.9960873350061515 \t 0.996274566968097\n",
      "21     \t [ 5.4370264   4.75741766 13.0574528   1.          5.81517222  0.42353796]. \t  0.7970503805878227 \t 0.996274566968097\n",
      "22     \t [ 5.14434965  4.694528   13.34375144  0.5         5.27801765  1.        ]. \t  0.9945318708536993 \t 0.996274566968097\n",
      "23     \t [ 4.95291505  4.4012087  12.92426946  0.57862608  5.78867232  1.        ]. \t  0.9952327898909085 \t 0.996274566968097\n",
      "24     \t [ 4.7512202   5.10398009 12.97560245  0.5         5.83235725  1.        ]. \t  0.9946903018023869 \t 0.996274566968097\n",
      "25     \t [ 4.47850964  4.75382397 13.26222401  0.95935652  5.62599616  1.        ]. \t  0.9961401437090259 \t 0.996274566968097\n",
      "26     \t [ 4.35341654  4.758789   12.70959127  0.59433443  5.27710804  1.        ]. \t  0.9953864168678117 \t 0.996274566968097\n",
      "27     \t [ 4.30058584  4.70876445 13.05502354  0.5         5.52572876  0.45175646]. \t  0.7917887084867287 \t 0.996274566968097\n",
      "28     \t [ 4.28993801  4.83317043 12.48933847  1.          5.86167326  1.        ]. \t  \u001b[92m0.9963945875079384\u001b[0m \t 0.9963945875079384\n",
      "29     \t [ 4.18692076  5.40772864 12.71374467  1.          5.42903197  1.        ]. \t  \u001b[92m0.9964185916020778\u001b[0m \t 0.9964185916020778\n",
      "30     \t [ 3.66797107  5.12804472 12.78424811  0.71630561  5.75776085  1.        ]. \t  0.9958808992572304 \t 0.9964185916020778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0603238218774876"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_9 = winner_9.getResult()[0]\n",
    "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
    "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
    "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
    "\n",
    "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
    "rmse_winner_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.99999999  3.0970682   8.56963517  0.84084525 19.30559557  0.83283556]. \t  0.9854775611438326 \t 0.9854871661142187\n",
      "2      \t [10.          3.33600814  8.42584803  0.85783293 18.65946715  0.75430253]. \t  \u001b[92m0.9857464098054324\u001b[0m \t 0.9857464098054324\n",
      "3      \t [ 9.41886488  2.95286306  8.58216463  0.80807025 18.86288935  0.80870642]. \t  0.9856263873295652 \t 0.9857464098054324\n",
      "4      \t [10.          2.82217207  8.91222918  0.999959   18.78161862  0.51692533]. \t  0.8899077630558327 \t 0.9857464098054324\n",
      "5      \t [ 9.59951943  3.43602401  8.5685586   0.99999997 19.10783959  0.32985068]. \t  0.8873392847412253 \t 0.9857464098054324\n",
      "6      \t [ 9.87217648  3.10844092  8.6257258   0.5        18.93713886  0.41057575]. \t  0.8927498521718357 \t 0.9857464098054324\n",
      "7      \t [ 9.51481427  3.62240137  8.27841389  0.62314141 19.12494861  1.        ]. \t  \u001b[92m0.9929380034833543\u001b[0m \t 0.9929380034833543\n",
      "8      \t [ 9.27455161  3.7143164   8.56712596  1.         18.57954464  1.        ]. \t  \u001b[92m0.9938309565182674\u001b[0m \t 0.9938309565182674\n",
      "9      \t [ 9.32618092  3.40290387  7.85918131  1.         18.74205953  1.        ]. \t  0.9909456575159798 \t 0.9938309565182674\n",
      "10     \t [ 9.22240142  3.5446587   8.10155152  0.5        18.32320709  1.        ]. \t  0.9921074688926227 \t 0.9938309565182674\n",
      "11     \t [ 8.72352631  3.64843688  8.06417694  0.70476215 18.69246311  0.72369822]. \t  0.9848342505635111 \t 0.9938309565182674\n",
      "12     \t [ 9.22999132  4.06616473  7.85186506  0.78803334 18.46033564  0.60521805]. \t  0.8889427994670956 \t 0.9938309565182674\n",
      "13     \t [ 8.75585709  3.93897591  7.53192168  0.5        18.6619478   1.        ]. \t  0.9900671219279226 \t 0.9938309565182674\n",
      "14     \t [ 8.55640486  3.85009917  7.67817668  0.93809684 18.14073822  1.        ]. \t  0.991814612679694 \t 0.9938309565182674\n",
      "15     \t [ 8.49185601  4.30674364  8.05619553  0.5        18.29466224  1.        ]. \t  0.9922034847160303 \t 0.9938309565182674\n",
      "16     \t [ 8.47696954  4.35816032  7.82472906  1.         18.78141315  1.        ]. \t  0.9909456575159798 \t 0.9938309565182674\n",
      "17     \t [ 7.87250863  4.17454249  7.61466133  0.67843519 18.5034351   1.        ]. \t  0.9916705870777003 \t 0.9938309565182674\n",
      "18     \t [ 8.18355948  4.51406404  7.42927997  0.74179895 18.33041794  0.57234108]. \t  0.8895044939009128 \t 0.9938309565182674\n",
      "19     \t [ 7.88599788  4.30139267  8.16951555  1.         18.2569727   0.80199744]. \t  0.9832259557204616 \t 0.9938309565182674\n",
      "20     \t [ 7.8463753   4.50446892  8.11582212  0.59671301 18.83034867  0.62214382]. \t  0.89000377521462 \t 0.9938309565182674\n",
      "21     \t [ 7.76775643  3.96698749  7.73351932  1.         18.60631842  0.3965937 ]. \t  0.8873680894882475 \t 0.9938309565182674\n",
      "22     \t [ 7.88789637  3.84916445  8.34478572  1.         18.80053294  1.        ]. \t  \u001b[92m0.9938309565182674\u001b[0m \t 0.9938309565182674\n",
      "23     \t [ 7.82645212  3.6558116   8.30840773  0.53367397 18.22771919  1.        ]. \t  0.9925539426788997 \t 0.9938309565182674\n",
      "24     \t [ 8.30510246  3.40408032  8.60775433  1.         18.07432274  1.        ]. \t  \u001b[92m0.99386456205646\u001b[0m \t 0.99386456205646\n",
      "25     \t [ 8.23940463  3.80304949  9.03464081  0.61934167 18.38838891  1.        ]. \t  0.9932692582815422 \t 0.99386456205646\n",
      "26     \t [ 7.80820513  3.6320756   8.90652414  0.90497962 18.33993674  0.45809149]. \t  0.8915784366545321 \t 0.99386456205646\n",
      "27     \t [ 8.43368424  3.76968249  8.76290574  0.54364746 17.73489785  0.68226108]. \t  0.9844117794193541 \t 0.99386456205646\n",
      "28     \t [ 9.01914029  3.24116217  9.22984239  0.57600797 18.07941879  0.75298834]. \t  0.9844357837900687 \t 0.99386456205646\n",
      "29     \t [ 8.17636515  3.2156053   9.30776314  0.5        17.73771497  0.95061061]. \t  0.9841477346603948 \t 0.99386456205646\n",
      "30     \t [ 8.49439541  3.60135017  9.55616493  0.97271315 17.69414733  0.86100629]. \t  0.9857080023697687 \t 0.99386456205646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0669731201677907"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_10 = winner_10.getResult()[0]\n",
    "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
    "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
    "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
    "\n",
    "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
    "rmse_winner_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [8.58481713 5.9162566  5.27779417 0.52815672 1.41669123 0.78892664]. \t  \u001b[92m0.9799470010383255\u001b[0m \t 0.9799470010383255\n",
      "2      \t [8.81590403 6.35570535 5.         0.50552946 1.         0.84444672]. \t  0.97960134338262 \t 0.9799470010383255\n",
      "3      \t [9.28539133 6.04041275 5.38589615 0.5000053  1.25231928 0.69076753]. \t  0.9793805030475866 \t 0.9799470010383255\n",
      "4      \t [9.05202046 6.02147312 5.         1.         1.3880526  1.        ]. \t  \u001b[92m0.983874058932987\u001b[0m \t 0.983874058932987\n",
      "5      \t [8.89419961 6.02431185 5.         0.87025813 1.19953574 0.27035155]. \t  0.8139015773677829 \t 0.983874058932987\n",
      "6      \t [9.13934005 6.03188496 5.         0.5        1.77698949 0.92435647]. \t  0.9793709014652459 \t 0.983874058932987\n",
      "7      \t [8.98891129 6.60659296 5.44934774 0.66329534 1.62643765 1.        ]. \t  \u001b[92m0.9839076650934736\u001b[0m \t 0.9839076650934736\n",
      "8      \t [8.80984354 6.12719838 5.6976336  0.8711255  1.         1.        ]. \t  \u001b[92m0.9840612916555139\u001b[0m \t 0.9840612916555139\n",
      "9      \t [9.109321   5.90504297 5.76363918 0.84012484 1.81254863 1.        ]. \t  0.9840180843275492 \t 0.9840612916555139\n",
      "10     \t [8.88970814 6.37617544 5.93259455 1.         1.66000927 0.56076538]. \t  0.7770405763699446 \t 0.9840612916555139\n",
      "11     \t [9.09773842 5.23407486 5.53662741 0.79683661 1.33959318 1.        ]. \t  0.983965275624675 \t 0.9840612916555139\n",
      "12     \t [9.17079807 5.16238994 5.16774275 0.5        1.86626322 1.        ]. \t  0.9838548567363183 \t 0.9840612916555139\n",
      "13     \t [9.23894978 5.24274333 5.2451486  0.97390417 1.88884828 0.53459792]. \t  0.813503110456053 \t 0.9840612916555139\n",
      "14     \t [9.76988939 5.3170183  5.26188759 0.561041   1.59365143 1.        ]. \t  0.9839604751100796 \t 0.9840612916555139\n",
      "15     \t [9.62126171 5.09710935 5.84313735 0.5        1.98548744 1.        ]. \t  0.9838548567363183 \t 0.9840612916555139\n",
      "16     \t [9.80251777 5.56632796 5.45437028 0.5        2.40671299 1.        ]. \t  0.9838884622745109 \t 0.9840612916555139\n",
      "17     \t [9.91186643 4.88711878 5.39832684 0.85817765 2.35970896 1.        ]. \t  0.9840084829526399 \t 0.9840612916555139\n",
      "18     \t [10.          5.51118057  5.81878779  1.          2.05828692  1.        ]. \t  0.9838596565594756 \t 0.9840612916555139\n",
      "19     \t [9.65863953 5.21738374 5.90016915 1.         2.73979549 1.        ]. \t  0.9838596565594756 \t 0.9840612916555139\n",
      "20     \t [10.          4.98973954  6.03368618  0.52258081  2.85217356  1.        ]. \t  \u001b[92m0.9883004086512103\u001b[0m \t 0.9883004086512103\n",
      "21     \t [10.          5.6587958   6.38649229  0.68731642  2.71585558  1.        ]. \t  \u001b[92m0.9885980605386692\u001b[0m \t 0.9885980605386692\n",
      "22     \t [10.          5.31203018  6.07054035  0.77085474  2.69616932  0.40904654]. \t  0.8201714133329131 \t 0.9885980605386692\n",
      "23     \t [10.          4.99040252  6.6160966   1.          2.55507677  1.        ]. \t  \u001b[92m0.9886748731282516\u001b[0m \t 0.9886748731282516\n",
      "24     \t [10.          5.22069449  6.70499111  1.          3.28618152  1.        ]. \t  0.9886316658694305 \t 0.9886748731282516\n",
      "25     \t [9.58793779 5.15623526 6.89252975 0.56673502 3.01627622 1.        ]. \t  0.9885884598551975 \t 0.9886748731282516\n",
      "26     \t [10.          5.41322027  7.39859389  0.78324419  2.9578453   1.        ]. \t  \u001b[92m0.9917329916239813\u001b[0m \t 0.9917329916239813\n",
      "27     \t [10.          4.76213093  7.30479515  0.5         3.18722852  1.        ]. \t  0.989841467042106 \t 0.9917329916239813\n",
      "28     \t [10.          5.14994237  7.23958536  0.55829902  3.17840607  0.40290358]. \t  0.8226198293448341 \t 0.9917329916239813\n",
      "29     \t [9.65123906 4.82870842 7.58536016 1.         3.20393721 1.        ]. \t  0.9912577118460938 \t 0.9917329916239813\n",
      "30     \t [9.76770226 5.19139748 7.63440951 0.65263835 3.71588647 1.        ]. \t  0.9897742556200017 \t 0.9917329916239813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07047333441968708"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_11 = winner_11.getResult()[0]\n",
    "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
    "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
    "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
    "\n",
    "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
    "rmse_winner_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.12084955  6.48679341  8.70526555  0.87184531 15.55775897  0.76328584]. \t  0.9901823525305562 \t 0.9904271937791528\n",
      "2      \t [ 2.14528164  5.82438056  8.9190362   0.82054903 15.52340474  0.64515806]. \t  0.8895620958426368 \t 0.9904271937791528\n",
      "3      \t [ 1.98472833  6.08385573  8.3110999   0.9406993  15.93554065  0.91733851]. \t  \u001b[92m0.9906960419567463\u001b[0m \t 0.9906960419567463\n",
      "4      \t [ 2.49441692  6.2638698   8.65116847  0.89476703 16.13872874  0.58961124]. \t  0.8899461646677702 \t 0.9906960419567463\n",
      "5      \t [ 1.72425966  6.27613551  8.55959069  0.61336517 15.91417418  0.38330165]. \t  0.8902726231691337 \t 0.9906960419567463\n",
      "6      \t [ 2.23041786  6.20374807  8.63999093  0.5        15.8591989   1.        ]. \t  \u001b[92m0.9932884611687284\u001b[0m \t 0.9932884611687284\n",
      "7      \t [ 1.90693568  6.78942002  8.43643166  0.7876304  16.20696837  1.        ]. \t  \u001b[92m0.9938981613017798\u001b[0m \t 0.9938981613017798\n",
      "8      \t [ 1.38592717  6.48082281  8.54669594  0.67058178 15.6348609   1.        ]. \t  0.9938549549418081 \t 0.9938981613017798\n",
      "9      \t [ 1.37333789  6.32344732  8.44835379  0.5        16.40277431  1.        ]. \t  0.9932020462362158 \t 0.9938981613017798\n",
      "10     \t [ 1.06913994  6.68102369  8.51937446  1.         16.28244712  1.        ]. \t  \u001b[92m0.9940613872336078\u001b[0m \t 0.9940613872336078\n",
      "11     \t [ 1.2143999   6.93194316  8.94076444  0.5        16.24029634  1.        ]. \t  0.9932308508449584 \t 0.9940613872336078\n",
      "12     \t [ 1.00744437  7.0801148   8.20305126  0.5        16.19839882  1.        ]. \t  0.9932308508449584 \t 0.9940613872336078\n",
      "13     \t [ 1.09955266  7.04751057  8.48861544  0.5639229  16.71219228  0.56702888]. \t  0.8902630214485053 \t 0.9940613872336078\n",
      "14     \t [ 0.5123256   6.71902681  8.59980377  0.5        16.19619794  0.76090168]. \t  0.9901535443263886 \t 0.9940613872336078\n",
      "15     \t [ 0.88075312  7.23370376  8.64290339  0.74716956 15.8615031   0.53097472]. \t  0.8896005046611482 \t 0.9940613872336078\n",
      "16     \t [ 0.38872382  7.34350807  8.68460053  0.6150413  16.44673417  1.        ]. \t  0.9932932634118984 \t 0.9940613872336078\n",
      "17     \t [ 3.1115769   7.74613985  7.28671281  0.77498698 13.45064451  0.2448139 ]. \t  0.8898117405789735 \t 0.9940613872336078\n",
      "18     \t [ 3.47542532  7.45082394  7.37474796  0.77383768 12.94230747  0.1       ]. \t  0.8899749698296553 \t 0.9940613872336078\n",
      "19     \t [ 2.85244998  7.80933699  7.51138677  0.71699954 12.80664654  0.100005  ]. \t  0.8884867050682611 \t 0.9940613872336078\n",
      "20     \t [ 3.4519782   8.10588616  7.74342553  0.79436234 13.10343692  0.1       ]. \t  0.8897061216520618 \t 0.9940613872336078\n",
      "21     \t [ 3.31094321  7.8644876   7.45300319  0.55529206 12.92062083  0.70473069]. \t  0.9882812288552714 \t 0.9940613872336078\n",
      "22     \t [ 3.17019126  7.86005515  7.36995706  1.         12.79013049  0.7178055 ]. \t  0.9862696497150821 \t 0.9940613872336078\n",
      "23     \t [ 3.81923102  7.89334347  7.32361546  1.         13.230336    0.7634111 ]. \t  0.9862504459972543 \t 0.9940613872336078\n",
      "24     \t [ 3.44780649  7.55584744  7.93214379  1.         13.24507689  0.84903613]. \t  0.9862648485781969 \t 0.9940613872336078\n",
      "25     \t [ 3.29907044  8.24448885  7.71704855  1.         13.43814949  1.        ]. \t  0.9917377951798115 \t 0.9940613872336078\n",
      "26     \t [ 3.76214919  8.16863673  7.9818992   1.         12.88351585  1.        ]. \t  0.991723392598869 \t 0.9940613872336078\n",
      "27     \t [ 3.93355213  8.0717137   8.04655434  0.66725238 13.5548961   1.        ]. \t  0.9931684487877223 \t 0.9940613872336078\n",
      "28     \t [ 3.31892515  8.14787227  8.43492268  0.58677591 13.23940403  1.        ]. \t  0.993250062445064 \t 0.9940613872336078\n",
      "29     \t [ 3.68139955  8.2847464   8.61194443  1.         13.56139494  0.83819219]. \t  0.9879739531906138 \t 0.9940613872336078\n",
      "30     \t [ 3.73239523  8.83733809  8.32471318  0.67172677 13.3630513   0.99772162]. \t  0.9896014471879716 \t 0.9940613872336078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06593434317511107"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_12 = winner_12.getResult()[0]\n",
    "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
    "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
    "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
    "\n",
    "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
    "rmse_winner_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [8.78933796 2.78683261 8.77321477 0.96550296 6.27852324 0.87706122]. \t  0.9879595504037993 \t 0.9879595510260936\n",
      "2      \t [8.72166442 2.83487447 9.40449474 0.99999706 5.97140662 0.90511956]. \t  0.9868073588634562 \t 0.9879595510260936\n",
      "3      \t [9.17020513 2.54644354 9.32285508 0.99929679 6.48498011 0.95787032]. \t  0.9875418884172104 \t 0.9879595510260936\n",
      "4      \t [9.2079009  2.69168132 9.12015267 0.72067454 6.00821288 0.48477152]. \t  0.8194704900779332 \t 0.9879595510260936\n",
      "5      \t [8.48499167 2.48859056 9.2935521  0.66381194 6.51742522 0.83124153]. \t  0.9865289151190312 \t 0.9879595510260936\n",
      "6      \t [8.61559552 2.47384361 9.23315686 1.         6.4335081  0.3478273 ]. \t  0.7993595739323779 \t 0.9879595510260936\n",
      "7      \t [ 6.15536885  8.87582689  7.99992434  0.77227022 10.39140902  0.79720554]. \t  0.987047398560263 \t 0.9879595510260936\n",
      "8      \t [ 6.74917439  8.7061268   7.99985782  0.83834941 10.05074022  0.79339861]. \t  \u001b[92m0.9879835517321874\u001b[0m \t 0.9879835517321874\n",
      "9      \t [ 6.61390347  9.04193069  7.45228947  0.78365003 10.36925212  0.9334464 ]. \t  0.9874026601491979 \t 0.9879835517321874\n",
      "10     \t [ 6.78827437  9.20501786  8.10535282  0.91391152 10.61901529  0.76158389]. \t  0.9877579155843362 \t 0.9879835517321874\n",
      "11     \t [ 6.60726546  9.1007305   7.81134892  0.51676444 10.28369501  0.29476992]. \t  0.8173005272139916 \t 0.9879835517321874\n",
      "12     \t [ 6.72327796  8.9363796   8.15336312  0.5        10.46370528  1.        ]. \t  \u001b[92m0.9931444385408376\u001b[0m \t 0.9931444385408376\n",
      "13     \t [8.40643286 2.72294854 8.9473389  0.5        5.84001207 1.        ]. \t  0.9925539351422272 \t 0.9931444385408376\n",
      "14     \t [8.88744841 3.1686348  9.28117472 0.5        6.44887332 1.        ]. \t  \u001b[92m0.9937589407786599\u001b[0m \t 0.9937589407786599\n",
      "15     \t [8.94945335 2.38868439 9.67458388 0.5        6.03371894 1.        ]. \t  \u001b[92m0.9937685427758632\u001b[0m \t 0.9937685427758632\n",
      "16     \t [9.08748125 2.33824576 8.79323205 0.5        6.51249595 1.        ]. \t  0.9925155283980015 \t 0.9937685427758632\n",
      "17     \t [ 6.71375452  8.49619659  7.84201797  1.         10.81136032  1.        ]. \t  0.9920306488355116 \t 0.9937685427758632\n",
      "18     \t [9.18342347 2.87619951 8.92904157 0.5        5.76189969 1.        ]. \t  0.9925539351422272 \t 0.9937685427758632\n",
      "19     \t [ 6.52423469  8.7051594   8.61419658  1.         10.46601074  1.        ]. \t  \u001b[92m0.9938453588226347\u001b[0m \t 0.9938453588226347\n",
      "20     \t [ 6.71486659  8.37682555  8.43310215  0.94156533 10.81291486  0.51399734]. \t  0.8210883925879765 \t 0.9938453588226347\n",
      "21     \t [ 6.30794896  8.87818192  8.29807336  1.         11.15825376  1.        ]. \t  \u001b[92m0.9938693629167744\u001b[0m \t 0.9938693629167744\n",
      "22     \t [ 6.16185501  9.48145344  7.74068617  1.         10.81776339  1.        ]. \t  0.9921458678236008 \t 0.9938693629167744\n",
      "23     \t [ 6.40718182  9.04616839  7.51372872  0.73570733 11.3288228   1.        ]. \t  0.9910416866149939 \t 0.9938693629167744\n",
      "24     \t [ 5.91170549  8.76491311  7.43049784  1.         11.19313405  1.        ]. \t  0.9922034775250768 \t 0.9938693629167744\n",
      "25     \t [ 6.18418816  8.98826944  7.48702922  1.         11.28369625  0.40042182]. \t  0.7993259683941852 \t 0.9938693629167744\n",
      "26     \t [6.77681166 9.079221   8.75753728 0.77192015 9.78444501 0.6831044 ]. \t  0.9868985755551444 \t 0.9938693629167744\n",
      "27     \t [7.18320758 9.24646108 8.44928505 1.         9.64730479 1.        ]. \t  0.9938165541447562 \t 0.9938693629167744\n",
      "28     \t [7.35692845 8.67680423 8.87180035 1.         9.96556407 1.        ]. \t  0.9938117533535858 \t 0.9938693629167744\n",
      "29     \t [ 7.14893298  9.34556311  9.13924823  1.         10.13840249  1.        ]. \t  \u001b[92m0.9944070526341579\u001b[0m \t 0.9944070526341579\n",
      "30     \t [7.54461017 9.22685974 8.93864499 0.5        9.86913503 1.        ]. \t  0.9924723183734292 \t 0.9944070526341579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06568492142484887"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_13 = winner_13.getResult()[0]\n",
    "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
    "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
    "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
    "\n",
    "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
    "rmse_winner_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  0.8869120213105831 \t 0.991771393943005\n",
      "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  0.9848582277607189 \t 0.991771393943005\n",
      "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  0.991771393943005 \t 0.991771393943005\n",
      "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  0.8867439936196196 \t 0.991771393943005\n",
      "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  0.8866719807149072 \t 0.991771393943005\n",
      "1      \t [1.80801696 4.32095236 9.62298986 0.96597678 3.00000001 0.99999965]. \t  0.9916321718287892 \t 0.991771393943005\n",
      "2      \t [2.28940292 3.89267048 9.39329933 0.88887721 2.88137655 0.88206593]. \t  0.9914785441604482 \t 0.991771393943005\n",
      "3      \t [2.40577692 4.62116871 9.4069845  0.9136396  2.77690831 0.90269559]. \t  0.9912048987871612 \t 0.991771393943005\n",
      "4      \t [2.03962663 4.32904253 9.42744905 0.9376004  2.88734989 0.30239259]. \t  0.8868256070695161 \t 0.991771393943005\n",
      "5      \t [2.34502431 4.34817055 9.39428888 1.         3.48902396 0.89810655]. \t  \u001b[92m0.9922802916358223\u001b[0m \t 0.9922802916358223\n",
      "6      \t [2.1386076  4.37803246 9.40821033 0.5        3.18971308 0.91988743]. \t  0.9906576068651427 \t 0.9922802916358223\n",
      "7      \t [1.21529534e-02 9.17598928e+00 5.00000000e+00 5.11552056e-01\n",
      " 1.67173833e+01 7.26188545e-01]. \t  0.983725221615105 \t 0.9922802916358223\n",
      "8      \t [ 0.09506101  8.87251121  5.          0.5        17.35096454  0.77437258]. \t  0.9835187875947784 \t 0.9922802916358223\n",
      "9      \t [ 0.424681    8.56018045  5.00000001  0.57988767 16.767088    0.71431123]. \t  0.9848966343666571 \t 0.9922802916358223\n",
      "10     \t [ 0.28244669  8.92502006  5.63001028  0.5456801  16.95474922  0.78111855]. \t  0.9834611783081652 \t 0.9922802916358223\n",
      "11     \t [ 0.19045365  8.93468463  5.13365691  1.         16.95747452  1.        ]. \t  0.9845269765580077 \t 0.9922802916358223\n",
      "12     \t [ 0.23926627  8.9501283   5.19206112  0.88795711 17.02090709  0.22570356]. \t  0.8869120213105831 \t 0.9922802916358223\n",
      "13     \t [2.72687614 4.27518589 8.85818725 0.79777141 3.10814171 0.71650986]. \t  0.9908400392113621 \t 0.9922802916358223\n",
      "14     \t [2.99242652 4.23686842 9.54456748 0.714473   3.14370127 0.59829034]. \t  0.8867439936196196 \t 0.9922802916358223\n",
      "15     \t [ 0.7738125   8.60757258  5.33674953  0.74579053 17.38822835  0.91296107]. \t  0.9833795676931637 \t 0.9922802916358223\n",
      "16     \t [ 0.17816432  8.1851492   5.48918417  0.796692   17.2801753   0.88962381]. \t  0.9837540303033229 \t 0.9922802916358223\n",
      "17     \t [ 0.54107318  8.21029683  5.53128379  0.5        17.45200383  0.36283097]. \t  0.8868304078606867 \t 0.9922802916358223\n",
      "18     \t [ 0.27011307  8.6225135   5.71894218  0.99979643 17.76404787  0.72270891]. \t  0.9839652655988264 \t 0.9922802916358223\n",
      "19     \t [ 0.32509198  8.3649706   5.86353724  0.5        17.89451878  1.        ]. \t  0.984128508539977 \t 0.9922802916358223\n",
      "20     \t [ 0.58764286  8.25784422  6.24282805  0.87331238 17.52530234  0.91953539]. \t  0.9894285833749578 \t 0.9922802916358223\n",
      "21     \t [ 0.62655765  7.87487172  5.92795234  1.         18.05830098  0.93520927]. \t  0.9892365743381596 \t 0.9922802916358223\n",
      "22     \t [2.41678353 3.83110177 9.01150958 0.6783409  3.50149533 0.45069992]. \t  0.8867439936196196 \t 0.9922802916358223\n",
      "23     \t [ 0.09937331  7.86546297  6.34325318  0.88563654 17.98138103  0.65878971]. \t  0.8868256070695161 \t 0.9922802916358223\n",
      "24     \t [2.56952271 4.76149983 9.01978775 0.69663552 3.41891165 0.36648573]. \t  0.8867439936196196 \t 0.9922802916358223\n",
      "25     \t [ 0.82837441  8.28728868  6.31072114  0.78777861 18.24171865  0.58866152]. \t  0.8868256070695161 \t 0.9922802916358223\n",
      "26     \t [ 0.78467308  7.60067232  6.27104328  0.5        17.82450298  0.89376051]. \t  0.9888188793008421 \t 0.9922802916358223\n",
      "27     \t [ 0.90355719  7.56330381  6.26855762  1.         17.68704666  0.42037065]. \t  0.890037356690776 \t 0.9922802916358223\n",
      "28     \t [ 1.40003714  7.86157608  6.04793668  0.66754563 17.75707231  0.95493891]. \t  0.9887996766893109 \t 0.9922802916358223\n",
      "29     \t [ 1.07302867  7.57004881  5.77341075  0.68395024 17.18571845  1.        ]. \t  0.9839220733442063 \t 0.9922802916358223\n",
      "30     \t [ 1.22497222  7.35488478  5.52903855  0.52264577 17.83269724  0.85768442]. \t  0.983470779544787 \t 0.9922802916358223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06066344917043695"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_14 = winner_14.getResult()[0]\n",
    "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
    "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
    "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
    "\n",
    "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
    "rmse_winner_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.84893943  3.83803297 13.58526996  0.50420256 15.58526996  0.85300941]. \t  \u001b[92m0.9871674273263862\u001b[0m \t 0.9871674273263862\n",
      "2      \t [ 7.26748215  3.43060234 13.97024399  0.5        15.48784844  0.90307271]. \t  0.9870186027309306 \t 0.9871674273263862\n",
      "3      \t [ 7.24537085  3.47160801 13.55827085  0.53361452 15.96302472  0.50647109]. \t  0.7999116693144428 \t 0.9871674273263862\n",
      "4      \t [ 6.57460162  3.18758724 13.80932321  0.5        15.44481597  0.8610555 ]. \t  0.9868265688715061 \t 0.9871674273263862\n",
      "5      \t [ 6.77869887  3.62675696 14.08587898  0.99996862 15.4528037   0.6597466 ]. \t  0.7997388411088441 \t 0.9871674273263862\n",
      "6      \t [ 6.91972979  3.2700384  13.50516946  0.88522026 15.74565135  1.        ]. \t  \u001b[92m0.9945990840036515\u001b[0m \t 0.9945990840036515\n",
      "7      \t [ 7.07187506  3.24086314 13.26666153  0.5        15.09993476  1.        ]. \t  0.992544335149168 \t 0.9945990840036515\n",
      "8      \t [ 7.17497391  2.6599512  13.53364172  0.5        15.53582954  1.        ]. \t  0.9925155304021396 \t 0.9945990840036515\n",
      "9      \t [ 6.73268605  2.96068736 12.96827777  0.5        15.7044646   1.        ]. \t  0.9925059289580824 \t 0.9945990840036515\n",
      "10     \t [ 6.71528097  2.78494841 13.60603398  0.5        16.25494985  1.        ]. \t  0.9923955115217113 \t 0.9945990840036515\n",
      "11     \t [ 6.27723011  3.44777525 13.35562865  0.5        16.23461936  1.        ]. \t  0.9930292250831814 \t 0.9945990840036515\n",
      "12     \t [ 6.91880086  3.34021898 13.10462908  0.5        16.46909811  1.        ]. \t  0.9923955115217113 \t 0.9945990840036515\n",
      "13     \t [ 7.55924742  3.10906574 13.03454198  0.5        15.73770818  1.        ]. \t  0.9925059289580824 \t 0.9945990840036515\n",
      "14     \t [ 6.98848532  3.72618775 12.68823871  0.5        15.78262077  1.        ]. \t  0.9924915263771399 \t 0.9945990840036515\n",
      "15     \t [ 7.49521149  2.84973463 13.90019629  0.5        16.18511184  1.        ]. \t  0.9924915263771399 \t 0.9945990840036515\n",
      "16     \t [ 6.22790028  3.83936912 12.89734828  0.5        15.40590875  1.        ]. \t  0.9932452611698932 \t 0.9945990840036515\n",
      "17     \t [ 6.23237139  4.22162177 12.77488798  0.5        16.10522812  1.        ]. \t  0.9931108373576677 \t 0.9945990840036515\n",
      "18     \t [ 6.18433717  3.80685341 12.5668781   0.5        15.95611974  0.51790712]. \t  0.7996620317689559 \t 0.9945990840036515\n",
      "19     \t [ 5.87482095  4.28829353 13.48439214  0.5        15.77837055  1.        ]. \t  0.9932260580052077 \t 0.9945990840036515\n",
      "20     \t [ 6.1133533   4.3156294  13.1185639   1.         15.77851962  1.        ]. \t  0.9945798812538227 \t 0.9945990840036515\n",
      "21     \t [ 7.00689899  2.80013494 14.5744163   0.5        15.87256145  1.        ]. \t  0.9924915263771399 \t 0.9945990840036515\n",
      "22     \t [ 6.23349825  4.78112196 13.03970803  0.52460446 15.45465502  1.        ]. \t  0.9932788623520971 \t 0.9945990840036515\n",
      "23     \t [ 5.92178867  4.69409694 13.14643097  0.65412393 15.6626957   0.40981595]. \t  0.8000268905151251 \t 0.9945990840036515\n",
      "24     \t [ 6.37337105  4.83607921 13.48147653  0.66477232 16.10329237  1.        ]. \t  0.9936245224970048 \t 0.9945990840036515\n",
      "25     \t [ 6.20783054  4.87278601 13.82614468  0.90328472 15.41186091  1.        ]. \t  \u001b[92m0.994608687245421\u001b[0m \t 0.994608687245421\n",
      "26     \t [ 6.25065166  4.30923309 13.56287361  0.78257488 14.80188762  1.        ]. \t  0.9945222745254775 \t 0.994608687245421\n",
      "27     \t [ 6.82014984  4.6708128  13.29349567  1.         15.03469032  1.        ]. \t  0.9945606782274226 \t 0.994608687245421\n",
      "28     \t [ 6.71013634  4.0718488  12.75086657  1.         14.72296021  1.        ]. \t  0.994560677121138 \t 0.994608687245421\n",
      "29     \t [ 6.90600971  4.33540874 13.01554877  0.5        14.45306252  1.        ]. \t  0.9926739554736533 \t 0.994608687245421\n",
      "30     \t [ 6.64100033  4.39940296 13.03069251  0.81713193 14.51416888  0.42395052]. \t  0.7996044195091878 \t 0.994608687245421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05904077444393976"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_15 = winner_15.getResult()[0]\n",
    "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
    "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
    "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
    "\n",
    "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
    "rmse_winner_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 3.88352763  9.59362321 13.87771857  0.92459776 16.75287944  0.73044978]. \t  \u001b[92m0.9861208442030359\u001b[0m \t 0.9861208442030359\n",
      "2      \t [ 3.55252076  9.69363139 13.32842057  0.92306351 17.02846292  0.79472882]. \t  \u001b[92m0.9861640516692923\u001b[0m \t 0.9861640516692923\n",
      "3      \t [ 3.42114933  9.90626619 13.60599611  0.99204167 16.40371112  0.94046694]. \t  0.985703176269982 \t 0.9861640516692923\n",
      "4      \t [ 3.26005678  9.26264149 13.64494252  0.86624866 16.5941229   0.62576764]. \t  0.8879201752026464 \t 0.9861640516692923\n",
      "5      \t [ 3.56967629  9.77464981 13.69690312  0.5        16.76051901  1.        ]. \t  \u001b[92m0.9936101181183498\u001b[0m \t 0.9936101181183498\n",
      "6      \t [ 3.99552167 10.         13.74891246  1.         17.19283639  1.        ]. \t  \u001b[92m0.9947815171104466\u001b[0m \t 0.9947815171104466\n",
      "7      \t [ 3.64394303  9.23313246 13.75633855  1.         17.09093946  1.        ]. \t  \u001b[92m0.9947815171104466\u001b[0m \t 0.9947815171104466\n",
      "8      \t [ 3.87927203  9.56172803 13.8457758   0.59484219 17.58633039  0.87759483]. \t  0.9852279002257274 \t 0.9947815171104466\n",
      "9      \t [ 4.03543867  9.6044392  14.44194406  0.88410994 17.4621991   1.        ]. \t  \u001b[92m0.9948295272347295\u001b[0m \t 0.9948295272347295\n",
      "10     \t [ 3.67988475  9.77040915 14.19028798  1.         17.99997344  1.        ]. \t  0.9946758997046761 \t 0.9948295272347295\n",
      "11     \t [ 3.73711618  9.41921068 13.12087946  0.89380855 16.23127131  1.        ]. \t  0.994791119384217 \t 0.9948295272347295\n",
      "12     \t [ 3.62096797 10.         14.53679074  0.5        17.75741879  1.        ]. \t  0.9932212485711878 \t 0.9948295272347295\n",
      "13     \t [ 3.64324461  9.23697959 13.74004956  1.         15.9039187   1.        ]. \t  \u001b[92m0.9948535289088745\u001b[0m \t 0.9948535289088745\n",
      "14     \t [ 3.47165982  9.5439836  13.42003163  0.54023436 15.62363483  1.        ]. \t  0.993965381228456 \t 0.9948535289088745\n",
      "15     \t [ 3.86864178 10.         14.38885187  0.87187382 17.78974085  0.42069063]. \t  0.8877857519435631 \t 0.9948535289088745\n",
      "16     \t [ 3.69341687  9.57626405 13.33652113  1.         15.54137194  0.53303335]. \t  0.7896091274858469 \t 0.9948535289088745\n",
      "17     \t [ 7.16096278  8.47359806 13.08339835  0.81683658 15.11952423  0.34143785]. \t  0.8866815730164282 \t 0.9948535289088745\n",
      "18     \t [ 3.02964168  9.27820826 13.19836422  0.98705465 15.70781612  1.        ]. \t  0.994848728463417 \t 0.9948535289088745\n",
      "19     \t [ 3.37029496  8.77468976 13.25707329  0.52829707 15.68056907  1.        ]. \t  0.9939125730095704 \t 0.9948535289088745\n",
      "20     \t [ 2.91167744  9.01384736 13.77878488  0.53207616 15.47231961  1.        ]. \t  0.9941766203268495 \t 0.9948535289088745\n",
      "21     \t [ 3.23394321  8.89364139 13.48560228  0.78439044 14.94479429  1.        ]. \t  \u001b[92m0.994915940922672\u001b[0m \t 0.994915940922672\n",
      "22     \t [ 2.97405723  8.40732399 13.63867857  1.         15.37933127  1.        ]. \t  \u001b[92m0.9949255415370158\u001b[0m \t 0.9949255415370158\n",
      "23     \t [ 2.93659286  8.67026215 13.47003923  0.75204385 15.21746132  0.4180566 ]. \t  0.8863647339362467 \t 0.9949255415370158\n",
      "24     \t [ 3.4366609   8.44474598 14.00925888  0.59014514 15.19056226  0.87394132]. \t  0.9852807076148995 \t 0.9949255415370158\n",
      "25     \t [ 2.92262155  8.13880281 13.65019673  0.5        14.8909458   1.        ]. \t  0.9943446465658493 \t 0.9949255415370158\n",
      "26     \t [ 3.38567834  9.36575239 14.62726835  0.69605042 17.81344665  0.81919069]. \t  0.984546173635977 \t 0.9949255415370158\n",
      "27     \t [ 2.81986832  8.39216474 14.19957245  0.83137128 14.71629317  0.94003697]. \t  0.9865529192804563 \t 0.9949255415370158\n",
      "28     \t [ 3.25415886  7.95982924 13.85927361  1.         14.56749263  0.77642488]. \t  0.9690923695359316 \t 0.9949255415370158\n",
      "29     \t [ 7.25831067  7.94929908 13.13771666  0.86801541 14.7165531   0.12036747]. \t  0.8878625643257342 \t 0.9949255415370158\n",
      "30     \t [ 7.39258807  7.90052722 13.15337537  0.88621771 15.44635925  0.1       ]. \t  0.8857550400260462 \t 0.9949255415370158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06058737795408081"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_16 = winner_16.getResult()[0]\n",
    "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
    "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
    "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
    "\n",
    "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
    "rmse_winner_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.90053806  0.80267343  6.          0.95614905 13.51015295  0.72846279]. \t  0.9866201190174384 \t 0.991204913651782\n",
      "2      \t [ 3.21404262  0.62291048  5.56618405  0.99347308 13.91082433  0.56825166]. \t  0.8932058195240776 \t 0.991204913651782\n",
      "3      \t [ 2.98431023  1.06803854  6.16978935  0.92909034 14.14600748  0.75924997]. \t  0.9906816248558185 \t 0.991204913651782\n",
      "4      \t [ 2.97809868  0.60549193  6.34585134  0.99998176 13.92412878  0.25097305]. \t  0.8924137151859163 \t 0.991204913651782\n",
      "5      \t [ 3.39737873  0.60700182  6.2825992   0.79095237 13.84076831  0.98383007]. \t  0.9904799990940414 \t 0.991204913651782\n",
      "6      \t [ 2.89542449  0.7222895   5.99496913  0.5        13.92416834  0.6349939 ]. \t  0.8910598576424787 \t 0.991204913651782\n",
      "7      \t [ 9.27333754  5.99519317 10.00093371  0.95745807 14.28392605  0.98671778]. \t  \u001b[92m0.9916081788656078\u001b[0m \t 0.9916081788656078\n",
      "8      \t [ 9.72964855  5.82865647 10.48001135  0.98645782 14.46952894  0.99999983]. \t  0.9915169629344952 \t 0.9916081788656078\n",
      "9      \t [ 9.55811599  6.16986362 10.37377335  0.50004779 14.22026446  0.68395835]. \t  0.9885644551372604 \t 0.9916081788656078\n",
      "10     \t [ 9.27187848  5.72204009 10.58701679  0.78719085 13.90575873  0.97892736]. \t  0.9907248342580727 \t 0.9916081788656078\n",
      "11     \t [ 9.35528454  5.45929639 10.22915055  0.52462678 14.44622914  0.74812305]. \t  0.9897502683262879 \t 0.9916081788656078\n",
      "12     \t [ 9.49293764  5.72069297 10.29462469  1.         14.11568488  0.34082137]. \t  0.8873777037082623 \t 0.9916081788656078\n",
      "13     \t [ 7.08360504  7.49104387 12.8439312   0.50000082 17.84392906  0.74890315]. \t  0.9896014351571266 \t 0.9916081788656078\n",
      "14     \t [ 6.56042713  7.53441237 12.66414526  0.52057054 17.43452626  0.5880138 ]. \t  0.8880978413290932 \t 0.9916081788656078\n",
      "15     \t [ 6.89910752  6.8302149  12.75274899  0.59596303 17.73903517  0.66484529]. \t  0.8888226565977907 \t 0.9916081788656078\n",
      "16     \t [ 6.8259504   7.3047499  13.36214184  0.51940776 17.51077466  0.9207468 ]. \t  0.9899614998189747 \t 0.9916081788656078\n",
      "17     \t [ 6.88880998  7.39803199 13.25212255  0.92127954 17.73306432  0.35014663]. \t  0.8899412632738429 \t 0.9916081788656078\n",
      "18     \t [ 6.73625078  7.40761447 12.8859773   1.         17.74129924  1.        ]. \t  \u001b[92m0.9944694621580247\u001b[0m \t 0.9944694621580247\n",
      "19     \t [ 2.80650405  0.77985943  6.77183422  1.         13.78868151  1.        ]. \t  0.9887180873690234 \t 0.9944694621580247\n",
      "20     \t [ 7.08436111  7.12375013 13.39444064  0.78925039 18.19640038  1.        ]. \t  0.993864563300144 \t 0.9944694621580247\n",
      "21     \t [ 6.81844419  7.88261157 13.47535148  0.72957606 18.06721374  1.        ]. \t  0.9936725281961495 \t 0.9944694621580247\n",
      "22     \t [ 9.01359408  5.9676983  10.69223733  0.59024374 14.64303231  1.        ]. \t  0.9934852975478939 \t 0.9944694621580247\n",
      "23     \t [ 6.45100561  7.33540673 13.77698789  1.         18.05265392  1.        ]. \t  0.9943638421248283 \t 0.9944694621580247\n",
      "24     \t [ 6.56853132  7.54098796 13.41302538  1.         18.69114258  1.        ]. \t  0.9942630240582307 \t 0.9944694621580247\n",
      "25     \t [ 9.50996281  6.05060589 10.10179431  0.5        14.95319186  1.        ]. \t  0.9930676243599882 \t 0.9944694621580247\n",
      "26     \t [ 6.59588581  7.4392981  13.93133716  0.5        18.60270783  1.        ]. \t  0.993048421748445 \t 0.9944694621580247\n",
      "27     \t [ 6.82792233  7.57790283 14.02432936  1.         18.63968126  0.59049228]. \t  0.8873777037082623 \t 0.9944694621580247\n",
      "28     \t [ 9.20569739  6.04963767 10.39252648  0.79777068 15.05574497  0.47873784]. \t  0.886964773715586 \t 0.9944694621580247\n",
      "29     \t [ 6.0873691   7.83952662 13.79460306  0.73073026 18.50746274  0.71791291]. \t  0.9905279987086207 \t 0.9944694621580247\n",
      "30     \t [ 6.13839955  7.10822602 13.81165158  0.81759648 18.77020622  0.5948328 ]. \t  0.8885970290235834 \t 0.9944694621580247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0627179309150946"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_17 = winner_17.getResult()[0]\n",
    "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
    "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
    "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
    "\n",
    "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
    "rmse_winner_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8299977850025021 \t 0.9872154265261087\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.984824605557074 \t 0.9872154265261087\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8300890052204674 \t 0.9872154265261087\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9802782361972252 \t 0.9872154265261087\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9872154265261087 \t 0.9872154265261087\n",
      "1      \t [ 3.25898935  2.33043215 10.71442904  0.76189972 13.91911639  0.99019641]. \t  0.9872154244518251 \t 0.9872154265261087\n",
      "2      \t [ 3.62888669  2.03388961 11.14231634  0.73489673 14.22413313  0.99948194]. \t  0.9865721097228688 \t 0.9872154265261087\n",
      "3      \t [ 3.39154077  2.14377766 11.31726895  0.72073183 13.54758808  0.99879895]. \t  0.9868409520924682 \t 0.9872154265261087\n",
      "4      \t [ 3.05258081  2.45144411 11.38699976  0.58991259 14.14844608  0.99613393]. \t  0.9853190937545766 \t 0.9872154265261087\n",
      "5      \t [ 3.25706821  2.21733362 11.20028584  0.94141975 13.98362639  0.41394337]. \t  0.8291576665297761 \t 0.9872154265261087\n",
      "6      \t [ 6.1715357   7.92375261  8.1390293   0.73006431 12.44415036  0.9996665 ]. \t  0.9846517693309121 \t 0.9872154265261087\n",
      "7      \t [ 5.69125827  7.62002444  7.74606954  0.77606345 12.50402493  0.86242035]. \t  0.9840660682445797 \t 0.9872154265261087\n",
      "8      \t [ 6.00681741  7.56256789  8.21656136  0.73632305 12.56937808  0.38610186]. \t  0.8298009569204711 \t 0.9872154265261087\n",
      "9      \t [ 5.55830487  8.05960955  8.21650257  0.8260973  12.83103175  1.        ]. \t  \u001b[92m0.9934852932610411\u001b[0m \t 0.9934852932610411\n",
      "10     \t [ 5.95581043  8.26109706  7.67774337  0.85760005 12.79079684  0.69869621]. \t  0.9846853745233978 \t 0.9934852932610411\n",
      "11     \t [ 5.65050381  8.08405618  7.63781188  0.5        12.97888633  1.        ]. \t  0.9900671246921662 \t 0.9934852932610411\n",
      "12     \t [10.          3.70448797 13.99999812  0.52511246  6.55039278  0.68900227]. \t  0.9805038726217096 \t 0.9934852932610411\n",
      "13     \t [10.          3.95965483 14.00000124  0.5001739   7.21021289  0.669694  ]. \t  0.980225434062905 \t 0.9934852932610411\n",
      "14     \t [ 9.98962406  3.66177199 14.59383752  0.50280684  6.95147734  0.8885197 ]. \t  0.9795101162475119 \t 0.9934852932610411\n",
      "15     \t [ 9.99963128  3.61211813 14.36942181  0.50493108  6.94924491  0.15325988]. \t  0.8292392703688422 \t 0.9934852932610411\n",
      "16     \t [ 9.37993013  3.95065379 14.24464558  0.50313982  6.83690642  0.79052241]. \t  0.9803358409895724 \t 0.9934852932610411\n",
      "17     \t [ 9.79983213  3.79559096 14.18531895  1.          6.9345712   0.86864916]. \t  0.984574991796706 \t 0.9934852932610411\n",
      "18     \t [ 3.47653019  1.89049408 10.85982381  0.5        13.78284907  0.68196725]. \t  0.985501519947093 \t 0.9934852932610411\n",
      "19     \t [ 5.56879683  8.30416693  7.96815603  0.5        12.32277514  0.67383402]. \t  0.9836291759891473 \t 0.9934852932610411\n",
      "20     \t [ 6.06876063  8.37280154  8.29759294  0.5        13.00735556  0.69123908]. \t  0.9835571809232739 \t 0.9934852932610411\n",
      "21     \t [ 5.46170995  8.00312081  7.92388804  0.5        13.06659004  0.34091217]. \t  0.8275062091633777 \t 0.9934852932610411\n",
      "22     \t [ 3.7680444   2.70433087 11.17326801  0.5        13.86972156  0.95246688]. \t  0.9851462575284146 \t 0.9934852932610411\n",
      "23     \t [ 6.32393901  7.86106203  7.50278454  0.5        12.55901218  0.67533939]. \t  0.9835283579225497 \t 0.9934852932610411\n",
      "24     \t [ 2.76921225  2.33968321 11.00670554  0.5        13.51738142  0.81881418]. \t  0.9851558593873286 \t 0.9934852932610411\n",
      "25     \t [ 3.14455678  3.02206503 11.1695641   0.5        13.45810652  1.        ]. \t  \u001b[92m0.9939029658958045\u001b[0m \t 0.9939029658958045\n",
      "26     \t [ 3.45275326  2.77323241 10.86749708  0.5        13.19736466  0.54706899]. \t  0.8285527704376071 \t 0.9939029658958045\n",
      "27     \t [ 9.98122308  4.42122695 14.35587962  0.53445679  6.70080987  1.        ]. \t  0.9936005179188628 \t 0.9939029658958045\n",
      "28     \t [ 9.84469355  4.44640125 14.64540221  0.8024547   6.78406925  0.48715449]. \t  0.8297913532638447 \t 0.9939029658958045\n",
      "29     \t [ 3.07479327  3.1080908  10.91715822  0.5        13.96670324  0.60219406]. \t  0.8285527704376071 \t 0.9939029658958045\n",
      "30     \t [ 6.24828478  8.66184928  7.77767983  0.5        12.4976573   1.        ]. \t  0.9900767261362233 \t 0.9939029658958045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06099515711402899"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_18 = winner_18.getResult()[0]\n",
    "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
    "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
    "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
    "\n",
    "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
    "rmse_winner_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.92265473  0.22715853 10.51172518  0.73846598  4.2558744   0.66803267]. \t  \u001b[92m0.9875803138994016\u001b[0m \t 0.9875803138994016\n",
      "2      \t [ 0.29479148  0.         10.48113014  0.85528391  4.24055305  0.87032761]. \t  0.9873498703225767 \t 0.9875803138994016\n",
      "3      \t [ 0.63908917  0.16038827 10.06063951  0.87133616  4.72446689  0.84199764]. \t  0.987349870875727 \t 0.9875803138994016\n",
      "4      \t [4.55991036e-01 3.31065886e-03 1.01956766e+01 6.17044919e-01\n",
      " 4.42262997e+00 2.41739907e-01]. \t  0.8908103104821331 \t 0.9875803138994016\n",
      "5      \t [ 0.37661448  0.71849282 10.35129936  0.80700135  4.39844127  0.71047101]. \t  0.9873354681564969 \t 0.9875803138994016\n",
      "6      \t [ 0.48948736  0.322524   10.29997386  0.5         4.46848427  1.        ]. \t  \u001b[92m0.9955448455347543\u001b[0m \t 0.9955448455347543\n",
      "7      \t [ 6.6873681   2.34025151  5.47513835  0.85117876 10.22320456  0.8126746 ]. \t  0.9714447939983524 \t 0.9955448455347543\n",
      "8      \t [ 6.95366471  1.69229236  5.55240727  0.83575079 10.1579147   0.78930449]. \t  0.9715648155062065 \t 0.9955448455347543\n",
      "9      \t [ 7.37086227  2.19809596  5.30012351  0.97850532 10.29303611  0.94367287]. \t  0.9714831992905587 \t 0.9955448455347543\n",
      "10     \t [7.11793429 2.19542865 5.54794912 0.91435843 9.61457647 0.89275822]. \t  0.9716992392492844 \t 0.9955448455347543\n",
      "11     \t [7.09512537 2.13160102 5.22623903 0.94800431 9.98467712 0.28268424]. \t  0.8919385172885962 \t 0.9955448455347543\n",
      "12     \t [ 7.21123934  2.19236852  5.28395154  0.5        10.01015037  0.84817653]. \t  0.9715792165659858 \t 0.9955448455347543\n",
      "13     \t [ 0.47160374  0.31158017 10.81162715  0.98153036  4.88838216  0.68605121]. \t  0.9872442528476842 \t 0.9955448455347543\n",
      "14     \t [ 3.76045655  5.81472191 12.00000007  0.99993007 12.47805833  0.14656835]. \t  0.891592832735959 \t 0.9955448455347543\n",
      "15     \t [ 4.43092025  5.64805126 12.13933297  0.9894013  12.45845962  0.21612474]. \t  0.891712855626689 \t 0.9955448455347543\n",
      "16     \t [ 4.07158952  5.97888729 12.57020766  0.99352008 12.80259728  0.1       ]. \t  0.8916312398950473 \t 0.9955448455347543\n",
      "17     \t [ 3.89602689  5.29367334 12.52890388  0.85992126 12.53573269  0.1376858 ]. \t  0.8917416627245998 \t 0.9955448455347543\n",
      "18     \t [ 3.98066702  5.70875641 12.32857219  0.77727351 12.68233261  0.74654433]. \t  0.9860728566906402 \t 0.9955448455347543\n",
      "19     \t [ 4.09011926  5.82861479 12.17663435  0.5        12.69387481  0.17739794]. \t  0.8856205938272675 \t 0.9955448455347543\n",
      "20     \t [ 0.89382784  0.6397702  10.42003225  1.          4.89463733  0.28593789]. \t  0.8898501209259223 \t 0.9955448455347543\n",
      "21     \t [ 0.93428299  0.75557634 10.66748294  1.          4.72230184  1.        ]. \t  \u001b[92m0.9967066425986338\u001b[0m \t 0.9967066425986338\n",
      "22     \t [1.10367117 0.80680951 9.97931976 1.         4.27365688 1.        ]. \t  0.9956264590537947 \t 0.9967066425986338\n",
      "23     \t [ 0.81809501  0.75572432 10.49035979  1.          3.78078117  1.        ]. \t  \u001b[92m0.9968266631384751\u001b[0m \t 0.9968266631384751\n",
      "24     \t [ 1.1292759   1.09848392 10.34624495  0.5         4.04152294  1.        ]. \t  0.9953432118907354 \t 0.9968266631384751\n",
      "25     \t [ 1.28328931  1.23271244 10.44328075  1.          3.99710037  0.55604966]. \t  0.8895044612650471 \t 0.9968266631384751\n",
      "26     \t [0.87152499 1.23694611 9.8690206  0.7309494  3.5709977  0.93229229]. \t  0.9873258703770641 \t 0.9968266631384751\n",
      "27     \t [1.44473089 0.92843493 9.97898839 0.68819586 3.36622964 1.        ]. \t  0.9951799829857726 \t 0.9968266631384751\n",
      "28     \t [1.24723682 0.95536662 9.7533565  0.5        3.65398193 0.37977274]. \t  0.8906950894197304 \t 0.9968266631384751\n",
      "29     \t [ 0.92194087  0.91109066 10.27767329  0.5         3.08966433  0.68388596]. \t  0.9869706068521032 \t 0.9968266631384751\n",
      "30     \t [ 1.35288974  1.56881786 10.2598203   0.5         3.07952988  0.80141037]. \t  0.9869177987715231 \t 0.9968266631384751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.052998228526471534"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_19 = winner_19.getResult()[0]\n",
    "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
    "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
    "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
    "\n",
    "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
    "rmse_winner_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.38908345  5.71876811 11.78020947  0.52347406 16.65937159  0.7316268 ]. \t  0.9911328909287936 \t 0.9913825280594238\n",
      "2      \t [ 5.65407413  6.27212064 11.95815158  0.50110069 16.32146891  0.79920742]. \t  0.9911904989017088 \t 0.9913825280594238\n",
      "3      \t [ 5.06155368  6.19606814 11.51490806  0.50154654 16.25322234  0.82785471]. \t  0.9913777284436799 \t 0.9913825280594238\n",
      "4      \t [ 5.69526077  5.83534526 11.39032884  0.53327601 16.12618453  1.        ]. \t  \u001b[92m0.9938453485194702\u001b[0m \t 0.9938453485194702\n",
      "5      \t [ 5.49460996  5.99233044 11.58729805  1.         16.19092886  0.46104783]. \t  0.8979874862152651 \t 0.9938453485194702\n",
      "6      \t [ 5.28066383  5.99745801 11.90280333  1.         16.39439361  1.        ]. \t  \u001b[92m0.9949687411209943\u001b[0m \t 0.9949687411209943\n",
      "7      \t [0.7425562  6.42152022 6.17146901 0.56525286 7.82853051 0.98274069]. \t  0.9911472937863074 \t 0.9949687411209943\n",
      "8      \t [0.77405674 6.11248287 6.56558784 0.58723864 8.31897257 0.88923002]. \t  0.9909312578378812 \t 0.9949687411209943\n",
      "9      \t [1.25106356 5.98682689 6.39344573 0.56387748 7.80838186 0.74054999]. \t  0.9910032730934484 \t 0.9949687411209943\n",
      "10     \t [1.10229033 6.3066296  5.97913203 0.50017054 8.32465108 0.58006645]. \t  0.8879729773369668 \t 0.9949687411209943\n",
      "11     \t [0.67338856 6.13065487 6.39433332 0.92168977 7.85680597 0.41246225]. \t  0.8900901357161075 \t 0.9949687411209943\n",
      "12     \t [1.13322874 6.09225629 6.16533484 1.         8.12846138 1.        ]. \t  0.9895582281057269 \t 0.9949687411209943\n",
      "13     \t [ 5.64136199  5.81059963 11.15677102  0.56536775 18.22106143  0.72439297]. \t  0.9903599730227631 \t 0.9949687411209943\n",
      "14     \t [ 5.6259565   6.26406026 11.22868569  0.71058474 17.08151424  1.        ]. \t  0.9943446341201477 \t 0.9949687411209943\n",
      "15     \t [ 5.87308724  6.238984   10.72219929  0.64728129 18.42280195  0.73433572]. \t  0.9915409582475038 \t 0.9949687411209943\n",
      "16     \t [ 5.49882525  6.20687231 11.14396334  0.9674     18.57462476  1.        ]. \t  0.9946374817593168 \t 0.9949687411209943\n",
      "17     \t [ 5.53291316  5.7380174  10.60318949  0.97832371 18.35541403  1.        ]. \t  0.9946710876432303 \t 0.9949687411209943\n",
      "18     \t [ 5.12653722  6.13311493 10.71554827  0.5        18.42642017  0.83816059]. \t  0.9912001024200497 \t 0.9949687411209943\n",
      "19     \t [ 5.35714057  6.05969366 10.83092954  1.         18.36663799  0.32984343]. \t  0.8979874862152651 \t 0.9949687411209943\n",
      "20     \t [ 4.99989316  6.01310056 11.0128179   0.95874823 17.5342587   1.        ]. \t  0.9947719052949712 \t 0.9949687411209943\n",
      "21     \t [ 5.35158944  5.84670323 10.58194506  0.5        17.3098451   0.94226352]. \t  0.9914017328835362 \t 0.9949687411209943\n",
      "22     \t [ 5.35181664  6.48690753 10.4302972   0.9382143  17.76828417  1.        ]. \t  0.9946326815904305 \t 0.9949687411209943\n",
      "23     \t [ 5.14646396  6.45720941 10.85836666  0.5        17.39009711  0.52478241]. \t  0.887228880633948 \t 0.9949687411209943\n",
      "24     \t [ 5.21558115  6.12213127 10.64369947  1.         16.68776739  1.        ]. \t  \u001b[92m0.9950023464517658\u001b[0m \t 0.9950023464517658\n",
      "25     \t [ 5.67635152  5.92769735 10.58962159  1.         17.24639064  0.56052937]. \t  0.8980258930977785 \t 0.9950023464517658\n",
      "26     \t [ 5.03294594  5.42826025 11.14461415  0.88368591 16.38617952  1.        ]. \t  0.9946614873054577 \t 0.9950023464517658\n",
      "27     \t [0.70001899 5.77510702 6.6811874  0.84513191 7.61023511 1.        ]. \t  0.9894670114831866 \t 0.9950023464517658\n",
      "28     \t [ 5.61999453  5.35032457 11.10473775  1.         16.95705729  1.        ]. \t  0.994944736957709 \t 0.9950023464517658\n",
      "29     \t [ 5.95455525  5.52441293 12.05749906  0.86597313 16.29282241  1.        ]. \t  0.9945318639386896 \t 0.9950023464517658\n",
      "30     \t [ 5.54318141  4.92772282 11.72985834  1.         16.19246984  1.        ]. \t  0.9949639402606802 \t 0.9950023464517658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06375093231474206"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_20 = winner_20.getResult()[0]\n",
    "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
    "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
    "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
    "\n",
    "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
    "rmse_winner_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.284481397689544, -4.957796770345985)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.268510647521853, -4.940865025039879)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.143403322367767, -5.39978206459989)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.597740686890408, -5.394482811667079)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.149175936750909, -4.49095788841416)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.30553523055892, -5.315253449748774)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.762089687372063, -5.831628530032438)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.12063935758606, -5.129520760473064)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.521691412270321, -5.631999148693028)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.165857084515026, -5.093673818926914)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.160822649297216, -4.7954825795249)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.174302685180055, -5.1262797138939495)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.1948697268771795, -5.186248873907126)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.275056695614438, -4.863978692319931)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.283534976025767, -5.2229663699903845)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.267580662291718, -5.283535466549013)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.221186582823342, -5.197470209230525)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.2105772917481366, -5.099952838469431)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.503844842737993, -5.752971607022524)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.297826632924897, -5.2987867670523725)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Yellow')\n",
    "plt.plot(median_winner, color = 'Green')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Yellow', alpha=0.4, label='GP CBM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Green', alpha=0.4, label='STP CBM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
