{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hartmann3 synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 3 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/hart3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Hartmann3'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Hartmann3':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = -3.86278\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 3\n",
    "\n",
    "# 3-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub]),\n",
    "             'x3_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test) \n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    x3_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training, x3_training):\n",
    "       \n",
    "        value = np.array([x1_training, x2_training, x3_training])\n",
    "      \n",
    "        a = np.array([[3.0, 10, 30],\n",
    "                      [0.1, 10, 35],\n",
    "                      [3.0, 10, 30],\n",
    "                      [0.1, 10, 35]])\n",
    "        \n",
    "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "      \n",
    "        p = np.array([[.3689, .1170, .2673],\n",
    "                      [.4699, .4387, .7470],\n",
    "                      [.1091, .8732, .5547],\n",
    "                      [.3810, .5743, .8828]])\n",
    "  \n",
    "        s = 0\n",
    "        for i in [0,1,2,3]:\n",
    "            sm = a[i,0]*(value[0]-p[i,0])**2\n",
    "            sm += a[i,1]*(value[1]-p[i,1])**2\n",
    "            sm += a[i,2]*(value[2]-p[i,2])**2\n",
    "            s += alpha[i]*np.exp(-sm)\n",
    "        result = -s\n",
    "        \n",
    "        return operator * result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0.         0.86482736 1.        ]. \t  0.8663380764047253 \t 2.3951473341797507\n",
      "2      \t [0.9694624  0.81729872 0.76150236]. \t  1.3879340688336017 \t 2.3951473341797507\n",
      "3      \t [0.00746842 0.39300358 0.94016277]. \t  2.339565033428072 \t 2.3951473341797507\n",
      "4      \t [0.99271916 0.97953791 0.19194934]. \t  0.0051249034030868 \t 2.3951473341797507\n",
      "5      \t [0.89539121 0.31310218 0.96935805]. \t  1.3903958518762796 \t 2.3951473341797507\n",
      "6      \t [0.43154031 0.53910525 0.95905966]. \t  \u001b[92m2.8079980209670032\u001b[0m \t 2.8079980209670032\n",
      "7      \t [0.48961894 0.53008348 0.60431267]. \t  1.3107116768279226 \t 2.8079980209670032\n",
      "8      \t [0.08393622 0.02257993 0.82276374]. \t  0.30471528635060674 \t 2.8079980209670032\n",
      "9      \t [0.85262173 0.90853957 0.974701  ]. \t  0.7859874906235119 \t 2.8079980209670032\n",
      "10     \t [0.97309633 0.19016164 0.29778079]. \t  0.3092980434831704 \t 2.8079980209670032\n",
      "11     \t [0.41453023 0.189136   0.98671538]. \t  0.5835678811098508 \t 2.8079980209670032\n",
      "12     \t [0.38250177 0.04518189 0.08113646]. \t  0.3356017180695572 \t 2.8079980209670032\n",
      "13     \t [0.13178792 0.01843852 0.37992013]. \t  0.5266166458834225 \t 2.8079980209670032\n",
      "14     \t [0.29631203 0.21583422 0.16238708]. \t  0.6420229022556283 \t 2.8079980209670032\n",
      "15     \t [0.27903694 0.29800729 0.85858851]. \t  2.1003956761271243 \t 2.8079980209670032\n",
      "16     \t [0.61325549 0.30243981 0.90825932]. \t  1.8874160789018661 \t 2.8079980209670032\n",
      "17     \t [0.2483362  0.96867155 0.62386134]. \t  2.345384616481004 \t 2.8079980209670032\n",
      "18     \t [0.75511073 0.31098202 0.11627379]. \t  0.22145948662279805 \t 2.8079980209670032\n",
      "19     \t [0.6988929  0.82224792 0.81285555]. \t  1.8184288957347945 \t 2.8079980209670032\n",
      "20     \t [0.74629129 0.39196569 0.04424157]. \t  0.06887424994128499 \t 2.8079980209670032\n",
      "21     \t [0.65530889 0.99070857 0.91077714]. \t  0.5916856864635031 \t 2.8079980209670032\n",
      "22     \t [0.085087   0.97222274 0.28108833]. \t  0.28790122550111297 \t 2.8079980209670032\n",
      "23     \t [0.83604037 0.55025856 0.68514336]. \t  1.8388292114924272 \t 2.8079980209670032\n",
      "24     \t [0.57095976 0.81673629 0.74317659]. \t  1.7102203992439355 \t 2.8079980209670032\n",
      "25     \t [0.63342171 0.0610053  0.61277941]. \t  0.1942697040063976 \t 2.8079980209670032\n",
      "26     \t [0.34353819 0.78447438 0.42476058]. \t  1.4333216348280542 \t 2.8079980209670032\n",
      "27     \t [0.51315413 0.21560717 0.66828498]. \t  0.7868197231909237 \t 2.8079980209670032\n",
      "28     \t [0.52769422 0.79558163 0.20534277]. \t  0.05118149121556392 \t 2.8079980209670032\n",
      "29     \t [0.65439246 0.40648609 0.80296072]. \t  \u001b[92m3.0000629220142074\u001b[0m \t 3.0000629220142074\n",
      "30     \t [0.31703275 0.38679538 0.87263802]. \t  2.925033424807816 \t 3.0000629220142074\n",
      "31     \t [0.20163824 0.26175231 0.28072878]. \t  0.7494070337353412 \t 3.0000629220142074\n",
      "32     \t [0.80015172 0.95292565 0.34818642]. \t  0.1877165275170739 \t 3.0000629220142074\n",
      "33     \t [0.19810728 0.12835955 0.30677975]. \t  0.8755510102119044 \t 3.0000629220142074\n",
      "34     \t [0.92713944 0.37449199 0.1992468 ]. \t  0.17686171776661236 \t 3.0000629220142074\n",
      "35     \t [0.97646817 0.72199139 0.23044572]. \t  0.01886879752069358 \t 3.0000629220142074\n",
      "36     \t [0.7132736  0.45084692 0.77419227]. \t  2.9988847266872862 \t 3.0000629220142074\n",
      "37     \t [0.62059475 0.19878146 0.39672362]. \t  0.48416330717961403 \t 3.0000629220142074\n",
      "38     \t [0.93771887 0.12502626 0.69952327]. \t  0.5349600897796973 \t 3.0000629220142074\n",
      "39     \t [0.91155269 0.4824978  0.5395311 ]. \t  0.4077509563328468 \t 3.0000629220142074\n",
      "40     \t [0.10508875 0.32971212 0.91216338]. \t  2.1021775737118262 \t 3.0000629220142074\n",
      "41     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.0000629220142074\n",
      "42     \t [0.33387899 0.53342683 0.11634561]. \t  0.09134636202859761 \t 3.0000629220142074\n",
      "43     \t [0.10933948 0.39434738 0.48843668]. \t  0.47463495201539446 \t 3.0000629220142074\n",
      "44     \t [0.18147797 0.00427161 0.93758596]. \t  0.16191639832850643 \t 3.0000629220142074\n",
      "45     \t [0.64603072 0.98356685 0.85169221]. \t  0.6965671981222739 \t 3.0000629220142074\n",
      "46     \t [0.60435642 0.84482559 0.20250671]. \t  0.03825490005082811 \t 3.0000629220142074\n",
      "47     \t [0.31632777 0.08667041 0.17033392]. \t  0.7412096226284234 \t 3.0000629220142074\n",
      "48     \t [0.11224833 0.81636102 0.05510213]. \t  0.0032231561413997357 \t 3.0000629220142074\n",
      "49     \t [0.18856892 0.58701826 0.71088235]. \t  2.6686946187175598 \t 3.0000629220142074\n",
      "50     \t [0.27851208 0.8787004  0.92779283]. \t  1.2761988007569391 \t 3.0000629220142074\n",
      "51     \t [0.86535767 0.22722627 0.11850734]. \t  0.2176305506090892 \t 3.0000629220142074\n",
      "52     \t [0.63427989 0.87306528 0.75324288]. \t  1.306823750539479 \t 3.0000629220142074\n",
      "53     \t [0.75308494 0.52011    0.17899753]. \t  0.10370341809742749 \t 3.0000629220142074\n",
      "54     \t [0.56886202 0.04790407 0.91984523]. \t  0.2817905816351073 \t 3.0000629220142074\n",
      "55     \t [0.72579525 0.35976761 0.33217317]. \t  0.3519326230068731 \t 3.0000629220142074\n",
      "56     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.0000629220142074\n",
      "57     \t [0.55035505 0.35769172 0.58668171]. \t  0.6870939250800232 \t 3.0000629220142074\n",
      "58     \t [0.02752459 0.61563494 0.42928929]. \t  0.9991490514822108 \t 3.0000629220142074\n",
      "59     \t [0.85815424 0.3766133  0.43959801]. \t  0.17764358099336147 \t 3.0000629220142074\n",
      "60     \t [0.66539115 0.70870415 0.14928367]. \t  0.02179028217114431 \t 3.0000629220142074\n",
      "61     \t [0.68751657 0.93769228 0.33449312]. \t  0.24729420645382472 \t 3.0000629220142074\n",
      "62     \t [0.61867025 0.21017493 0.90488381]. \t  1.1280061567678494 \t 3.0000629220142074\n",
      "63     \t [0.45139881 0.75764658 0.1536751 ]. \t  0.025809063494270642 \t 3.0000629220142074\n",
      "64     \t [0.90472964 0.9160513  0.18428633]. \t  0.007771975311035936 \t 3.0000629220142074\n",
      "65     \t [0.58464209 0.25549428 0.4077631 ]. \t  0.4304226878949818 \t 3.0000629220142074\n",
      "66     \t [0.73167071 0.04646788 0.63602023]. \t  0.20109147747427045 \t 3.0000629220142074\n",
      "67     \t [0.50159174 0.39568288 0.3605373 ]. \t  0.4049837552433268 \t 3.0000629220142074\n",
      "68     \t [0.284838   0.7672172  0.22291601]. \t  0.1034113167291186 \t 3.0000629220142074\n",
      "69     \t [0.1213989  0.52283173 0.43351905]. \t  0.6735992321504352 \t 3.0000629220142074\n",
      "70     \t [0.98892067 0.72767522 0.86826642]. \t  2.734855490734007 \t 3.0000629220142074\n",
      "71     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.0000629220142074\n",
      "72     \t [0.98804586 0.0449322  0.30230819]. \t  0.29004380755883535 \t 3.0000629220142074\n",
      "73     \t [0.95901186 0.25076518 0.23941641]. \t  0.28784139865955954 \t 3.0000629220142074\n",
      "74     \t [0.19916068 0.6271735  0.40368678]. \t  0.8597929973599497 \t 3.0000629220142074\n",
      "75     \t [0.11165879 0.28524441 0.48207772]. \t  0.3208186689144355 \t 3.0000629220142074\n",
      "76     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.0000629220142074\n",
      "77     \t [0.4699236  0.89020772 0.37975838]. \t  0.8113985516385133 \t 3.0000629220142074\n",
      "78     \t [0.4392073  0.78519199 0.10636872]. \t  0.010029344977685532 \t 3.0000629220142074\n",
      "79     \t [0.21961907 0.6015288  0.09956847]. \t  0.04121021759401271 \t 3.0000629220142074\n",
      "80     \t [0.95655451 0.88808101 0.58809126]. \t  0.4553290610022084 \t 3.0000629220142074\n",
      "81     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.0000629220142074\n",
      "82     \t [0.78933727 0.90472402 0.82215516]. \t  1.126411336305204 \t 3.0000629220142074\n",
      "83     \t [0.8777519  0.14790529 0.76351869]. \t  0.8108427181301892 \t 3.0000629220142074\n",
      "84     \t [0.33935565 0.09674464 0.31095635]. \t  0.9396204683691252 \t 3.0000629220142074\n",
      "85     \t [0.43361283 0.20523394 0.62461828]. \t  0.5329662580521853 \t 3.0000629220142074\n",
      "86     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.0000629220142074\n",
      "87     \t [0.39871357 0.2735528  0.87510818]. \t  1.8095336899165377 \t 3.0000629220142074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.52645571 0.75007053 0.81331628]. \t  2.5756383452991445 \t 3.0000629220142074\n",
      "89     \t [0.67162765 0.16588182 0.85316793]. \t  0.9636196694046057 \t 3.0000629220142074\n",
      "90     \t [0.1353118  0.41816142 0.57977477]. \t  0.9330594111440559 \t 3.0000629220142074\n",
      "91     \t [0.84515891 0.7984235  0.27681832]. \t  0.06007456863337144 \t 3.0000629220142074\n",
      "92     \t [0.65259893 0.40543277 0.61169084]. \t  0.9410805313922265 \t 3.0000629220142074\n",
      "93     \t [0.89712384 0.86740939 0.57199769]. \t  0.5705142395729424 \t 3.0000629220142074\n",
      "94     \t [0.10050002 0.70316347 0.1260103 ]. \t  0.02331172194873839 \t 3.0000629220142074\n",
      "95     \t [0.49266886 0.21165411 0.91177336]. \t  1.110705187350084 \t 3.0000629220142074\n",
      "96     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.0000629220142074\n",
      "97     \t [0.09047156 0.52056546 0.37167556]. \t  0.43681458744751706 \t 3.0000629220142074\n",
      "98     \t [0.30024739 0.21118253 0.95814266]. \t  0.8514100172171297 \t 3.0000629220142074\n",
      "99     \t [0.05766926 0.62078636 0.81547445]. \t  \u001b[92m3.5677144595940287\u001b[0m \t 3.5677144595940287\n",
      "100    \t [0.31882033 0.71137107 0.41807763]. \t  1.1847704621319368 \t 3.5677144595940287\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0.         0.88842232 1.        ]. \t  0.7510512573785252 \t 2.6229838112516717\n",
      "3      \t [0.03171049 0.81766905 0.30766729]. \t  0.4633094918758506 \t 2.6229838112516717\n",
      "4      \t [0.30290203 0.50085936 0.98998561]. \t  2.1749524963713047 \t 2.6229838112516717\n",
      "5      \t [0.92029033 0.04265395 0.04009595]. \t  0.08078082202939513 \t 2.6229838112516717\n",
      "6      \t [0.08503078 0.11774937 0.01943383]. \t  0.12432388056630428 \t 2.6229838112516717\n",
      "7      \t [0.42498819 0.6784016  0.00833422]. \t  0.005863578553280072 \t 2.6229838112516717\n",
      "8      \t [0.68025983 0.57148002 0.98342386]. \t  2.3683751921405296 \t 2.6229838112516717\n",
      "9      \t [0.00287034 0.00960506 0.94199974]. \t  0.16425706845727325 \t 2.6229838112516717\n",
      "10     \t [0.34967458 0.9663794  0.97964562]. \t  0.5167517317875341 \t 2.6229838112516717\n",
      "11     \t [0.9449635  0.59504613 0.16844385]. \t  0.029987329500278628 \t 2.6229838112516717\n",
      "12     \t [0.07723703 0.42935057 0.68862569]. \t  1.979554528781844 \t 2.6229838112516717\n",
      "13     \t [0.81284857 0.45466138 0.81303408]. \t  \u001b[92m3.327086283949069\u001b[0m \t 3.327086283949069\n",
      "14     \t [0.06392242 1.         0.67266609]. \t  1.824308008802697 \t 3.327086283949069\n",
      "15     \t [1.         0.52461502 0.92322125]. \t  3.204514986898651 \t 3.327086283949069\n",
      "16     \t [0.49889293 0.38898862 0.80587178]. \t  2.907130697178981 \t 3.327086283949069\n",
      "17     \t [0.96647132 0.75221483 0.78488307]. \t  2.0857966270751653 \t 3.327086283949069\n",
      "18     \t [0.84647935 0.85606054 0.01601542]. \t  0.0004190810508713835 \t 3.327086283949069\n",
      "19     \t [0.02101621 0.3103691  0.98987519]. \t  1.1806830067459866 \t 3.327086283949069\n",
      "20     \t [0.67074769 0.50407663 0.10950069]. \t  0.08134994605282261 \t 3.327086283949069\n",
      "21     \t [0.99776615 0.20786608 0.19673105]. \t  0.24219398588023455 \t 3.327086283949069\n",
      "22     \t [0.18451048 0.43115052 0.82357876]. \t  3.313692032179021 \t 3.327086283949069\n",
      "23     \t [0.17455699 0.98486952 0.17098058]. \t  0.031913330206764436 \t 3.327086283949069\n",
      "24     \t [0.95239638 0.36144772 0.94554421]. \t  1.9936188205498406 \t 3.327086283949069\n",
      "25     \t [0.311088   0.10086237 0.85501238]. \t  0.5855724326415652 \t 3.327086283949069\n",
      "26     \t [0.17618559 0.68440006 0.8314362 ]. \t  3.2889860093354377 \t 3.327086283949069\n",
      "27     \t [0.64697655 0.9959314  0.73510199]. \t  0.7117122301240999 \t 3.327086283949069\n",
      "28     \t [0.71430163 0.98804758 0.82488168]. \t  0.6532725282053791 \t 3.327086283949069\n",
      "29     \t [0.24906671 0.42368708 0.15161783]. \t  0.25316392589076253 \t 3.327086283949069\n",
      "30     \t [0.51674399 0.65932498 0.64410161]. \t  1.821659570287034 \t 3.327086283949069\n",
      "31     \t [0.73365766 0.53991925 0.82607642]. \t  \u001b[92m3.68852662283979\u001b[0m \t 3.68852662283979\n",
      "32     \t [0.1268655  0.41178033 0.26678082]. \t  0.38181628623354774 \t 3.68852662283979\n",
      "33     \t [0.64809123 0.52606378 0.86501151]. \t  \u001b[92m3.771634489655683\u001b[0m \t 3.771634489655683\n",
      "34     \t [0.4115952  0.00448034 0.06480005]. \t  0.256081721264955 \t 3.771634489655683\n",
      "35     \t [0.68065875 0.06935073 0.27964859]. \t  0.727306180729379 \t 3.771634489655683\n",
      "36     \t [0.80834889 0.13656897 0.75847908]. \t  0.7445052723203085 \t 3.771634489655683\n",
      "37     \t [0.73667086 0.41211989 0.68574058]. \t  1.7285172452558117 \t 3.771634489655683\n",
      "38     \t [0.14154474 0.71312893 0.82279243]. \t  3.038390692600929 \t 3.771634489655683\n",
      "39     \t [0.85618534 0.25398188 0.5061637 ]. \t  0.2029365073720305 \t 3.771634489655683\n",
      "40     \t [0.68891349 0.07170848 0.91273754]. \t  0.3644836785822796 \t 3.771634489655683\n",
      "41     \t [0.70664598 0.00359851 0.80572512]. \t  0.2584865256522639 \t 3.771634489655683\n",
      "42     \t [0.28937801 0.46416963 0.24158263]. \t  0.3153279295858197 \t 3.771634489655683\n",
      "43     \t [0.57580235 0.03307013 0.7990242 ]. \t  0.34400695897946443 \t 3.771634489655683\n",
      "44     \t [0.09739252 0.48744649 0.98239158]. \t  2.249316846427416 \t 3.771634489655683\n",
      "45     \t [0.57215748 0.81772148 0.08081551]. \t  0.004107945210182036 \t 3.771634489655683\n",
      "46     \t [0.25029555 0.03218495 0.41040645]. \t  0.48831139871669466 \t 3.771634489655683\n",
      "47     \t [0.48144191 0.66129613 0.32669206]. \t  0.3118766076901997 \t 3.771634489655683\n",
      "48     \t [0.54383562 0.68843494 0.39754072]. \t  0.6071596355594916 \t 3.771634489655683\n",
      "49     \t [0.61632653 0.80762409 0.11658522]. \t  0.007764003637543907 \t 3.771634489655683\n",
      "50     \t [0.11943269 0.79208637 0.67105988]. \t  2.5604362371269542 \t 3.771634489655683\n",
      "51     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.771634489655683\n",
      "52     \t [0.35694243 0.13628735 0.97680919]. \t  0.4204855119916151 \t 3.771634489655683\n",
      "53     \t [0.16520131 0.15697552 0.28504892]. \t  0.8630790546054824 \t 3.771634489655683\n",
      "54     \t [0.89807939 0.20808284 0.13597905]. \t  0.2368611982249034 \t 3.771634489655683\n",
      "55     \t [0.14556332 0.74958193 0.86294748]. \t  2.7389053935290635 \t 3.771634489655683\n",
      "56     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.771634489655683\n",
      "57     \t [0.93793433 0.96289914 0.62095157]. \t  0.41431461357234256 \t 3.771634489655683\n",
      "58     \t [0.73769945 0.64403488 0.34053379]. \t  0.1746857730921407 \t 3.771634489655683\n",
      "59     \t [0.17637354 0.45353782 0.90834784]. \t  3.181216311396711 \t 3.771634489655683\n",
      "60     \t [0.1102904  0.72905619 0.7105759 ]. \t  2.5478252541413138 \t 3.771634489655683\n",
      "61     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.771634489655683\n",
      "62     \t [0.92199931 0.75913036 0.59764878]. \t  0.6646765953627011 \t 3.771634489655683\n",
      "63     \t [0.19705171 0.73435699 0.7803774 ]. \t  2.712353150515283 \t 3.771634489655683\n",
      "64     \t [0.75147761 0.48911937 0.01747507]. \t  0.024852224218635303 \t 3.771634489655683\n",
      "65     \t [0.41907116 0.64224203 0.25228123]. \t  0.14748024004701676 \t 3.771634489655683\n",
      "66     \t [0.09918691 0.51251405 0.82103803]. \t  3.6962924579227248 \t 3.771634489655683\n",
      "67     \t [0.42471671 0.88158675 0.23989252]. \t  0.11653903631853006 \t 3.771634489655683\n",
      "68     \t [0.48388965 0.08673052 0.61339832]. \t  0.2393791775025995 \t 3.771634489655683\n",
      "69     \t [0.0713518  0.62234148 0.90537674]. \t  3.43256432178603 \t 3.771634489655683\n",
      "70     \t [0.86144225 0.25484801 0.5945089 ]. \t  0.4624660965502681 \t 3.771634489655683\n",
      "71     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.771634489655683\n",
      "72     \t [0.40849331 0.37194864 0.5961946 ]. \t  0.8341827724971607 \t 3.771634489655683\n",
      "73     \t [0.89971495 0.71543465 0.28623959]. \t  0.053432034071667286 \t 3.771634489655683\n",
      "74     \t [0.5574833  0.60674016 0.63907891]. \t  1.6491914725936576 \t 3.771634489655683\n",
      "75     \t [0.52544185 0.57976028 0.5353924 ]. \t  1.0100049618567128 \t 3.771634489655683\n",
      "76     \t [0.2030483  0.60445376 0.6118741 ]. \t  2.008738081126147 \t 3.771634489655683\n",
      "77     \t [0.83960497 0.7178898  0.51860813]. \t  0.5713437535535434 \t 3.771634489655683\n",
      "78     \t [0.12568486 0.48898317 0.39101566]. \t  0.4535341902267533 \t 3.771634489655683\n",
      "79     \t [0.40356815 0.91222831 0.4853063 ]. \t  1.9875464421362756 \t 3.771634489655683\n",
      "80     \t [0.30448895 0.14236495 0.57839827]. \t  0.26961305933473323 \t 3.771634489655683\n",
      "81     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.771634489655683\n",
      "82     \t [0.98963774 0.39128212 0.0483968 ]. \t  0.03524480693066017 \t 3.771634489655683\n",
      "83     \t [0.46545686 0.78536145 0.41128135]. \t  1.0375274839054927 \t 3.771634489655683\n",
      "84     \t [0.15183179 0.68326864 0.73319422]. \t  2.740138846743 \t 3.771634489655683\n",
      "85     \t [0.76625153 0.90636263 0.16262998]. \t  0.008953763578063236 \t 3.771634489655683\n",
      "86     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.771634489655683\n",
      "87     \t [0.88177672 0.84891331 0.85058481]. \t  1.6023093149208276 \t 3.771634489655683\n",
      "88     \t [0.7690034  0.94956963 0.54386002]. \t  0.7981680382972645 \t 3.771634489655683\n",
      "89     \t [0.42584837 0.27642666 0.84476741]. \t  1.91730519849934 \t 3.771634489655683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.59454665 0.45537968 0.58468917]. \t  0.8630606019833844 \t 3.771634489655683\n",
      "91     \t [0.40142707 0.84353954 0.4657078 ]. \t  1.8342725319658606 \t 3.771634489655683\n",
      "92     \t [0.84081615 0.99488463 0.97315247]. \t  0.41314251161269006 \t 3.771634489655683\n",
      "93     \t [0.87104222 0.54797113 0.12798409]. \t  0.04169874636833335 \t 3.771634489655683\n",
      "94     \t [0.45601705 0.61857631 0.12320264]. \t  0.04646537162221157 \t 3.771634489655683\n",
      "95     \t [0.92791424 0.4076477  0.77780579]. \t  2.735659286442517 \t 3.771634489655683\n",
      "96     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.771634489655683\n",
      "97     \t [0.37566745 0.03245529 0.93029623]. \t  0.22800922636108983 \t 3.771634489655683\n",
      "98     \t [0.02696673 0.47445665 0.50263357]. \t  0.7518362739769882 \t 3.771634489655683\n",
      "99     \t [0.32353418 0.39124557 0.82877661]. \t  3.019316440408473 \t 3.771634489655683\n",
      "100    \t [0.13645963 0.68909868 0.32313889]. \t  0.45748233319260295 \t 3.771634489655683\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [0.84818564 0.94085197 0.99354084]. \t  0.5448385422791876 \t 0.5647137279144399\n",
      "2      \t [0.00227319 0.27141826 0.95291977]. \t  \u001b[92m1.263004128201619\u001b[0m \t 1.263004128201619\n",
      "3      \t [0.00798031 0.03065721 0.14136081]. \t  0.39018974818183505 \t 1.263004128201619\n",
      "4      \t [0.08579033 0.76749811 0.9456053 ]. \t  \u001b[92m2.0306218951148267\u001b[0m \t 2.0306218951148267\n",
      "5      \t [0.04738879 0.88300606 0.04039215]. \t  0.0015031669884217013 \t 2.0306218951148267\n",
      "6      \t [0.9395453  0.92040294 0.01364733]. \t  0.0001428288445807452 \t 2.0306218951148267\n",
      "7      \t [0.77850495 0.03166667 0.08677142]. \t  0.21143052308139257 \t 2.0306218951148267\n",
      "8      \t [0.93601229 0.21307273 0.95402798]. \t  0.8621344872119134 \t 2.0306218951148267\n",
      "9      \t [0.05132907 0.95144077 0.74570392]. \t  1.4155608156504336 \t 2.0306218951148267\n",
      "10     \t [0.36440183 0.64658727 0.9861336 ]. \t  \u001b[92m2.200649424947822\u001b[0m \t 2.200649424947822\n",
      "11     \t [0.01606995 0.70223025 0.52824604]. \t  \u001b[92m2.2826705169024453\u001b[0m \t 2.2826705169024453\n",
      "12     \t [0.99009257 0.71650847 0.39851076]. \t  0.12354186056992493 \t 2.2826705169024453\n",
      "13     \t [0.98320704 0.25377295 0.0649609 ]. \t  0.0782882581977021 \t 2.2826705169024453\n",
      "14     \t [0.83729419 0.97610451 0.06943465]. \t  0.0005698155093462204 \t 2.2826705169024453\n",
      "15     \t [0.7906421  0.54259562 0.28658985]. \t  0.12432963415046257 \t 2.2826705169024453\n",
      "16     \t [0.79490539 0.06382176 0.19744044]. \t  0.4872052488139809 \t 2.2826705169024453\n",
      "17     \t [0.6838507  0.23022748 0.79615579]. \t  1.4602972829198642 \t 2.2826705169024453\n",
      "18     \t [0.22636183 0.64061672 0.198285  ]. \t  0.08967756666680651 \t 2.2826705169024453\n",
      "19     \t [0.0691621  0.46326016 0.50833668]. \t  0.7423073923540788 \t 2.2826705169024453\n",
      "20     \t [0.83170682 0.17888937 0.36366687]. \t  0.3883592928907606 \t 2.2826705169024453\n",
      "21     \t [0.16170007 0.68023358 0.6872927 ]. \t  \u001b[92m2.542547919759233\u001b[0m \t 2.542547919759233\n",
      "22     \t [0.2639102  0.23353625 0.79583519]. \t  1.497806538156057 \t 2.542547919759233\n",
      "23     \t [0.28682474 0.64334271 0.66487252]. \t  2.317991104262553 \t 2.542547919759233\n",
      "24     \t [0.23133169 0.39329039 0.65341517]. \t  1.444128581846464 \t 2.542547919759233\n",
      "25     \t [0.96725291 0.67978249 0.99568319]. \t  1.8467876437501514 \t 2.542547919759233\n",
      "26     \t [0.94842633 0.89564128 0.87267219]. \t  1.200400456562727 \t 2.542547919759233\n",
      "27     \t [0.09099856 0.64101224 0.05424805]. \t  0.01399937644003779 \t 2.542547919759233\n",
      "28     \t [0.59874655 0.29218002 0.45897895]. \t  0.3021734390962647 \t 2.542547919759233\n",
      "29     \t [0.64496072 0.22245524 0.27928371]. \t  0.7110696406898894 \t 2.542547919759233\n",
      "30     \t [0.52697739 0.7663314  0.85444265]. \t  2.527873327885665 \t 2.542547919759233\n",
      "31     \t [0.08489481 0.18980925 0.71654586]. \t  0.9054809583985677 \t 2.542547919759233\n",
      "32     \t [0.27760652 0.85506997 0.97266512]. \t  1.145464938616533 \t 2.542547919759233\n",
      "33     \t [0.79321136 0.08824052 0.45244894]. \t  0.22491499230957876 \t 2.542547919759233\n",
      "34     \t [0.15568853 0.49689242 0.63048503]. \t  1.6499720569649918 \t 2.542547919759233\n",
      "35     \t [0.23233499 0.26917355 0.48381664]. \t  0.33202393393512286 \t 2.542547919759233\n",
      "36     \t [0.88845153 0.69037472 0.38714919]. \t  0.1676020084418643 \t 2.542547919759233\n",
      "37     \t [0.97764727 0.19556086 0.15078478]. \t  0.2058548030755244 \t 2.542547919759233\n",
      "38     \t [0.10534516 0.35382056 0.37061701]. \t  0.41746701641257455 \t 2.542547919759233\n",
      "39     \t [0.53849731 0.10961919 0.44318802]. \t  0.3823976204649163 \t 2.542547919759233\n",
      "40     \t [0.76599538 0.12989459 0.05405983]. \t  0.15900068899346548 \t 2.542547919759233\n",
      "41     \t [0.76260918 0.64093003 0.56663145]. \t  0.8307494998265441 \t 2.542547919759233\n",
      "42     \t [0.36382114 0.14709907 0.68928492]. \t  0.6070017544805645 \t 2.542547919759233\n",
      "43     \t [0.70167635 0.07015801 0.52279586]. \t  0.1561225370415103 \t 2.542547919759233\n",
      "44     \t [0.05170108 0.1134196  0.63952085]. \t  0.33990734323546096 \t 2.542547919759233\n",
      "45     \t [0.839324   0.06403955 0.40024224]. \t  0.2994111813182178 \t 2.542547919759233\n",
      "46     \t [0.45411088 0.59508763 0.96650425]. \t  \u001b[92m2.671951945047823\u001b[0m \t 2.671951945047823\n",
      "47     \t [0.78906737 0.38659812 0.15240793]. \t  0.1921299403310432 \t 2.671951945047823\n",
      "48     \t [0.38384218 0.0751017  0.74184332]. \t  0.45404500777371515 \t 2.671951945047823\n",
      "49     \t [0.79007954 0.40272769 0.71645872]. \t  2.062790581082308 \t 2.671951945047823\n",
      "50     \t [0.03878896 0.35923014 0.02009634]. \t  0.06416205097137362 \t 2.671951945047823\n",
      "51     \t [0.20506765 0.03084034 0.10531936]. \t  0.3898960934927274 \t 2.671951945047823\n",
      "52     \t [0.26498722 0.11267496 0.61787615]. \t  0.2946864112690853 \t 2.671951945047823\n",
      "53     \t [0.09033347 0.62724944 0.40843099]. \t  0.909831714731411 \t 2.671951945047823\n",
      "54     \t [0.62065724 0.12402137 0.97715591]. \t  0.37643420052178506 \t 2.671951945047823\n",
      "55     \t [0.61411986 0.75466171 0.15501918]. \t  0.01986646070580702 \t 2.671951945047823\n",
      "56     \t [0.57568572 0.03836061 0.34427041]. \t  0.6934393647900274 \t 2.671951945047823\n",
      "57     \t [0.59188961 0.39128668 0.67530293]. \t  1.580913872707931 \t 2.671951945047823\n",
      "58     \t [0.42475792 0.97387134 0.00902368]. \t  0.00035204927402394324 \t 2.671951945047823\n",
      "59     \t [0.15778838 0.18300772 0.09020225]. \t  0.32691265266507724 \t 2.671951945047823\n",
      "60     \t [0.87275814 0.84250628 0.84964085]. \t  1.6618514972948977 \t 2.671951945047823\n",
      "61     \t [0.14574635 0.38726702 0.24731617]. \t  0.42664930173697285 \t 2.671951945047823\n",
      "62     \t [0.81638382 0.34391524 0.37149758]. \t  0.2594376062979833 \t 2.671951945047823\n",
      "63     \t [0.02564778 0.55046707 0.80272242]. \t  \u001b[92m3.6055589342778753\u001b[0m \t 3.6055589342778753\n",
      "64     \t [0.29217694 0.60262025 0.33532167]. \t  0.39133002431244923 \t 3.6055589342778753\n",
      "65     \t [0.39043185 0.52941375 0.10714522]. \t  0.08622169483458463 \t 3.6055589342778753\n",
      "66     \t [0.63444916 0.36426605 0.85539499]. \t  2.7493518034550775 \t 3.6055589342778753\n",
      "67     \t [0.0465056  0.60854331 0.92882691]. \t  3.2037911489156845 \t 3.6055589342778753\n",
      "68     \t [0.47191162 0.13818727 0.7444622 ]. \t  0.7346908833612771 \t 3.6055589342778753\n",
      "69     \t [0.55406155 0.09043208 0.11680137]. \t  0.45412533126015486 \t 3.6055589342778753\n",
      "70     \t [0.34222285 0.83045299 0.18503117]. \t  0.04650710902298375 \t 3.6055589342778753\n",
      "71     \t [0.43584131 0.50778002 0.05015357]. \t  0.05234924350693408 \t 3.6055589342778753\n",
      "72     \t [0.1067718  0.54409883 0.40585873]. \t  0.6154702295468619 \t 3.6055589342778753\n",
      "73     \t [0.11137856 0.94406589 0.09120133]. \t  0.004877999350758647 \t 3.6055589342778753\n",
      "74     \t [0.47318103 0.36048154 0.0183971 ]. \t  0.08343197041809045 \t 3.6055589342778753\n",
      "75     \t [0.67612549 0.38549353 0.03872676]. \t  0.07646283841794146 \t 3.6055589342778753\n",
      "76     \t [0.42596168 0.35197751 0.56469788]. \t  0.5906940593598009 \t 3.6055589342778753\n",
      "77     \t [0.60053513 0.61777627 0.11920594]. \t  0.03847212463219365 \t 3.6055589342778753\n",
      "78     \t [0.20200377 0.47480006 0.44549866]. \t  0.5690413955300141 \t 3.6055589342778753\n",
      "79     \t [0.27845204 0.01145598 0.09828696]. \t  0.3705038669599746 \t 3.6055589342778753\n",
      "80     \t [0.12125842 0.59471263 0.86922824]. \t  \u001b[92m3.766861277872451\u001b[0m \t 3.766861277872451\n",
      "81     \t [0.66688846 0.22554472 0.58509531]. \t  0.3956915884135331 \t 3.766861277872451\n",
      "82     \t [0.70037105 0.44356148 0.04236935]. \t  0.05432916266670234 \t 3.766861277872451\n",
      "83     \t [0.83545233 0.46071611 0.8940389 ]. \t  3.2994723316848176 \t 3.766861277872451\n",
      "84     \t [0.66383305 0.13344965 0.87018013]. \t  0.7291879962461709 \t 3.766861277872451\n",
      "85     \t [0.64278847 0.51956449 0.17011568]. \t  0.1232935205513317 \t 3.766861277872451\n",
      "86     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.766861277872451\n",
      "87     \t [0.46295686 0.00718677 0.2675899 ]. \t  0.8633361727835598 \t 3.766861277872451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.63661989 0.6262248  0.89263547]. \t  3.507866049799997 \t 3.766861277872451\n",
      "89     \t [0.50620969 0.60759504 0.05899758]. \t  0.023746032625237397 \t 3.766861277872451\n",
      "90     \t [0.69190312 0.33390545 0.79015211]. \t  2.330472349785727 \t 3.766861277872451\n",
      "91     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.766861277872451\n",
      "92     \t [0.2571393  0.48793247 0.20039213]. \t  0.2275105901066279 \t 3.766861277872451\n",
      "93     \t [0.70093933 0.69291867 0.53900539]. \t  0.936698712797758 \t 3.766861277872451\n",
      "94     \t [0.65575744 0.69334793 0.64502505]. \t  1.509331797337941 \t 3.766861277872451\n",
      "95     \t [0.03451085 0.63602022 0.93854795]. \t  2.971034254042888 \t 3.766861277872451\n",
      "96     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.766861277872451\n",
      "97     \t [0.16505998 0.56790177 0.200132  ]. \t  0.127912546004578 \t 3.766861277872451\n",
      "98     \t [0.42483603 0.22137435 0.08625916]. \t  0.3323829907804243 \t 3.766861277872451\n",
      "99     \t [0.81538527 0.49290493 0.42253205]. \t  0.18928231253683095 \t 3.766861277872451\n",
      "100    \t [0.77252383 0.70572381 0.59796455]. \t  0.9956774010817171 \t 3.766861277872451\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [0.47647277 0.92393606 0.15672912]. \t  0.017844770068977132 \t 1.9592421489197056\n",
      "2      \t [0.72176492 0.         0.96425917]. \t  0.12600618381348344 \t 1.9592421489197056\n",
      "3      \t [0.12891537 0.10450196 0.76439878]. \t  0.6009821728805325 \t 1.9592421489197056\n",
      "4      \t [0.07213246 0.95861543 0.39059898]. \t  1.2397133678239307 \t 1.9592421489197056\n",
      "5      \t [0.98597041 0.2856864  0.41065736]. \t  0.15284929019920618 \t 1.9592421489197056\n",
      "6      \t [0.35984016 0.39345196 0.95312626]. \t  \u001b[92m2.2080405720154364\u001b[0m \t 2.2080405720154364\n",
      "7      \t [0.80886206 0.5371506  0.97065275]. \t  \u001b[92m2.5535236020960204\u001b[0m \t 2.5535236020960204\n",
      "8      \t [0.78616262 0.98348255 0.89699467]. \t  0.6336596985860946 \t 2.5535236020960204\n",
      "9      \t [0.04681794 0.56517096 0.99093683]. \t  2.2288293321682677 \t 2.5535236020960204\n",
      "10     \t [0.92671631 0.30850458 0.73208369]. \t  1.6831956823568888 \t 2.5535236020960204\n",
      "11     \t [0.99251564 0.53200473 0.97139017]. \t  2.4848356404702816 \t 2.5535236020960204\n",
      "12     \t [0.46379372 0.38690137 0.37719231]. \t  0.4121445056585975 \t 2.5535236020960204\n",
      "13     \t [0.01989029 0.49631073 0.04396718]. \t  0.03714686443394807 \t 2.5535236020960204\n",
      "14     \t [0.004404   0.09896688 0.094363  ]. \t  0.27280620176167747 \t 2.5535236020960204\n",
      "15     \t [0.00680257 0.83486728 0.96935618]. \t  1.2909852498742977 \t 2.5535236020960204\n",
      "16     \t [0.45769784 0.80757882 0.94089557]. \t  1.7544268191834966 \t 2.5535236020960204\n",
      "17     \t [0.95230129 0.38134913 0.4858654 ]. \t  0.18305807391691747 \t 2.5535236020960204\n",
      "18     \t [0.6537332  0.60354243 0.75153684]. \t  \u001b[92m2.8201029650726914\u001b[0m \t 2.8201029650726914\n",
      "19     \t [0.969245   0.14050121 0.02213768]. \t  0.055581951405792945 \t 2.8201029650726914\n",
      "20     \t [0.61043042 0.76901763 0.23691638]. \t  0.07288343079196587 \t 2.8201029650726914\n",
      "21     \t [0.72091588 0.81569726 0.01051573]. \t  0.0008541583076826821 \t 2.8201029650726914\n",
      "22     \t [0.71083172 0.23505423 0.94873069]. \t  1.0498890486494339 \t 2.8201029650726914\n",
      "23     \t [0.72226889 0.20156952 0.09222259]. \t  0.25522171878427014 \t 2.8201029650726914\n",
      "24     \t [0.39269752 0.54762669 0.83297827]. \t  \u001b[92m3.8151893325258737\u001b[0m \t 3.8151893325258737\n",
      "25     \t [0.15506875 0.7398081  0.79992413]. \t  2.7493877272612255 \t 3.8151893325258737\n",
      "26     \t [0.47922685 0.39281578 0.00439121]. \t  0.056670893979183526 \t 3.8151893325258737\n",
      "27     \t [0.93341652 0.98195286 0.01379805]. \t  8.502821513842029e-05 \t 3.8151893325258737\n",
      "28     \t [0.58161244 0.85353531 0.4330298 ]. \t  0.9907374713535583 \t 3.8151893325258737\n",
      "29     \t [0.36698684 0.71903546 0.38399462]. \t  0.8319079231681563 \t 3.8151893325258737\n",
      "30     \t [0.96710442 0.0449519  0.9665492 ]. \t  0.19275441918756014 \t 3.8151893325258737\n",
      "31     \t [0.47944205 0.33257806 0.96390037]. \t  1.623099451158217 \t 3.8151893325258737\n",
      "32     \t [0.09745536 0.94251266 0.16993469]. \t  0.03433223941386708 \t 3.8151893325258737\n",
      "33     \t [0.71625866 0.87709437 0.30436503]. \t  0.1537142166541831 \t 3.8151893325258737\n",
      "34     \t [0.19012126 0.12058599 0.11795816]. \t  0.4653194865578239 \t 3.8151893325258737\n",
      "35     \t [0.74774734 0.04970054 0.42040843]. \t  0.31451243264143736 \t 3.8151893325258737\n",
      "36     \t [0.84958006 0.02224249 0.22693705]. \t  0.4352870206613002 \t 3.8151893325258737\n",
      "37     \t [0.73814083 0.75071711 0.80438852]. \t  2.3887537283639095 \t 3.8151893325258737\n",
      "38     \t [0.64913429 0.33518378 0.30617113]. \t  0.48114562793001553 \t 3.8151893325258737\n",
      "39     \t [0.79436457 0.7512464  0.20992399]. \t  0.02730621755927814 \t 3.8151893325258737\n",
      "40     \t [0.76449008 0.18849949 0.41987   ]. \t  0.3153807348262885 \t 3.8151893325258737\n",
      "41     \t [0.07141796 0.17239542 0.26121577]. \t  0.7446444819706365 \t 3.8151893325258737\n",
      "42     \t [0.38180059 0.43843081 0.22180373]. \t  0.3474035754294986 \t 3.8151893325258737\n",
      "43     \t [0.08482858 0.82164654 0.49044133]. \t  2.6127388005142196 \t 3.8151893325258737\n",
      "44     \t [0.         0.59972128 0.65899086]. \t  2.222863886254104 \t 3.8151893325258737\n",
      "45     \t [0.32959084 0.36001994 0.87349593]. \t  2.667005212854527 \t 3.8151893325258737\n",
      "46     \t [0.99747198 0.15794026 0.84858456]. \t  0.8922482324898482 \t 3.8151893325258737\n",
      "47     \t [0.59205156 0.12922306 0.02366767]. \t  0.14491954357998088 \t 3.8151893325258737\n",
      "48     \t [0.99623678 0.57079417 0.8806454 ]. \t  3.6095994722521034 \t 3.8151893325258737\n",
      "49     \t [0.49316125 0.64947007 0.46837257]. \t  1.0091456887671393 \t 3.8151893325258737\n",
      "50     \t [0.77070132 0.78203712 0.49392271]. \t  0.7154307154874395 \t 3.8151893325258737\n",
      "51     \t [0.08966898 0.76237058 0.22546117]. \t  0.11425353032662568 \t 3.8151893325258737\n",
      "52     \t [0.85449185 0.45695142 0.28387514]. \t  0.16567414047062834 \t 3.8151893325258737\n",
      "53     \t [0.78613408 0.08167713 0.18378513]. \t  0.47525089404355164 \t 3.8151893325258737\n",
      "54     \t [0.29687815 0.87933742 0.87351696]. \t  1.4831575364258622 \t 3.8151893325258737\n",
      "55     \t [0.65080677 0.50894956 0.83584544]. \t  3.7124054670165427 \t 3.8151893325258737\n",
      "56     \t [0.25056999 0.23593882 0.47352493]. \t  0.33300666000299306 \t 3.8151893325258737\n",
      "57     \t [0.95419104 0.99391944 0.31777697]. \t  0.05673756586560111 \t 3.8151893325258737\n",
      "58     \t [0.68517702 0.04633665 0.7821367 ]. \t  0.3828038218861167 \t 3.8151893325258737\n",
      "59     \t [0.2365173  0.19077569 0.7414871 ]. \t  1.0200235283282992 \t 3.8151893325258737\n",
      "60     \t [0.6926002  0.01666735 0.52810415]. \t  0.12583141797854358 \t 3.8151893325258737\n",
      "61     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8151893325258737\n",
      "62     \t [0.29621598 0.79790783 0.02054715]. \t  0.002024858861289507 \t 3.8151893325258737\n",
      "63     \t [0.97243547 0.39776351 0.90290875]. \t  2.7231173646354363 \t 3.8151893325258737\n",
      "64     \t [0.67144269 0.06370543 0.88091737]. \t  0.3904208308121737 \t 3.8151893325258737\n",
      "65     \t [0.25392813 0.87188832 0.77817404]. \t  1.7048137935058674 \t 3.8151893325258737\n",
      "66     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8151893325258737\n",
      "67     \t [0.02004482 0.15447473 0.24703621]. \t  0.6771287708568413 \t 3.8151893325258737\n",
      "68     \t [0.03434399 0.61503807 0.84049126]. \t  3.6869648987519668 \t 3.8151893325258737\n",
      "69     \t [0.05458561 0.38780484 0.02342249]. \t  0.06002266829451044 \t 3.8151893325258737\n",
      "70     \t [0.93801884 0.55742704 0.78516778]. \t  3.2134915628582204 \t 3.8151893325258737\n",
      "71     \t [0.34682653 0.0185079  0.84163519]. \t  0.2873547834147019 \t 3.8151893325258737\n",
      "72     \t [0.04159497 0.06039998 0.78206446]. \t  0.4289808497990568 \t 3.8151893325258737\n",
      "73     \t [0.74001869 0.56057665 0.76221524]. \t  3.0080293794656545 \t 3.8151893325258737\n",
      "74     \t [0.78632777 0.4286754  0.5041643 ]. \t  0.3065319733912303 \t 3.8151893325258737\n",
      "75     \t [0.57431136 0.18655666 0.37108816]. \t  0.6173683076972666 \t 3.8151893325258737\n",
      "76     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8151893325258737\n",
      "77     \t [0.63036437 0.10444753 0.50685531]. \t  0.2032777599608422 \t 3.8151893325258737\n",
      "78     \t [0.94754388 0.93130913 0.02533518]. \t  0.00016201526987493124 \t 3.8151893325258737\n",
      "79     \t [0.72749109 0.29778109 0.07816875]. \t  0.16771807928609878 \t 3.8151893325258737\n",
      "80     \t [0.24104627 0.05784681 0.35761507]. \t  0.7223868418715442 \t 3.8151893325258737\n",
      "81     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8151893325258737\n",
      "82     \t [0.50645353 0.96469724 0.40230175]. \t  0.8577685518309991 \t 3.8151893325258737\n",
      "83     \t [0.47760278 0.03970655 0.12418723]. \t  0.4918340278072209 \t 3.8151893325258737\n",
      "84     \t [0.7042768  0.90279546 0.76947643]. \t  1.0802503344142702 \t 3.8151893325258737\n",
      "85     \t [0.93029315 0.72624329 0.18884353]. \t  0.013666408812849307 \t 3.8151893325258737\n",
      "86     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8151893325258737\n",
      "87     \t [0.36123269 0.05714129 0.24442103]. \t  0.9498213276855749 \t 3.8151893325258737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.42658279 0.36008033 0.44964381]. \t  0.37058270938451565 \t 3.8151893325258737\n",
      "89     \t [0.07821525 0.28395456 0.59400979]. \t  0.5962106261528902 \t 3.8151893325258737\n",
      "90     \t [0.35700609 0.85474401 0.67761676]. \t  2.0935319664822267 \t 3.8151893325258737\n",
      "91     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8151893325258737\n",
      "92     \t [0.89335318 0.48621021 0.25719843]. \t  0.11947728525669717 \t 3.8151893325258737\n",
      "93     \t [0.25767767 0.32707301 0.11423834]. \t  0.30731655934584096 \t 3.8151893325258737\n",
      "94     \t [0.08343446 0.38720798 0.21858599]. \t  0.3610006026372162 \t 3.8151893325258737\n",
      "95     \t [0.74268025 0.72842831 0.68601941]. \t  1.5289709164819008 \t 3.8151893325258737\n",
      "96     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8151893325258737\n",
      "97     \t [0.69612463 0.41825352 0.06053865]. \t  0.08125363518274566 \t 3.8151893325258737\n",
      "98     \t [0.17078721 0.27754479 0.64585559]. \t  0.9021637496207571 \t 3.8151893325258737\n",
      "99     \t [0.67232233 0.97100767 0.99615183]. \t  0.4304963204316718 \t 3.8151893325258737\n",
      "100    \t [0.33960335 0.24229867 0.9555809 ]. \t  1.0608480606353172 \t 3.8151893325258737\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.70996449]. \t  \u001b[92m1.4248573182815316\u001b[0m \t 1.4248573182815316\n",
      "2      \t [0.92779976 0.07015106 0.89872933]. \t  0.3773619776486492 \t 1.4248573182815316\n",
      "3      \t [0.02311239 0.85121629 0.01672445]. \t  0.000979175972205631 \t 1.4248573182815316\n",
      "4      \t [0.12010582 0.59106076 0.95578026]. \t  \u001b[92m2.8456094195030626\u001b[0m \t 2.8456094195030626\n",
      "5      \t [0.46435384 0.46360957 0.98170214]. \t  2.1839422331213445 \t 2.8456094195030626\n",
      "6      \t [0.00198933 0.4672197  0.7091874 ]. \t  2.360146381618021 \t 2.8456094195030626\n",
      "7      \t [0.10422717 0.07186274 0.09377813]. \t  0.32180875456045027 \t 2.8456094195030626\n",
      "8      \t [0.9288332  0.06863405 0.08101552]. \t  0.13465766405489876 \t 2.8456094195030626\n",
      "9      \t [0.95865942 0.72495028 0.83845002]. \t  2.712478201608403 \t 2.8456094195030626\n",
      "10     \t [0.95644514 0.36678771 0.46830963]. \t  0.15636536592706055 \t 2.8456094195030626\n",
      "11     \t [0.97048148 0.5725626  0.97381238]. \t  2.4750997190639303 \t 2.8456094195030626\n",
      "12     \t [0.98431778 0.97728845 0.65410684]. \t  0.3460698616746582 \t 2.8456094195030626\n",
      "13     \t [0.39684022 0.46533085 0.29798335]. \t  0.35068722282708686 \t 2.8456094195030626\n",
      "14     \t [0.06927026 0.24993547 0.9685129 ]. \t  1.004531629071045 \t 2.8456094195030626\n",
      "15     \t [0.29117701 0.79906396 0.21189735]. \t  0.08424253943987578 \t 2.8456094195030626\n",
      "16     \t [0.73542419 0.17154116 0.13117851]. \t  0.3721146732362513 \t 2.8456094195030626\n",
      "17     \t [0.03538692 0.87260147 0.91229935]. \t  1.3922845520042249 \t 2.8456094195030626\n",
      "18     \t [0.0097183  0.73342815 0.65880477]. \t  2.5293795831783044 \t 2.8456094195030626\n",
      "19     \t [0.16605575 0.31484971 0.61033204]. \t  0.7886849227311512 \t 2.8456094195030626\n",
      "20     \t [0.78937418 0.94870826 0.39243625]. \t  0.3225235715036547 \t 2.8456094195030626\n",
      "21     \t [0.95838034 0.79362036 0.08587379]. \t  0.0017922585961232876 \t 2.8456094195030626\n",
      "22     \t [0.32959478 0.97977578 0.31699981]. \t  0.42556300209858455 \t 2.8456094195030626\n",
      "23     \t [0.12594363 0.68422837 0.07078373]. \t  0.012399148292579262 \t 2.8456094195030626\n",
      "24     \t [0.40475859 0.4658762  0.25740992]. \t  0.3253303343023408 \t 2.8456094195030626\n",
      "25     \t [0.71548507 0.98750527 0.46697302]. \t  0.6987097690521482 \t 2.8456094195030626\n",
      "26     \t [0.97590941 0.99579231 0.53571642]. \t  0.2867894657820969 \t 2.8456094195030626\n",
      "27     \t [0.64593789 0.48116769 0.76663233]. \t  \u001b[92m3.046702602216973\u001b[0m \t 3.046702602216973\n",
      "28     \t [0.67537018 0.05927558 0.00145966]. \t  0.08757814118957351 \t 3.046702602216973\n",
      "29     \t [0.84624275 0.55502505 0.96251073]. \t  2.7026173611483757 \t 3.046702602216973\n",
      "30     \t [0.86200592 0.93431391 0.21993853]. \t  0.0188642484337634 \t 3.046702602216973\n",
      "31     \t [0.28519781 0.40292405 0.13698136]. \t  0.26134883725309527 \t 3.046702602216973\n",
      "32     \t [0.5252679  0.54306335 0.56921457]. \t  1.0631090833926988 \t 3.046702602216973\n",
      "33     \t [0.04496167 0.15128783 0.07163235]. \t  0.22876159277394795 \t 3.046702602216973\n",
      "34     \t [0.17807663 0.98302126 0.19239364]. \t  0.05150638057792035 \t 3.046702602216973\n",
      "35     \t [0.10563847 0.51965299 0.35410946]. \t  0.3902774762175974 \t 3.046702602216973\n",
      "36     \t [0.36961533 0.38808606 0.64315993]. \t  1.2951624379659934 \t 3.046702602216973\n",
      "37     \t [0.75607284 0.43117565 0.85118749]. \t  \u001b[92m3.304700873877996\u001b[0m \t 3.304700873877996\n",
      "38     \t [0.08613721 0.892057   0.89648729]. \t  1.3071904162918941 \t 3.304700873877996\n",
      "39     \t [0.19581017 0.82604754 0.02575648]. \t  0.0016902112440418018 \t 3.304700873877996\n",
      "40     \t [0.9628041  0.00165349 0.86583751]. \t  0.22112418189446642 \t 3.304700873877996\n",
      "41     \t [0.61891633 0.22574784 0.45757517]. \t  0.3065004557945397 \t 3.304700873877996\n",
      "42     \t [0.05983348 0.37532816 0.45309878]. \t  0.3784370368464777 \t 3.304700873877996\n",
      "43     \t [0.70141011 0.22837892 0.54719061]. \t  0.28500028019394896 \t 3.304700873877996\n",
      "44     \t [0.90068885 0.51601282 0.81339203]. \t  \u001b[92m3.5116799583321114\u001b[0m \t 3.5116799583321114\n",
      "45     \t [0.99817048 0.97270388 0.19931396]. \t  0.005914829519934096 \t 3.5116799583321114\n",
      "46     \t [0.9406422  0.3104256  0.12201668]. \t  0.1370253403613483 \t 3.5116799583321114\n",
      "47     \t [0.78008751 0.91217743 0.09891658]. \t  0.001965946817253484 \t 3.5116799583321114\n",
      "48     \t [0.54025253 0.58494373 0.35184001]. \t  0.3047095942676279 \t 3.5116799583321114\n",
      "49     \t [0.34329903 0.88702102 0.03721005]. \t  0.0013660761576769008 \t 3.5116799583321114\n",
      "50     \t [0.03360824 0.00802081 0.36067209]. \t  0.48946990766554466 \t 3.5116799583321114\n",
      "51     \t [0.20787217 0.61004851 0.74845814]. \t  3.0360388299352037 \t 3.5116799583321114\n",
      "52     \t [0.89340077 0.85748575 0.59961431]. \t  0.6248569322835986 \t 3.5116799583321114\n",
      "53     \t [0.18926891 0.95140736 0.7658503 ]. \t  1.2879568215548085 \t 3.5116799583321114\n",
      "54     \t [0.40381616 0.65269767 0.05450903]. \t  0.015308395907646804 \t 3.5116799583321114\n",
      "55     \t [0.45580338 0.64567726 0.65293851]. \t  1.9857258482938995 \t 3.5116799583321114\n",
      "56     \t [0.54251355 0.85179015 0.45787374]. \t  1.2987563774658661 \t 3.5116799583321114\n",
      "57     \t [0.98766017 0.04640037 0.82327905]. \t  0.3724708599416452 \t 3.5116799583321114\n",
      "58     \t [0.11902505 0.69838178 0.02175544]. \t  0.00506596019128309 \t 3.5116799583321114\n",
      "59     \t [0.24986741 0.16910196 0.38781282]. \t  0.6183420024255487 \t 3.5116799583321114\n",
      "60     \t [0.03513873 0.21439434 0.78423176]. \t  1.3025923811717548 \t 3.5116799583321114\n",
      "61     \t [0.6361924  0.27829844 0.47523219]. \t  0.27509536117455013 \t 3.5116799583321114\n",
      "62     \t [0.59626604 0.05648247 0.91943951]. \t  0.3063033169372537 \t 3.5116799583321114\n",
      "63     \t [0.70429363 0.00312427 0.24408351]. \t  0.6167915982559065 \t 3.5116799583321114\n",
      "64     \t [0.88808978 0.22863736 0.34092134]. \t  0.33856711999431305 \t 3.5116799583321114\n",
      "65     \t [0.09824742 0.23858652 0.2844683 ]. \t  0.6927292643999329 \t 3.5116799583321114\n",
      "66     \t [0.14501681 0.47075112 0.91461001]. \t  3.2110042352024024 \t 3.5116799583321114\n",
      "67     \t [0.78434316 0.3956329  0.08473852]. \t  0.10096567793274311 \t 3.5116799583321114\n",
      "68     \t [0.18659218 0.45043702 0.89531113]. \t  3.285246583751448 \t 3.5116799583321114\n",
      "69     \t [0.03261414 0.66221918 0.05884548]. \t  0.011079641212285937 \t 3.5116799583321114\n",
      "70     \t [0.54468173 0.29590807 0.9311682 ]. \t  1.6539672441123188 \t 3.5116799583321114\n",
      "71     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.5116799583321114\n",
      "72     \t [0.78235941 0.60061819 0.19284479]. \t  0.0561269513606585 \t 3.5116799583321114\n",
      "73     \t [0.34871399 0.92072751 0.39317276]. \t  1.1313363399092695 \t 3.5116799583321114\n",
      "74     \t [0.61380648 0.96105248 0.51387535]. \t  1.2481539430553115 \t 3.5116799583321114\n",
      "75     \t [0.7139411  0.86961409 0.66357915]. \t  1.0935382974348675 \t 3.5116799583321114\n",
      "76     \t [0.10004329 0.77380573 0.36903565]. \t  0.9767257832757625 \t 3.5116799583321114\n",
      "77     \t [0.32555747 0.87119144 0.02391585]. \t  0.001125942403551703 \t 3.5116799583321114\n",
      "78     \t [0.76932015 0.93434117 0.12890602]. \t  0.003831929221092921 \t 3.5116799583321114\n",
      "79     \t [0.20831485 0.14504721 0.92687713]. \t  0.6345433658676212 \t 3.5116799583321114\n",
      "80     \t [0.86204741 0.58837699 0.24105523]. \t  0.06403281689301106 \t 3.5116799583321114\n",
      "81     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.5116799583321114\n",
      "82     \t [0.21069057 0.77407452 0.23344649]. \t  0.13121712040533615 \t 3.5116799583321114\n",
      "83     \t [0.1001701  0.47295882 0.12013719]. \t  0.12052624207518559 \t 3.5116799583321114\n",
      "84     \t [0.43359098 0.97200977 0.13762089]. \t  0.011141220909150746 \t 3.5116799583321114\n",
      "85     \t [0.73812931 0.77157111 0.42711039]. \t  0.5232747468742454 \t 3.5116799583321114\n",
      "86     \t [0.27448468 0.17200083 0.38396597]. \t  0.6423027740941983 \t 3.5116799583321114\n",
      "87     \t [0.62358364 0.8260888  0.13237028]. \t  0.009415497102037882 \t 3.5116799583321114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.75099457 0.65441938 0.12255032]. \t  0.021155966226623357 \t 3.5116799583321114\n",
      "89     \t [0.77697783 0.89252243 0.43042056]. \t  0.4993825044385206 \t 3.5116799583321114\n",
      "90     \t [0.10858734 0.34765346 0.2913764 ]. \t  0.4955393359039446 \t 3.5116799583321114\n",
      "91     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.5116799583321114\n",
      "92     \t [0.57018049 0.66606921 0.11723059]. \t  0.02542004122614448 \t 3.5116799583321114\n",
      "93     \t [0.22195332 0.64359576 0.87193586]. \t  \u001b[92m3.5668333327348387\u001b[0m \t 3.5668333327348387\n",
      "94     \t [0.83406038 0.49926751 0.96336306]. \t  2.5839795928023706 \t 3.5668333327348387\n",
      "95     \t [0.85906739 0.60411887 0.59820621]. \t  0.8522079827951599 \t 3.5668333327348387\n",
      "96     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.5668333327348387\n",
      "97     \t [0.51508278 0.2294295  0.66213859]. \t  0.806875982384233 \t 3.5668333327348387\n",
      "98     \t [0.17960787 0.36340497 0.29699497]. \t  0.5075096027937913 \t 3.5668333327348387\n",
      "99     \t [0.59243786 0.96893281 0.2754484 ]. \t  0.13153647472057536 \t 3.5668333327348387\n",
      "100    \t [0.08917121 0.15241197 0.42408264]. \t  0.39739031089502425 \t 3.5668333327348387\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.46426722 0.31959039 0.90909418]. \t  2.0488314609076865 \t 2.5106636917702634\n",
      "2      \t [0.70898738 0.93302993 0.97435582]. \t  0.6738626249496847 \t 2.5106636917702634\n",
      "3      \t [0.75678295 0.02003835 0.75290539]. \t  0.287644796163147 \t 2.5106636917702634\n",
      "4      \t [0.97147912 0.88821771 0.207725  ]. \t  0.00947812438476387 \t 2.5106636917702634\n",
      "5      \t [0.99710485 0.42387924 0.61273974]. \t  0.8481431923526283 \t 2.5106636917702634\n",
      "6      \t [0.82762613 0.51322514 0.9423355 ]. \t  \u001b[92m2.9660960227381628\u001b[0m \t 2.9660960227381628\n",
      "7      \t [0.52154146 0.94289715 0.0141488 ]. \t  0.00041627150510049974 \t 2.9660960227381628\n",
      "8      \t [0.00676464 0.36240414 0.07004012]. \t  0.11516576142645876 \t 2.9660960227381628\n",
      "9      \t [0.95559854 0.87998361 0.87392421]. \t  1.3243935639269881 \t 2.9660960227381628\n",
      "10     \t [0.01988521 0.01189723 0.06101082]. \t  0.17333141682770897 \t 2.9660960227381628\n",
      "11     \t [0.76472106 0.79860774 0.67357588]. \t  1.1931746091910338 \t 2.9660960227381628\n",
      "12     \t [0.86589409 0.02143342 0.12579018]. \t  0.23857241047298577 \t 2.9660960227381628\n",
      "13     \t [0.06768473 0.98835432 0.43637529]. \t  1.7201092789518635 \t 2.9660960227381628\n",
      "14     \t [0.00960474 0.73501483 0.97670187]. \t  1.8789565432346857 \t 2.9660960227381628\n",
      "15     \t [0.14660221 0.55525094 0.39612766]. \t  0.6027790366540361 \t 2.9660960227381628\n",
      "16     \t [0.29280178 0.381848   0.94314101]. \t  2.2475614624833087 \t 2.9660960227381628\n",
      "17     \t [0.02884926 0.92107045 0.10038463]. \t  0.006359984812937052 \t 2.9660960227381628\n",
      "18     \t [0.68552777 0.90217347 0.08984633]. \t  0.0022844537103321114 \t 2.9660960227381628\n",
      "19     \t [0.24778727 0.51703487 0.30986352]. \t  0.3162293495446352 \t 2.9660960227381628\n",
      "20     \t [0.0332157  0.13370625 0.36932453]. \t  0.528056615697629 \t 2.9660960227381628\n",
      "21     \t [0.72570127 0.27053989 0.41939798]. \t  0.30570172226294623 \t 2.9660960227381628\n",
      "22     \t [0.22899771 0.88855773 0.66144849]. \t  2.3726789291100885 \t 2.9660960227381628\n",
      "23     \t [0.75660248 0.31819385 0.65511557]. \t  1.066387731221408 \t 2.9660960227381628\n",
      "24     \t [0.43367485 0.         0.        ]. \t  0.10096766099588854 \t 2.9660960227381628\n",
      "25     \t [1.         0.50488478 0.        ]. \t  0.007891266389360713 \t 2.9660960227381628\n",
      "26     \t [0.28925137 0.22910633 0.78929155]. \t  1.4476945024604517 \t 2.9660960227381628\n",
      "27     \t [0.04296544 0.92313739 0.7371167 ]. \t  1.6223698093850776 \t 2.9660960227381628\n",
      "28     \t [0.04117115 0.25557529 0.08207371]. \t  0.21370963201810345 \t 2.9660960227381628\n",
      "29     \t [0.52607309 0.28318212 0.15243849]. \t  0.4746576052019127 \t 2.9660960227381628\n",
      "30     \t [0.63939221 0.87737546 0.55554324]. \t  1.3686936883414307 \t 2.9660960227381628\n",
      "31     \t [0.4346958  0.60889655 0.8201429 ]. \t  \u001b[92m3.631042399892129\u001b[0m \t 3.631042399892129\n",
      "32     \t [0.13441687 0.86376956 0.57986455]. \t  3.0641504484097006 \t 3.631042399892129\n",
      "33     \t [0.55121056 0.62659953 0.55014174]. \t  1.195689113378892 \t 3.631042399892129\n",
      "34     \t [0.84182303 0.18550484 0.5241892 ]. \t  0.18986218486785345 \t 3.631042399892129\n",
      "35     \t [0.         0.68008636 0.78019863]. \t  3.0151899942303695 \t 3.631042399892129\n",
      "36     \t [0.90344081 0.17303917 0.15773467]. \t  0.2868981272674761 \t 3.631042399892129\n",
      "37     \t [0.35182468 0.62347921 0.93828355]. \t  3.0572181407413375 \t 3.631042399892129\n",
      "38     \t [0.04759745 0.15637077 0.32808732]. \t  0.6514615880268911 \t 3.631042399892129\n",
      "39     \t [0.19036093 0.6111129  0.7109371 ]. \t  2.675535818618081 \t 3.631042399892129\n",
      "40     \t [0.1862487  0.35049309 0.99584252]. \t  1.3618145428628112 \t 3.631042399892129\n",
      "41     \t [0.26109554 0.84873653 0.17389629]. \t  0.039416154328806276 \t 3.631042399892129\n",
      "42     \t [0.2411947  0.71089079 0.94830097]. \t  2.43940638666352 \t 3.631042399892129\n",
      "43     \t [0.95149478 0.05018079 0.53974879]. \t  0.098529041682867 \t 3.631042399892129\n",
      "44     \t [0.3233854  0.63321874 0.09207473]. \t  0.029931103458442163 \t 3.631042399892129\n",
      "45     \t [0.424223   0.12519416 0.67754617]. \t  0.4882972596028541 \t 3.631042399892129\n",
      "46     \t [0.1008011  0.69100806 0.11987073]. \t  0.02297185267312819 \t 3.631042399892129\n",
      "47     \t [0.85188265 0.04011227 0.89388837]. \t  0.2931216008636774 \t 3.631042399892129\n",
      "48     \t [0.01411906 0.26307292 0.27083666]. \t  0.5601798116996003 \t 3.631042399892129\n",
      "49     \t [0.07417431 0.28464511 0.69847017]. \t  1.327660116354567 \t 3.631042399892129\n",
      "50     \t [0.26391802 0.55233291 0.32548463]. \t  0.3396901722182805 \t 3.631042399892129\n",
      "51     \t [0.22836016 0.89651158 0.89707304]. \t  1.2740290385689355 \t 3.631042399892129\n",
      "52     \t [0.79818438 0.16817142 0.70514572]. \t  0.7416214286597129 \t 3.631042399892129\n",
      "53     \t [0.83527072 0.19572695 0.83014434]. \t  1.1944209938070887 \t 3.631042399892129\n",
      "54     \t [0.59300454 0.3111237  0.40598328]. \t  0.3818134617640583 \t 3.631042399892129\n",
      "55     \t [0.86854957 0.05055449 0.32404131]. \t  0.411411898931723 \t 3.631042399892129\n",
      "56     \t [0.37957024 0.35889641 0.92977225]. \t  2.2145067119344692 \t 3.631042399892129\n",
      "57     \t [0.13634234 0.05691787 0.98232037]. \t  0.19448113363052155 \t 3.631042399892129\n",
      "58     \t [0.5075492  0.04414528 0.82694882]. \t  0.3748726186448098 \t 3.631042399892129\n",
      "59     \t [0.05523261 0.01074321 0.57580439]. \t  0.11260749788850297 \t 3.631042399892129\n",
      "60     \t [0.18406096 0.38254387 0.22959094]. \t  0.43853690353229396 \t 3.631042399892129\n",
      "61     \t [0.68966182 0.15913004 0.20567779]. \t  0.6440137859907251 \t 3.631042399892129\n",
      "62     \t [0.65944577 0.19945516 0.65270416]. \t  0.6343995370203407 \t 3.631042399892129\n",
      "63     \t [0.21789836 0.46273006 0.66346585]. \t  1.8309817451061736 \t 3.631042399892129\n",
      "64     \t [0.07858971 0.62389709 0.13486611]. \t  0.04325978992777502 \t 3.631042399892129\n",
      "65     \t [0.56068032 0.24475387 0.08301478]. \t  0.2746559213717259 \t 3.631042399892129\n",
      "66     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.631042399892129\n",
      "67     \t [0.66767275 0.75152206 0.79946861]. \t  2.3942920894158 \t 3.631042399892129\n",
      "68     \t [0.66563017 0.97212126 0.85647134]. \t  0.7520519051826682 \t 3.631042399892129\n",
      "69     \t [0.36162685 0.03053591 0.18498491]. \t  0.7571886080164373 \t 3.631042399892129\n",
      "70     \t [0.23469791 0.19626658 0.46905979]. \t  0.33214498871143167 \t 3.631042399892129\n",
      "71     \t [0.59802231 0.44790879 0.9045159 ]. \t  3.1786990245245974 \t 3.631042399892129\n",
      "72     \t [0.05790537 0.31858793 0.16691747]. \t  0.36982693021208274 \t 3.631042399892129\n",
      "73     \t [0.48766107 0.82537961 0.10713698]. \t  0.007622744217565988 \t 3.631042399892129\n",
      "74     \t [0.28141591 0.1240646  0.78396034]. \t  0.7251109616876069 \t 3.631042399892129\n",
      "75     \t [0.80008984 0.66469512 0.7276977 ]. \t  2.1404400065437885 \t 3.631042399892129\n",
      "76     \t [0.83401196 0.42203083 0.26259397]. \t  0.2125427448756169 \t 3.631042399892129\n",
      "77     \t [0.10405245 0.38935579 0.28404873]. \t  0.4153620553364183 \t 3.631042399892129\n",
      "78     \t [0.19158302 0.59681087 0.89878592]. \t  3.597480156692398 \t 3.631042399892129\n",
      "79     \t [0.26700239 0.05492885 0.29627968]. \t  0.9101782778992026 \t 3.631042399892129\n",
      "80     \t [0.27094016 0.33545742 0.61111408]. \t  0.8566533054218431 \t 3.631042399892129\n",
      "81     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.631042399892129\n",
      "82     \t [0.41379935 0.4010954  0.50652043]. \t  0.4806340370735916 \t 3.631042399892129\n",
      "83     \t [0.06417411 0.59519414 0.71752662]. \t  2.730494212921118 \t 3.631042399892129\n",
      "84     \t [0.3349528  0.03801653 0.69238951]. \t  0.27286909381495894 \t 3.631042399892129\n",
      "85     \t [0.86768521 0.78698273 0.50741913]. \t  0.5257951628418611 \t 3.631042399892129\n",
      "86     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.631042399892129\n",
      "87     \t [0.81845974 0.30297021 0.8933031 ]. \t  1.964727173041564 \t 3.631042399892129\n",
      "88     \t [0.69823095 0.49754794 0.62276322]. \t  1.1805592927479978 \t 3.631042399892129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.11019052 0.78583376 0.0661553 ]. \t  0.004931920308375605 \t 3.631042399892129\n",
      "90     \t [0.52215181 0.38692792 0.85436559]. \t  2.9769880833994264 \t 3.631042399892129\n",
      "91     \t [0.82915108 0.43047211 0.72997315]. \t  2.3337490349988457 \t 3.631042399892129\n",
      "92     \t [0.49310147 0.63063374 0.28767256]. \t  0.19396851355912742 \t 3.631042399892129\n",
      "93     \t [0.04932664 0.6633557  0.68319437]. \t  2.5067863857295816 \t 3.631042399892129\n",
      "94     \t [0.57858158 0.81528447 0.46693551]. \t  1.2134303056389917 \t 3.631042399892129\n",
      "95     \t [0.78572918 0.17154958 0.0568002 ]. \t  0.1525474308893347 \t 3.631042399892129\n",
      "96     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.631042399892129\n",
      "97     \t [0.37050387 0.23864815 0.37059348]. \t  0.6477204306487983 \t 3.631042399892129\n",
      "98     \t [0.41873938 0.60890389 0.89831832]. \t  3.57023334698979 \t 3.631042399892129\n",
      "99     \t [0.21683791 0.49017501 0.76770751]. \t  3.185509555298485 \t 3.631042399892129\n",
      "100    \t [0.785572   0.05093827 0.50973963]. \t  0.1367271173738578 \t 3.631042399892129\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0.0095305  0.06847696 0.97789675]. \t  0.22422993094430546 \t 1.6237282255098657\n",
      "2      \t [0.00962764 0.96956788 0.65396657]. \t  \u001b[92m2.132425580713128\u001b[0m \t 2.132425580713128\n",
      "3      \t [0.97409469 0.03143273 0.64953327]. \t  0.1879512798009964 \t 2.132425580713128\n",
      "4      \t [0.22595933 0.95348785 0.12294433]. \t  0.010520248665944764 \t 2.132425580713128\n",
      "5      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 2.132425580713128\n",
      "6      \t [0.3714494  0.05992124 0.47930973]. \t  0.2780879042129226 \t 2.132425580713128\n",
      "7      \t [0.08978758 0.58671321 0.7206821 ]. \t  \u001b[92m2.7673013179295816\u001b[0m \t 2.7673013179295816\n",
      "8      \t [0.03685289 0.49116928 0.9040006 ]. \t  \u001b[92m3.4062663378427063\u001b[0m \t 3.4062663378427063\n",
      "9      \t [9.85660232e-01 7.88246377e-01 8.84621003e-04]. \t  0.00044769353083181577 \t 3.4062663378427063\n",
      "10     \t [0.9465703  0.61593804 0.8230332 ]. \t  \u001b[92m3.409651114729523\u001b[0m \t 3.409651114729523\n",
      "11     \t [0.         0.72985365 1.        ]. \t  1.590649948255382 \t 3.409651114729523\n",
      "12     \t [0.98382715 0.27993618 0.29751896]. \t  0.24196930797294067 \t 3.409651114729523\n",
      "13     \t [0.00580799 0.3712785  0.82623368]. \t  2.794577363798754 \t 3.409651114729523\n",
      "14     \t [0.32910241 0.02426588 0.07541194]. \t  0.3025913601388581 \t 3.409651114729523\n",
      "15     \t [0.48586458 0.50925965 0.78303676]. \t  3.362885414060049 \t 3.409651114729523\n",
      "16     \t [0.02599962 0.80037721 0.76595196]. \t  2.2200012557864595 \t 3.409651114729523\n",
      "17     \t [0.20125009 0.47131934 0.88759595]. \t  \u001b[92m3.4774994382775026\u001b[0m \t 3.4774994382775026\n",
      "18     \t [0.98211039 0.65919527 0.59261324]. \t  0.6482813411459536 \t 3.4774994382775026\n",
      "19     \t [0.98410255 0.31170284 0.82693474]. \t  2.184864208659012 \t 3.4774994382775026\n",
      "20     \t [0.4130355  0.00785469 0.12824955]. \t  0.4941035975615608 \t 3.4774994382775026\n",
      "21     \t [0.30533773 0.01277795 0.22343655]. \t  0.8366289500550946 \t 3.4774994382775026\n",
      "22     \t [0.78705306 0.66304608 0.9348571 ]. \t  2.8612161320195604 \t 3.4774994382775026\n",
      "23     \t [0.75381232 0.99392978 0.70725623]. \t  0.6068270173563677 \t 3.4774994382775026\n",
      "24     \t [0.02404341 0.0942877  0.62108141]. \t  0.25719593506293176 \t 3.4774994382775026\n",
      "25     \t [0.6520758  0.26029302 0.04619241]. \t  0.1477154540736463 \t 3.4774994382775026\n",
      "26     \t [0.43665953 0.03481133 0.11106203]. \t  0.4432502741230613 \t 3.4774994382775026\n",
      "27     \t [0.21798613 0.03979611 0.46539245]. \t  0.28887198045196705 \t 3.4774994382775026\n",
      "28     \t [0.98696718 0.46573214 0.96253201]. \t  2.4232975691625884 \t 3.4774994382775026\n",
      "29     \t [0.29933128 0.32524601 0.26997656]. \t  0.6507621925232626 \t 3.4774994382775026\n",
      "30     \t [0.04246869 0.2875836  0.97840552]. \t  1.1541184522251169 \t 3.4774994382775026\n",
      "31     \t [0.41417838 0.19311361 0.76261502]. \t  1.1087495702662022 \t 3.4774994382775026\n",
      "32     \t [0.87203038 0.42640354 0.3821151 ]. \t  0.16160955464686083 \t 3.4774994382775026\n",
      "33     \t [0.72418083 1.         0.        ]. \t  0.00011344635306565488 \t 3.4774994382775026\n",
      "34     \t [0.81241931 0.36064909 0.72616583]. \t  1.963127413588715 \t 3.4774994382775026\n",
      "35     \t [0.34502756 0.00681989 0.10873278]. \t  0.41586096294873914 \t 3.4774994382775026\n",
      "36     \t [0.43447726 0.04955604 0.96719786]. \t  0.20718758555534678 \t 3.4774994382775026\n",
      "37     \t [0.25432862 0.76856548 0.28473518]. \t  0.29737793386262007 \t 3.4774994382775026\n",
      "38     \t [0.04794507 0.26936512 0.54927872]. \t  0.3816210624341058 \t 3.4774994382775026\n",
      "39     \t [0.58522174 0.58869764 0.78062516]. \t  3.2728937092037884 \t 3.4774994382775026\n",
      "40     \t [0.93189822 0.24939339 0.75233946]. \t  1.4182079501843896 \t 3.4774994382775026\n",
      "41     \t [0.17802938 0.0294758  0.79506928]. \t  0.3313348891871022 \t 3.4774994382775026\n",
      "42     \t [0.97651534 0.4841796  0.21944172]. \t  0.08253771839622574 \t 3.4774994382775026\n",
      "43     \t [0.71300613 0.53677098 0.93514556]. \t  3.1533828886945177 \t 3.4774994382775026\n",
      "44     \t [0.62700145 0.8012262  0.97846886]. \t  1.4347536734805706 \t 3.4774994382775026\n",
      "45     \t [0.22934906 0.18684188 0.3254738 ]. \t  0.8182322861277761 \t 3.4774994382775026\n",
      "46     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.4774994382775026\n",
      "47     \t [0.0643714  0.08522135 0.27871535]. \t  0.7473716413693425 \t 3.4774994382775026\n",
      "48     \t [0.46313356 0.47927302 0.88073863]. \t  \u001b[92m3.5704663420411387\u001b[0m \t 3.5704663420411387\n",
      "49     \t [0.23583495 0.52125632 0.26348166]. \t  0.25030329997336537 \t 3.5704663420411387\n",
      "50     \t [0.77135093 0.65990973 0.91322584]. \t  3.1234636405375196 \t 3.5704663420411387\n",
      "51     \t [0.59853585 0.3089528  0.64636139]. \t  0.9878666569123074 \t 3.5704663420411387\n",
      "52     \t [0.65619053 0.28700976 0.08111451]. \t  0.20672553862945914 \t 3.5704663420411387\n",
      "53     \t [0.11224669 0.42759766 0.89438893]. \t  3.1159234782761778 \t 3.5704663420411387\n",
      "54     \t [0.56877483 0.15510009 0.37961578]. \t  0.6073058194865852 \t 3.5704663420411387\n",
      "55     \t [0.88504033 0.87772647 0.67648178]. \t  0.7400437002684781 \t 3.5704663420411387\n",
      "56     \t [0.88778347 0.032233   0.87823258]. \t  0.2887325615180171 \t 3.5704663420411387\n",
      "57     \t [0.63252512 0.39155287 0.29382626]. \t  0.3917616767011882 \t 3.5704663420411387\n",
      "58     \t [0.78302483 0.84043631 0.11391371]. \t  0.0038097353465152486 \t 3.5704663420411387\n",
      "59     \t [0.30983316 0.54263777 0.86647888]. \t  \u001b[92m3.83722875791526\u001b[0m \t 3.83722875791526\n",
      "60     \t [0.93884062 0.6907784  0.40388567]. \t  0.15680004897093414 \t 3.83722875791526\n",
      "61     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.83722875791526\n",
      "62     \t [0.39141075 0.6682249  0.32772495]. \t  0.37526470986621746 \t 3.83722875791526\n",
      "63     \t [0.93984625 0.87229769 0.12249176]. \t  0.0020613546464448135 \t 3.83722875791526\n",
      "64     \t [0.10799289 0.49298973 0.43224513]. \t  0.5767401050317421 \t 3.83722875791526\n",
      "65     \t [0.3998278  0.62664926 0.90508839]. \t  3.44288732608451 \t 3.83722875791526\n",
      "66     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.83722875791526\n",
      "67     \t [0.61241343 0.97717877 0.79372164]. \t  0.7635983690255415 \t 3.83722875791526\n",
      "68     \t [0.44986045 0.04096689 0.43530225]. \t  0.40659493777246697 \t 3.83722875791526\n",
      "69     \t [0.39279359 0.18140795 0.97664482]. \t  0.5999814463912994 \t 3.83722875791526\n",
      "70     \t [0.94246849 0.08998696 0.21367445]. \t  0.33946349176507806 \t 3.83722875791526\n",
      "71     \t [0.38151606 0.64937266 0.67849598]. \t  2.2736534531887607 \t 3.83722875791526\n",
      "72     \t [0.93543722 0.25526505 0.61043679]. \t  0.5372273868015285 \t 3.83722875791526\n",
      "73     \t [0.93182818 0.58102934 0.27465297]. \t  0.061145639167543266 \t 3.83722875791526\n",
      "74     \t [0.75148069 0.37839204 0.86510762]. \t  2.8356358916260005 \t 3.83722875791526\n",
      "75     \t [0.53842689 0.20209545 0.03517025]. \t  0.16946266418547523 \t 3.83722875791526\n",
      "76     \t [0.66666872 0.9879886  0.97856614]. \t  0.4295463085574229 \t 3.83722875791526\n",
      "77     \t [0.81938583 0.15484641 0.33275164]. \t  0.47376936237519857 \t 3.83722875791526\n",
      "78     \t [0.12984349 0.95836702 0.14229128]. \t  0.017391810552035626 \t 3.83722875791526\n",
      "79     \t [0.60561798 0.49346954 0.56466118]. \t  0.8014164770549999 \t 3.83722875791526\n",
      "80     \t [0.41979638 0.58009508 0.62413737]. \t  1.712321970347566 \t 3.83722875791526\n",
      "81     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.83722875791526\n",
      "82     \t [0.9833747  0.07992838 0.06819232]. \t  0.09673389388243601 \t 3.83722875791526\n",
      "83     \t [0.68086621 0.0946363  0.20945249]. \t  0.6721802110522616 \t 3.83722875791526\n",
      "84     \t [0.01984272 0.53509364 0.47590447]. \t  0.8991836318454244 \t 3.83722875791526\n",
      "85     \t [0.18697394 0.06951492 0.33744015]. \t  0.7658020018088214 \t 3.83722875791526\n",
      "86     \t [0.29234884 0.38878995 0.02698622]. \t  0.08307067463903102 \t 3.83722875791526\n",
      "87     \t [0.9501523  0.45799694 0.58188769]. \t  0.6322108433550255 \t 3.83722875791526\n",
      "88     \t [0.99377564 0.68733491 0.46147929]. \t  0.20195144177128935 \t 3.83722875791526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.12129095 0.65963091 0.57861225]. \t  2.2560765840740222 \t 3.83722875791526\n",
      "90     \t [0.61533137 0.16596426 0.11691732]. \t  0.41290596966549614 \t 3.83722875791526\n",
      "91     \t [0.1908268  0.52539051 0.95233098]. \t  2.8886375190571583 \t 3.83722875791526\n",
      "92     \t [0.71387159 0.68811182 0.42912745]. \t  0.4759020563078621 \t 3.83722875791526\n",
      "93     \t [0.25601229 0.57490552 0.53964237]. \t  1.4320519550088604 \t 3.83722875791526\n",
      "94     \t [0.66192095 0.37186563 0.85521659]. \t  2.8173963307276444 \t 3.83722875791526\n",
      "95     \t [0.3834012  0.00784596 0.41197929]. \t  0.47790389594330634 \t 3.83722875791526\n",
      "96     \t [0.28167736 0.09390389 0.82019296]. \t  0.5800442303293563 \t 3.83722875791526\n",
      "97     \t [0.53381469 0.51381329 0.15333844]. \t  0.1331021987449858 \t 3.83722875791526\n",
      "98     \t [0.43305606 0.25897579 0.04973795]. \t  0.19518771727017228 \t 3.83722875791526\n",
      "99     \t [0.06620756 0.07810312 0.39393038]. \t  0.4691539503144004 \t 3.83722875791526\n",
      "100    \t [0.51140729 0.95279908 0.47172793]. \t  1.417731105179221 \t 3.83722875791526\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [0.82808055 0.95668265 0.90976448]. \t  0.7541633809323007 \t 0.8830091449513892\n",
      "2      \t [0.09250474 0.65862611 0.98455533]. \t  \u001b[92m2.16574947683527\u001b[0m \t 2.16574947683527\n",
      "3      \t [0.69332868 0.16959446 0.9783922 ]. \t  0.5362860668682835 \t 2.16574947683527\n",
      "4      \t [0.68307375 0.03550334 0.03657775]. \t  0.1409270525778632 \t 2.16574947683527\n",
      "5      \t [0.11068717 0.14200085 0.01146429]. \t  0.11419914532673142 \t 2.16574947683527\n",
      "6      \t [0.03242071 0.97346284 0.89384909]. \t  0.7560821232330165 \t 2.16574947683527\n",
      "7      \t [0.03589703 0.52950655 0.58126809]. \t  1.4370584070410368 \t 2.16574947683527\n",
      "8      \t [0.10377104 0.79581918 0.09490675]. \t  0.008284831979292443 \t 2.16574947683527\n",
      "9      \t [0.48739873 0.58974076 0.98082375]. \t  \u001b[92m2.4227838480742547\u001b[0m \t 2.4227838480742547\n",
      "10     \t [0.97332082 0.99909138 0.02867902]. \t  9.292333815586823e-05 \t 2.4227838480742547\n",
      "11     \t [0.9906633  0.6042244  0.77715191]. \t  \u001b[92m2.9598095568459186\u001b[0m \t 2.9598095568459186\n",
      "12     \t [0.77613797 0.53878338 0.64743613]. \t  1.4084814358616347 \t 2.9598095568459186\n",
      "13     \t [1.         0.57213136 1.        ]. \t  2.0083577428636286 \t 2.9598095568459186\n",
      "14     \t [0.24616264 0.07820646 0.98944629]. \t  0.2247034423793938 \t 2.9598095568459186\n",
      "15     \t [0.1914126  0.64581687 0.86492381]. \t  \u001b[92m3.5701181855273822\u001b[0m \t 3.5701181855273822\n",
      "16     \t [0.99580667 0.27194136 0.01705326]. \t  0.036965452639109435 \t 3.5701181855273822\n",
      "17     \t [0.31650606 0.54822602 0.74681844]. \t  3.028280798543296 \t 3.5701181855273822\n",
      "18     \t [0.30792028 0.85027456 0.92878414]. \t  1.495799410590166 \t 3.5701181855273822\n",
      "19     \t [0.72669299 0.91658801 0.82717177]. \t  1.0773200898112971 \t 3.5701181855273822\n",
      "20     \t [0.48588757 0.09949652 0.33749825]. \t  0.8276222574425918 \t 3.5701181855273822\n",
      "21     \t [0.98788057 0.77798366 0.7741983 ]. \t  1.7717396794624731 \t 3.5701181855273822\n",
      "22     \t [0.18037212 0.90226405 0.62511662]. \t  2.713771239385097 \t 3.5701181855273822\n",
      "23     \t [0.90632425 0.12502009 0.68561426]. \t  0.4949941621548021 \t 3.5701181855273822\n",
      "24     \t [0.55003502 0.01057428 0.80649535]. \t  0.2782023000581064 \t 3.5701181855273822\n",
      "25     \t [0.381375   0.68999117 0.39132503]. \t  0.8027532689164403 \t 3.5701181855273822\n",
      "26     \t [0.00312984 0.89922714 0.71033415]. \t  1.9149173508753687 \t 3.5701181855273822\n",
      "27     \t [0.19484077 0.78744943 0.43747096]. \t  1.8239120365786972 \t 3.5701181855273822\n",
      "28     \t [0.59115682 0.52130064 0.74278386]. \t  2.8282431935809615 \t 3.5701181855273822\n",
      "29     \t [0.77437245 0.53905503 0.38781696]. \t  0.1917749111377073 \t 3.5701181855273822\n",
      "30     \t [0.94176069 0.20788213 0.78219447]. \t  1.2289722585901697 \t 3.5701181855273822\n",
      "31     \t [0.76372796 0.88253182 0.97112984]. \t  0.9612361076011148 \t 3.5701181855273822\n",
      "32     \t [0.97102822 0.5982502  0.91255014]. \t  3.3300000947761403 \t 3.5701181855273822\n",
      "33     \t [0.50935375 0.65674599 0.06578369]. \t  0.01602800707847871 \t 3.5701181855273822\n",
      "34     \t [0.25736721 0.06043309 0.54387752]. \t  0.16930147927337816 \t 3.5701181855273822\n",
      "35     \t [0.28007331 0.56914378 0.822542  ]. \t  \u001b[92m3.7672150031822547\u001b[0m \t 3.7672150031822547\n",
      "36     \t [1.         0.69326112 0.        ]. \t  0.0013019479939693722 \t 3.7672150031822547\n",
      "37     \t [0.43612477 0.74315716 0.99982543]. \t  1.5449453903180206 \t 3.7672150031822547\n",
      "38     \t [0.06549346 0.38284032 0.59027247]. \t  0.8699407615245407 \t 3.7672150031822547\n",
      "39     \t [0.74109156 0.91761747 0.86118205]. \t  1.0853144889793076 \t 3.7672150031822547\n",
      "40     \t [0.73796804 0.9196598  0.81750362]. \t  1.037619834944521 \t 3.7672150031822547\n",
      "41     \t [0.87099602 0.3752066  0.34801774]. \t  0.21483581963381862 \t 3.7672150031822547\n",
      "42     \t [0.85677037 0.91573528 0.12124502]. \t  0.0024014498313956016 \t 3.7672150031822547\n",
      "43     \t [0.09669857 0.27306876 0.92663206]. \t  1.4897573498295789 \t 3.7672150031822547\n",
      "44     \t [0.13321294 0.88617355 0.83118368]. \t  1.5221620399944809 \t 3.7672150031822547\n",
      "45     \t [0.74090445 0.65481877 0.37955992]. \t  0.2561334240916374 \t 3.7672150031822547\n",
      "46     \t [0.         1.         0.30996995]. \t  0.4090723705146404 \t 3.7672150031822547\n",
      "47     \t [0.55373763 0.4016182  0.50618592]. \t  0.41152024536352294 \t 3.7672150031822547\n",
      "48     \t [0.88979299 0.5876564  0.16428358]. \t  0.0373805242846916 \t 3.7672150031822547\n",
      "49     \t [0.83734842 0.94579949 0.63069426]. \t  0.6291403642545439 \t 3.7672150031822547\n",
      "50     \t [0.83356731 0.99580414 0.24430581]. \t  0.029936131053122515 \t 3.7672150031822547\n",
      "51     \t [0.26806168 0.92072454 0.57991583]. \t  2.7504536542836218 \t 3.7672150031822547\n",
      "52     \t [0.94182564 0.0228358  0.47456489]. \t  0.1103708972030579 \t 3.7672150031822547\n",
      "53     \t [0.12816539 0.9651931  0.92527938]. \t  0.7167927946681384 \t 3.7672150031822547\n",
      "54     \t [0.39932832 0.94334256 0.46489544]. \t  1.7494874580703168 \t 3.7672150031822547\n",
      "55     \t [0.48724739 0.10690739 0.74174769]. \t  0.5810135538097464 \t 3.7672150031822547\n",
      "56     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.7672150031822547\n",
      "57     \t [0.51834049 0.15609498 0.94885667]. \t  0.6066989659617829 \t 3.7672150031822547\n",
      "58     \t [0.080072   0.77547307 0.31861164]. \t  0.5209809020416881 \t 3.7672150031822547\n",
      "59     \t [0.54906896 0.90971067 0.16178112]. \t  0.01734433727723192 \t 3.7672150031822547\n",
      "60     \t [0.07648824 0.87617784 0.86502903]. \t  1.5338808447104273 \t 3.7672150031822547\n",
      "61     \t [0.65244562 0.21314933 0.18035824]. \t  0.5712228373905991 \t 3.7672150031822547\n",
      "62     \t [0.47071596 0.08439746 0.03367211]. \t  0.18652644999599177 \t 3.7672150031822547\n",
      "63     \t [0.08626338 0.32835822 0.30513615]. \t  0.50713427508067 \t 3.7672150031822547\n",
      "64     \t [0.07627262 0.60592078 0.41965131]. \t  0.9049507477790698 \t 3.7672150031822547\n",
      "65     \t [0.94888622 0.80625696 0.31376992]. \t  0.06400865401218608 \t 3.7672150031822547\n",
      "66     \t [0.49843812 0.60544155 0.73993733]. \t  2.788419670954484 \t 3.7672150031822547\n",
      "67     \t [0.77344706 0.99505961 0.61890148]. \t  0.6851708749626142 \t 3.7672150031822547\n",
      "68     \t [0.16201451 0.72884377 0.99655492]. \t  1.6594071824084362 \t 3.7672150031822547\n",
      "69     \t [0.6653514  0.13682213 0.54446613]. \t  0.20445606621572807 \t 3.7672150031822547\n",
      "70     \t [0.36867418 0.71232602 0.73035973]. \t  2.483995828835785 \t 3.7672150031822547\n",
      "71     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.7672150031822547\n",
      "72     \t [0.27952306 0.33902367 0.34938724]. \t  0.5363309575637881 \t 3.7672150031822547\n",
      "73     \t [0.75972655 0.23410555 0.83296686]. \t  1.5149225869359015 \t 3.7672150031822547\n",
      "74     \t [0.59815987 0.48809715 0.61795398]. \t  1.2066492035041683 \t 3.7672150031822547\n",
      "75     \t [0.67189194 0.85942577 0.68460476]. \t  1.2313802906482274 \t 3.7672150031822547\n",
      "76     \t [0.60008011 0.92496505 0.52429065]. \t  1.4087284930885242 \t 3.7672150031822547\n",
      "77     \t [0.2444321  0.50485488 0.50788661]. \t  0.8987089976929616 \t 3.7672150031822547\n",
      "78     \t [0.81769418 0.13001474 0.88255499]. \t  0.6765649630943437 \t 3.7672150031822547\n",
      "79     \t [0.11151363 0.36185785 0.05050592]. \t  0.1099957916682531 \t 3.7672150031822547\n",
      "80     \t [0.15873662 0.25637374 0.46219008]. \t  0.3342996983425587 \t 3.7672150031822547\n",
      "81     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.7672150031822547\n",
      "82     \t [0.98610069 0.4425689  0.12887743]. \t  0.062393181353264854 \t 3.7672150031822547\n",
      "83     \t [0.93180025 0.65465301 0.58391503]. \t  0.6572760948948935 \t 3.7672150031822547\n",
      "84     \t [0.58271614 0.30385449 0.08007295]. \t  0.21489779549585372 \t 3.7672150031822547\n",
      "85     \t [0.10362691 0.06223745 0.63789004]. \t  0.233616068741885 \t 3.7672150031822547\n",
      "86     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.7672150031822547\n",
      "87     \t [0.51712563 0.9804653  0.41579239]. \t  0.9115376651984629 \t 3.7672150031822547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.77901981 0.30643979 0.83347927]. \t  2.1826575662159646 \t 3.7672150031822547\n",
      "89     \t [0.22800456 0.11021078 0.54544231]. \t  0.2056988119882285 \t 3.7672150031822547\n",
      "90     \t [0.16234545 0.15090905 0.73449207]. \t  0.7693729649432159 \t 3.7672150031822547\n",
      "91     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.7672150031822547\n",
      "92     \t [0.54932523 0.36911071 0.26815621]. \t  0.4919621604955257 \t 3.7672150031822547\n",
      "93     \t [0.81733701 0.48742779 0.94661913]. \t  2.8131570579414014 \t 3.7672150031822547\n",
      "94     \t [0.21311319 0.7802126  0.09413853]. \t  0.009240500093104067 \t 3.7672150031822547\n",
      "95     \t [0.8763063  0.41730623 0.28379792]. \t  0.19368493148320307 \t 3.7672150031822547\n",
      "96     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.7672150031822547\n",
      "97     \t [0.67963353 0.3424413  0.72058914]. \t  1.830922880788777 \t 3.7672150031822547\n",
      "98     \t [0.13579278 0.3600425  0.32872856]. \t  0.4692130012754629 \t 3.7672150031822547\n",
      "99     \t [0.30148919 0.01337579 0.65699551]. \t  0.18123347020220654 \t 3.7672150031822547\n",
      "100    \t [0.55912399 0.9183065  0.48880292]. \t  1.4218084320990023 \t 3.7672150031822547\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
      "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
      "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
      "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
      "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
      "1      \t [0.58444074 0.99050166 0.47658358]. \t  1.1115612161599435 \t 3.595021899183128\n",
      "2      \t [0.         0.55738032 0.        ]. \t  0.011313047963942802 \t 3.595021899183128\n",
      "3      \t [0.75987333 0.47216222 1.        ]. \t  1.8828686907446737 \t 3.595021899183128\n",
      "4      \t [0.47253623 0.43756399 0.55627173]. \t  0.7302848080952353 \t 3.595021899183128\n",
      "5      \t [0.59153848 0.65473503 0.79345695]. \t  3.1225321370050105 \t 3.595021899183128\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.595021899183128\n",
      "7      \t [0.99980609 0.4161656  0.62696752]. \t  0.9757371065192627 \t 3.595021899183128\n",
      "8      \t [0.79508012 0.16582114 0.91622805]. \t  0.7773262025762627 \t 3.595021899183128\n",
      "9      \t [0.         0.56516844 0.63800499]. \t  1.9585390684307677 \t 3.595021899183128\n",
      "10     \t [0.98896185 0.76454532 0.58106913]. \t  0.49904318819013554 \t 3.595021899183128\n",
      "11     \t [0.36543417 0.95348505 0.09961531]. \t  0.0050196065781756526 \t 3.595021899183128\n",
      "12     \t [0.33046766 0.19829821 0.7391559 ]. \t  1.0594276320106952 \t 3.595021899183128\n",
      "13     \t [0.10000995 0.78361592 0.19528553]. \t  0.06553206290358828 \t 3.595021899183128\n",
      "14     \t [0.63796431 0.49553304 0.944787  ]. \t  2.9096373673656992 \t 3.595021899183128\n",
      "15     \t [0.06396978 0.46216013 0.09938513]. \t  0.09974740170445946 \t 3.595021899183128\n",
      "16     \t [0.28095921 0.95472615 0.6397489 ]. \t  2.218804440613135 \t 3.595021899183128\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
      "18     \t [0.28694241 0.39169052 0.85685861]. \t  3.021421380809735 \t 3.595021899183128\n",
      "19     \t [0.77352728 0.82355039 0.4683024 ]. \t  0.6454934775689937 \t 3.595021899183128\n",
      "20     \t [0.80824595 0.86678053 0.77477962]. \t  1.2344434935722344 \t 3.595021899183128\n",
      "21     \t [0.29766289 0.76762905 0.35821761]. \t  0.7708859721106268 \t 3.595021899183128\n",
      "22     \t [0.40563726 0.68505335 0.1754113 ]. \t  0.05228717190381487 \t 3.595021899183128\n",
      "23     \t [0.02067804 0.91099811 0.59912263]. \t  2.8424606783464554 \t 3.595021899183128\n",
      "24     \t [ 6.29688233e-01  4.15267304e-01 -8.67361738e-19]. \t  0.039291920191968 \t 3.595021899183128\n",
      "25     \t [0.06995427 0.01511866 0.72442506]. \t  0.25274913388809944 \t 3.595021899183128\n",
      "26     \t [0.67804479 0.77078918 0.56902073]. \t  1.2168931215562688 \t 3.595021899183128\n",
      "27     \t [0.86589987 0.92639659 0.87757439]. \t  0.9871183347890559 \t 3.595021899183128\n",
      "28     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.595021899183128\n",
      "29     \t [0.91335427 0.23583841 0.44450227]. \t  0.17714105470247507 \t 3.595021899183128\n",
      "30     \t [0.7506065  0.40634044 0.41486966]. \t  0.22632380203095379 \t 3.595021899183128\n",
      "31     \t [0.39977128 0.23813587 0.91268602]. \t  1.3095751931212423 \t 3.595021899183128\n",
      "32     \t [0.79450272 0.7126485  0.50934612]. \t  0.6327343940160386 \t 3.595021899183128\n",
      "33     \t [0.25772358 0.57968761 0.83700438]. \t  \u001b[92m3.814263786797786\u001b[0m \t 3.814263786797786\n",
      "34     \t [0.32687437 0.32426142 0.58229107]. \t  0.6370136299924598 \t 3.814263786797786\n",
      "35     \t [0.83192678 0.29758518 0.65755643]. \t  1.000957092930762 \t 3.814263786797786\n",
      "36     \t [0.2968788  0.59922979 0.79615695]. \t  3.5149009200745946 \t 3.814263786797786\n",
      "37     \t [0.00791438 0.7725744  0.76452674]. \t  2.388326089004248 \t 3.814263786797786\n",
      "38     \t [0.7831771  0.11393476 0.34031502]. \t  0.511090303764253 \t 3.814263786797786\n",
      "39     \t [0.58315801 0.99991139 0.65434991]. \t  1.0884058985143514 \t 3.814263786797786\n",
      "40     \t [0.53521012 0.82353753 0.11870219]. \t  0.008887745710349901 \t 3.814263786797786\n",
      "41     \t [0.43814079 0.78787176 0.17906828]. \t  0.037915227463000994 \t 3.814263786797786\n",
      "42     \t [0.3960321  0.29571919 0.3165956 ]. \t  0.690757086165492 \t 3.814263786797786\n",
      "43     \t [0.32673024 0.         0.        ]. \t  0.10170260750742209 \t 3.814263786797786\n",
      "44     \t [0.84324654 0.40795512 0.18468329]. \t  0.17907967355038906 \t 3.814263786797786\n",
      "45     \t [0.02685762 0.60280665 0.71467612]. \t  2.6889666815575426 \t 3.814263786797786\n",
      "46     \t [0.24561221 0.33233123 0.41387818]. \t  0.42211076022411576 \t 3.814263786797786\n",
      "47     \t [0.36447976 0.70156883 0.48132649]. \t  1.6321558603521293 \t 3.814263786797786\n",
      "48     \t [0.84842124 0.18218914 0.97560176]. \t  0.5961544894303186 \t 3.814263786797786\n",
      "49     \t [0.12984436 0.69852893 0.3573845 ]. \t  0.7123474682865216 \t 3.814263786797786\n",
      "50     \t [0.52534405 0.66792719 0.72398521]. \t  2.4018638450447756 \t 3.814263786797786\n",
      "51     \t [0.07786434 0.27164166 0.97193436]. \t  1.1132327915269886 \t 3.814263786797786\n",
      "52     \t [0.10962047 0.04483307 0.86191811]. \t  0.34796024304865303 \t 3.814263786797786\n",
      "53     \t [0.75000236 0.47560137 0.71576149]. \t  2.2966766459338115 \t 3.814263786797786\n",
      "54     \t [0.81693946 0.05488837 0.12012989]. \t  0.27511765755182194 \t 3.814263786797786\n",
      "55     \t [0.69690756 0.44841209 0.00256636]. \t  0.029511061790492753 \t 3.814263786797786\n",
      "56     \t [0.25528111 0.48415676 0.36037925]. \t  0.39866790747783953 \t 3.814263786797786\n",
      "57     \t [0.82551278 0.291437   0.69478736]. \t  1.2898866826883002 \t 3.814263786797786\n",
      "58     \t [0.81351655 0.42959636 0.4443309 ]. \t  0.19793697261499427 \t 3.814263786797786\n",
      "59     \t [0.35840926 0.15876879 0.42501352]. \t  0.48984090207098496 \t 3.814263786797786\n",
      "60     \t [0.42659616 0.26231807 0.48581641]. \t  0.3230057993212272 \t 3.814263786797786\n",
      "61     \t [0.27036096 0.91077795 0.38093927]. \t  1.1084812363376046 \t 3.814263786797786\n",
      "62     \t [0.70522617 0.92820702 0.95321304]. \t  0.7938744310416895 \t 3.814263786797786\n",
      "63     \t [0.91437015 0.15733016 0.32743429]. \t  0.36322474065860144 \t 3.814263786797786\n",
      "64     \t [0.04977767 0.70862733 0.4274287 ]. \t  1.4208589676070935 \t 3.814263786797786\n",
      "65     \t [0.5571242  0.20250162 0.69783511]. \t  0.8855766304236897 \t 3.814263786797786\n",
      "66     \t [0.87335527 0.39530656 0.35284998]. \t  0.19324353876702555 \t 3.814263786797786\n",
      "67     \t [0.50717256 0.63300668 0.98809787]. \t  2.2051804036179177 \t 3.814263786797786\n",
      "68     \t [0.74282132 0.09519347 0.86185807]. \t  0.5440572247124906 \t 3.814263786797786\n",
      "69     \t [0.88782106 0.036925   0.914746  ]. \t  0.2553094281926955 \t 3.814263786797786\n",
      "70     \t [0.98808529 0.39123085 0.14039753]. \t  0.09223007154309171 \t 3.814263786797786\n",
      "71     \t [0.33463186 0.69083589 0.55812411]. \t  2.1008406865618463 \t 3.814263786797786\n",
      "72     \t [0.94354885 0.93226586 0.79809   ]. \t  0.8239992082452622 \t 3.814263786797786\n",
      "73     \t [0.56098831 0.07435879 0.0274089 ]. \t  0.15640531297667126 \t 3.814263786797786\n",
      "74     \t [0.60219522 0.34545451 0.04807679]. \t  0.11923968083605012 \t 3.814263786797786\n",
      "75     \t [0.56539164 0.65339196 0.39425583]. \t  0.4990868816110166 \t 3.814263786797786\n",
      "76     \t [0.70344285 0.22611978 0.55108985]. \t  0.2906825411389715 \t 3.814263786797786\n",
      "77     \t [0.98769389 0.78995211 0.05810395]. \t  0.001089988756417544 \t 3.814263786797786\n",
      "78     \t [0.29520878 0.62503722 0.76480711]. \t  3.1389751822029917 \t 3.814263786797786\n",
      "79     \t [0.32313746 0.64663479 0.92416753]. \t  3.1444568439838494 \t 3.814263786797786\n",
      "80     \t [0.55755299 0.62021345 0.03969792]. \t  0.015402661237968218 \t 3.814263786797786\n",
      "81     \t [0.63083083 0.80771709 0.94964549]. \t  1.66189665408206 \t 3.814263786797786\n",
      "82     \t [0.67765879 0.80613949 0.87291363]. \t  2.0767113122745466 \t 3.814263786797786\n",
      "83     \t [0.24793462 0.03299496 0.24597322]. \t  0.8799301960103929 \t 3.814263786797786\n",
      "84     \t [0.26700692 0.69085293 0.69224926]. \t  2.4844544736083507 \t 3.814263786797786\n",
      "85     \t [0.45074871 0.7214667  0.66789062]. \t  2.0882138077978514 \t 3.814263786797786\n",
      "86     \t [0.61687711 0.09510488 0.91547047]. \t  0.44474067889320423 \t 3.814263786797786\n",
      "87     \t [0.23894909 0.69959286 0.52875898]. \t  2.2198741184287027 \t 3.814263786797786\n",
      "88     \t [0.0743263  0.81617286 0.52043236]. \t  2.8591383042700267 \t 3.814263786797786\n",
      "89     \t [0.70526481 0.82125252 0.89421046]. \t  1.8739824287549274 \t 3.814263786797786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.00271189 0.87698195 0.90354972]. \t  1.391326979456286 \t 3.814263786797786\n",
      "91     \t [0.45342712 0.29246917 0.08607081]. \t  0.26866794578786274 \t 3.814263786797786\n",
      "92     \t [0.27530935 0.29470349 0.96624981]. \t  1.3275575893186944 \t 3.814263786797786\n",
      "93     \t [0.53828561 0.01858217 0.9900779 ]. \t  0.12322679462156946 \t 3.814263786797786\n",
      "94     \t [0.83153634 0.0114916  0.86397406]. \t  0.24871690613153696 \t 3.814263786797786\n",
      "95     \t [0.25861071 0.05033044 0.506291  ]. \t  0.20544651675794345 \t 3.814263786797786\n",
      "96     \t [0.54742893 0.33386352 0.6356025 ]. \t  0.9920591266767318 \t 3.814263786797786\n",
      "97     \t [0.75344941 0.5909561  0.48116115]. \t  0.43885322229831075 \t 3.814263786797786\n",
      "98     \t [0.23922697 0.30041013 0.98878037]. \t  1.1458272347770426 \t 3.814263786797786\n",
      "99     \t [0.39628605 0.66448595 0.09270071]. \t  0.02246596304889642 \t 3.814263786797786\n",
      "100    \t [0.70606998 0.59208069 0.14527219]. \t  0.05066806780491258 \t 3.814263786797786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
      "2      \t [0.13064464 0.91108377 0.95539833]. \t  0.902351403909183 \t 1.1029187088185965\n",
      "3      \t [0.96893897 0.97974629 0.99865138]. \t  0.381043852508212 \t 1.1029187088185965\n",
      "4      \t [0.98139485 0.81725946 0.12404836]. \t  0.0024384188819132563 \t 1.1029187088185965\n",
      "5      \t [0.10355689 0.02892137 0.41095543]. \t  0.4089719355888731 \t 1.1029187088185965\n",
      "6      \t [0.02218954 0.56584252 0.99967923]. \t  \u001b[92m2.067141380028605\u001b[0m \t 2.067141380028605\n",
      "7      \t [0.05955692 0.03232917 0.89086014]. \t  0.27726435960872364 \t 2.067141380028605\n",
      "8      \t [0.98484883 0.56561537 0.99985617]. \t  2.015018280106272 \t 2.067141380028605\n",
      "9      \t [0.63184882 0.63023147 0.86946882]. \t  \u001b[92m3.5909750349985345\u001b[0m \t 3.5909750349985345\n",
      "10     \t [0.57930165 0.65764517 0.96407083]. \t  2.508814976030725 \t 3.5909750349985345\n",
      "11     \t [0.8078532  0.7622684  0.63227738]. \t  1.0200554009147913 \t 3.5909750349985345\n",
      "12     \t [0.         0.61493441 0.71370723]. \t  2.6640858280248736 \t 3.5909750349985345\n",
      "13     \t [0.68277949 0.58597741 0.        ]. \t  0.009720792904080396 \t 3.5909750349985345\n",
      "14     \t [0.66430626 0.44457057 0.93318238]. \t  2.8124401992306645 \t 3.5909750349985345\n",
      "15     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5909750349985345\n",
      "16     \t [0.34880716 0.62390797 0.74426788]. \t  2.9064433751584025 \t 3.5909750349985345\n",
      "17     \t [0.56383078 0.96512321 0.01989752]. \t  0.00038527602653866237 \t 3.5909750349985345\n",
      "18     \t [0.93467313 0.59513961 0.96751112]. \t  2.57238835070557 \t 3.5909750349985345\n",
      "19     \t [0.55401803 0.59081142 0.69702173]. \t  2.2293241627036107 \t 3.5909750349985345\n",
      "20     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5909750349985345\n",
      "21     \t [0.49838547 0.91412823 0.84602665]. \t  1.1959894323347209 \t 3.5909750349985345\n",
      "22     \t [0.93481963 0.15852344 0.01178922]. \t  0.05304559981970046 \t 3.5909750349985345\n",
      "23     \t [0.88131572 0.44114157 0.73043951]. \t  2.359362749579099 \t 3.5909750349985345\n",
      "24     \t [0.56774596 0.16752077 0.11092578]. \t  0.415755972019419 \t 3.5909750349985345\n",
      "25     \t [0.82727226 0.01716246 0.19996375]. \t  0.4206393454321978 \t 3.5909750349985345\n",
      "26     \t [0.71908065 0.38259867 0.3836828 ]. \t  0.27627310137628897 \t 3.5909750349985345\n",
      "27     \t [0.4990589  0.30974256 0.24859155]. \t  0.6536270178128792 \t 3.5909750349985345\n",
      "28     \t [0.85541574 0.11938326 0.13548969]. \t  0.2919036809750154 \t 3.5909750349985345\n",
      "29     \t [0.02059083 0.05214196 0.52110823]. \t  0.1461744279058926 \t 3.5909750349985345\n",
      "30     \t [0.54390625 0.96535455 0.95093515]. \t  0.6194232780140717 \t 3.5909750349985345\n",
      "31     \t [0.36931778 0.2013132  0.99354813]. \t  0.5996045984881392 \t 3.5909750349985345\n",
      "32     \t [0.37977161 0.12368017 0.6437221 ]. \t  0.3839468807083688 \t 3.5909750349985345\n",
      "33     \t [0.8047245  0.59171294 0.41136894]. \t  0.22314224425083254 \t 3.5909750349985345\n",
      "34     \t [0.44638342 0.89418366 0.11933375]. \t  0.008414858529026797 \t 3.5909750349985345\n",
      "35     \t [0.52924027 0.07235039 0.7635873 ]. \t  0.46807615193261143 \t 3.5909750349985345\n",
      "36     \t [0.7411776  0.1226393  0.57007957]. \t  0.20544785956289124 \t 3.5909750349985345\n",
      "37     \t [0.40426692 0.43087405 0.72122275]. \t  2.358668318450057 \t 3.5909750349985345\n",
      "38     \t [0.34323487 0.61523632 0.05239501]. \t  0.0215357272331486 \t 3.5909750349985345\n",
      "39     \t [0.35604083 0.00039971 0.11023479]. \t  0.41622529501959665 \t 3.5909750349985345\n",
      "40     \t [0.54347741 0.80014797 0.14840083]. \t  0.017026410789868152 \t 3.5909750349985345\n",
      "41     \t [0.92786235 0.39842196 0.1261783 ]. \t  0.09778381676669251 \t 3.5909750349985345\n",
      "42     \t [0.35526549 0.85732693 0.77594706]. \t  1.7393881605971755 \t 3.5909750349985345\n",
      "43     \t [0.13928129 0.037642   0.31364399]. \t  0.7523955330830968 \t 3.5909750349985345\n",
      "44     \t [0.43943336 0.35058606 0.65226717]. \t  1.2254093193788633 \t 3.5909750349985345\n",
      "45     \t [0.41287946 0.11110351 0.80531922]. \t  0.668777513157417 \t 3.5909750349985345\n",
      "46     \t [0.75394388 0.67847445 0.46045262]. \t  0.5040847138299055 \t 3.5909750349985345\n",
      "47     \t [0.41261872 0.95628583 0.82782702]. \t  0.9611467135683864 \t 3.5909750349985345\n",
      "48     \t [0.23896224 0.39284001 0.62793845]. \t  1.1987990305882834 \t 3.5909750349985345\n",
      "49     \t [0.90063347 0.85098353 0.19437089]. \t  0.010944354013264199 \t 3.5909750349985345\n",
      "50     \t [0.3981577  0.45222437 0.68939628]. \t  2.042203289076502 \t 3.5909750349985345\n",
      "51     \t [0.27320175 0.32743207 0.54484384]. \t  0.4868679427897529 \t 3.5909750349985345\n",
      "52     \t [0.70426147 0.94977949 0.91975555]. \t  0.7859185485075025 \t 3.5909750349985345\n",
      "53     \t [0.29938903 0.33951197 0.10493827]. \t  0.2727704257910028 \t 3.5909750349985345\n",
      "54     \t [0.04227152 0.51632077 0.06614304]. \t  0.04442441130910269 \t 3.5909750349985345\n",
      "55     \t [0.99248764 0.16035644 0.36489517]. \t  0.233568848059692 \t 3.5909750349985345\n",
      "56     \t [0.7384411  0.96115012 0.40008681]. \t  0.4147517720316982 \t 3.5909750349985345\n",
      "57     \t [0.21183532 0.81474443 0.39789438]. \t  1.352137891327086 \t 3.5909750349985345\n",
      "58     \t [0.22519599 0.91835787 0.61079726]. \t  2.704469251438837 \t 3.5909750349985345\n",
      "59     \t [0.86614404 0.60244622 0.09429213]. \t  0.018831020154467 \t 3.5909750349985345\n",
      "60     \t [0.         1.         0.52421648]. \t  2.4116683076339074 \t 3.5909750349985345\n",
      "61     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.5909750349985345\n",
      "62     \t [0.91427037 0.73030932 0.37220707]. \t  0.13958483090937676 \t 3.5909750349985345\n",
      "63     \t [0.3715256  0.97661857 0.83547791]. \t  0.842797440952053 \t 3.5909750349985345\n",
      "64     \t [0.94554547 0.95144071 0.33096856]. \t  0.07758239833381178 \t 3.5909750349985345\n",
      "65     \t [0.62945952 0.81213003 0.07417604]. \t  0.0033818119848788857 \t 3.5909750349985345\n",
      "66     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.5909750349985345\n",
      "67     \t [0.14245061 0.4693838  0.2147183 ]. \t  0.2462921799924161 \t 3.5909750349985345\n",
      "68     \t [0.34945726 0.51161119 0.28253862]. \t  0.28359048954848454 \t 3.5909750349985345\n",
      "69     \t [0.65959258 0.15693117 0.49238012]. \t  0.23201619425484993 \t 3.5909750349985345\n",
      "70     \t [0.07867937 0.90892995 0.60080682]. \t  2.896572730126719 \t 3.5909750349985345\n",
      "71     \t [0.39040893 0.32429144 0.01740015]. \t  0.0998205204225004 \t 3.5909750349985345\n",
      "72     \t [0.61052577 0.39268847 0.46810742]. \t  0.31167649069899306 \t 3.5909750349985345\n",
      "73     \t [0.36260274 0.54841149 0.04248908]. \t  0.03446239000851175 \t 3.5909750349985345\n",
      "74     \t [0.00589812 0.27171843 0.98890298]. \t  0.9666106673936532 \t 3.5909750349985345\n",
      "75     \t [0.89290771 0.30965735 0.87323794]. \t  2.1147184202197984 \t 3.5909750349985345\n",
      "76     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.5909750349985345\n",
      "77     \t [0.1100138  0.86478416 0.52766993]. \t  2.9856105882883006 \t 3.5909750349985345\n",
      "78     \t [0.84135491 0.32663725 0.05460281]. \t  0.08491332223000136 \t 3.5909750349985345\n",
      "79     \t [0.46396811 0.57353837 0.95369955]. \t  2.9132276860785464 \t 3.5909750349985345\n",
      "80     \t [0.75053175 0.07049347 0.35769157]. \t  0.496737531071639 \t 3.5909750349985345\n",
      "81     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.5909750349985345\n",
      "82     \t [0.91763429 0.51515885 0.54189505]. \t  0.43106468558061406 \t 3.5909750349985345\n",
      "83     \t [0.67524817 0.56782453 0.72559978]. \t  2.5187639523066165 \t 3.5909750349985345\n",
      "84     \t [0.1837141  0.8699628  0.15373861]. \t  0.02583793048070997 \t 3.5909750349985345\n",
      "85     \t [0.96033402 0.15484275 0.55027791]. \t  0.17937950478392242 \t 3.5909750349985345\n",
      "86     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.5909750349985345\n",
      "87     \t [0.96856976 0.08974637 0.01914393]. \t  0.053200915564779426 \t 3.5909750349985345\n",
      "88     \t [0.95030821 0.19386301 0.97179269]. \t  0.6620893120506729 \t 3.5909750349985345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.62693302 0.84101297 0.42004557]. \t  0.7795783747167534 \t 3.5909750349985345\n",
      "90     \t [0.71755206 0.26757393 0.39225026]. \t  0.3691089467773529 \t 3.5909750349985345\n",
      "91     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.5909750349985345\n",
      "92     \t [0.9270585  0.78468855 0.49436225]. \t  0.3833080971914929 \t 3.5909750349985345\n",
      "93     \t [0.06804019 0.76189743 0.21971389]. \t  0.10216064197127789 \t 3.5909750349985345\n",
      "94     \t [0.0962197  0.81525186 0.35072737]. \t  0.838479048398054 \t 3.5909750349985345\n",
      "95     \t [0.49508671 0.39037781 0.57475127]. \t  0.7079714096790843 \t 3.5909750349985345\n",
      "96     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.5909750349985345\n",
      "97     \t [0.71358188 0.73749073 0.55743709]. \t  1.0333471264077945 \t 3.5909750349985345\n",
      "98     \t [0.42325437 0.52968222 0.00214746]. \t  0.021975714792266725 \t 3.5909750349985345\n",
      "99     \t [0.55324297 0.28908356 0.80797698]. \t  2.0124792267115676 \t 3.5909750349985345\n",
      "100    \t [0.54656951 0.65652098 0.75129329]. \t  2.7054350876838593 \t 3.5909750349985345\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0.54383213 0.99819891 0.96605385]. \t  0.4340597074596305 \t 0.687459437576373\n",
      "2      \t [0.95983675 0.98162579 0.03051357]. \t  0.00011698065361110391 \t 0.687459437576373\n",
      "3      \t [0.99072322 0.00568382 0.85196183]. \t  0.23940397454961343 \t 0.687459437576373\n",
      "4      \t [0.96635878 0.05065822 0.14510777]. \t  0.2095493914518875 \t 0.687459437576373\n",
      "5      \t [0.11626065 0.11270374 0.98686942]. \t  0.3129882692341237 \t 0.687459437576373\n",
      "6      \t [0.0325319  0.0681414  0.34792992]. \t  0.574554419912479 \t 0.687459437576373\n",
      "7      \t [0.09903838 0.94911004 0.96556234]. \t  0.6473387535315734 \t 0.687459437576373\n",
      "8      \t [0.52665542 0.99837455 0.28378765]. \t  0.16856345575833884 \t 0.687459437576373\n",
      "9      \t [0.82239546 0.2976596  0.97693775]. \t  \u001b[92m1.223356686192842\u001b[0m \t 1.223356686192842\n",
      "10     \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 1.223356686192842\n",
      "11     \t [0.00147484 0.93964119 0.00205779]. \t  0.0003838484044950603 \t 1.223356686192842\n",
      "12     \t [0.53661097 0.44177042 0.00933445]. \t  0.0435128262406222 \t 1.223356686192842\n",
      "13     \t [0.58824586 0.92223861 0.52464089]. \t  \u001b[92m1.4628210506084425\u001b[0m \t 1.4628210506084425\n",
      "14     \t [0.07441325 0.30230799 0.04195948]. \t  0.11924842505494702 \t 1.4628210506084425\n",
      "15     \t [0.74469128 0.72600472 0.94859131]. \t  \u001b[92m2.2887077007421595\u001b[0m \t 2.2887077007421595\n",
      "16     \t [0.02080374 0.92826588 0.45911101]. \t  2.169348725091859 \t 2.2887077007421595\n",
      "17     \t [0.92520218 0.35168958 0.64241465]. \t  1.0181546173599993 \t 2.2887077007421595\n",
      "18     \t [0.83823964 0.78751959 0.86158897]. \t  2.2128807060592215 \t 2.2887077007421595\n",
      "19     \t [0.88340505 0.19776439 0.93043219]. \t  0.9015758371143319 \t 2.2887077007421595\n",
      "20     \t [0.73695676 0.88091896 0.60843112]. \t  1.0173206108842925 \t 2.2887077007421595\n",
      "21     \t [0.57403855 0.83920549 0.88971474]. \t  1.7493958325783987 \t 2.2887077007421595\n",
      "22     \t [1.         0.58989505 1.        ]. \t  1.9987300911710584 \t 2.2887077007421595\n",
      "23     \t [0.13354403 0.337556   0.94263112]. \t  1.8844777407532414 \t 2.2887077007421595\n",
      "24     \t [0.08209724 0.85599915 0.30246339]. \t  0.445951009497801 \t 2.2887077007421595\n",
      "25     \t [0.61480638 0.32456524 0.1872003 ]. \t  0.44842461410223 \t 2.2887077007421595\n",
      "26     \t [0.48455829 0.36987274 0.17834187]. \t  0.401964192772876 \t 2.2887077007421595\n",
      "27     \t [0.58695784 0.81532927 0.17236437]. \t  0.023264371249665364 \t 2.2887077007421595\n",
      "28     \t [0.46048979 0.81421302 0.16711461]. \t  0.027664804050156293 \t 2.2887077007421595\n",
      "29     \t [0.23701218 0.59591573 0.89894031]. \t  \u001b[92m3.6029773715032065\u001b[0m \t 3.6029773715032065\n",
      "30     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.6029773715032065\n",
      "31     \t [0.         0.6853504  0.81053212]. \t  3.1623900291466653 \t 3.6029773715032065\n",
      "32     \t [0.43877667 0.3797842  0.02039755]. \t  0.07937499454939982 \t 3.6029773715032065\n",
      "33     \t [0.61121167 0.00107461 0.95751915]. \t  0.13534741531845035 \t 3.6029773715032065\n",
      "34     \t [0.00813916 0.40917623 0.83503383]. \t  3.138115703694798 \t 3.6029773715032065\n",
      "35     \t [0.49982731 0.90579256 0.2331232 ]. \t  0.08621967538113952 \t 3.6029773715032065\n",
      "36     \t [0.15800007 1.         0.66457775]. \t  1.9038778526831794 \t 3.6029773715032065\n",
      "37     \t [0.6752069  0.81796695 0.45441109]. \t  0.8417381747877882 \t 3.6029773715032065\n",
      "38     \t [0.86240987 0.78842564 0.51778972]. \t  0.5631960416924139 \t 3.6029773715032065\n",
      "39     \t [0.92713127 0.86718465 0.91675105]. \t  1.3413636838856968 \t 3.6029773715032065\n",
      "40     \t [0.35357211 0.51185782 0.02878209]. \t  0.03830683776229002 \t 3.6029773715032065\n",
      "41     \t [0.05762137 0.95472163 0.49114906]. \t  2.479043323684774 \t 3.6029773715032065\n",
      "42     \t [0.0088501  0.85339864 0.51987391]. \t  2.845347029086422 \t 3.6029773715032065\n",
      "43     \t [0.92527545 0.18887922 0.95619654]. \t  0.7185987031641645 \t 3.6029773715032065\n",
      "44     \t [0.52578548 0.1949655  0.37561227]. \t  0.6269693854008805 \t 3.6029773715032065\n",
      "45     \t [0.54450649 0.49039642 0.84509756]. \t  \u001b[92m3.6952070502329595\u001b[0m \t 3.6952070502329595\n",
      "46     \t [0.99263914 0.15298368 0.10210187]. \t  0.13549979390863812 \t 3.6952070502329595\n",
      "47     \t [0.00204512 0.53637534 0.90481753]. \t  3.5269212761465907 \t 3.6952070502329595\n",
      "48     \t [0.55971228 0.39288025 0.98728663]. \t  1.7225165152213575 \t 3.6952070502329595\n",
      "49     \t [0.49657515 0.58416467 0.779257  ]. \t  3.3128019824062505 \t 3.6952070502329595\n",
      "50     \t [0.47915436 0.73689439 0.09459543]. \t  0.01133009927251797 \t 3.6952070502329595\n",
      "51     \t [0.21412015 0.87387982 0.69532366]. \t  2.1471688886727778 \t 3.6952070502329595\n",
      "52     \t [0.12271917 0.37553842 0.4272115 ]. \t  0.3864043016317239 \t 3.6952070502329595\n",
      "53     \t [0.82080203 0.29618673 0.7727641 ]. \t  1.8990264295842576 \t 3.6952070502329595\n",
      "54     \t [0.81894502 0.14217946 0.85586045]. \t  0.7982175892835341 \t 3.6952070502329595\n",
      "55     \t [0.64443437 0.22217795 0.8371342 ]. \t  1.4200274340407004 \t 3.6952070502329595\n",
      "56     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.6952070502329595\n",
      "57     \t [0.11362845 0.73784761 0.58499413]. \t  2.7329829476515304 \t 3.6952070502329595\n",
      "58     \t [0.30562469 0.08565643 0.2132352 ]. \t  0.8964390815744543 \t 3.6952070502329595\n",
      "59     \t [0.1711343  0.78616482 0.67357901]. \t  2.5332181225533814 \t 3.6952070502329595\n",
      "60     \t [0.78645719 0.57463958 0.04022363]. \t  0.01565100397479039 \t 3.6952070502329595\n",
      "61     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.6952070502329595\n",
      "62     \t [0.97409418 0.46679145 0.23632156]. \t  0.09830335271047452 \t 3.6952070502329595\n",
      "63     \t [0.51847515 0.10331076 0.94773397]. \t  0.3949299809870835 \t 3.6952070502329595\n",
      "64     \t [0.51606021 0.93594717 0.69369794]. \t  1.3216254492453405 \t 3.6952070502329595\n",
      "65     \t [0.25034073 0.283311   0.84914915]. \t  1.9741592008394155 \t 3.6952070502329595\n",
      "66     \t [0.62661822 0.79305607 0.71171919]. \t  1.6353891844691524 \t 3.6952070502329595\n",
      "67     \t [0.99209761 0.52724509 0.59182   ]. \t  0.706813714232192 \t 3.6952070502329595\n",
      "68     \t [0.39815523 0.60099155 0.45032258]. \t  0.8846101320937436 \t 3.6952070502329595\n",
      "69     \t [0.42039382 0.75207267 0.10391814]. \t  0.012253459456886143 \t 3.6952070502329595\n",
      "70     \t [0.48486484 0.39706945 0.52374185]. \t  0.4903912089210618 \t 3.6952070502329595\n",
      "71     \t [0.65255185 0.37046885 0.60672359]. \t  0.8229914938001928 \t 3.6952070502329595\n",
      "72     \t [0.53269428 0.5021177  0.41613   ]. \t  0.38257475846236266 \t 3.6952070502329595\n",
      "73     \t [0.23292542 0.65429981 0.5694923 ]. \t  2.1114534977548267 \t 3.6952070502329595\n",
      "74     \t [0.06094383 0.75006948 0.91544775]. \t  2.4577926427867203 \t 3.6952070502329595\n",
      "75     \t [0.11839277 0.40045652 0.80436571]. \t  2.9845651362088654 \t 3.6952070502329595\n",
      "76     \t [0.47931034 0.84014484 0.67227953]. \t  1.8305430269273972 \t 3.6952070502329595\n",
      "77     \t [0.98152923 0.00976713 0.28631693]. \t  0.28612596862520334 \t 3.6952070502329595\n",
      "78     \t [0.31402171 0.04764952 0.37383835]. \t  0.6749958469159306 \t 3.6952070502329595\n",
      "79     \t [0.75106957 0.15814369 0.38654257]. \t  0.42218406246182705 \t 3.6952070502329595\n",
      "80     \t [0.27461031 0.94774274 0.29159062]. \t  0.328639689249174 \t 3.6952070502329595\n",
      "81     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.6952070502329595\n",
      "82     \t [0.70673888 0.8113937  0.57526345]. \t  1.1486811700958668 \t 3.6952070502329595\n",
      "83     \t [0.76981789 0.64524744 0.58542349]. \t  0.9169360326409772 \t 3.6952070502329595\n",
      "84     \t [0.84248597 0.87114236 0.23971618]. \t  0.03217010731508287 \t 3.6952070502329595\n",
      "85     \t [0.60098498 0.42754807 0.83812588]. \t  3.3069210488382543 \t 3.6952070502329595\n",
      "86     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.6952070502329595\n",
      "87     \t [0.98928277 0.83515542 0.82849782]. \t  1.6312442936979132 \t 3.6952070502329595\n",
      "88     \t [0.49240399 0.4472185  0.9468286 ]. \t  2.655421694864316 \t 3.6952070502329595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.23368604 0.82427426 0.64991027]. \t  2.5802624328376487 \t 3.6952070502329595\n",
      "90     \t [0.16053329 0.96913501 0.73523832]. \t  1.4046142063188611 \t 3.6952070502329595\n",
      "91     \t [0.29146938 0.27435274 0.85713041]. \t  1.8728266859564773 \t 3.6952070502329595\n",
      "92     \t [0.36158656 0.87476252 0.15758473]. \t  0.024083704335378493 \t 3.6952070502329595\n",
      "93     \t [0.15810814 0.76485029 0.44684918]. \t  1.8937374352144585 \t 3.6952070502329595\n",
      "94     \t [0.42975675 0.29917632 0.6583835 ]. \t  1.0741313974916602 \t 3.6952070502329595\n",
      "95     \t [0.47695731 0.57196415 0.97239096]. \t  2.587977947486678 \t 3.6952070502329595\n",
      "96     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.6952070502329595\n",
      "97     \t [0.70341769 0.04770042 0.57490573]. \t  0.13992091944485147 \t 3.6952070502329595\n",
      "98     \t [0.01379356 0.86797625 0.38832868]. \t  1.2760355953003144 \t 3.6952070502329595\n",
      "99     \t [0.85408766 0.561019   0.45399772]. \t  0.23760051016737732 \t 3.6952070502329595\n",
      "100    \t [0.21927488 0.77810318 0.82141228]. \t  2.469706919173057 \t 3.6952070502329595\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0.00218943 0.04196524 0.14178738]. \t  0.3936517086962368 \t 1.6482992955272024\n",
      "2      \t [0.77928559 0.03180806 0.96261853]. \t  0.1774348183168348 \t 1.6482992955272024\n",
      "3      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 1.6482992955272024\n",
      "4      \t [0.81035323 0.96410121 0.95771406]. \t  0.5855693863872368 \t 1.6482992955272024\n",
      "5      \t [0.02515426 0.98910186 0.07077422]. \t  0.002392320430631601 \t 1.6482992955272024\n",
      "6      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 1.6482992955272024\n",
      "7      \t [0.57560745 0.079372   0.04232202]. \t  0.18998890510184638 \t 1.6482992955272024\n",
      "8      \t [0.14372017 0.45284623 0.00447107]. \t  0.03505772621982825 \t 1.6482992955272024\n",
      "9      \t [0.71425984 0.42838582 0.97182332]. \t  \u001b[92m2.142089118725871\u001b[0m \t 2.142089118725871\n",
      "10     \t [0.54971999 0.123454   0.34646439]. \t  0.7541630909040608 \t 2.142089118725871\n",
      "11     \t [0.28834187 0.66519844 0.82381613]. \t  \u001b[92m3.38993374702056\u001b[0m \t 3.38993374702056\n",
      "12     \t [0.29815323 0.44576715 0.84251532]. \t  \u001b[92m3.4662727508063926\u001b[0m \t 3.4662727508063926\n",
      "13     \t [0.02263787 0.72332989 0.77237958]. \t  2.7282178756611355 \t 3.4662727508063926\n",
      "14     \t [0.99156486 0.9086604  0.67066223]. \t  0.5046079730032078 \t 3.4662727508063926\n",
      "15     \t [0.49169148 0.43063243 0.71336222]. \t  2.2335820971778686 \t 3.4662727508063926\n",
      "16     \t [0.98395189 0.24594206 0.91930385]. \t  1.287223865695118 \t 3.4662727508063926\n",
      "17     \t [0.         0.54975567 1.        ]. \t  2.05146612034035 \t 3.4662727508063926\n",
      "18     \t [0.48930669 0.81426489 0.0195969 ]. \t  0.0015248017944159383 \t 3.4662727508063926\n",
      "19     \t [0.92356452 0.06609808 0.14107916]. \t  0.24007567894345905 \t 3.4662727508063926\n",
      "20     \t [0.01212006 0.74721957 0.66784193]. \t  2.5238887834047836 \t 3.4662727508063926\n",
      "21     \t [0.14912285 0.91183835 0.63297933]. \t  2.642458610255681 \t 3.4662727508063926\n",
      "22     \t [0.3824515  0.21765801 0.46043515]. \t  0.36322495039594555 \t 3.4662727508063926\n",
      "23     \t [0.85718337 0.57784222 0.63080965]. \t  1.1438848837877669 \t 3.4662727508063926\n",
      "24     \t [1.         0.73409037 1.        ]. \t  1.5276219985332578 \t 3.4662727508063926\n",
      "25     \t [0.37636552 0.0107913  0.25592252]. \t  0.889855102438802 \t 3.4662727508063926\n",
      "26     \t [0.77722177 0.51750901 0.95112688]. \t  2.8522184098251007 \t 3.4662727508063926\n",
      "27     \t [0.84430204 0.0931376  0.5407072 ]. \t  0.1409227125821996 \t 3.4662727508063926\n",
      "28     \t [0.36505371 0.86257435 0.39257391]. \t  1.124125831322702 \t 3.4662727508063926\n",
      "29     \t [0.07611994 0.17403627 0.16964161]. \t  0.5625197850100055 \t 3.4662727508063926\n",
      "30     \t [0.15040217 0.27487737 0.68886371]. \t  1.2068112176054355 \t 3.4662727508063926\n",
      "31     \t [0.18487098 0.79798731 0.69640597]. \t  2.3978639708547576 \t 3.4662727508063926\n",
      "32     \t [0.47005646 0.43008523 0.76644683]. \t  2.8747164606798186 \t 3.4662727508063926\n",
      "33     \t [0.4830335  0.3589971  0.01957983]. \t  0.08497849431200012 \t 3.4662727508063926\n",
      "34     \t [0.39065506 0.36531876 0.33968319]. \t  0.5088841364966702 \t 3.4662727508063926\n",
      "35     \t [0.16606988 0.85341586 0.08747172]. \t  0.005714791267828612 \t 3.4662727508063926\n",
      "36     \t [0.97116965 0.66728227 0.49654319]. \t  0.28674280245595724 \t 3.4662727508063926\n",
      "37     \t [0.49621793 0.89029341 0.85221119]. \t  1.3795236492204368 \t 3.4662727508063926\n",
      "38     \t [0.06580719 0.65240766 0.56850218]. \t  2.163646036187145 \t 3.4662727508063926\n",
      "39     \t [0.37502837 0.5797235  0.55210511]. \t  1.365261717231631 \t 3.4662727508063926\n",
      "40     \t [0.36536699 0.60093531 0.35029954]. \t  0.41726358763676213 \t 3.4662727508063926\n",
      "41     \t [0.99071082 0.51863531 0.06184161]. \t  0.017663949149639254 \t 3.4662727508063926\n",
      "42     \t [0.57705539 0.11731267 0.24619877]. \t  0.8668166475679191 \t 3.4662727508063926\n",
      "43     \t [0.09674508 0.79030655 0.88036037]. \t  2.2909610613092406 \t 3.4662727508063926\n",
      "44     \t [0.         1.         0.54103637]. \t  2.4711157335918816 \t 3.4662727508063926\n",
      "45     \t [0.82893282 0.38042007 0.29612236]. \t  0.26674348201046777 \t 3.4662727508063926\n",
      "46     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.4662727508063926\n",
      "47     \t [0.14903979 0.14004251 0.06309594]. \t  0.24628545011331074 \t 3.4662727508063926\n",
      "48     \t [0.33649642 0.79002893 0.11813537]. \t  0.01339568728156521 \t 3.4662727508063926\n",
      "49     \t [0.19911323 0.00237939 0.87097552]. \t  0.22425533538853948 \t 3.4662727508063926\n",
      "50     \t [0.63719903 0.11168066 0.68479935]. \t  0.4601969693558089 \t 3.4662727508063926\n",
      "51     \t [0.38792124 0.61712546 0.28586968]. \t  0.2226509244019392 \t 3.4662727508063926\n",
      "52     \t [0.25904267 0.93828389 0.5850829 ]. \t  2.6921736979674034 \t 3.4662727508063926\n",
      "53     \t [0.4504184  0.03605091 0.84411583]. \t  0.3381776110807855 \t 3.4662727508063926\n",
      "54     \t [0.65381146 0.55497032 0.11298225]. \t  0.05763780126846553 \t 3.4662727508063926\n",
      "55     \t [0.25048847 0.35816706 0.75953867]. \t  2.3465209235283924 \t 3.4662727508063926\n",
      "56     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.4662727508063926\n",
      "57     \t [0.80933285 0.26153161 0.72892783]. \t  1.3799198877907282 \t 3.4662727508063926\n",
      "58     \t [0.06935917 0.04562295 0.11885838]. \t  0.37488370638369756 \t 3.4662727508063926\n",
      "59     \t [0.19125182 0.06925342 0.12400518]. \t  0.4802546651737729 \t 3.4662727508063926\n",
      "60     \t [0.39667605 0.31587947 0.77118847]. \t  2.09764552636701 \t 3.4662727508063926\n",
      "61     \t [0.75257917 0.76666177 0.77475694]. \t  2.0249713671027507 \t 3.4662727508063926\n",
      "62     \t [0.15119604 0.56926613 0.79512519]. \t  \u001b[92m3.5645605148548265\u001b[0m \t 3.5645605148548265\n",
      "63     \t [0.95058887 0.60792326 0.4672321 ]. \t  0.21497354455016082 \t 3.5645605148548265\n",
      "64     \t [0.77521251 0.31322511 0.07287485]. \t  0.13344020497519432 \t 3.5645605148548265\n",
      "65     \t [0.95386747 0.78640043 0.70238134]. \t  1.1284191028386925 \t 3.5645605148548265\n",
      "66     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.5645605148548265\n",
      "67     \t [0.31648226 0.21977504 0.63165128]. \t  0.6129235792428898 \t 3.5645605148548265\n",
      "68     \t [0.21123292 0.96019313 0.16994722]. \t  0.032336079936821825 \t 3.5645605148548265\n",
      "69     \t [0.7204923  0.85641747 0.12714917]. \t  0.005664526266435488 \t 3.5645605148548265\n",
      "70     \t [0.16132399 0.18463979 0.39212205]. \t  0.5455106698921409 \t 3.5645605148548265\n",
      "71     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.5645605148548265\n",
      "72     \t [0.97000739 0.8651114  0.1323205 ]. \t  0.0022642447975693637 \t 3.5645605148548265\n",
      "73     \t [0.88402471 0.55017171 0.74039155]. \t  2.627584398234073 \t 3.5645605148548265\n",
      "74     \t [0.20725764 0.44661577 0.43293425]. \t  0.47973243351049316 \t 3.5645605148548265\n",
      "75     \t [0.70277869 0.98631596 0.48471729]. \t  0.7993628237820469 \t 3.5645605148548265\n",
      "76     \t [0.38963959 0.50754173 0.62896835]. \t  1.5548694758764974 \t 3.5645605148548265\n",
      "77     \t [0.81937767 0.82390328 0.37279227]. \t  0.24367774267758802 \t 3.5645605148548265\n",
      "78     \t [0.96814049 0.77015168 0.31479389]. \t  0.05749856420429448 \t 3.5645605148548265\n",
      "79     \t [0.88648415 0.64118345 0.12676937]. \t  0.017038199366140965 \t 3.5645605148548265\n",
      "80     \t [0.32415789 0.63931755 0.71370113]. \t  2.6053987677052417 \t 3.5645605148548265\n",
      "81     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.5645605148548265\n",
      "82     \t [0.98790758 0.95210545 0.779161  ]. \t  0.6501774865339243 \t 3.5645605148548265\n",
      "83     \t [0.33721187 0.97906121 0.32228894]. \t  0.45450899396014893 \t 3.5645605148548265\n",
      "84     \t [0.07194143 0.57218387 0.00097635]. \t  0.011634653775367711 \t 3.5645605148548265\n",
      "85     \t [0.74376934 0.45618499 0.90031264]. \t  3.243537437346223 \t 3.5645605148548265\n",
      "86     \t [0.60832238 0.55700038 0.81874419]. \t  \u001b[92m3.683568939368378\u001b[0m \t 3.683568939368378\n",
      "87     \t [0.42324854 0.44603074 0.25738932]. \t  0.36038375658699134 \t 3.683568939368378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.4927525  0.5385138  0.27851717]. \t  0.2253183040613936 \t 3.683568939368378\n",
      "89     \t [0.04815468 0.70016081 0.19228493]. \t  0.06345418758305794 \t 3.683568939368378\n",
      "90     \t [0.96808327 0.58718331 0.88105544]. \t  3.592656151513638 \t 3.683568939368378\n",
      "91     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.683568939368378\n",
      "92     \t [0.45240892 0.93089545 0.88793639]. \t  1.0218699268839455 \t 3.683568939368378\n",
      "93     \t [0.40131836 0.529441   0.64123593]. \t  1.7253094510661566 \t 3.683568939368378\n",
      "94     \t [0.3152538  0.63830075 0.01828794]. \t  0.010460757853160165 \t 3.683568939368378\n",
      "95     \t [0.29793335 0.26841084 0.64559196]. \t  0.8646917429051725 \t 3.683568939368378\n",
      "96     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.683568939368378\n",
      "97     \t [4.03393898e-01 9.87857342e-01 9.78709217e-04]. \t  0.0002656821721122154 \t 3.683568939368378\n",
      "98     \t [0.43401217 0.70958608 0.81826318]. \t  2.9929994017946773 \t 3.683568939368378\n",
      "99     \t [0.52938386 0.10117051 0.74540064]. \t  0.5622130609904947 \t 3.683568939368378\n",
      "100    \t [0.32680064 0.74037044 0.49418899]. \t  2.0226489388748736 \t 3.683568939368378\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [0.41009259 1.         1.        ]. \t  0.33361389892979315 \t 2.6697919207500047\n",
      "2      \t [0.91418486 0.13498533 0.09072509]. \t  0.16031997849670143 \t 2.6697919207500047\n",
      "3      \t [0.05541546 0.94065901 0.80220177]. \t  1.1967277289563476 \t 2.6697919207500047\n",
      "4      \t [0.04415727 0.04815351 0.41655995]. \t  0.3638074619101327 \t 2.6697919207500047\n",
      "5      \t [0.99455258 0.92788983 0.2811148 ]. \t  0.029823578890430132 \t 2.6697919207500047\n",
      "6      \t [0.64946068 0.97212608 0.53549274]. \t  1.1445885705215288 \t 2.6697919207500047\n",
      "7      \t [0.00280639 0.56643874 0.56974004]. \t  1.5638719753560206 \t 2.6697919207500047\n",
      "8      \t [3.74128618e-01 2.90520797e-04 6.79335015e-03]. \t  0.1139263390418666 \t 2.6697919207500047\n",
      "9      \t [0.3540029  0.43577344 0.60362303]. \t  1.1125244414958395 \t 2.6697919207500047\n",
      "10     \t [1.         0.45118349 1.        ]. \t  1.7605076920363132 \t 2.6697919207500047\n",
      "11     \t [0.06668372 0.01516675 0.0183777 ]. \t  0.10682425407575023 \t 2.6697919207500047\n",
      "12     \t [0.5196858  0.69149659 0.95483845]. \t  2.4718493595269835 \t 2.6697919207500047\n",
      "13     \t [0.99985351 0.03121148 0.88559058]. \t  0.27446242085498995 \t 2.6697919207500047\n",
      "14     \t [0.99325398 0.62676208 0.55356945]. \t  0.4473890126884814 \t 2.6697919207500047\n",
      "15     \t [0.96394442 0.91735249 0.01956559]. \t  0.00015162727779548937 \t 2.6697919207500047\n",
      "16     \t [0.73330929 0.10079917 0.1790754 ]. \t  0.5302323258196112 \t 2.6697919207500047\n",
      "17     \t [0.80849755 0.15516763 0.55071728]. \t  0.20286319974479652 \t 2.6697919207500047\n",
      "18     \t [0.58308557 0.87109283 0.5936674 ]. \t  1.6129452256882726 \t 2.6697919207500047\n",
      "19     \t [0.19794147 0.6599606  0.46264745]. \t  1.5064933507193867 \t 2.6697919207500047\n",
      "20     \t [0.37910729 0.34556537 0.24838855]. \t  0.5956795291066099 \t 2.6697919207500047\n",
      "21     \t [0.15676912 0.2266729  0.71458064]. \t  1.1069290346962903 \t 2.6697919207500047\n",
      "22     \t [0.52038061 0.95614372 0.33226602]. \t  0.3830866615916619 \t 2.6697919207500047\n",
      "23     \t [0.92986348 0.40325218 0.60196825]. \t  0.7490893375906799 \t 2.6697919207500047\n",
      "24     \t [0.53542066 0.22558072 0.30585522]. \t  0.7871123420942476 \t 2.6697919207500047\n",
      "25     \t [0.5823161  0.00148667 0.37270086]. \t  0.5485870456558988 \t 2.6697919207500047\n",
      "26     \t [0.59357954 0.56878291 0.68481091]. \t  2.0452040645303198 \t 2.6697919207500047\n",
      "27     \t [0.12076705 0.86206906 0.00611273]. \t  0.0007762036397133444 \t 2.6697919207500047\n",
      "28     \t [0.9337669  0.48487116 0.36928721]. \t  0.11147461455551527 \t 2.6697919207500047\n",
      "29     \t [0.57331052 0.99917895 0.69537268]. \t  0.9411566287997121 \t 2.6697919207500047\n",
      "30     \t [0.34088191 0.45891124 0.17408725]. \t  0.24477884525232038 \t 2.6697919207500047\n",
      "31     \t [0.45346533 0.24195019 0.21707166]. \t  0.777584452254225 \t 2.6697919207500047\n",
      "32     \t [0.7336959  0.19992914 0.83601768]. \t  1.2323835870354274 \t 2.6697919207500047\n",
      "33     \t [0.62578916 0.9972927  0.83172719]. \t  0.6417088167630212 \t 2.6697919207500047\n",
      "34     \t [0.51438648 0.13364806 0.62148412]. \t  0.3430355524950182 \t 2.6697919207500047\n",
      "35     \t [0.06293522 0.83845774 0.66680146]. \t  2.518751420246468 \t 2.6697919207500047\n",
      "36     \t [0.82803501 0.75130533 0.12609098]. \t  0.0074436256194782115 \t 2.6697919207500047\n",
      "37     \t [0.9733965  0.04824388 0.53741531]. \t  0.09379129043720691 \t 2.6697919207500047\n",
      "38     \t [0.48614181 0.08087374 0.51203787]. \t  0.21111004333001018 \t 2.6697919207500047\n",
      "39     \t [0.1045581  0.03693532 0.28305847]. \t  0.7553175789879043 \t 2.6697919207500047\n",
      "40     \t [0.63740215 0.69850905 0.29943415]. \t  0.1626293940305324 \t 2.6697919207500047\n",
      "41     \t [0.01492616 0.79914268 0.07117118]. \t  0.004550701126035047 \t 2.6697919207500047\n",
      "42     \t [0.49810016 0.89864423 0.91889104]. \t  1.1531476462011079 \t 2.6697919207500047\n",
      "43     \t [0.71927048 0.24035136 0.84679427]. \t  1.5604251042519215 \t 2.6697919207500047\n",
      "44     \t [0.51772007 0.16459242 0.54156564]. \t  0.24703368892906002 \t 2.6697919207500047\n",
      "45     \t [0.91307953 0.54224691 0.01255567]. \t  0.009644528294269164 \t 2.6697919207500047\n",
      "46     \t [0.32018525 0.96995705 0.07045838]. \t  0.002320141225838857 \t 2.6697919207500047\n",
      "47     \t [0.63775282 0.84122823 0.98907381]. \t  1.0848821100342865 \t 2.6697919207500047\n",
      "48     \t [0.41687774 0.47665256 0.8930494 ]. \t  \u001b[92m3.4735349593341613\u001b[0m \t 3.4735349593341613\n",
      "49     \t [0.87544484 0.55099794 0.57339059]. \t  0.6560061477708956 \t 3.4735349593341613\n",
      "50     \t [0.82259705 0.18715503 0.24239246]. \t  0.5043136364337744 \t 3.4735349593341613\n",
      "51     \t [0.75320002 0.16866405 0.87634165]. \t  0.9279718806791477 \t 3.4735349593341613\n",
      "52     \t [0.14283124 0.96982648 0.46066381]. \t  2.0942187826157297 \t 3.4735349593341613\n",
      "53     \t [0.07023171 0.70890922 0.27658173]. \t  0.2471697766124015 \t 3.4735349593341613\n",
      "54     \t [0.92469231 0.95738827 0.17634737]. \t  0.005448549580207616 \t 3.4735349593341613\n",
      "55     \t [0.48102739 0.82261573 0.75166446]. \t  1.822862214107297 \t 3.4735349593341613\n",
      "56     \t [0.67393751 0.46596782 0.65886108]. \t  1.552198018826415 \t 3.4735349593341613\n",
      "57     \t [0.02708937 0.30626316 0.66384568]. \t  1.1499309828643371 \t 3.4735349593341613\n",
      "58     \t [0.83932096 0.82726693 0.50711291]. \t  0.6015910919907921 \t 3.4735349593341613\n",
      "59     \t [0.27701155 0.64444347 0.89608859]. \t  3.433467982743001 \t 3.4735349593341613\n",
      "60     \t [0.85806739 0.42003711 0.1988972 ]. \t  0.170858558567319 \t 3.4735349593341613\n",
      "61     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.4735349593341613\n",
      "62     \t [0.92301807 0.88585592 0.01980938]. \t  0.00024849928122262397 \t 3.4735349593341613\n",
      "63     \t [0.43194257 0.06871556 0.38614572]. \t  0.6366288501403745 \t 3.4735349593341613\n",
      "64     \t [0.31122698 0.46711811 0.02088006]. \t  0.04710443979578095 \t 3.4735349593341613\n",
      "65     \t [0.53570239 0.57571895 0.01116363]. \t  0.015773597038891984 \t 3.4735349593341613\n",
      "66     \t [0.41249547 0.65940938 0.78237046]. \t  3.1007216815605227 \t 3.4735349593341613\n",
      "67     \t [0.77935401 0.26930086 0.86935596]. \t  1.7638500802766433 \t 3.4735349593341613\n",
      "68     \t [0.43283647 0.76036953 0.41310426]. \t  1.0749063091057893 \t 3.4735349593341613\n",
      "69     \t [0.22385249 0.80812974 0.91181421]. \t  1.972010837229594 \t 3.4735349593341613\n",
      "70     \t [0.60743554 0.68562361 0.20006046]. \t  0.05206336040242722 \t 3.4735349593341613\n",
      "71     \t [0.25804814 0.13996467 0.81905318]. \t  0.8299592570505872 \t 3.4735349593341613\n",
      "72     \t [0.5821718  0.35246179 0.89182358]. \t  2.4801548321341045 \t 3.4735349593341613\n",
      "73     \t [0.23357874 0.73727731 0.3789872 ]. \t  0.9613318277059854 \t 3.4735349593341613\n",
      "74     \t [0.88888774 0.41874628 0.84717674]. \t  3.1732249560976977 \t 3.4735349593341613\n",
      "75     \t [0.02769668 0.13806226 0.81265602]. \t  0.8084523199876115 \t 3.4735349593341613\n",
      "76     \t [0.63695442 0.19345711 0.67316354]. \t  0.7158065553199365 \t 3.4735349593341613\n",
      "77     \t [0.50884202 0.31174631 0.03642599]. \t  0.13043191317951816 \t 3.4735349593341613\n",
      "78     \t [0.5557356  0.45374772 0.24007706]. \t  0.29810480269679684 \t 3.4735349593341613\n",
      "79     \t [0.10194547 0.48337965 0.67262343]. \t  2.0130384067844784 \t 3.4735349593341613\n",
      "80     \t [0.99357982 0.258955   0.11226195]. \t  0.123299984534561 \t 3.4735349593341613\n",
      "81     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.4735349593341613\n",
      "82     \t [0.92160388 0.16911816 0.78896864]. \t  0.9773645613769912 \t 3.4735349593341613\n",
      "83     \t [0.90765207 0.08229488 0.34841664]. \t  0.3410480358603376 \t 3.4735349593341613\n",
      "84     \t [0.6177773  0.45181939 0.55973338]. \t  0.6756980288286273 \t 3.4735349593341613\n",
      "85     \t [0.7953169  0.48600213 0.60033648]. \t  0.8837266789465817 \t 3.4735349593341613\n",
      "86     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.4735349593341613\n",
      "87     \t [0.63723314 0.7948855  0.96558784]. \t  1.6083594950866844 \t 3.4735349593341613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.17952302 0.7183282  0.35107356]. \t  0.6922487666296709 \t 3.4735349593341613\n",
      "89     \t [0.18214649 0.81090828 0.03565757]. \t  0.002337368518537894 \t 3.4735349593341613\n",
      "90     \t [0.91265406 0.94025652 0.6566742 ]. \t  0.5102062827977603 \t 3.4735349593341613\n",
      "91     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.4735349593341613\n",
      "92     \t [0.71659102 0.29215869 0.45843115]. \t  0.25157630039769635 \t 3.4735349593341613\n",
      "93     \t [0.65917194 0.14424592 0.1377917 ]. \t  0.46612716240634694 \t 3.4735349593341613\n",
      "94     \t [0.98883086 0.81042525 0.54541417]. \t  0.3860550087247636 \t 3.4735349593341613\n",
      "95     \t [0.28215278 0.36647051 0.59496722]. \t  0.8409719875188555 \t 3.4735349593341613\n",
      "96     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.4735349593341613\n",
      "97     \t [0.77153192 0.85421568 0.09593543]. \t  0.002562590911587586 \t 3.4735349593341613\n",
      "98     \t [0.13704579 0.51048051 0.96517546]. \t  2.626575319576149 \t 3.4735349593341613\n",
      "99     \t [0.10408355 0.96486528 0.16520414]. \t  0.029557288512020097 \t 3.4735349593341613\n",
      "100    \t [0.30368637 0.3434045  0.0622976 ]. \t  0.16772013086145363 \t 3.4735349593341613\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [0.05593593 0.92782293 0.12748656]. \t  0.012672955321826789 \t 2.610000357863649\n",
      "4      \t [1.         1.         0.48695631]. \t  0.21258181210219015 \t 2.610000357863649\n",
      "5      \t [0.80829604 0.99997905 0.88385512]. \t  0.5623859876062987 \t 2.610000357863649\n",
      "6      \t [0.02146419 0.63896915 0.69822208]. \t  2.557184178249518 \t 2.610000357863649\n",
      "7      \t [0.98377295 0.45156257 0.12592043]. \t  0.05786822271687149 \t 2.610000357863649\n",
      "8      \t [0.98535496 0.31687436 0.75694447]. \t  1.921552441041293 \t 2.610000357863649\n",
      "9      \t [0.96994182 0.59808976 0.95922333]. \t  \u001b[92m2.694038523821318\u001b[0m \t 2.694038523821318\n",
      "10     \t [0.08960719 0.07019454 0.86353831]. \t  0.4361042465421674 \t 2.694038523821318\n",
      "11     \t [0.82661661 0.02338278 0.01499173]. \t  0.07237182666742638 \t 2.694038523821318\n",
      "12     \t [0.09208514 0.07037796 0.44908954]. \t  0.30589771667488297 \t 2.694038523821318\n",
      "13     \t [0.31388854 0.56171835 0.65470991]. \t  2.0249055367371938 \t 2.694038523821318\n",
      "14     \t [0.11218565 0.44702431 0.03432975]. \t  0.05434212210126889 \t 2.694038523821318\n",
      "15     \t [0.80939243 0.79010088 0.08327394]. \t  0.002997202454549464 \t 2.694038523821318\n",
      "16     \t [0.10052363 0.02367888 0.33536347]. \t  0.6437424979875838 \t 2.694038523821318\n",
      "17     \t [0.33319617 0.02386789 0.01858048]. \t  0.14278722241783126 \t 2.694038523821318\n",
      "18     \t [0.19115446 0.65697421 0.90020293]. \t  \u001b[92m3.322905191093616\u001b[0m \t 3.322905191093616\n",
      "19     \t [0.19675717 0.23104748 0.6933584 ]. \t  1.009417664252229 \t 3.322905191093616\n",
      "20     \t [0.93674272 0.00715582 0.42178829]. \t  0.16933941525743085 \t 3.322905191093616\n",
      "21     \t [0.36214764 0.49508642 0.97420567]. \t  2.436937429241984 \t 3.322905191093616\n",
      "22     \t [0.33814324 0.06018895 0.88562386]. \t  0.37360227617833996 \t 3.322905191093616\n",
      "23     \t [0.51299824 0.53435554 0.05110948]. \t  0.04079586528950364 \t 3.322905191093616\n",
      "24     \t [0.72598471 0.55989489 0.83116069]. \t  \u001b[92m3.7139472168367194\u001b[0m \t 3.7139472168367194\n",
      "25     \t [0.04591849 1.         0.68603563]. \t  1.6819784261276431 \t 3.7139472168367194\n",
      "26     \t [0.26032784 0.27375084 0.81176359]. \t  1.8818104124373705 \t 3.7139472168367194\n",
      "27     \t [0.73141003 0.25188286 0.79355714]. \t  1.6288419673048349 \t 3.7139472168367194\n",
      "28     \t [0.79772135 0.5752789  0.97944833]. \t  2.4178653602124673 \t 3.7139472168367194\n",
      "29     \t [0.40488625 0.56336259 0.72591955]. \t  2.728428649987446 \t 3.7139472168367194\n",
      "30     \t [0.80901536 0.21371557 0.00255886]. \t  0.06220844726521293 \t 3.7139472168367194\n",
      "31     \t [0.77487825 0.12440358 0.49843522]. \t  0.17868446358946294 \t 3.7139472168367194\n",
      "32     \t [0.58296738 0.76497902 0.24063865]. \t  0.08343007839597481 \t 3.7139472168367194\n",
      "33     \t [0.00420557 0.45156882 0.50342476]. \t  0.659104502994991 \t 3.7139472168367194\n",
      "34     \t [0.18285003 0.37844441 0.00287031]. \t  0.05587853797714881 \t 3.7139472168367194\n",
      "35     \t [0.14642965 0.91598237 0.2559379 ]. \t  0.20304412268097038 \t 3.7139472168367194\n",
      "36     \t [0.45615361 0.73727042 0.84871959]. \t  2.827008634102651 \t 3.7139472168367194\n",
      "37     \t [0.50993243 0.79119257 0.84788879]. \t  2.286440058928742 \t 3.7139472168367194\n",
      "38     \t [0.93623618 0.29605179 0.33829222]. \t  0.2437372125777743 \t 3.7139472168367194\n",
      "39     \t [0.7034763  0.7681872  0.59752879]. \t  1.191916990360724 \t 3.7139472168367194\n",
      "40     \t [0.60051018 0.29926604 0.66535384]. \t  1.1093207085561936 \t 3.7139472168367194\n",
      "41     \t [0.65646725 0.49265972 0.9653974 ]. \t  2.5605517011544756 \t 3.7139472168367194\n",
      "42     \t [0.14906706 0.07891761 0.45078817]. \t  0.3299405071818477 \t 3.7139472168367194\n",
      "43     \t [0.06361983 0.49685449 0.5492252 ]. \t  1.0900625695977924 \t 3.7139472168367194\n",
      "44     \t [0.48465398 0.53084226 0.76783388]. \t  3.216582404179128 \t 3.7139472168367194\n",
      "45     \t [0.49675483 0.31947015 0.54955814]. \t  0.44711691558888944 \t 3.7139472168367194\n",
      "46     \t [0.57531671 0.77714039 0.79601549]. \t  2.222017309872367 \t 3.7139472168367194\n",
      "47     \t [0.26885881 0.07215766 0.98682835]. \t  0.21748198695025048 \t 3.7139472168367194\n",
      "48     \t [0.78473699 0.80022914 0.48037126]. \t  0.6473946693924508 \t 3.7139472168367194\n",
      "49     \t [0.91186375 0.95770045 0.68081817]. \t  0.49059545288520023 \t 3.7139472168367194\n",
      "50     \t [0.76346494 0.19713688 0.57715695]. \t  0.3122080478524831 \t 3.7139472168367194\n",
      "51     \t [0.19454059 0.18159557 0.72237632]. \t  0.8914520401243811 \t 3.7139472168367194\n",
      "52     \t [0.44777979 0.55156613 0.32905811]. \t  0.2989331694716544 \t 3.7139472168367194\n",
      "53     \t [0.10993639 0.85226217 0.04170515]. \t  0.0019104756240350366 \t 3.7139472168367194\n",
      "54     \t [0.26209026 0.4376069  0.36118454]. \t  0.4084725546606177 \t 3.7139472168367194\n",
      "55     \t [0.06882112 0.21835609 0.21992618]. \t  0.6453713415558252 \t 3.7139472168367194\n",
      "56     \t [0.40019455 0.99076213 0.36883095]. \t  0.7195529257060763 \t 3.7139472168367194\n",
      "57     \t [0.74937503 0.30475534 0.21494596]. \t  0.4204924501714821 \t 3.7139472168367194\n",
      "58     \t [0.8803298  0.04036959 0.23308407]. \t  0.4154404081189263 \t 3.7139472168367194\n",
      "59     \t [0.70824533 0.16272843 0.55621025]. \t  0.23298576797584025 \t 3.7139472168367194\n",
      "60     \t [0.88090148 0.98500405 0.81756814]. \t  0.6036992108633918 \t 3.7139472168367194\n",
      "61     \t [0.53512709 0.16829529 0.9421505 ]. \t  0.6952432576112471 \t 3.7139472168367194\n",
      "62     \t [0.64401662 0.07139507 0.48504836]. \t  0.21907733667470577 \t 3.7139472168367194\n",
      "63     \t [0.87962619 0.97940685 0.19199823]. \t  0.0089505544210135 \t 3.7139472168367194\n",
      "64     \t [0.06312619 0.70383427 0.70474941]. \t  2.57097876829957 \t 3.7139472168367194\n",
      "65     \t [0.04272913 0.29614461 0.82233246]. \t  2.0848611009640106 \t 3.7139472168367194\n",
      "66     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.7139472168367194\n",
      "67     \t [0.62690373 0.00985957 0.62415573]. \t  0.14153873039834525 \t 3.7139472168367194\n",
      "68     \t [0.51894156 0.10204151 0.11673319]. \t  0.47243581539184043 \t 3.7139472168367194\n",
      "69     \t [0.82250668 0.92070626 0.57262634]. \t  0.703700240487046 \t 3.7139472168367194\n",
      "70     \t [0.80836442 0.11740861 0.54025285]. \t  0.1632805156707197 \t 3.7139472168367194\n",
      "71     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.7139472168367194\n",
      "72     \t [0.82454869 0.54477795 0.6759933 ]. \t  1.7256223502834849 \t 3.7139472168367194\n",
      "73     \t [0.05346613 0.14611429 0.75967501]. \t  0.8007182747420258 \t 3.7139472168367194\n",
      "74     \t [0.23981051 0.67263057 0.13086409]. \t  0.03353502772872412 \t 3.7139472168367194\n",
      "75     \t [0.9019401  0.82825263 0.81074213]. \t  1.6489711770283817 \t 3.7139472168367194\n",
      "76     \t [0.96803703 0.49412009 0.97246297]. \t  2.3800799393998964 \t 3.7139472168367194\n",
      "77     \t [0.11502712 0.97940057 0.38983881]. \t  1.1868805497135508 \t 3.7139472168367194\n",
      "78     \t [0.58754603 0.62029323 0.99348871]. \t  2.1370125930544903 \t 3.7139472168367194\n",
      "79     \t [0.9654962  0.13244232 0.08087639]. \t  0.12090328344314215 \t 3.7139472168367194\n",
      "80     \t [0.88177703 0.43718365 0.67654305]. \t  1.623967464378314 \t 3.7139472168367194\n",
      "81     \t [0.01465277 0.63397872 0.83901536]. \t  3.591801365983141 \t 3.7139472168367194\n",
      "82     \t [0.         0.53510087 1.        ]. \t  2.036606672273514 \t 3.7139472168367194\n",
      "83     \t [0.88256879 0.24667527 0.44384711]. \t  0.19112385305007243 \t 3.7139472168367194\n",
      "84     \t [0.02025095 0.62794742 0.34387863]. \t  0.468892639341909 \t 3.7139472168367194\n",
      "85     \t [0.66323936 0.67504463 0.77717188]. \t  2.786061946768787 \t 3.7139472168367194\n",
      "86     \t [0.96232362 0.52546593 0.40224017]. \t  0.10604193758342897 \t 3.7139472168367194\n",
      "87     \t [0.43523261 0.5618435  0.1594736 ]. \t  0.10387864219465551 \t 3.7139472168367194\n",
      "88     \t [0.71480031 0.33219295 0.31557587]. \t  0.4210776703142361 \t 3.7139472168367194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.58636839 0.44979184 0.79863073]. \t  3.2622995894011355 \t 3.7139472168367194\n",
      "90     \t [0.12175792 0.18456227 0.18306323]. \t  0.6433284296425059 \t 3.7139472168367194\n",
      "91     \t [0.28318428 0.71248563 0.53696315]. \t  2.2598699965857656 \t 3.7139472168367194\n",
      "92     \t [0.1244726  0.52626388 0.13790217]. \t  0.09965785828484391 \t 3.7139472168367194\n",
      "93     \t [0.38390603 0.72920448 0.50681676]. \t  1.9052606070746767 \t 3.7139472168367194\n",
      "94     \t [0.66904527 0.57598451 0.26215034]. \t  0.1301749742887242 \t 3.7139472168367194\n",
      "95     \t [0.96727165 0.74151729 0.17567326]. \t  0.00909902576108185 \t 3.7139472168367194\n",
      "96     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.7139472168367194\n",
      "97     \t [0.38754664 0.11656231 0.04666429]. \t  0.23190351032409634 \t 3.7139472168367194\n",
      "98     \t [0.38700234 0.62982399 0.08687214]. \t  0.028972305826335615 \t 3.7139472168367194\n",
      "99     \t [0.20064203 0.83355521 0.68312582]. \t  2.3765749506506375 \t 3.7139472168367194\n",
      "100    \t [0.13333789 0.45299801 0.76109183]. \t  2.9534189917965623 \t 3.7139472168367194\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0.08128701 1.         1.        ]. \t  0.3322425706424324 \t 1.540625560354162\n",
      "2      \t [0.61363169 0.18622958 0.87189166]. \t  1.0703600264488446 \t 1.540625560354162\n",
      "3      \t [0.09227183 0.81916727 0.00548775]. \t  0.001076579463298919 \t 1.540625560354162\n",
      "4      \t [0.0825884  0.10762681 0.97600228]. \t  0.32817121618245837 \t 1.540625560354162\n",
      "5      \t [0.99573902 0.01517269 0.64475385]. \t  0.15727336529876818 \t 1.540625560354162\n",
      "6      \t [0.6820242  0.83219259 0.97548164]. \t  1.2535582322531822 \t 1.540625560354162\n",
      "7      \t [0.48764269 0.93337811 0.0397321 ]. \t  0.0009185125389504879 \t 1.540625560354162\n",
      "8      \t [0.00105504 0.08339973 0.03624692]. \t  0.13281849367369486 \t 1.540625560354162\n",
      "9      \t [0.931463   0.05328064 0.13719876]. \t  0.22361958586423464 \t 1.540625560354162\n",
      "10     \t [0.04241685 0.57427846 0.86643881]. \t  \u001b[92m3.794734222483946\u001b[0m \t 3.794734222483946\n",
      "11     \t [0.         0.56479452 1.        ]. \t  2.0578130359176217 \t 3.794734222483946\n",
      "12     \t [0.14721214 0.6361499  0.74123246]. \t  2.921782726048466 \t 3.794734222483946\n",
      "13     \t [0.         1.         0.60780303]. \t  2.326909418848248 \t 3.794734222483946\n",
      "14     \t [0.70530993 1.         0.76314685]. \t  0.6027370549656079 \t 3.794734222483946\n",
      "15     \t [0.         0.27212117 0.75115939]. \t  1.6038604953515234 \t 3.794734222483946\n",
      "16     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.794734222483946\n",
      "17     \t [0.         0.71546873 0.69006733]. \t  2.4940029134996697 \t 3.794734222483946\n",
      "18     \t [0.29143127 0.39138629 1.        ]. \t  1.5400258342261837 \t 3.794734222483946\n",
      "19     \t [1.         0.62118487 1.        ]. \t  1.9522178911307033 \t 3.794734222483946\n",
      "20     \t [0.77580129 0.95488433 0.83877889]. \t  0.8189802946083149 \t 3.794734222483946\n",
      "21     \t [0.1745419  0.58861564 0.46678452]. \t  1.1425460938241916 \t 3.794734222483946\n",
      "22     \t [0.04590261 0.99449647 0.92188582]. \t  0.5763601772225795 \t 3.794734222483946\n",
      "23     \t [0.61034429 0.01704907 0.91874193]. \t  0.20844592155048333 \t 3.794734222483946\n",
      "24     \t [0.        1.        0.1115677]. \t  0.006945422843502955 \t 3.794734222483946\n",
      "25     \t [0.0003486  0.34853228 0.        ]. \t  0.045654879939926514 \t 3.794734222483946\n",
      "26     \t [0.15158623 0.95060302 0.64613729]. \t  2.356182250610943 \t 3.794734222483946\n",
      "27     \t [0.93902116 0.98300982 0.40163048]. \t  0.16799325633713869 \t 3.794734222483946\n",
      "28     \t [0.81029591 0.24678794 0.31937025]. \t  0.4381419149067207 \t 3.794734222483946\n",
      "29     \t [0.07219438 0.54592438 0.46719698]. \t  0.92582274020682 \t 3.794734222483946\n",
      "30     \t [0.25031903 0.57176203 0.29226148]. \t  0.26396392768388777 \t 3.794734222483946\n",
      "31     \t [0.96747193 0.73696735 0.56512663]. \t  0.49336417137675836 \t 3.794734222483946\n",
      "32     \t [0.30092112 0.04207641 0.49286411]. \t  0.23182925780625033 \t 3.794734222483946\n",
      "33     \t [0.49284216 0.24798455 0.17518899]. \t  0.6241596018331336 \t 3.794734222483946\n",
      "34     \t [0.94991439 0.25344513 0.47231953]. \t  0.15411571556249676 \t 3.794734222483946\n",
      "35     \t [0.42395109 0.46333711 0.69317732]. \t  2.1160544025142434 \t 3.794734222483946\n",
      "36     \t [0.57566654 0.86638367 0.42873776]. \t  0.9775902812427747 \t 3.794734222483946\n",
      "37     \t [0.62567376 0.48586688 0.12032655]. \t  0.11113690706796829 \t 3.794734222483946\n",
      "38     \t [0.42621411 0.29267786 0.06988823]. \t  0.2259749665115015 \t 3.794734222483946\n",
      "39     \t [0.76185184 0.23768587 0.64088868]. \t  0.6866343913709799 \t 3.794734222483946\n",
      "40     \t [0.65280619 0.29583628 0.3662211 ]. \t  0.4466117291379827 \t 3.794734222483946\n",
      "41     \t [0.95742796 0.23987328 0.12207954]. \t  0.16160564315557036 \t 3.794734222483946\n",
      "42     \t [0.9721892  0.6532657  0.66550756]. \t  1.2784958775059452 \t 3.794734222483946\n",
      "43     \t [0.25515437 0.53313499 0.73361559]. \t  2.8663227839682435 \t 3.794734222483946\n",
      "44     \t [0.06563885 0.5164127  0.68482816]. \t  2.251330443417286 \t 3.794734222483946\n",
      "45     \t [0.16197517 0.07401815 0.40257298]. \t  0.506158595836589 \t 3.794734222483946\n",
      "46     \t [0.94932689 0.7082576  0.15828948]. \t  0.010194875724603343 \t 3.794734222483946\n",
      "47     \t [0.03612574 0.20492158 0.96675868]. \t  0.7573473904144807 \t 3.794734222483946\n",
      "48     \t [0.31478473 0.75239736 0.59539435]. \t  2.503108446494391 \t 3.794734222483946\n",
      "49     \t [0.12807809 0.71043487 0.78441767]. \t  2.8944366251963025 \t 3.794734222483946\n",
      "50     \t [0.78169698 0.9679582  0.12423329]. \t  0.0029518948623640582 \t 3.794734222483946\n",
      "51     \t [0.17049127 0.30955908 0.5032451 ]. \t  0.36574896563945947 \t 3.794734222483946\n",
      "52     \t [0.62694206 0.23855795 0.03932852]. \t  0.14858552811675604 \t 3.794734222483946\n",
      "53     \t [0.02949578 0.52997778 0.92497741]. \t  3.2843093921713233 \t 3.794734222483946\n",
      "54     \t [0.39571091 0.76513454 0.11735628]. \t  0.014336754993934943 \t 3.794734222483946\n",
      "55     \t [0.49912256 0.57359354 0.07987976]. \t  0.0420897873141118 \t 3.794734222483946\n",
      "56     \t [0.71298415 0.07796515 0.02175991]. \t  0.1131417516293181 \t 3.794734222483946\n",
      "57     \t [0.08532404 0.07999008 0.5542384 ]. \t  0.16634200233107438 \t 3.794734222483946\n",
      "58     \t [0.31537954 0.50614944 0.14901243]. \t  0.1482404676065216 \t 3.794734222483946\n",
      "59     \t [0.95595308 0.70300921 0.2873902 ]. \t  0.04232035400132968 \t 3.794734222483946\n",
      "60     \t [0.13282356 0.51499461 0.7746907 ]. \t  3.323744793940068 \t 3.794734222483946\n",
      "61     \t [0.94502108 0.39990792 0.46043258]. \t  0.1539566760694928 \t 3.794734222483946\n",
      "62     \t [0.8196895  0.72686006 0.93511731]. \t  2.416662369877793 \t 3.794734222483946\n",
      "63     \t [0.21142111 0.13065041 0.63887592]. \t  0.3862661188276634 \t 3.794734222483946\n",
      "64     \t [0.89506211 0.53577709 0.97892019]. \t  2.3861619625231607 \t 3.794734222483946\n",
      "65     \t [0.07179407 0.307412   0.54957737]. \t  0.45644969200223207 \t 3.794734222483946\n",
      "66     \t [0.74102955 0.10310999 0.55334669]. \t  0.17062597575798433 \t 3.794734222483946\n",
      "67     \t [0.32117172 0.15156373 0.36337591]. \t  0.7518827261601073 \t 3.794734222483946\n",
      "68     \t [0.23987767 0.03561525 0.82833568]. \t  0.3449567060631165 \t 3.794734222483946\n",
      "69     \t [0.56859761 0.55077975 0.36159905]. \t  0.29355795375216914 \t 3.794734222483946\n",
      "70     \t [0.68819035 0.64189538 0.06515681]. \t  0.01423296030403576 \t 3.794734222483946\n",
      "71     \t [0.64977247 0.99241348 0.0401207 ]. \t  0.0004630520725081235 \t 3.794734222483946\n",
      "72     \t [0.35024029 0.27870533 0.4139076 ]. \t  0.4638658250097487 \t 3.794734222483946\n",
      "73     \t [0.70710257 0.39199623 0.73546271]. \t  2.262804163662324 \t 3.794734222483946\n",
      "74     \t [0.17094858 0.30939972 0.91923819]. \t  1.8661890406393606 \t 3.794734222483946\n",
      "75     \t [0.12129095 0.05188576 0.52608347]. \t  0.1609168422448141 \t 3.794734222483946\n",
      "76     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.794734222483946\n",
      "77     \t [0.76748649 0.65576792 0.81978304]. \t  3.246026808706147 \t 3.794734222483946\n",
      "78     \t [0.78474209 0.09469402 0.27343527]. \t  0.5919355161280437 \t 3.794734222483946\n",
      "79     \t [0.3432652  0.99233688 0.29913662]. \t  0.311745229322761 \t 3.794734222483946\n",
      "80     \t [0.42594991 0.64093961 0.79628817]. \t  3.311886121166303 \t 3.794734222483946\n",
      "81     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.794734222483946\n",
      "82     \t [0.01785545 0.43979856 0.33346611]. \t  0.31975513224542707 \t 3.794734222483946\n",
      "83     \t [0.31647395 0.46802472 0.27590367]. \t  0.3387216842663361 \t 3.794734222483946\n",
      "84     \t [0.41326538 0.23479662 0.33965651]. \t  0.7515899184306771 \t 3.794734222483946\n",
      "85     \t [0.32277329 0.62037369 0.19391519]. \t  0.0949095772444279 \t 3.794734222483946\n",
      "86     \t [0.24974013 0.69021691 0.23669921]. \t  0.1322942127617831 \t 3.794734222483946\n",
      "87     \t [0.27770224 0.88136147 0.45737355]. \t  2.084065645762579 \t 3.794734222483946\n",
      "88     \t [0.68300943 0.804197   0.27631666]. \t  0.11087677703003533 \t 3.794734222483946\n",
      "89     \t [0.63672142 0.53245896 0.5901552 ]. \t  1.017995686820874 \t 3.794734222483946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.84436452 0.90046832 0.78427641]. \t  1.02422684648505 \t 3.794734222483946\n",
      "91     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.794734222483946\n",
      "92     \t [0.3594319  0.77854922 0.53836509]. \t  2.3715610925954347 \t 3.794734222483946\n",
      "93     \t [0.84754239 0.32899248 0.26725522]. \t  0.3237456416348651 \t 3.794734222483946\n",
      "94     \t [0.16133936 0.95229344 0.03212184]. \t  0.0009293574810958403 \t 3.794734222483946\n",
      "95     \t [0.16932797 0.89291599 0.43779014]. \t  1.9690371859422988 \t 3.794734222483946\n",
      "96     \t [0.45842086 0.96216734 0.79621177]. \t  0.9516993946869766 \t 3.794734222483946\n",
      "97     \t [0.88281347 0.56274476 0.02425408]. \t  0.01059400172578875 \t 3.794734222483946\n",
      "98     \t [0.50015117 0.62194574 0.45523951]. \t  0.8241009240885171 \t 3.794734222483946\n",
      "99     \t [0.24256493 0.3609237  0.17845439]. \t  0.4178813319039115 \t 3.794734222483946\n",
      "100    \t [0.8987482  0.08554098 0.91272598]. \t  0.40642636439750535 \t 3.794734222483946\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.08056179 0.45877099 1.        ]. \t  1.8425209175454709 \t 3.8084053754826726\n",
      "2      \t [0.         1.         0.75308252]. \t  1.0928770389243105 \t 3.8084053754826726\n",
      "3      \t [0.         0.22301295 0.34500758]. \t  0.5095972391781063 \t 3.8084053754826726\n",
      "4      \t [0.24662864 0.55095703 0.66400731]. \t  2.124428548649976 \t 3.8084053754826726\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "6      \t [1.         0.61092457 0.        ]. \t  0.0031086994070658506 \t 3.8084053754826726\n",
      "7      \t [0.         0.78457803 1.        ]. \t  1.2979609366217637 \t 3.8084053754826726\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "9      \t [0.79982814 1.         0.        ]. \t  8.742179832770534e-05 \t 3.8084053754826726\n",
      "10     \t [0.69045809 0.68193354 1.        ]. \t  1.8176811195343365 \t 3.8084053754826726\n",
      "11     \t [0.58953027 1.         0.77429489]. \t  0.6953772508985578 \t 3.8084053754826726\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "14     \t [0.25533903 0.76562693 0.84282053]. \t  2.6001998657892154 \t 3.8084053754826726\n",
      "15     \t [0.4369566  0.31390697 0.94432149]. \t  1.6861912722697086 \t 3.8084053754826726\n",
      "16     \t [0.97582172 0.68367138 0.60172171]. \t  0.6855234655190374 \t 3.8084053754826726\n",
      "17     \t [0.53799443 0.42946953 0.00548847]. \t  0.04425139695826853 \t 3.8084053754826726\n",
      "18     \t [0.56661062 0.34269926 0.26922376]. \t  0.5430076964051797 \t 3.8084053754826726\n",
      "19     \t [0.90848465 0.94727411 0.66475276]. \t  0.5069273890831931 \t 3.8084053754826726\n",
      "20     \t [0.83448508 0.12077406 0.25541979]. \t  0.519844097608949 \t 3.8084053754826726\n",
      "21     \t [0.04023156 0.45894557 0.70851272]. \t  2.3323726899042696 \t 3.8084053754826726\n",
      "22     \t [1.         0.45063554 1.        ]. \t  1.7583124130343637 \t 3.8084053754826726\n",
      "23     \t [0.86840893 0.50347298 0.18496287]. \t  0.08894346754563025 \t 3.8084053754826726\n",
      "24     \t [0.15507555 0.57267199 0.87068821]. \t  \u001b[92m3.8090213016418613\u001b[0m \t 3.8090213016418613\n",
      "25     \t [0.21207837 0.9958074  0.01335284]. \t  0.00043942655710574075 \t 3.8090213016418613\n",
      "26     \t [0.05885562 0.6198107  0.89982702]. \t  3.489981524852336 \t 3.8090213016418613\n",
      "27     \t [1.39120962e-01 2.82077871e-04 4.22672869e-01]. \t  0.36632513317281906 \t 3.8090213016418613\n",
      "28     \t [0.71969094 0.12141137 0.88384231]. \t  0.633205118123529 \t 3.8090213016418613\n",
      "29     \t [0.3437446  0.42971995 0.46588041]. \t  0.4770678941020929 \t 3.8090213016418613\n",
      "30     \t [0.43569532 0.69456793 0.64220519]. \t  2.048398826348916 \t 3.8090213016418613\n",
      "31     \t [0.38982814 0.55892186 0.89282072]. \t  3.702871815898514 \t 3.8090213016418613\n",
      "32     \t [0.99656962 0.06250794 0.78544588]. \t  0.43045263052217053 \t 3.8090213016418613\n",
      "33     \t [0.88781016 0.04638648 0.8933457 ]. \t  0.310998536788925 \t 3.8090213016418613\n",
      "34     \t [0.46625412 0.34168985 0.1594325 ]. \t  0.41493325966955924 \t 3.8090213016418613\n",
      "35     \t [0.75458476 0.91661048 0.98789737]. \t  0.6831435742486909 \t 3.8090213016418613\n",
      "36     \t [0.78144854 0.69357355 0.53307138]. \t  0.7174412868429118 \t 3.8090213016418613\n",
      "37     \t [0.49477579 0.98071266 0.09779905]. \t  0.003492061104007494 \t 3.8090213016418613\n",
      "38     \t [0.38225454 0.54813721 0.85226666]. \t  \u001b[92m3.8564430066401205\u001b[0m \t 3.8564430066401205\n",
      "39     \t [0.5272702  0.65121755 0.97036025]. \t  2.440683465823292 \t 3.8564430066401205\n",
      "40     \t [0.05773691 0.81600649 0.83255251]. \t  2.1203882471503688 \t 3.8564430066401205\n",
      "41     \t [0.56054202 0.5367984  0.09062523]. \t  0.06109184484329125 \t 3.8564430066401205\n",
      "42     \t [0.44468499 0.00937981 0.74491765]. \t  0.25889728035744847 \t 3.8564430066401205\n",
      "43     \t [0.64608835 0.5730285  0.80072763]. \t  3.496451410603565 \t 3.8564430066401205\n",
      "44     \t [0.87162801 0.47921632 0.9186832 ]. \t  3.1441940720976573 \t 3.8564430066401205\n",
      "45     \t [0.96714671 0.31877611 0.8109514 ]. \t  2.224195426707524 \t 3.8564430066401205\n",
      "46     \t [0.06196911 0.9379467  0.18851526]. \t  0.05190659427104055 \t 3.8564430066401205\n",
      "47     \t [0.25289096 0.16187821 0.28078821]. \t  0.9383184957295628 \t 3.8564430066401205\n",
      "48     \t [0.43728799 0.33399021 0.54974035]. \t  0.48720358065663055 \t 3.8564430066401205\n",
      "49     \t [0.31540751 0.4925806  0.18533984]. \t  0.20812756425445086 \t 3.8564430066401205\n",
      "50     \t [0.55388742 0.80271077 0.8283371 ]. \t  2.1264732229035648 \t 3.8564430066401205\n",
      "51     \t [0.01141799 0.7958181  0.04024629]. \t  0.0024257263715949007 \t 3.8564430066401205\n",
      "52     \t [0.38568653 0.43133829 0.84687915]. \t  3.36473683347935 \t 3.8564430066401205\n",
      "53     \t [0.96522448 0.72693724 0.54356502]. \t  0.43203529307071536 \t 3.8564430066401205\n",
      "54     \t [0.78805564 0.61380732 0.41349227]. \t  0.256670904458279 \t 3.8564430066401205\n",
      "55     \t [0.97318955 0.43407487 0.41427682]. \t  0.11515215923246044 \t 3.8564430066401205\n",
      "56     \t [0.90603606 0.25244893 0.40295125]. \t  0.21999200267847413 \t 3.8564430066401205\n",
      "57     \t [0.51003172 0.40408284 0.06036259]. \t  0.11447049881241596 \t 3.8564430066401205\n",
      "58     \t [0.04398814 0.28878408 0.93202463]. \t  1.5716951794842502 \t 3.8564430066401205\n",
      "59     \t [0.97372688 0.12096872 0.72139876]. \t  0.5768109144263086 \t 3.8564430066401205\n",
      "60     \t [0.93617363 0.31811842 0.34290007]. \t  0.22210349730127474 \t 3.8564430066401205\n",
      "61     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8564430066401205\n",
      "62     \t [0.58821847 0.87099    0.19609719]. \t  0.0343397149464509 \t 3.8564430066401205\n",
      "63     \t [0.49773967 0.30088814 0.81176042]. \t  2.135118119974003 \t 3.8564430066401205\n",
      "64     \t [0.13141619 0.40399005 0.92938214]. \t  2.5762376586942803 \t 3.8564430066401205\n",
      "65     \t [0.05166908 0.89712758 0.83222533]. \t  1.4256201907466846 \t 3.8564430066401205\n",
      "66     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8564430066401205\n",
      "67     \t [0.3279515  0.0692452  0.67134912]. \t  0.31247617797407107 \t 3.8564430066401205\n",
      "68     \t [0.0961328  0.21997547 0.097758  ]. \t  0.30382130390439555 \t 3.8564430066401205\n",
      "69     \t [0.00527737 0.84067748 0.99465264]. \t  1.0374991182043767 \t 3.8564430066401205\n",
      "70     \t [0.73828086 0.51416956 0.16225059]. \t  0.10097721825159194 \t 3.8564430066401205\n",
      "71     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8564430066401205\n",
      "72     \t [0.69179496 0.9626543  0.04608057]. \t  0.00055817181923092 \t 3.8564430066401205\n",
      "73     \t [0.01135413 0.13766301 0.42708938]. \t  0.3369781391191165 \t 3.8564430066401205\n",
      "74     \t [0.01821837 0.06125134 0.50305178]. \t  0.16691697532744534 \t 3.8564430066401205\n",
      "75     \t [0.75887025 0.98992728 0.04830322]. \t  0.00041008097929826304 \t 3.8564430066401205\n",
      "76     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8564430066401205\n",
      "77     \t [0.34516894 0.87245412 0.01261572]. \t  0.0008504359352652114 \t 3.8564430066401205\n",
      "78     \t [0.5402775  0.11084036 0.71951798]. \t  0.5495022323459491 \t 3.8564430066401205\n",
      "79     \t [0.85245056 0.8370809  0.2820212 ]. \t  0.06353125045560243 \t 3.8564430066401205\n",
      "80     \t [0.65723325 0.81839565 0.23693994]. \t  0.0627265516022317 \t 3.8564430066401205\n",
      "81     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8564430066401205\n",
      "82     \t [0.10264431 0.13774606 0.43768167]. \t  0.36310734409873674 \t 3.8564430066401205\n",
      "83     \t [0.44394081 0.80240406 0.74670846]. \t  1.9883099573903038 \t 3.8564430066401205\n",
      "84     \t [0.1070112  0.18095404 0.22045493]. \t  0.7325303941307997 \t 3.8564430066401205\n",
      "85     \t [0.98222071 0.56216458 0.37437893]. \t  0.08341834321777766 \t 3.8564430066401205\n",
      "86     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8564430066401205\n",
      "87     \t [0.63531704 0.15550047 0.43784079]. \t  0.3572604518527468 \t 3.8564430066401205\n",
      "88     \t [0.7149028  0.61242382 0.36241602]. \t  0.21763972793491507 \t 3.8564430066401205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.72451742 0.87865968 0.01700706]. \t  0.00048062661902852907 \t 3.8564430066401205\n",
      "90     \t [0.06980833 0.06735126 0.19189161]. \t  0.6291019355032829 \t 3.8564430066401205\n",
      "91     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8564430066401205\n",
      "92     \t [0.057924   0.12644384 0.43121119]. \t  0.35490505058061267 \t 3.8564430066401205\n",
      "93     \t [0.23209014 0.82288377 0.68282029]. \t  2.3689475747338604 \t 3.8564430066401205\n",
      "94     \t [0.35882936 0.0239903  0.06063885]. \t  0.25459726411542455 \t 3.8564430066401205\n",
      "95     \t [0.73959719 0.3018457  0.40047305]. \t  0.3086314264790407 \t 3.8564430066401205\n",
      "96     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8564430066401205\n",
      "97     \t [0.58961367 0.45489445 0.43543263]. \t  0.33085397053812077 \t 3.8564430066401205\n",
      "98     \t [0.51236923 0.53830879 0.23496533]. \t  0.18246633710710705 \t 3.8564430066401205\n",
      "99     \t [0.97546352 0.36504299 0.38991969]. \t  0.13791651637606442 \t 3.8564430066401205\n",
      "100    \t [0.73376058 0.30712204 0.79077983]. \t  2.0952712170015175 \t 3.8564430066401205\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.29618333 0.33186883 1.        ]. \t  1.2124713708531745 \t 3.1179188940604616\n",
      "2      \t [1.         0.81744379 1.        ]. \t  1.0845992897598118 \t 3.1179188940604616\n",
      "3      \t [0.79727087 0.0538872  0.56023603]. \t  0.12825765959665458 \t 3.1179188940604616\n",
      "4      \t [0.04397697 0.85806207 0.53127678]. \t  2.965962353565724 \t 3.1179188940604616\n",
      "5      \t [0.05386169 0.67093251 0.88330276]. \t  \u001b[92m3.319999030720213\u001b[0m \t 3.319999030720213\n",
      "6      \t [0.0463506  0.87431407 0.95426815]. \t  1.1397777956785817 \t 3.319999030720213\n",
      "7      \t [0.         0.4527632  0.59860873]. \t  1.1775579862167573 \t 3.319999030720213\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.319999030720213\n",
      "9      \t [0.22300354 0.69323876 0.70266475]. \t  2.554835191736643 \t 3.319999030720213\n",
      "10     \t [0.         0.87574253 0.        ]. \t  0.0005299851578312664 \t 3.319999030720213\n",
      "11     \t [0.85890902 0.5169442  0.83052597]. \t  \u001b[92m3.6373255607522275\u001b[0m \t 3.6373255607522275\n",
      "12     \t [0.99170155 0.28892913 0.96914478]. \t  1.2177982335921864 \t 3.6373255607522275\n",
      "13     \t [0.83176945 0.61943008 0.72386123]. \t  2.2470532647540855 \t 3.6373255607522275\n",
      "14     \t [0.7773786  0.5273069  0.96831959]. \t  2.584725706260578 \t 3.6373255607522275\n",
      "15     \t [ 3.57055135e-01  1.00000000e+00 -1.38777878e-17]. \t  0.00025629310098966324 \t 3.6373255607522275\n",
      "16     \t [0.6186153  0.29607359 0.        ]. \t  0.07056951859788228 \t 3.6373255607522275\n",
      "17     \t [1.         0.26109414 0.67430872]. \t  0.9655659596693915 \t 3.6373255607522275\n",
      "18     \t [0.00520203 0.07833183 0.04023799]. \t  0.1410723662314045 \t 3.6373255607522275\n",
      "19     \t [0.         0.28104542 1.        ]. \t  0.9228789175236554 \t 3.6373255607522275\n",
      "20     \t [0.66708377 0.40556294 0.81743887]. \t  3.0664547158610658 \t 3.6373255607522275\n",
      "21     \t [0.94977414 0.19142298 0.76712315]. \t  1.0761414609657656 \t 3.6373255607522275\n",
      "22     \t [0.17428315 0.26781481 0.19964769]. \t  0.6215373191924979 \t 3.6373255607522275\n",
      "23     \t [0.43316865 0.33781837 0.16281783]. \t  0.43839356627360937 \t 3.6373255607522275\n",
      "24     \t [0.68987068 0.86716863 0.78450816]. \t  1.363504216858666 \t 3.6373255607522275\n",
      "25     \t [0.93893893 0.51145611 0.8570888 ]. \t  \u001b[92m3.648694769757588\u001b[0m \t 3.648694769757588\n",
      "26     \t [0.92303707 0.8869093  0.64767486]. \t  0.597178916481855 \t 3.648694769757588\n",
      "27     \t [0.61524298 0.8274235  0.25854383]. \t  0.10347423077592531 \t 3.648694769757588\n",
      "28     \t [0.49057397 0.00677537 0.68830688]. \t  0.20338784302438667 \t 3.648694769757588\n",
      "29     \t [0.47400839 0.40920829 0.45088014]. \t  0.37781778834869073 \t 3.648694769757588\n",
      "30     \t [0.55549822 0.02602027 0.4454878 ]. \t  0.33003912092135945 \t 3.648694769757588\n",
      "31     \t [0.11848612 0.07103457 0.02780259]. \t  0.14514648175093361 \t 3.648694769757588\n",
      "32     \t [0.12261411 0.9737731  0.40050435]. \t  1.3294543986203815 \t 3.648694769757588\n",
      "33     \t [0.47389126 0.18744075 0.81971356]. \t  1.1555089935923426 \t 3.648694769757588\n",
      "34     \t [0.16645579 0.969506   0.91332713]. \t  0.7307621502627861 \t 3.648694769757588\n",
      "35     \t [0.7200592  0.87547006 0.07956411]. \t  0.001882277042286545 \t 3.648694769757588\n",
      "36     \t [0.67785326 0.70861535 0.25442474]. \t  0.080663966526304 \t 3.648694769757588\n",
      "37     \t [0.3067936  0.89540176 0.06161733]. \t  0.002453658715214689 \t 3.648694769757588\n",
      "38     \t [0.1434548  0.08706172 0.50526542]. \t  0.20799888594699673 \t 3.648694769757588\n",
      "39     \t [0.05493339 0.34112824 0.90162177]. \t  2.284827208633714 \t 3.648694769757588\n",
      "40     \t [0.96431293 0.25428421 0.04062655]. \t  0.06121262862655542 \t 3.648694769757588\n",
      "41     \t [0.40092944 0.57225613 0.92167914]. \t  3.396291615289509 \t 3.648694769757588\n",
      "42     \t [0.73138278 0.52690713 0.45886613]. \t  0.3226847570174443 \t 3.648694769757588\n",
      "43     \t [0.74865906 0.59823871 0.87480389]. \t  \u001b[92m3.672251381247662\u001b[0m \t 3.672251381247662\n",
      "44     \t [0.33218932 0.55692604 0.34266385]. \t  0.37144478515930707 \t 3.672251381247662\n",
      "45     \t [0.83725307 0.79959111 0.70013841]. \t  1.1919591658793827 \t 3.672251381247662\n",
      "46     \t [0.         1.         0.69013001]. \t  1.6070938226630656 \t 3.672251381247662\n",
      "47     \t [0.66828    0.71894607 0.06229252]. \t  0.006423128990915802 \t 3.672251381247662\n",
      "48     \t [0.48565841 0.5055407  0.52301778]. \t  0.7532456792463784 \t 3.672251381247662\n",
      "49     \t [0.19068887 0.54653876 0.22469722]. \t  0.17468431327761985 \t 3.672251381247662\n",
      "50     \t [0.2511088 0.3079115 0.6974723]. \t  1.4612392662713283 \t 3.672251381247662\n",
      "51     \t [0.82399351 0.15394836 0.64722767]. \t  0.4584541049637956 \t 3.672251381247662\n",
      "52     \t [0.35348394 0.54054037 0.79326966]. \t  3.5423436991380184 \t 3.672251381247662\n",
      "53     \t [0.54033536 0.55708343 0.80453791]. \t  3.5940782171099377 \t 3.672251381247662\n",
      "54     \t [0.97518443 0.16518228 0.1573454 ]. \t  0.22569657611171431 \t 3.672251381247662\n",
      "55     \t [0.12084503 0.23291589 0.02030243]. \t  0.11658770253805333 \t 3.672251381247662\n",
      "56     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.672251381247662\n",
      "57     \t [0.16425101 0.29241238 0.19842371]. \t  0.5646226947998655 \t 3.672251381247662\n",
      "58     \t [0.02994214 0.90579619 0.36261184]. \t  0.9648630155089746 \t 3.672251381247662\n",
      "59     \t [0.39197389 0.50386445 0.47102974]. \t  0.6412848508887127 \t 3.672251381247662\n",
      "60     \t [0.6745912  0.3495636  0.27472958]. \t  0.44668356148041866 \t 3.672251381247662\n",
      "61     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.672251381247662\n",
      "62     \t [0.19529861 0.25854139 0.58264935]. \t  0.4881278080966765 \t 3.672251381247662\n",
      "63     \t [0.01115601 0.37581528 0.5813964 ]. \t  0.779550659614829 \t 3.672251381247662\n",
      "64     \t [0.6247172  0.31633328 0.10535845]. \t  0.251616622079824 \t 3.672251381247662\n",
      "65     \t [0.24468336 0.35194759 0.03991848]. \t  0.11662447727743262 \t 3.672251381247662\n",
      "66     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.672251381247662\n",
      "67     \t [0.58547314 0.77591115 0.66615084]. \t  1.668138336827412 \t 3.672251381247662\n",
      "68     \t [0.25904834 0.35509101 0.92005275]. \t  2.2764830385778954 \t 3.672251381247662\n",
      "69     \t [0.38886039 0.25135511 0.94066634]. \t  1.2307586237532115 \t 3.672251381247662\n",
      "70     \t [0.7748074  0.52998796 0.05385526]. \t  0.028382679565202167 \t 3.672251381247662\n",
      "71     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.672251381247662\n",
      "72     \t [0.99412436 0.61009789 0.90025713]. \t  3.3969057743859796 \t 3.672251381247662\n",
      "73     \t [0.49427874 0.06078479 0.09831404]. \t  0.39241495499378787 \t 3.672251381247662\n",
      "74     \t [0.2380556  0.22872624 0.76367594]. \t  1.3619292087267638 \t 3.672251381247662\n",
      "75     \t [0.13132677 0.60403356 0.5170896 ]. \t  1.5744761173600037 \t 3.672251381247662\n",
      "76     \t [0.23072881 0.85476936 0.97600168]. \t  1.1203921741915033 \t 3.672251381247662\n",
      "77     \t [0.81168948 0.57008726 0.64103874]. \t  1.298527861838243 \t 3.672251381247662\n",
      "78     \t [0.16755414 0.78062145 0.89354321]. \t  2.3339433408261065 \t 3.672251381247662\n",
      "79     \t [0.2554513  0.19549951 0.23507527]. \t  0.878276890884476 \t 3.672251381247662\n",
      "80     \t [0.49204468 0.87623059 0.93071709]. \t  1.267310045025293 \t 3.672251381247662\n",
      "81     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.672251381247662\n",
      "82     \t [0.25052621 0.17010318 0.71783121]. \t  0.8151619535206089 \t 3.672251381247662\n",
      "83     \t [0.96408758 0.63174507 0.05861642]. \t  0.0067284576879286274 \t 3.672251381247662\n",
      "84     \t [0.89258929 0.04884562 0.85635767]. \t  0.36203911780863607 \t 3.672251381247662\n",
      "85     \t [0.60846194 0.72990991 0.70982028]. \t  1.9273562235638337 \t 3.672251381247662\n",
      "86     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.672251381247662\n",
      "87     \t [0.16752079 0.2572927  0.51443825]. \t  0.31906035279084605 \t 3.672251381247662\n",
      "88     \t [0.89954628 0.74870152 0.06193817]. \t  0.0025124416832664736 \t 3.672251381247662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.7557668  0.08660438 0.81475771]. \t  0.542425360440532 \t 3.672251381247662\n",
      "90     \t [0.33748504 0.79789433 0.85234081]. \t  2.2721529590135434 \t 3.672251381247662\n",
      "91     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.672251381247662\n",
      "92     \t [0.17671046 0.56556359 0.67002609]. \t  2.2476536673077527 \t 3.672251381247662\n",
      "93     \t [0.18882945 0.49185784 0.18836753]. \t  0.1969260192761571 \t 3.672251381247662\n",
      "94     \t [0.66936525 0.53565213 0.74427858]. \t  2.8125319218943465 \t 3.672251381247662\n",
      "95     \t [0.92825386 0.75906919 0.23961872]. \t  0.024146434416881628 \t 3.672251381247662\n",
      "96     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.672251381247662\n",
      "97     \t [0.40367685 0.02504577 0.86102811]. \t  0.29168278131574493 \t 3.672251381247662\n",
      "98     \t [0.95993186 0.62888357 0.6627181 ]. \t  1.3205584756640187 \t 3.672251381247662\n",
      "99     \t [0.55281618 0.37893901 0.09183871]. \t  0.18088350815242976 \t 3.672251381247662\n",
      "100    \t [0.12728804 0.15490575 0.95500312]. \t  0.5730027320889911 \t 3.672251381247662\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
      "2      \t [0.00482474 0.64977529 0.94137026]. \t  \u001b[92m2.8634726533487465\u001b[0m \t 2.8634726533487465\n",
      "3      \t [0.41366768 0.97572252 0.94506975]. \t  0.5957593168891541 \t 2.8634726533487465\n",
      "4      \t [0.91119252 0.82772995 0.04090539]. \t  0.0007242881617636472 \t 2.8634726533487465\n",
      "5      \t [0.00124238 0.82714185 0.61084561]. \t  2.841218998812702 \t 2.8634726533487465\n",
      "6      \t [0.00806648 0.63086884 0.22273634]. \t  0.10481434203252675 \t 2.8634726533487465\n",
      "7      \t [0.96201929 0.19512248 0.17695609]. \t  0.2563913325924201 \t 2.8634726533487465\n",
      "8      \t [0.96186762 0.13180742 0.88285608]. \t  0.6761048728778654 \t 2.8634726533487465\n",
      "9      \t [0.08949126 0.0011825  0.8944142 ]. \t  0.19984817886284917 \t 2.8634726533487465\n",
      "10     \t [0.99940633 0.63248472 0.88650376]. \t  \u001b[92m3.387313380954288\u001b[0m \t 3.387313380954288\n",
      "11     \t [0.82662258 0.3037438  0.98623006]. \t  1.1708834329555582 \t 3.387313380954288\n",
      "12     \t [0.25588814 0.88061097 0.09237205]. \t  0.005740506407401016 \t 3.387313380954288\n",
      "13     \t [0.25792464 0.57693525 0.93129787]. \t  3.259598672522413 \t 3.387313380954288\n",
      "14     \t [0.01308225 0.54489594 0.90087337]. \t  \u001b[92m3.5798885554583455\u001b[0m \t 3.5798885554583455\n",
      "15     \t [0.92269907 0.77484678 0.19716845]. \t  0.012624191177223057 \t 3.5798885554583455\n",
      "16     \t [0.05792257 0.64083285 0.66966906]. \t  2.4211052253648244 \t 3.5798885554583455\n",
      "17     \t [0.        0.2994974 1.       ]. \t  1.0196129140814847 \t 3.5798885554583455\n",
      "18     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5798885554583455\n",
      "19     \t [0.74762959 0.78914022 0.9857386 ]. \t  1.4239384966954582 \t 3.5798885554583455\n",
      "20     \t [0.00241175 0.97553421 0.78813045]. \t  1.0319974245624777 \t 3.5798885554583455\n",
      "21     \t [0.82568555 0.50352912 0.        ]. \t  0.014089229598992755 \t 3.5798885554583455\n",
      "22     \t [0.5745334  0.39027396 0.25373722]. \t  0.42541178255113576 \t 3.5798885554583455\n",
      "23     \t [0.69830022 0.0325326  0.49661531]. \t  0.16612375230900428 \t 3.5798885554583455\n",
      "24     \t [0.91663665 0.30295198 0.68005739]. \t  1.2015675769353986 \t 3.5798885554583455\n",
      "25     \t [0.44093437 0.04103895 0.73854636]. \t  0.33804505675103763 \t 3.5798885554583455\n",
      "26     \t [0.66340188 0.65801611 0.51603536]. \t  0.8655767782185495 \t 3.5798885554583455\n",
      "27     \t [0.15782908 0.54829002 0.67059706]. \t  2.2068851609836475 \t 3.5798885554583455\n",
      "28     \t [0.12998841 0.09323376 0.57671251]. \t  0.19631576362123299 \t 3.5798885554583455\n",
      "29     \t [0.13552027 0.89559607 0.21119359]. \t  0.0882385543282937 \t 3.5798885554583455\n",
      "30     \t [0.67508432 0.6080857  0.43994567]. \t  0.44681163801031826 \t 3.5798885554583455\n",
      "31     \t [0.36937662 0.06674274 0.68650969]. \t  0.3347794499589437 \t 3.5798885554583455\n",
      "32     \t [0.83329247 0.96765954 0.6498969 ]. \t  0.5853295207957866 \t 3.5798885554583455\n",
      "33     \t [0.90057306 0.04836091 0.75216672]. \t  0.3647904827951455 \t 3.5798885554583455\n",
      "34     \t [0.2230882 0.5989508 0.0977612]. \t  0.04140865508964812 \t 3.5798885554583455\n",
      "35     \t [0.47192915 0.47188413 0.51625333]. \t  0.6393275343949238 \t 3.5798885554583455\n",
      "36     \t [0.6562836  0.15725733 0.53242462]. \t  0.21607031060941992 \t 3.5798885554583455\n",
      "37     \t [0.59843347 0.59788546 0.13763625]. \t  0.054765100999425394 \t 3.5798885554583455\n",
      "38     \t [0.65581236 0.64433593 0.95466435]. \t  2.7030270738326463 \t 3.5798885554583455\n",
      "39     \t [0.76631698 0.37096161 0.31690851]. \t  0.3172903185290855 \t 3.5798885554583455\n",
      "40     \t [0.84720801 0.95276625 0.45241843]. \t  0.40668663748846934 \t 3.5798885554583455\n",
      "41     \t [0.4542793  0.27686867 0.87438832]. \t  1.8432876495954358 \t 3.5798885554583455\n",
      "42     \t [0.19937222 0.9446533  0.85827678]. \t  1.027241170393896 \t 3.5798885554583455\n",
      "43     \t [0.03788695 0.18369452 0.41445365]. \t  0.3868243184919618 \t 3.5798885554583455\n",
      "44     \t [0.53763434 0.54191688 0.09155918]. \t  0.060681180418439194 \t 3.5798885554583455\n",
      "45     \t [0.06481073 0.03109829 0.12946029]. \t  0.39805420104150985 \t 3.5798885554583455\n",
      "46     \t [0.8902259 0.5955858 0.0539357]. \t  0.011550329052696968 \t 3.5798885554583455\n",
      "47     \t [0.23498364 0.58358687 0.44877488]. \t  0.9705215381374465 \t 3.5798885554583455\n",
      "48     \t [0.14186784 0.0525015  0.09480535]. \t  0.3366051581818347 \t 3.5798885554583455\n",
      "49     \t [0.39394695 0.8794093  0.46626307]. \t  1.8739818935835015 \t 3.5798885554583455\n",
      "50     \t [0.98928275 0.19954874 0.27394902]. \t  0.2945825304755498 \t 3.5798885554583455\n",
      "51     \t [0.84397882 0.33832146 0.89353692]. \t  2.2931951790596177 \t 3.5798885554583455\n",
      "52     \t [0.55994313 0.65723271 0.65286312]. \t  1.7798879528313765 \t 3.5798885554583455\n",
      "53     \t [0.80052371 0.98971188 0.58128943]. \t  0.6561209309416411 \t 3.5798885554583455\n",
      "54     \t [0.3631394  0.22614052 0.92951751]. \t  1.120423704818359 \t 3.5798885554583455\n",
      "55     \t [0.01323057 0.10392014 0.56915771]. \t  0.18989175487929383 \t 3.5798885554583455\n",
      "56     \t [0.27856709 0.95133089 0.42404761]. \t  1.5548092352299898 \t 3.5798885554583455\n",
      "57     \t [0.51484935 0.37870079 0.17637177]. \t  0.3712327765620692 \t 3.5798885554583455\n",
      "58     \t [0.92378982 0.62698725 0.81919386]. \t  3.3375600803889807 \t 3.5798885554583455\n",
      "59     \t [0.9008106  0.89863301 0.50635468]. \t  0.4503584829037369 \t 3.5798885554583455\n",
      "60     \t [0.88708557 0.1325105  0.55939045]. \t  0.18261463847821882 \t 3.5798885554583455\n",
      "61     \t [0.04865049 0.09823542 0.4212928 ]. \t  0.37315811158047807 \t 3.5798885554583455\n",
      "62     \t [0.5390818  0.84958602 0.46101446]. \t  1.333636522568597 \t 3.5798885554583455\n",
      "63     \t [0.16612704 0.48916367 0.99398218]. \t  2.0611999549721984 \t 3.5798885554583455\n",
      "64     \t [0.10352517 0.56434995 0.45093582]. \t  0.9280919263469136 \t 3.5798885554583455\n",
      "65     \t [0.58532372 0.33593381 0.18464687]. \t  0.43973928736740575 \t 3.5798885554583455\n",
      "66     \t [0.42698077 0.79457181 0.04596618]. \t  0.0031936492213742503 \t 3.5798885554583455\n",
      "67     \t [0.85006149 0.81031404 0.60016   ]. \t  0.7714548886829327 \t 3.5798885554583455\n",
      "68     \t [0.73305282 0.20500763 0.01658051]. \t  0.09431982467160967 \t 3.5798885554583455\n",
      "69     \t [0.75377633 0.75683595 0.3869172 ]. \t  0.3356325003406052 \t 3.5798885554583455\n",
      "70     \t [0.87848359 0.5184638  0.78023268]. \t  3.190703770224456 \t 3.5798885554583455\n",
      "71     \t [0.77931996 0.70884583 0.55471014]. \t  0.8143693810560588 \t 3.5798885554583455\n",
      "72     \t [0.67753011 0.19888946 0.09034316]. \t  0.2746770093846463 \t 3.5798885554583455\n",
      "73     \t [0.76804294 0.3062466  0.78708901]. \t  2.065211349310254 \t 3.5798885554583455\n",
      "74     \t [0.42441418 0.124083   0.18432184]. \t  0.8056265166744665 \t 3.5798885554583455\n",
      "75     \t [0.46018519 0.96389053 0.21788154]. \t  0.0641936011807335 \t 3.5798885554583455\n",
      "76     \t [0.01807392 0.81368688 0.98082843]. \t  1.3266707709398322 \t 3.5798885554583455\n",
      "77     \t [0.93180825 0.88940732 0.61191999]. \t  0.5255793471524116 \t 3.5798885554583455\n",
      "78     \t [0.91670262 0.85636471 0.60385514]. \t  0.5857504506744929 \t 3.5798885554583455\n",
      "79     \t [0.08982395 0.22815226 0.07746712]. \t  0.23738274009980176 \t 3.5798885554583455\n",
      "80     \t [0.19465177 0.80094751 0.13652537]. \t  0.01975742475111471 \t 3.5798885554583455\n",
      "81     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 3.5798885554583455\n",
      "82     \t [7.23789033e-01 5.75054111e-04 9.75390987e-01]. \t  0.11531495431160955 \t 3.5798885554583455\n",
      "83     \t [0.96954862 0.75827886 0.8010741 ]. \t  2.1707938772494266 \t 3.5798885554583455\n",
      "84     \t [0.08647243 0.96293921 0.32710117]. \t  0.5849356951338773 \t 3.5798885554583455\n",
      "85     \t [0.92456447 0.27602085 0.57067914]. \t  0.3769508605474541 \t 3.5798885554583455\n",
      "86     \t [0.64262746 0.50518779 0.79424085]. \t  3.4202876002328155 \t 3.5798885554583455\n",
      "87     \t [0.55959504 0.56036878 0.86034904]. \t  \u001b[92m3.824705320265667\u001b[0m \t 3.824705320265667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.56335103 0.63177929 0.83477455]. \t  3.5628077247782612 \t 3.824705320265667\n",
      "89     \t [0.12885795 0.47757019 0.43556527]. \t  0.5490663111180263 \t 3.824705320265667\n",
      "90     \t [0.8308906  0.29135679 0.79257402]. \t  1.949566771585567 \t 3.824705320265667\n",
      "91     \t [0.67338267 0.79139192 0.42711586]. \t  0.6770942979099852 \t 3.824705320265667\n",
      "92     \t [0.61770194 0.45288384 0.78835474]. \t  3.1810884718557237 \t 3.824705320265667\n",
      "93     \t [0.20510133 0.30126271 0.36898501]. \t  0.527957285978602 \t 3.824705320265667\n",
      "94     \t [0.32041972 0.09926547 0.11026052]. \t  0.47236637369737755 \t 3.824705320265667\n",
      "95     \t [0.42093232 0.75086645 0.04728742]. \t  0.005030015612563284 \t 3.824705320265667\n",
      "96     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.824705320265667\n",
      "97     \t [0.70670786 0.2444462  0.48689606]. \t  0.24031854225005608 \t 3.824705320265667\n",
      "98     \t [0.33735701 0.43188609 0.10161533]. \t  0.163115079042684 \t 3.824705320265667\n",
      "99     \t [0.92682191 0.50050062 0.5879895 ]. \t  0.7087530192057957 \t 3.824705320265667\n",
      "100    \t [0.97993502 0.40425479 0.05151405]. \t  0.03537802264890189 \t 3.824705320265667\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [ 1.00000000e+00 -1.73472348e-18  1.00000000e+00]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [0.68990349 0.776492   1.        ]. \t  1.3453739209325697 \t 2.524990008735946\n",
      "3      \t [0.03364059 0.33501849 0.98410297]. \t  1.3937191686454828 \t 2.524990008735946\n",
      "4      \t [0.97920914 0.18641683 0.18896551]. \t  0.2593776366232241 \t 2.524990008735946\n",
      "5      \t [0.94980803 0.46087189 0.73882356]. \t  2.5067790538732235 \t 2.524990008735946\n",
      "6      \t [0.05527369 0.97794093 0.91437772]. \t  0.6786579002918043 \t 2.524990008735946\n",
      "7      \t [0.20854414 0.01247687 0.97683317]. \t  0.13019357750163824 \t 2.524990008735946\n",
      "8      \t [0.11992636 0.02082448 0.08122865]. \t  0.26790366205121774 \t 2.524990008735946\n",
      "9      \t [0.97764934 0.11376053 0.5752718 ]. \t  0.17853364585453999 \t 2.524990008735946\n",
      "10     \t [0.98239807 0.67839943 0.19235616]. \t  0.01575624859524108 \t 2.524990008735946\n",
      "11     \t [0.5420832  0.44256445 0.98376186]. \t  2.0477303896288763 \t 2.524990008735946\n",
      "12     \t [0.39416909 0.74193082 0.99616816]. \t  1.6009524125311398 \t 2.524990008735946\n",
      "13     \t [0.27237542 0.29248624 0.3262406 ]. \t  0.6658020777975745 \t 2.524990008735946\n",
      "14     \t [0.04351292 0.03821347 0.57005865]. \t  0.1315384795371757 \t 2.524990008735946\n",
      "15     \t [0.0193673  0.94170574 0.10132114]. \t  0.006200678197065262 \t 2.524990008735946\n",
      "16     \t [0.91803025 0.98243139 0.63030969]. \t  0.41606156342182465 \t 2.524990008735946\n",
      "17     \t [0.63913968 0.03275057 0.2196956 ]. \t  0.6990875164937015 \t 2.524990008735946\n",
      "18     \t [0.89959783 0.18640486 0.32532141]. \t  0.3721595728327397 \t 2.524990008735946\n",
      "19     \t [0.91429314 0.61394615 0.47079134]. \t  0.2553092982531287 \t 2.524990008735946\n",
      "20     \t [0.94077828 0.98246466 0.09758458]. \t  0.0007216140393015158 \t 2.524990008735946\n",
      "21     \t [0.20373518 0.10396556 0.26788834]. \t  0.9206405090054817 \t 2.524990008735946\n",
      "22     \t [0.51558025 0.23594348 0.914284  ]. \t  1.2812801110476435 \t 2.524990008735946\n",
      "23     \t [0.22373008 0.98392242 0.62774342]. \t  2.272209256080007 \t 2.524990008735946\n",
      "24     \t [0.9365443  0.31151369 0.92102654]. \t  1.824166726454697 \t 2.524990008735946\n",
      "25     \t [0.19715124 0.00517215 0.06584252]. \t  0.23904680250278545 \t 2.524990008735946\n",
      "26     \t [0.80542875 0.42263181 0.01153591]. \t  0.031185090331500635 \t 2.524990008735946\n",
      "27     \t [0.00146875 0.83148045 0.56000057]. \t  \u001b[92m2.96169336072112\u001b[0m \t 2.96169336072112\n",
      "28     \t [0.         0.72338906 0.80323217]. \t  2.853187203022144 \t 2.96169336072112\n",
      "29     \t [0.11647277 0.45705134 0.48522976]. \t  0.6400691058291559 \t 2.96169336072112\n",
      "30     \t [0.59033076 0.06363243 0.49189957]. \t  0.21782042126473788 \t 2.96169336072112\n",
      "31     \t [0.68179021 0.95676262 0.86552895]. \t  0.834328952573477 \t 2.96169336072112\n",
      "32     \t [0.56369632 0.30227669 0.67556081]. \t  1.2152410437939047 \t 2.96169336072112\n",
      "33     \t [0.11717895 0.51838382 0.90175872]. \t  \u001b[92m3.5454474450820195\u001b[0m \t 3.5454474450820195\n",
      "34     \t [0.5481816  0.50370555 0.2956876 ]. \t  0.25703180611592874 \t 3.5454474450820195\n",
      "35     \t [0.36087767 0.61068344 0.74838167]. \t  2.973602527068601 \t 3.5454474450820195\n",
      "36     \t [0.         1.         0.55144538]. \t  2.48828899435288 \t 3.5454474450820195\n",
      "37     \t [0.14524698 0.73480978 0.78685137]. \t  2.739250492595711 \t 3.5454474450820195\n",
      "38     \t [0.69855006 0.19290808 0.00415494]. \t  0.08535178549929102 \t 3.5454474450820195\n",
      "39     \t [0.03721759 0.89689153 0.75204622]. \t  1.6713578827737994 \t 3.5454474450820195\n",
      "40     \t [0.11450535 0.38877479 0.23858203]. \t  0.39830622429073204 \t 3.5454474450820195\n",
      "41     \t [0.09816446 0.74341611 0.27680073]. \t  0.2658459977900028 \t 3.5454474450820195\n",
      "42     \t [0.00960704 0.26918501 0.17780091]. \t  0.4245888949045053 \t 3.5454474450820195\n",
      "43     \t [0.49653534 0.94903274 0.57404065]. \t  1.8443640336229983 \t 3.5454474450820195\n",
      "44     \t [0.00791927 0.3957235  0.08991408]. \t  0.12148097787753802 \t 3.5454474450820195\n",
      "45     \t [0.59474975 0.87943433 0.23196203]. \t  0.06743639217319873 \t 3.5454474450820195\n",
      "46     \t [0.65894428 0.76166503 0.23018257]. \t  0.05712577600016635 \t 3.5454474450820195\n",
      "47     \t [0.62753153 0.55511343 0.47924242]. \t  0.5373316096470497 \t 3.5454474450820195\n",
      "48     \t [0.45682491 0.64570631 0.4459672 ]. \t  0.9320541455221174 \t 3.5454474450820195\n",
      "49     \t [0.94807395 0.46458315 0.60378518]. \t  0.8156892453882129 \t 3.5454474450820195\n",
      "50     \t [0.27308196 0.64427215 0.40757537]. \t  0.9044354849926574 \t 3.5454474450820195\n",
      "51     \t [0.28314539 0.00285815 0.07177572]. \t  0.27274097784952667 \t 3.5454474450820195\n",
      "52     \t [0.99554601 0.57829    0.9322286 ]. \t  3.119155902893649 \t 3.5454474450820195\n",
      "53     \t [0.56115138 0.04485817 0.05606849]. \t  0.2227992161080657 \t 3.5454474450820195\n",
      "54     \t [0.98920116 0.0248215  0.36647867]. \t  0.2170044321424345 \t 3.5454474450820195\n",
      "55     \t [0.91249575 0.33840021 0.15488226]. \t  0.17297843146191016 \t 3.5454474450820195\n",
      "56     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.5454474450820195\n",
      "57     \t [0.422907   0.7620658  0.41076796]. \t  1.0772340142030297 \t 3.5454474450820195\n",
      "58     \t [0.55878957 0.66713579 0.56502813]. \t  1.3779589664532825 \t 3.5454474450820195\n",
      "59     \t [0.37090652 0.18889771 0.88455317]. \t  1.0565871932050406 \t 3.5454474450820195\n",
      "60     \t [0.69739797 0.08915418 0.06916519]. \t  0.22108680367723801 \t 3.5454474450820195\n",
      "61     \t [0.85557198 0.57367251 0.68673724]. \t  1.819144295018671 \t 3.5454474450820195\n",
      "62     \t [0.23203378 0.41794496 0.03128611]. \t  0.0719632455882965 \t 3.5454474450820195\n",
      "63     \t [0.38558226 0.19148405 0.25973858]. \t  0.9454651432780736 \t 3.5454474450820195\n",
      "64     \t [0.26345614 0.45403693 0.83321725]. \t  3.503332877541431 \t 3.5454474450820195\n",
      "65     \t [0.06710588 0.57006299 0.27212872]. \t  0.20650622491275084 \t 3.5454474450820195\n",
      "66     \t [0.56925757 0.90150448 0.25170707]. \t  0.10227822235163753 \t 3.5454474450820195\n",
      "67     \t [0.07837724 0.81772399 0.98030338]. \t  1.3114332086406393 \t 3.5454474450820195\n",
      "68     \t [0.50016584 0.29638218 0.64331495]. \t  0.934582442883535 \t 3.5454474450820195\n",
      "69     \t [0.83381273 0.97179445 0.73062929]. \t  0.5780568126236292 \t 3.5454474450820195\n",
      "70     \t [0.78997844 0.45990799 0.47618796]. \t  0.2606657712959982 \t 3.5454474450820195\n",
      "71     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.5454474450820195\n",
      "72     \t [0.83868331 0.14753063 0.5079043 ]. \t  0.16519592078183637 \t 3.5454474450820195\n",
      "73     \t [0.3306307  0.45461128 0.25566971]. \t  0.34814944560713534 \t 3.5454474450820195\n",
      "74     \t [0.11257769 0.26654768 0.54955919]. \t  0.386115116736546 \t 3.5454474450820195\n",
      "75     \t [0.65076837 0.18819651 0.9704999 ]. \t  0.6577995178961773 \t 3.5454474450820195\n",
      "76     \t [0.04413445 0.32111221 0.5440949 ]. \t  0.46131651131415014 \t 3.5454474450820195\n",
      "77     \t [0.26588489 0.88394182 0.37817198]. \t  1.0963580191182685 \t 3.5454474450820195\n",
      "78     \t [0.24453041 0.88008028 0.70832336]. \t  1.9916542845969574 \t 3.5454474450820195\n",
      "79     \t [3.53470836e-01 5.04884895e-01 2.06142090e-04]. \t  0.026173774956853777 \t 3.5454474450820195\n",
      "80     \t [0.24131355 0.88185973 0.48672481]. \t  2.497795720181425 \t 3.5454474450820195\n",
      "81     \t [0.24364365 0.35900578 0.90424936]. \t  2.4538321445628757 \t 3.5454474450820195\n",
      "82     \t [0.26084987 0.36019883 0.97777207]. \t  1.6486021935148831 \t 3.5454474450820195\n",
      "83     \t [0.34523056 0.03967511 0.23774049]. \t  0.916197015229224 \t 3.5454474450820195\n",
      "84     \t [0.58560567 0.22961408 0.50522468]. \t  0.26909669789205626 \t 3.5454474450820195\n",
      "85     \t [0.24556394 0.49812268 0.95143745]. \t  2.828978729166592 \t 3.5454474450820195\n",
      "86     \t [0.01895849 0.29888338 0.15063556]. \t  0.33152174386330074 \t 3.5454474450820195\n",
      "87     \t [0.35944912 0.36778263 0.62847366]. \t  1.088953658732462 \t 3.5454474450820195\n",
      "88     \t [0.07735909 0.2245395  0.63856711]. \t  0.6577166796780516 \t 3.5454474450820195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.06917974 0.06494529 0.49902502]. \t  0.1877271021091782 \t 3.5454474450820195\n",
      "90     \t [0.19703774 0.39527822 0.70485073]. \t  2.0156820397306303 \t 3.5454474450820195\n",
      "91     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.5454474450820195\n",
      "92     \t [0.98347018 0.28556126 0.76232988]. \t  1.7263497444515101 \t 3.5454474450820195\n",
      "93     \t [0.94852366 0.85046182 0.41574563]. \t  0.20814695647165224 \t 3.5454474450820195\n",
      "94     \t [0.09395706 0.81511356 0.72220645]. \t  2.2505063092552415 \t 3.5454474450820195\n",
      "95     \t [0.72400274 0.58656412 0.47478491]. \t  0.4520181952185668 \t 3.5454474450820195\n",
      "96     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.5454474450820195\n",
      "97     \t [0.2572601  0.94126286 0.53286899]. \t  2.6743523315883073 \t 3.5454474450820195\n",
      "98     \t [0.62637209 0.36806122 0.12779578]. \t  0.24384738077880563 \t 3.5454474450820195\n",
      "99     \t [0.61627522 0.99132052 0.20447026]. \t  0.030783093232522537 \t 3.5454474450820195\n",
      "100    \t [0.03781833 0.95433331 0.95485782]. \t  0.6629899424553869 \t 3.5454474450820195\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [0.54663172 0.95772822 0.99370791]. \t  0.4914614738653129 \t 0.675391399411646\n",
      "2      \t [0.91932094 0.0125958  0.17618825]. \t  0.2816999755052942 \t 0.675391399411646\n",
      "3      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
      "4      \t [0.83546183 0.15352723 0.88175242]. \t  \u001b[92m0.8118727969835325\u001b[0m \t 0.8118727969835325\n",
      "5      \t [0.04282238 0.98714938 0.99662789]. \t  0.3795921540393995 \t 0.8118727969835325\n",
      "6      \t [0.00657248 0.04994934 0.95603609]. \t  0.22346365116669764 \t 0.8118727969835325\n",
      "7      \t [0.0140605  0.99189634 0.09256457]. \t  0.0043141598608878945 \t 0.8118727969835325\n",
      "8      \t [0.97302872 0.76767204 0.83300021]. \t  \u001b[92m2.2831423836103286\u001b[0m \t 2.2831423836103286\n",
      "9      \t [0.75244603 0.97391009 0.75870158]. \t  0.6651080101915691 \t 2.2831423836103286\n",
      "10     \t [0.00407095 0.01602185 0.22787245]. \t  0.5782528840560536 \t 2.2831423836103286\n",
      "11     \t [0.99078418 0.22121079 0.03507944]. \t  0.05576618977127451 \t 2.2831423836103286\n",
      "12     \t [0.98562056 0.3532819  0.97238955]. \t  1.6129586582258908 \t 2.2831423836103286\n",
      "13     \t [0.72818255 0.04068818 0.51966219]. \t  0.1375604883842715 \t 2.2831423836103286\n",
      "14     \t [0.21561842 0.81296904 0.1185526 ]. \t  0.013073952399992744 \t 2.2831423836103286\n",
      "15     \t [0.11631352 0.02556041 0.60513699]. \t  0.14369463056464749 \t 2.2831423836103286\n",
      "16     \t [0.7664446 0.6360279 0.767858 ]. \t  \u001b[92m2.8246323331682794\u001b[0m \t 2.8246323331682794\n",
      "17     \t [0.48254864 0.61420849 0.90966464]. \t  \u001b[92m3.4400563356463536\u001b[0m \t 3.4400563356463536\n",
      "18     \t [0.02592365 0.78832521 0.71176665]. \t  2.3541425416158104 \t 3.4400563356463536\n",
      "19     \t [0.73772067 0.50014702 0.94045278]. \t  2.9743049960010755 \t 3.4400563356463536\n",
      "20     \t [0.26761052 0.8975585  0.76765919]. \t  1.559581278102397 \t 3.4400563356463536\n",
      "21     \t [0.86095053 0.97464702 0.91172435]. \t  0.6481411249424998 \t 3.4400563356463536\n",
      "22     \t [0.97581581 0.14663182 0.44064067]. \t  0.15356793092299648 \t 3.4400563356463536\n",
      "23     \t [0.02843244 0.5384091  0.99128982]. \t  2.2016692511646556 \t 3.4400563356463536\n",
      "24     \t [0.70238185 0.1683291  0.57459002]. \t  0.2731710807913837 \t 3.4400563356463536\n",
      "25     \t [0.86044025 0.53007868 0.38860755]. \t  0.1435598115427928 \t 3.4400563356463536\n",
      "26     \t [0.01537935 0.26817864 0.34648792]. \t  0.4768440140289208 \t 3.4400563356463536\n",
      "27     \t [0.16104933 0.76869031 0.71439035]. \t  2.4356809957041783 \t 3.4400563356463536\n",
      "28     \t [0.5990851  0.44034166 0.16274615]. \t  0.2182581964935925 \t 3.4400563356463536\n",
      "29     \t [0.46911668 0.41725862 0.78373556]. \t  2.9645303334571738 \t 3.4400563356463536\n",
      "30     \t [0.55919641 0.15706264 0.21489171]. \t  0.8132787973204357 \t 3.4400563356463536\n",
      "31     \t [0.4171144  0.28870291 0.69791646]. \t  1.3512214601488175 \t 3.4400563356463536\n",
      "32     \t [0.36057657 0.23489665 0.43238393]. \t  0.43665360851459445 \t 3.4400563356463536\n",
      "33     \t [0.23169721 0.43114394 0.52938959]. \t  0.7036496618670851 \t 3.4400563356463536\n",
      "34     \t [0.16399227 0.29837935 0.1023087 ]. \t  0.2806156700136531 \t 3.4400563356463536\n",
      "35     \t [0.95184173 0.6875272  0.16795367]. \t  0.01319718155864041 \t 3.4400563356463536\n",
      "36     \t [0.65257082 0.75698146 0.23990171]. \t  0.06810984282763881 \t 3.4400563356463536\n",
      "37     \t [0.39538804 0.9042213  0.87995339]. \t  1.2484154431092795 \t 3.4400563356463536\n",
      "38     \t [0.85638566 0.3105256  0.90613037]. \t  1.9447564882408375 \t 3.4400563356463536\n",
      "39     \t [0.30206208 0.26687395 0.3378858 ]. \t  0.6979162454139326 \t 3.4400563356463536\n",
      "40     \t [0.35788206 0.31794543 0.8975966 ]. \t  2.11756620489918 \t 3.4400563356463536\n",
      "41     \t [0.31241449 0.381452   0.83839052]. \t  2.943967331975257 \t 3.4400563356463536\n",
      "42     \t [0.16182161 0.3510125  0.21914025]. \t  0.481039053682242 \t 3.4400563356463536\n",
      "43     \t [0.36338206 0.66509103 0.51988038]. \t  1.6999027543704892 \t 3.4400563356463536\n",
      "44     \t [0.46310391 0.20153928 0.66258093]. \t  0.7030942743589284 \t 3.4400563356463536\n",
      "45     \t [0.13314688 0.36283131 0.34132913]. \t  0.4524887204207148 \t 3.4400563356463536\n",
      "46     \t [0.66223288 0.96022554 0.91042743]. \t  0.752947491537213 \t 3.4400563356463536\n",
      "47     \t [0.27687362 0.79075161 0.9371752 ]. \t  1.933725641921452 \t 3.4400563356463536\n",
      "48     \t [0.51236514 0.15357309 0.12380179]. \t  0.5001764571324175 \t 3.4400563356463536\n",
      "49     \t [0.70041028 0.53058182 0.02232165]. \t  0.02154604077917894 \t 3.4400563356463536\n",
      "50     \t [0.3855267  0.32194759 0.06225079]. \t  0.186039907957913 \t 3.4400563356463536\n",
      "51     \t [0.614879   0.01897242 0.51500566]. \t  0.15367501249511611 \t 3.4400563356463536\n",
      "52     \t [0.41167536 0.16385165 0.98067855]. \t  0.5078843597974345 \t 3.4400563356463536\n",
      "53     \t [0.57461273 0.4549721  0.31272816]. \t  0.31286063907099904 \t 3.4400563356463536\n",
      "54     \t [0.37499163 0.63554262 0.10271291]. \t  0.03315445338769701 \t 3.4400563356463536\n",
      "55     \t [0.10772508 0.86123022 0.85728881]. \t  1.684811366751597 \t 3.4400563356463536\n",
      "56     \t [0.1392827  0.79366321 0.04088121]. \t  0.0029033334872716663 \t 3.4400563356463536\n",
      "57     \t [0.29673248 0.72120036 0.82671988]. \t  2.97236620024631 \t 3.4400563356463536\n",
      "58     \t [0.79655151 0.05901964 0.00650744]. \t  0.07260977253929579 \t 3.4400563356463536\n",
      "59     \t [0.95046973 0.09796401 0.89088472]. \t  0.49761056322740577 \t 3.4400563356463536\n",
      "60     \t [0.55213051 0.37210301 0.83852039]. \t  2.8470984778881414 \t 3.4400563356463536\n",
      "61     \t [0.19277311 0.22411141 0.23644552]. \t  0.7916688656986456 \t 3.4400563356463536\n",
      "62     \t [0.02302266 0.55759926 0.85307513]. \t  \u001b[92m3.8182835594078135\u001b[0m \t 3.8182835594078135\n",
      "63     \t [0.23443089 0.12249257 0.53249562]. \t  0.2182628279975289 \t 3.8182835594078135\n",
      "64     \t [0.25033324 0.17223292 0.66275429]. \t  0.5976263057593222 \t 3.8182835594078135\n",
      "65     \t [0.88551001 0.19374512 0.28819094]. \t  0.4188322926123537 \t 3.8182835594078135\n",
      "66     \t [0.29894393 0.69254729 0.07318243]. \t  0.013441052540799628 \t 3.8182835594078135\n",
      "67     \t [0.69204271 0.2617181  0.17389616]. \t  0.4567228690017591 \t 3.8182835594078135\n",
      "68     \t [0.09887742 0.80146926 0.92508264]. \t  1.9310060526836044 \t 3.8182835594078135\n",
      "69     \t [0.26022755 0.94512016 0.60619099]. \t  2.55818934338015 \t 3.8182835594078135\n",
      "70     \t [0.0419944  0.272961   0.60340039]. \t  0.6123246864289482 \t 3.8182835594078135\n",
      "71     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.8182835594078135\n",
      "72     \t [0.79919213 0.55481786 0.02960821]. \t  0.015561981839875073 \t 3.8182835594078135\n",
      "73     \t [0.53323659 0.07847673 0.42246853]. \t  0.4514836825569653 \t 3.8182835594078135\n",
      "74     \t [0.85153165 0.80186875 0.91415992]. \t  1.9319188912520728 \t 3.8182835594078135\n",
      "75     \t [0.18191887 0.98086372 0.22579752]. \t  0.10293195576328541 \t 3.8182835594078135\n",
      "76     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.8182835594078135\n",
      "77     \t [0.05313924 0.61417086 0.68369727]. \t  2.45400306920748 \t 3.8182835594078135\n",
      "78     \t [0.29137267 0.69414151 0.36532438]. \t  0.7023140456625757 \t 3.8182835594078135\n",
      "79     \t [0.56143615 0.11198211 0.34216714]. \t  0.7586939620952345 \t 3.8182835594078135\n",
      "80     \t [0.43159082 0.58183171 0.77795281]. \t  3.33220213936571 \t 3.8182835594078135\n",
      "81     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.8182835594078135\n",
      "82     \t [0.71742585 0.28987715 0.84530822]. \t  2.0254191443497147 \t 3.8182835594078135\n",
      "83     \t [0.87402104 0.06437169 0.84856424]. \t  0.4253112016867315 \t 3.8182835594078135\n",
      "84     \t [0.67455954 0.21569822 0.55133985]. \t  0.28522871971445246 \t 3.8182835594078135\n",
      "85     \t [0.42274541 0.65986703 0.21753009]. \t  0.09514203604510278 \t 3.8182835594078135\n",
      "86     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.8182835594078135\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8182835594078135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.87891146 0.43847854 0.82573944]. \t  3.2745639011521805 \t 3.8182835594078135\n",
      "89     \t [0.07926141 0.02468115 0.92539295]. \t  0.21503412497346253 \t 3.8182835594078135\n",
      "90     \t [0.34679652 0.50690133 0.93526117]. \t  3.116157716524637 \t 3.8182835594078135\n",
      "91     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.8182835594078135\n",
      "92     \t [0.26933286 0.18897611 0.34563904]. \t  0.7759469617211912 \t 3.8182835594078135\n",
      "93     \t [0.99474059 0.75673791 0.07898431]. \t  0.002059736298411999 \t 3.8182835594078135\n",
      "94     \t [0.05616427 0.79463796 0.3497037 ]. \t  0.8003549758232651 \t 3.8182835594078135\n",
      "95     \t [0.69138323 0.75223357 0.59086131]. \t  1.2089808454992501 \t 3.8182835594078135\n",
      "96     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.8182835594078135\n",
      "97     \t [0.64636594 0.08580935 0.32084508]. \t  0.7223969230433356 \t 3.8182835594078135\n",
      "98     \t [0.21669174 0.21331674 0.18901403]. \t  0.708106387725855 \t 3.8182835594078135\n",
      "99     \t [0.73369361 0.68157457 0.39638145]. \t  0.3304032399723793 \t 3.8182835594078135\n",
      "100    \t [0.3689577  0.85363475 0.49223597]. \t  2.201032730366967 \t 3.8182835594078135\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [5.55111512e-17 1.00000000e+00 1.00000000e+00]. \t  0.330219860606422 \t 2.3951473341797507\n",
      "2      \t [1.         0.78315918 1.        ]. \t  1.2696136888697387 \t 2.3951473341797507\n",
      "3      \t [0.         0.09441028 1.        ]. \t  0.2331386467651292 \t 2.3951473341797507\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.3951473341797507\n",
      "5      \t [1.        1.        0.2630082]. \t  0.0185315287765745 \t 2.3951473341797507\n",
      "6      \t [0.4538513  0.53103549 1.        ]. \t  2.059860953174396 \t 2.3951473341797507\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.3951473341797507\n",
      "8      \t [1.         0.44199231 0.58398753]. \t  0.6211391028651039 \t 2.3951473341797507\n",
      "9      \t [0.58304391 1.         0.81188114]. \t  0.6597197106589741 \t 2.3951473341797507\n",
      "10     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.3951473341797507\n",
      "11     \t [0.22286213 0.         0.        ]. \t  0.09590967354051118 \t 2.3951473341797507\n",
      "12     \t [0.         0.55601488 1.        ]. \t  2.0552108106046374 \t 2.3951473341797507\n",
      "13     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.3951473341797507\n",
      "14     \t [0.         0.         0.46761404]. \t  0.18651625739782185 \t 2.3951473341797507\n",
      "15     \t [0.         1.         0.49351143]. \t  2.2108792751556194 \t 2.3951473341797507\n",
      "16     \t [0.51565295 0.         1.        ]. \t  0.09161240246395266 \t 2.3951473341797507\n",
      "17     \t [0.86574336 0.3776575  1.        ]. \t  1.434117188061115 \t 2.3951473341797507\n",
      "18     \t [1.         0.55516297 0.        ]. \t  0.005214508116807005 \t 2.3951473341797507\n",
      "19     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.3951473341797507\n",
      "20     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3951473341797507\n",
      "21     \t [1.         0.         0.42556338]. \t  0.12926987241900426 \t 2.3951473341797507\n",
      "22     \t [0.58613391 0.71362158 0.        ]. \t  0.003010552113144552 \t 2.3951473341797507\n",
      "23     \t [0.         0.3562957  0.74805294]. \t  2.2008552388643547 \t 2.3951473341797507\n",
      "24     \t [0.7147299  0.79651383 1.        ]. \t  1.2319318773132777 \t 2.3951473341797507\n",
      "25     \t [0.69586082 0.         0.        ]. \t  0.07419354354629586 \t 2.3951473341797507\n",
      "26     \t [0.         0.47699718 0.        ]. \t  0.02138733277809581 \t 2.3951473341797507\n",
      "27     \t [0.33635659 1.         1.        ]. \t  0.33419857853162505 \t 2.3951473341797507\n",
      "28     \t [0.68363275 1.         0.        ]. \t  0.0001287744845956681 \t 2.3951473341797507\n",
      "29     \t [0.23686476 0.         0.79112275]. \t  0.25110740350121263 \t 2.3951473341797507\n",
      "30     \t [1.         1.         0.71620665]. \t  0.346697071623864 \t 2.3951473341797507\n",
      "31     \t [0.34147899 0.30324202 0.        ]. \t  0.08270477722671124 \t 2.3951473341797507\n",
      "32     \t [0.75361626 0.         0.7938338 ]. \t  0.24948901132369156 \t 2.3951473341797507\n",
      "33     \t [0.01406967 0.74507084 0.77612789]. \t  \u001b[92m2.5992293824787875\u001b[0m \t 2.5992293824787875\n",
      "34     \t [0.0433834  0.99999999 0.75749597]. \t  1.0827175949796217 \t 2.5992293824787875\n",
      "35     \t [0.74361225 0.75222185 0.46491478]. \t  0.6443150209923949 \t 2.5992293824787875\n",
      "36     \t [0.18918464 0.36004819 1.        ]. \t  1.3653349629240756 \t 2.5992293824787875\n",
      "37     \t [0.97981675 0.10774182 0.69195737]. \t  0.45149321951333465 \t 2.5992293824787875\n",
      "38     \t [0.54243539 0.5414994  0.79105192]. \t  \u001b[92m3.466405347267814\u001b[0m \t 3.466405347267814\n",
      "39     \t [0.35407807 0.65070911 0.81999315]. \t  3.4484516611015783 \t 3.466405347267814\n",
      "40     \t [0.         0.78843857 0.15572137]. \t  0.027764324113983163 \t 3.466405347267814\n",
      "41     \t [0.28965227 0.53553129 0.79757512]. \t  \u001b[92m3.5866746338302997\u001b[0m \t 3.5866746338302997\n",
      "42     \t [0.42165032 0.56549021 0.7801747 ]. \t  3.383244291711323 \t 3.5866746338302997\n",
      "43     \t [0.17184129 0.69007273 0.94621976]. \t  2.5994550962303333 \t 3.5866746338302997\n",
      "44     \t [0.42801352 0.         0.24283669]. \t  0.8476990214635901 \t 3.5866746338302997\n",
      "45     \t [0.34815611 0.5965089  0.81655624]. \t  \u001b[92m3.6694354747583926\u001b[0m \t 3.6694354747583926\n",
      "46     \t [0.40370328 0.63771729 0.76671879]. \t  3.0590597237812185 \t 3.6694354747583926\n",
      "47     \t [1.         0.78214435 0.10107451]. \t  0.002116009691316463 \t 3.6694354747583926\n",
      "48     \t [0.83300991 0.51144661 0.83976683]. \t  \u001b[92m3.6705134383795683\u001b[0m \t 3.6705134383795683\n",
      "49     \t [0.62306214 0.46020946 0.79495449]. \t  3.275182106811876 \t 3.6705134383795683\n",
      "50     \t [0.98833441 0.47650782 0.91576759]. \t  3.124413529168141 \t 3.6705134383795683\n",
      "51     \t [0.65243997 0.55732902 0.78055008]. \t  3.2944134786881425 \t 3.6705134383795683\n",
      "52     \t [0.9687182  0.5787138  0.81179599]. \t  3.4403410185630867 \t 3.6705134383795683\n",
      "53     \t [0.26212259 0.54450501 0.85321913]. \t  \u001b[92m3.856996655758139\u001b[0m \t 3.856996655758139\n",
      "54     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.856996655758139\n",
      "55     \t [0.84903047 0.63062366 0.85083166]. \t  3.5107214991574005 \t 3.856996655758139\n",
      "56     \t [0.35909713 0.49702421 0.7359181 ]. \t  2.7959986265999497 \t 3.856996655758139\n",
      "57     \t [0.24627376 0.52695639 0.84597662]. \t  3.8296340158213 \t 3.856996655758139\n",
      "58     \t [0.4073267  0.58302285 0.90208244]. \t  3.6022454628205707 \t 3.856996655758139\n",
      "59     \t [0.69347949 0.54470352 0.84827183]. \t  3.7857533348717944 \t 3.856996655758139\n",
      "60     \t [0.88818115 0.57691842 0.87719895]. \t  3.662210086264213 \t 3.856996655758139\n",
      "61     \t [0.73597311 0.56345636 0.89913418]. \t  3.5905790126284205 \t 3.856996655758139\n",
      "62     \t [0.84443171 0.61102665 0.84551792]. \t  3.5929005224784207 \t 3.856996655758139\n",
      "63     \t [0.82562056 0.70649131 0.82780613]. \t  2.8819690116407592 \t 3.856996655758139\n",
      "64     \t [0.15112422 0.57169865 0.8695877 ]. \t  3.8131284252695528 \t 3.856996655758139\n",
      "65     \t [0.77587353 0.51426965 0.80072071]. \t  3.4510627829451606 \t 3.856996655758139\n",
      "66     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.856996655758139\n",
      "67     \t [0.48825941 0.54786943 0.84111149]. \t  3.825718612689603 \t 3.856996655758139\n",
      "68     \t [0.50699119 0.56642632 0.82044433]. \t  3.718089006813016 \t 3.856996655758139\n",
      "69     \t [0.96035166 0.57332921 0.8520918 ]. \t  3.6676231391512597 \t 3.856996655758139\n",
      "70     \t [0.18793813 0.64691773 0.77900431]. \t  3.208883002062578 \t 3.856996655758139\n",
      "71     \t [0.87121634 0.51465973 0.8645512 ]. \t  3.675271968418247 \t 3.856996655758139\n",
      "72     \t [0.57211507 0.59941331 0.88418629]. \t  3.6759366690353437 \t 3.856996655758139\n",
      "73     \t [0.23277105 0.57522965 0.8168357 ]. \t  3.726925377999931 \t 3.856996655758139\n",
      "74     \t [0.87332471 0.52849178 0.83137071]. \t  3.6530186803957037 \t 3.856996655758139\n",
      "75     \t [0.61272774 0.59809361 0.85115438]. \t  3.7383989306004715 \t 3.856996655758139\n",
      "76     \t [0.18204843 0.56439098 0.87397583]. \t  3.806974273732531 \t 3.856996655758139\n",
      "77     \t [0.47435298 0.46178707 0.7846849 ]. \t  3.222785205805443 \t 3.856996655758139\n",
      "78     \t [0.13110044 0.58499679 0.79378143]. \t  3.5291900259338043 \t 3.856996655758139\n",
      "79     \t [0.18477611 0.54516117 0.86883023]. \t  3.8234605269924815 \t 3.856996655758139\n",
      "80     \t [0.83193751 0.62176461 0.89276836]. \t  3.468557609229556 \t 3.856996655758139\n",
      "81     \t [0.43129676 0.5334239  0.82135133]. \t  3.742661626962778 \t 3.856996655758139\n",
      "82     \t [0.44806228 0.58681829 0.86771351]. \t  3.7963371090209606 \t 3.856996655758139\n",
      "83     \t [0.49917783 0.45547798 0.90041505]. \t  3.2790073437094103 \t 3.856996655758139\n",
      "84     \t [0.19968569 0.65146239 0.79328599]. \t  3.2975905939014947 \t 3.856996655758139\n",
      "85     \t [0.84681221 0.56751278 0.80904111]. \t  3.4961617709728046 \t 3.856996655758139\n",
      "86     \t [0.233326   0.62877496 0.83387497]. \t  3.6411072632475796 \t 3.856996655758139\n",
      "87     \t [0.83826163 0.55417167 0.85918682]. \t  3.7413436192615395 \t 3.856996655758139\n",
      "88     \t [0.89334596 0.44237723 0.86156751]. \t  3.327007719824448 \t 3.856996655758139\n",
      "89     \t [0.13983935 0.60982176 0.8363651 ]. \t  3.7212576581323322 \t 3.856996655758139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.61686258 0.61564013 0.82760354]. \t  3.585722208299774 \t 3.856996655758139\n",
      "91     \t [0.70822321 0.5796581  0.85518904]. \t  3.759830622179515 \t 3.856996655758139\n",
      "92     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.856996655758139\n",
      "93     \t [0.39429329 0.5761656  0.87345389]. \t  3.8035064243724515 \t 3.856996655758139\n",
      "94     \t [0.03374698 0.59886308 0.8778744 ]. \t  3.700730318087712 \t 3.856996655758139\n",
      "95     \t [0.78403207 0.56596904 0.87503171]. \t  3.7226792510223095 \t 3.856996655758139\n",
      "96     \t [0.24940295 0.48230415 0.79459474]. \t  3.4271376855800293 \t 3.856996655758139\n",
      "97     \t [0.51412726 0.57571    0.8428894 ]. \t  3.804511547600334 \t 3.856996655758139\n",
      "98     \t [0.35321781 0.54229618 0.81064778]. \t  3.691339919948006 \t 3.856996655758139\n",
      "99     \t [0.05766926 0.62078636 0.81547445]. \t  3.5677144595940287 \t 3.856996655758139\n",
      "100    \t [0.33421404 0.4973874  0.83747485]. \t  3.7322947199585452 \t 3.856996655758139\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0. 1. 1.]. \t  0.33021986060642144 \t 2.6229838112516717\n",
      "3      \t [0.        0.7377818 0.       ]. \t  0.0018886469256373063 \t 2.6229838112516717\n",
      "4      \t [0.         0.26102476 1.        ]. \t  0.8221415467856201 \t 2.6229838112516717\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.6229838112516717\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.6229838112516717\n",
      "7      \t [0.54378253 0.5672007  1.        ]. \t  2.0823361198988826 \t 2.6229838112516717\n",
      "8      \t [0.         0.62263047 0.56603114]. \t  1.9005701215968387 \t 2.6229838112516717\n",
      "9      \t [1.         1.         0.39603819]. \t  0.111843166083788 \t 2.6229838112516717\n",
      "10     \t [0.         1.         0.39351285]. \t  1.131446995910848 \t 2.6229838112516717\n",
      "11     \t [1.         0.54543698 0.        ]. \t  0.005671597723928386 \t 2.6229838112516717\n",
      "12     \t [0.48117077 0.26787031 0.        ]. \t  0.0899180964393137 \t 2.6229838112516717\n",
      "13     \t [0.43098202 1.         0.74394691]. \t  0.9566798907709799 \t 2.6229838112516717\n",
      "14     \t [1.         0.54444364 1.        ]. \t  1.998616342092032 \t 2.6229838112516717\n",
      "15     \t [0.         0.         0.64562209]. \t  0.1448346050485284 \t 2.6229838112516717\n",
      "16     \t [0.32702615 0.         1.        ]. \t  0.09169043043641632 \t 2.6229838112516717\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.6229838112516717\n",
      "18     \t [1.         0.         0.48304381]. \t  0.08074480696775062 \t 2.6229838112516717\n",
      "19     \t [0.56840265 0.         0.        ]. \t  0.09073865979701096 \t 2.6229838112516717\n",
      "20     \t [0.18927271 1.         0.        ]. \t  0.00028924317191065125 \t 2.6229838112516717\n",
      "21     \t [0.33104015 0.69346943 0.41091591]. \t  1.0401040618109614 \t 2.6229838112516717\n",
      "22     \t [0.         0.74903586 0.83657511]. \t  \u001b[92m2.723737550400976\u001b[0m \t 2.723737550400976\n",
      "23     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.723737550400976\n",
      "24     \t [1.         0.72238291 0.76717943]. \t  2.120489811134976 \t 2.723737550400976\n",
      "25     \t [0.27025882 0.48603113 0.81592612]. \t  \u001b[92m3.5979052797342783\u001b[0m \t 3.5979052797342783\n",
      "26     \t [0.         1.         0.72672568]. \t  1.2835499448095318 \t 3.5979052797342783\n",
      "27     \t [0.22785876 0.63043242 0.8311443 ]. \t  \u001b[92m3.6234746802099984\u001b[0m \t 3.6234746802099984\n",
      "28     \t [0.         0.29441799 0.        ]. \t  0.056907006171083664 \t 3.6234746802099984\n",
      "29     \t [0.61001842 0.98265572 1.        ]. \t  0.38131391958304656 \t 3.6234746802099984\n",
      "30     \t [0.23907798 0.         0.        ]. \t  0.0972054439942352 \t 3.6234746802099984\n",
      "31     \t [1.        0.        0.7798217]. \t  0.24261137110631148 \t 3.6234746802099984\n",
      "32     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.6234746802099984\n",
      "33     \t [0.11791012 0.54162945 0.83537133]. \t  \u001b[92m3.8112729552377447\u001b[0m \t 3.8112729552377447\n",
      "34     \t [0.         0.         0.24537722]. \t  0.5715606200732086 \t 3.8112729552377447\n",
      "35     \t [0.19462365 0.4626822  0.92054385]. \t  3.100911329131283 \t 3.8112729552377447\n",
      "36     \t [1.         0.32719825 0.23233863]. \t  0.1883354276759473 \t 3.8112729552377447\n",
      "37     \t [0.00000000e+00 1.69182407e-14 1.00000000e+00]. \t  0.09028946765488205 \t 3.8112729552377447\n",
      "38     \t [1.         1.         0.81807872]. \t  0.50565910241385 \t 3.8112729552377447\n",
      "39     \t [0.21838043 0.64462621 0.81509108]. \t  3.475196415868434 \t 3.8112729552377447\n",
      "40     \t [0.         0.63449877 0.99999979]. \t  1.9701118592197593 \t 3.8112729552377447\n",
      "41     \t [0.26232865 0.56170039 0.82889159]. \t  3.805167143061916 \t 3.8112729552377447\n",
      "42     \t [0.21547983 0.50716192 0.78621987]. \t  3.431097558208819 \t 3.8112729552377447\n",
      "43     \t [1.         0.80402148 0.17745506]. \t  0.005820962755150969 \t 3.8112729552377447\n",
      "44     \t [0.00158273 0.47620799 0.81176623]. \t  3.483175193897295 \t 3.8112729552377447\n",
      "45     \t [0.17818074 0.56309976 0.81491434]. \t  3.7243375495271125 \t 3.8112729552377447\n",
      "46     \t [0.73375462 0.         0.28283792]. \t  0.580860532347121 \t 3.8112729552377447\n",
      "47     \t [0.18070678 0.51176841 0.8205742 ]. \t  3.705926011200442 \t 3.8112729552377447\n",
      "48     \t [0.17599678 0.5717011  0.83693779]. \t  \u001b[92m3.8205602756004584\u001b[0m \t 3.8205602756004584\n",
      "49     \t [0.45311606 0.57665961 0.7813888 ]. \t  3.3700679483280775 \t 3.8205602756004584\n",
      "50     \t [0.17641689 0.58032486 0.84848025]. \t  \u001b[92m3.8299521735236515\u001b[0m \t 3.8299521735236515\n",
      "51     \t [1.         0.21196871 1.        ]. \t  0.5866143530062768 \t 3.8299521735236515\n",
      "52     \t [0.27086107 0.58003252 0.81542543]. \t  3.7094113558392126 \t 3.8299521735236515\n",
      "53     \t [0.1001112  0.59143974 0.80407399]. \t  3.594538419837141 \t 3.8299521735236515\n",
      "54     \t [0.2594834  0.5421124  0.81181112]. \t  3.7066852102116314 \t 3.8299521735236515\n",
      "55     \t [0.7396465  0.81582991 0.        ]. \t  0.0006738846048574538 \t 3.8299521735236515\n",
      "56     \t [0.21991799 0.62667498 0.80592198]. \t  3.50387574580012 \t 3.8299521735236515\n",
      "57     \t [0.14516444 0.53118912 0.79171353]. \t  3.5248450143389145 \t 3.8299521735236515\n",
      "58     \t [0.12547343 0.58598869 0.77601423]. \t  3.3592035367884128 \t 3.8299521735236515\n",
      "59     \t [0.00555605 0.50229191 0.21064378]. \t  0.15960970122424306 \t 3.8299521735236515\n",
      "60     \t [0.27661312 0.63517949 0.82127092]. \t  3.5535785620356437 \t 3.8299521735236515\n",
      "61     \t [0.48557621 0.53183717 0.83641186]. \t  3.799903161805562 \t 3.8299521735236515\n",
      "62     \t [0.30013156 1.         0.99999994]. \t  0.3343113932302973 \t 3.8299521735236515\n",
      "63     \t [0.25629721 0.57441184 0.82577019]. \t  3.7779278960340115 \t 3.8299521735236515\n",
      "64     \t [0.01435182 0.52547785 0.83829492]. \t  3.7687741672252457 \t 3.8299521735236515\n",
      "65     \t [0.42816897 0.46804587 0.90267452]. \t  3.3391451977633273 \t 3.8299521735236515\n",
      "66     \t [0.09918691 0.51251405 0.82103803]. \t  3.6962924579227248 \t 3.8299521735236515\n",
      "67     \t [0.48361502 0.55251784 0.80167312]. \t  3.58939416404602 \t 3.8299521735236515\n",
      "68     \t [0.38364572 0.58167674 0.85193783]. \t  \u001b[92m3.8304342164670575\u001b[0m \t 3.8304342164670575\n",
      "69     \t [0.16320906 0.56551466 0.83080553]. \t  3.80387620738451 \t 3.8304342164670575\n",
      "70     \t [0.48225058 0.57478998 0.81533606]. \t  3.6784019818936726 \t 3.8304342164670575\n",
      "71     \t [0.34524529 0.56222715 0.80580203]. \t  3.653136348505176 \t 3.8304342164670575\n",
      "72     \t [0.47846732 0.62957361 0.87197169]. \t  3.62434765148135 \t 3.8304342164670575\n",
      "73     \t [0.30557368 0.58689026 0.82214487]. \t  3.7335266186702007 \t 3.8304342164670575\n",
      "74     \t [0.37779688 0.63857208 0.86269943]. \t  3.611615471646295 \t 3.8304342164670575\n",
      "75     \t [0.00395153 0.50110001 0.85495737]. \t  3.7098941715628513 \t 3.8304342164670575\n",
      "76     \t [0.00644994 0.58259736 0.77864353]. \t  3.3564867701344787 \t 3.8304342164670575\n",
      "77     \t [0.22467551 0.4784806  0.78191485]. \t  3.2935120165094602 \t 3.8304342164670575\n",
      "78     \t [0.06407978 0.52158803 0.89692159]. \t  3.587393217535568 \t 3.8304342164670575\n",
      "79     \t [0.1634043  0.5930341  0.85038782]. \t  3.802050213939024 \t 3.8304342164670575\n",
      "80     \t [0.16875912 0.52945233 0.87197877]. \t  3.7877986386736087 \t 3.8304342164670575\n",
      "81     \t [0.45449189 0.58273501 0.81921612]. \t  3.6973423767602296 \t 3.8304342164670575\n",
      "82     \t [0.32703631 0.60537309 0.81654878]. \t  3.645298443319394 \t 3.8304342164670575\n",
      "83     \t [0.51440356 0.60125863 0.81327972]. \t  3.5852498576025784 \t 3.8304342164670575\n",
      "84     \t [0.02081793 0.55126355 0.8728142 ]. \t  3.776391429765297 \t 3.8304342164670575\n",
      "85     \t [0.41531383 0.49719586 0.89941974]. \t  3.515643998933507 \t 3.8304342164670575\n",
      "86     \t [0.28163422 0.54169825 0.85321745]. \t  \u001b[92m3.8553017463291472\u001b[0m \t 3.8553017463291472\n",
      "87     \t [0.10443614 0.61709583 0.87175505]. \t  3.6805680680282102 \t 3.8553017463291472\n",
      "88     \t [0.25171353 0.6330804  0.00746913]. \t  0.009025791040852426 \t 3.8553017463291472\n",
      "89     \t [0.33299929 0.61259981 0.80549341]. \t  3.5412527482896605 \t 3.8553017463291472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.77516159 0.52573268 0.85581973]. \t  3.7436054413783744 \t 3.8553017463291472\n",
      "91     \t [0.50402864 0.55315445 0.85273797]. \t  3.8405261276392246 \t 3.8553017463291472\n",
      "92     \t [0.25995998 0.55844724 0.88172998]. \t  3.7775920363955837 \t 3.8553017463291472\n",
      "93     \t [0.41348323 0.4890495  0.815733  ]. \t  3.600601976648708 \t 3.8553017463291472\n",
      "94     \t [0.73320515 0.64578794 0.90285158]. \t  3.307445103709022 \t 3.8553017463291472\n",
      "95     \t [0.61679303 0.45459485 0.85515962]. \t  3.493774654786743 \t 3.8553017463291472\n",
      "96     \t [0.72721288 0.60204012 0.89428006]. \t  3.5648575282173596 \t 3.8553017463291472\n",
      "97     \t [0.30397503 0.4859721  0.77150994]. \t  3.209459153279128 \t 3.8553017463291472\n",
      "98     \t [0.44860852 0.57145803 0.87709243]. \t  3.7887995344257606 \t 3.8553017463291472\n",
      "99     \t [0.2538122  0.49403225 0.9046972 ]. \t  3.4477101677204187 \t 3.8553017463291472\n",
      "100    \t [0.75583183 0.54919833 0.05118267]. \t  0.024424117920646648 \t 3.8553017463291472\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.5647137279144399\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.5647137279144399\n",
      "3      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.5647137279144399\n",
      "4      \t [-5.55111512e-17  0.00000000e+00  0.00000000e+00]. \t  0.06797411659013229 \t 0.5647137279144399\n",
      "5      \t [ 1.00000000e+00 -1.11022302e-16  1.00000000e+00]. \t  0.08848201872702724 \t 0.5647137279144399\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.5647137279144399\n",
      "7      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.5647137279144399\n",
      "8      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 0.5647137279144399\n",
      "9      \t [1.         0.50909691 1.        ]. \t  \u001b[92m1.9433340578071387\u001b[0m \t 1.9433340578071387\n",
      "10     \t [0.38292376 0.57377321 1.        ]. \t  \u001b[92m2.0874885170486737\u001b[0m \t 2.0874885170486737\n",
      "11     \t [1.         0.66019004 0.5130736 ]. \t  0.2989872774675264 \t 2.0874885170486737\n",
      "12     \t [0.         0.50096845 1.        ]. \t  1.9700617620952268 \t 2.0874885170486737\n",
      "13     \t [0.47286902 0.         0.        ]. \t  0.09898406626721644 \t 2.0874885170486737\n",
      "14     \t [0.         1.         0.50514696]. \t  \u001b[92m2.2998196141378733\u001b[0m \t 2.2998196141378733\n",
      "15     \t [0.         0.         0.49948482]. \t  0.13708326263714657 \t 2.2998196141378733\n",
      "16     \t [0.54193025 1.         0.69033479]. \t  1.0269802879644434 \t 2.2998196141378733\n",
      "17     \t [0.         0.66888443 0.65179615]. \t  \u001b[92m2.3861508171331205\u001b[0m \t 2.3861508171331205\n",
      "18     \t [1.         0.52006742 0.        ]. \t  0.006999916076454797 \t 2.3861508171331205\n",
      "19     \t [0.         0.49585387 0.        ]. \t  0.018622794389432516 \t 2.3861508171331205\n",
      "20     \t [0.49172612 1.         0.        ]. \t  0.00020737402161531964 \t 2.3861508171331205\n",
      "21     \t [0.54757003 0.         1.        ]. \t  0.09153475329023587 \t 2.3861508171331205\n",
      "22     \t [0.74474605 0.69767208 1.        ]. \t  1.7434465413301312 \t 2.3861508171331205\n",
      "23     \t [0.74869945 0.30251143 1.        ]. \t  1.0379241776280224 \t 2.3861508171331205\n",
      "24     \t [0.44583908 1.         1.        ]. \t  0.33317136528597474 \t 2.3861508171331205\n",
      "25     \t [1.         1.         0.35228784]. \t  0.06942704206690767 \t 2.3861508171331205\n",
      "26     \t [0.         0.29944366 0.74721614]. \t  1.7813784726577588 \t 2.3861508171331205\n",
      "27     \t [1.         0.         0.60267028]. \t  0.09859817703346042 \t 2.3861508171331205\n",
      "28     \t [0.         0.4258058  0.33036649]. \t  0.31654306711244073 \t 2.3861508171331205\n",
      "29     \t [0.30027278 0.         0.38417559]. \t  0.5730535014708381 \t 2.3861508171331205\n",
      "30     \t [0.12730701 1.         0.74895851]. \t  1.1507640806307475 \t 2.3861508171331205\n",
      "31     \t [1.         0.25313484 0.26315889]. \t  0.25209518613387716 \t 2.3861508171331205\n",
      "32     \t [0.20949992 0.1152157  1.        ]. \t  0.2843292033945748 \t 2.3861508171331205\n",
      "33     \t [0.15384172 0.29530564 0.        ]. \t  0.07427221314369875 \t 2.3861508171331205\n",
      "34     \t [0.62509209 1.         0.2935262 ]. \t  0.14885752618689213 \t 2.3861508171331205\n",
      "35     \t [0.94084104 0.79738863 0.83021748]. \t  2.0023363584190825 \t 2.3861508171331205\n",
      "36     \t [0.99675742 0.30556151 0.750839  ]. \t  1.7942656420292966 \t 2.3861508171331205\n",
      "37     \t [7.73497550e-01 7.53321577e-01 3.57923064e-04]. \t  0.0013269615699016053 \t 2.3861508171331205\n",
      "38     \t [0.20719228 0.98815692 0.20647272]. \t  0.06760800787986383 \t 2.3861508171331205\n",
      "39     \t [0.50223746 0.55971449 0.77367579]. \t  \u001b[92m3.2803148020554986\u001b[0m \t 3.2803148020554986\n",
      "40     \t [0.63222491 0.25522537 0.        ]. \t  0.07866750266917386 \t 3.2803148020554986\n",
      "41     \t [0.21390053 0.         0.79205914]. \t  0.2509046149018324 \t 3.2803148020554986\n",
      "42     \t [0.78999218 0.53972365 0.78174464]. \t  3.255517721884081 \t 3.2803148020554986\n",
      "43     \t [0.15449297 0.51907531 0.82292793]. \t  \u001b[92m3.7324535326706134\u001b[0m \t 3.7324535326706134\n",
      "44     \t [0.26627691 0.59083369 0.81000318]. \t  3.6502789110316143 \t 3.7324535326706134\n",
      "45     \t [8.03737508e-01 5.31784813e-08 1.16728473e-01]. \t  0.2505017420229359 \t 3.7324535326706134\n",
      "46     \t [0.20271549 0.53351225 0.71755674]. \t  2.6792010292556427 \t 3.7324535326706134\n",
      "47     \t [0.26329789 0.59382723 0.86367468]. \t  \u001b[92m3.799229020832191\u001b[0m \t 3.799229020832191\n",
      "48     \t [0.         0.73111697 0.85451305]. \t  2.890775734435132 \t 3.799229020832191\n",
      "49     \t [0.15394435 0.53302725 0.86310736]. \t  \u001b[92m3.819717587928337\u001b[0m \t 3.819717587928337\n",
      "50     \t [0.25970086 0.52903409 0.84973846]. \t  \u001b[92m3.8369407232457906\u001b[0m \t 3.8369407232457906\n",
      "51     \t [0.11727442 0.55055644 0.86281119]. \t  3.8314563990564294 \t 3.8369407232457906\n",
      "52     \t [0.27670919 0.52289528 0.86485438]. \t  3.808312349314517 \t 3.8369407232457906\n",
      "53     \t [0.2216276  0.47444207 0.87475812]. \t  3.5747313378838466 \t 3.8369407232457906\n",
      "54     \t [0.58811907 0.64427527 0.81444378]. \t  3.363942764087925 \t 3.8369407232457906\n",
      "55     \t [0.5075295  0.5299826  0.85958244]. \t  3.817874267825693 \t 3.8369407232457906\n",
      "56     \t [0.35788288 0.52804503 0.85734898]. \t  3.8329496689399973 \t 3.8369407232457906\n",
      "57     \t [0.36291565 0.60634218 0.88231056]. \t  3.6924544247186954 \t 3.8369407232457906\n",
      "58     \t [0.20559758 0.51317366 0.8685735 ]. \t  3.7650697601661958 \t 3.8369407232457906\n",
      "59     \t [0.28322249 0.50328082 0.86146526]. \t  3.7581805058970947 \t 3.8369407232457906\n",
      "60     \t [0.31319461 0.53285902 0.85365387]. \t  \u001b[92m3.844612025746662\u001b[0m \t 3.844612025746662\n",
      "61     \t [0.38805383 0.54402555 0.86762466]. \t  3.8329064005076345 \t 3.844612025746662\n",
      "62     \t [0.07707203 0.49791812 0.88968304]. \t  3.570673941664955 \t 3.844612025746662\n",
      "63     \t [0.34771915 0.63526066 0.87279548]. \t  3.609077255816796 \t 3.844612025746662\n",
      "64     \t [0.22085471 0.62046255 0.82264437]. \t  3.631348382529702 \t 3.844612025746662\n",
      "65     \t [0.27864082 0.58777323 0.84343583]. \t  3.8137201347840946 \t 3.844612025746662\n",
      "66     \t [0.49797042 0.54814926 0.82751153]. \t  3.771257319485722 \t 3.844612025746662\n",
      "67     \t [0.28728705 0.55313217 0.79621593]. \t  3.5824537748489185 \t 3.844612025746662\n",
      "68     \t [0.34688249 0.46735149 0.86771699]. \t  3.5705717541078696 \t 3.844612025746662\n",
      "69     \t [0.42900547 0.62846122 0.        ]. \t  0.0085971183349662 \t 3.844612025746662\n",
      "70     \t [0.32728497 0.66209167 0.83103027]. \t  3.43297373403694 \t 3.844612025746662\n",
      "71     \t [0.04278871 0.48416445 0.85163294]. \t  3.6523348138771854 \t 3.844612025746662\n",
      "72     \t [0.53063382 0.6016113  0.81888066]. \t  3.618356221806928 \t 3.844612025746662\n",
      "73     \t [0.48060546 0.64944957 0.85212793]. \t  3.5307626273104047 \t 3.844612025746662\n",
      "74     \t [0.49615083 0.56714246 0.83883278]. \t  3.809122608160155 \t 3.844612025746662\n",
      "75     \t [0.49625873 0.65302754 0.83810015]. \t  3.4736051096480525 \t 3.844612025746662\n",
      "76     \t [0.72272844 0.99694337 0.83552732]. \t  0.6084077238083911 \t 3.844612025746662\n",
      "77     \t [0.23863015 0.59204473 0.86968537]. \t  3.7874536955201155 \t 3.844612025746662\n",
      "78     \t [0.16027789 0.55694087 0.83317501]. \t  3.8165136607904513 \t 3.844612025746662\n",
      "79     \t [0.33698922 0.5975874  0.82707255]. \t  3.7276227744909494 \t 3.844612025746662\n",
      "80     \t [0.12126507 0.59471558 0.8692209 ]. \t  3.7668773978701235 \t 3.844612025746662\n",
      "81     \t [0.37133005 0.50762501 0.87410252]. \t  3.7322761984047457 \t 3.844612025746662\n",
      "82     \t [0.13717311 0.50563222 0.85500839]. \t  3.759777093862554 \t 3.844612025746662\n",
      "83     \t [0.23629012 0.55805526 0.87813113]. \t  3.7953423524067977 \t 3.844612025746662\n",
      "84     \t [0.15010992 0.45065388 0.83599841]. \t  3.4756929481425063 \t 3.844612025746662\n",
      "85     \t [0.50115519 0.52666634 0.83816197]. \t  3.794643078401573 \t 3.844612025746662\n",
      "86     \t [0.26054006 0.62774591 0.85186853]. \t  3.6802388525548717 \t 3.844612025746662\n",
      "87     \t [0.42325143 0.51275926 0.86073251]. \t  3.7873070245278697 \t 3.844612025746662\n",
      "88     \t [0.04526332 0.53483161 0.79514675]. \t  3.537975140431649 \t 3.844612025746662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.12867569 0.6140194  0.89088936]. \t  3.596919807667831 \t 3.844612025746662\n",
      "90     \t [0.22236784 0.55235408 0.84878838]. \t  \u001b[92m3.8569275115013726\u001b[0m \t 3.8569275115013726\n",
      "91     \t [0.46642822 0.53798483 0.8852377 ]. \t  3.7372490242008842 \t 3.8569275115013726\n",
      "92     \t [0.27683599 0.58482713 0.85206127]. \t  3.8299211683753036 \t 3.8569275115013726\n",
      "93     \t [0.62128776 0.58337726 0.81558953]. \t  3.614300625629847 \t 3.8569275115013726\n",
      "94     \t [0.00671729 0.74485106 0.16687828]. \t  0.03673916291455573 \t 3.8569275115013726\n",
      "95     \t [0.1387833  0.57554093 0.90153822]. \t  3.6024449952793187 \t 3.8569275115013726\n",
      "96     \t [0.45539508 0.5019613  0.83303289]. \t  3.724586236813872 \t 3.8569275115013726\n",
      "97     \t [0.82531367 0.52460042 0.84448228]. \t  3.712192099793732 \t 3.8569275115013726\n",
      "98     \t [0.17207473 0.51920585 0.85439079]. \t  3.8064718298775064 \t 3.8569275115013726\n",
      "99     \t [0.30441371 0.60621378 0.8849237 ]. \t  3.679484543921439 \t 3.8569275115013726\n",
      "100    \t [0.44711506 0.60559085 0.85921244]. \t  3.7553093492139897 \t 3.8569275115013726\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [ 0.00000000e+00  1.00000000e+00 -1.11022302e-16]. \t  0.00027353676804544496 \t 1.9592421489197056\n",
      "2      \t [0.62509099 0.         1.        ]. \t  0.09126899860100553 \t 1.9592421489197056\n",
      "3      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.9592421489197056\n",
      "4      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.9592421489197056\n",
      "5      \t [1.         0.         0.51081783]. \t  0.06975907686187692 \t 1.9592421489197056\n",
      "6      \t [1.         0.41654422 0.        ]. \t  0.014474393922420604 \t 1.9592421489197056\n",
      "7      \t [0.         1.         0.58765452]. \t  \u001b[92m2.4309178584015765\u001b[0m \t 2.4309178584015765\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.4309178584015765\n",
      "9      \t [1.         0.40754345 1.        ]. \t  1.565013757811742 \t 2.4309178584015765\n",
      "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.4309178584015765\n",
      "11     \t [0.         0.55901066 0.64689651]. \t  2.000988169379626 \t 2.4309178584015765\n",
      "12     \t [0.         0.         0.50484658]. \t  0.13076925656688437 \t 2.4309178584015765\n",
      "13     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4309178584015765\n",
      "14     \t [0.529329 1.       0.      ]. \t  0.00019195739355334237 \t 2.4309178584015765\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4309178584015765\n",
      "16     \t [0. 1. 1.]. \t  0.330219860606422 \t 2.4309178584015765\n",
      "17     \t [0.58644728 1.         1.        ]. \t  0.33056069639350594 \t 2.4309178584015765\n",
      "18     \t [0.23121743 0.41064546 1.        ]. \t  1.6371949747621004 \t 2.4309178584015765\n",
      "19     \t [0.5309946  0.33511245 0.71877982]. \t  1.7952218673627856 \t 2.4309178584015765\n",
      "20     \t [0.36235722 1.         0.50817806]. \t  1.985722457501246 \t 2.4309178584015765\n",
      "21     \t [0.         0.55530543 0.        ]. \t  0.011517905126239994 \t 2.4309178584015765\n",
      "22     \t [1.         1.         0.38238167]. \t  0.0975373925015914 \t 2.4309178584015765\n",
      "23     \t [0.30809662 0.         0.69127213]. \t  0.19407371634421217 \t 2.4309178584015765\n",
      "24     \t [1.         0.43783056 0.60469931]. \t  0.7871856351242446 \t 2.4309178584015765\n",
      "25     \t [0.         0.63415015 1.        ]. \t  1.9710045872056379 \t 2.4309178584015765\n",
      "26     \t [0.39211704 0.56060218 0.        ]. \t  0.016446858302837136 \t 2.4309178584015765\n",
      "27     \t [0.69774041 0.64001982 1.        ]. \t  1.9623789175901873 \t 2.4309178584015765\n",
      "28     \t [0.         0.9036276  0.31010889]. \t  0.4780632253381209 \t 2.4309178584015765\n",
      "29     \t [0.78724777 0.53361273 0.26943041]. \t  0.12538106140243147 \t 2.4309178584015765\n",
      "30     \t [0.         0.26906739 0.8926295 ]. \t  1.6595982940989742 \t 2.4309178584015765\n",
      "31     \t [0.43776031 0.         0.29731178]. \t  0.83712186846209 \t 2.4309178584015765\n",
      "32     \t [1.         0.72535331 1.        ]. \t  1.570925883172638 \t 2.4309178584015765\n",
      "33     \t [1.         0.80598168 0.18616005]. \t  0.006667525011457558 \t 2.4309178584015765\n",
      "34     \t [1.         0.17063256 0.22531917]. \t  0.27913128543538185 \t 2.4309178584015765\n",
      "35     \t [0.37329358 0.         0.        ]. \t  0.1022406974653381 \t 2.4309178584015765\n",
      "36     \t [0.17616409 0.93404689 0.04732381]. \t  0.0015265065734099035 \t 2.4309178584015765\n",
      "37     \t [1.13987433e-08 2.66361323e-01 0.00000000e+00]. \t  0.06236731827257289 \t 2.4309178584015765\n",
      "38     \t [0.81456312 0.00226894 0.80668485]. \t  0.25319999278873473 \t 2.4309178584015765\n",
      "39     \t [0.50683069 0.57631662 0.81579539]. \t  \u001b[92m3.671533100150011\u001b[0m \t 3.671533100150011\n",
      "40     \t [1.         1.         0.79122546]. \t  0.4657311509084099 \t 3.671533100150011\n",
      "41     \t [0.43789859 0.60617784 0.81953027]. \t  3.636289942635324 \t 3.671533100150011\n",
      "42     \t [0.50119154 0.69158594 0.82203391]. \t  3.1262797292581492 \t 3.671533100150011\n",
      "43     \t [0.19396839 0.52506406 0.84273912]. \t  \u001b[92m3.8169317083358627\u001b[0m \t 3.8169317083358627\n",
      "44     \t [0.24118285 0.55585636 0.78542595]. \t  3.482875910364374 \t 3.8169317083358627\n",
      "45     \t [0.35254141 0.53975265 0.837634  ]. \t  \u001b[92m3.8303910813782536\u001b[0m \t 3.8303910813782536\n",
      "46     \t [0.34669764 0.53840299 0.84049573]. \t  \u001b[92m3.8369747015618754\u001b[0m \t 3.8369747015618754\n",
      "47     \t [0.34583896 0.56537029 0.72283229]. \t  2.726638029289515 \t 3.8369747015618754\n",
      "48     \t [0.58278043 0.98296534 0.27801473]. \t  0.13698306608422572 \t 3.8369747015618754\n",
      "49     \t [0.40718224 0.52209877 0.85595648]. \t  3.8187378945955297 \t 3.8369747015618754\n",
      "50     \t [0.        0.        0.2417697]. \t  0.5686250747638151 \t 3.8369747015618754\n",
      "51     \t [0.25020725 0.60018614 0.79296841]. \t  3.491157229884421 \t 3.8369747015618754\n",
      "52     \t [0.60468849 0.42265256 0.89881956]. \t  3.0459145710786655 \t 3.8369747015618754\n",
      "53     \t [0.56280717 0.99343567 0.65695098]. \t  1.157053850692582 \t 3.8369747015618754\n",
      "54     \t [0.32436383 0.54125149 0.80617419]. \t  3.6601454710924193 \t 3.8369747015618754\n",
      "55     \t [0.47433576 0.52126925 0.79040874]. \t  3.465183073923222 \t 3.8369747015618754\n",
      "56     \t [0.00361544 0.61760212 0.85461232]. \t  3.6813321324021167 \t 3.8369747015618754\n",
      "57     \t [0.36791987 0.58306459 0.89876105]. \t  3.6340830387913625 \t 3.8369747015618754\n",
      "58     \t [0.43775782 0.56235632 0.84066936]. \t  3.830470981307728 \t 3.8369747015618754\n",
      "59     \t [0.52390434 0.57116564 0.84955576]. \t  3.8215643878026837 \t 3.8369747015618754\n",
      "60     \t [0.35732572 0.5704984  0.79716394]. \t  3.5685461966709218 \t 3.8369747015618754\n",
      "61     \t [0.48633675 0.62352658 0.8066559 ]. \t  3.456061298285689 \t 3.8369747015618754\n",
      "62     \t [0.10814934 0.58528213 0.84437903]. \t  3.80388839120915 \t 3.8369747015618754\n",
      "63     \t [0.1396996 0.5275263 0.8484843]. \t  3.819842137758962 \t 3.8369747015618754\n",
      "64     \t [0.07891014 0.57582835 0.8226097 ]. \t  3.738346657774174 \t 3.8369747015618754\n",
      "65     \t [0.29638354 0.46240627 0.8689885 ]. \t  3.5351270243662034 \t 3.8369747015618754\n",
      "66     \t [0.35955485 0.51797221 0.82401753]. \t  3.741218642778799 \t 3.8369747015618754\n",
      "67     \t [0.45617853 0.50451343 0.85198186]. \t  3.7657917700765324 \t 3.8369747015618754\n",
      "68     \t [0.03434399 0.61503807 0.84049126]. \t  3.6869648987519668 \t 3.8369747015618754\n",
      "69     \t [0.99231588 0.24909206 0.79800495]. \t  1.5775541397006907 \t 3.8369747015618754\n",
      "70     \t [0.49550672 0.4808993  0.82965343]. \t  3.6238019484476287 \t 3.8369747015618754\n",
      "71     \t [0.51524657 0.51060444 0.88154452]. \t  3.6916091683005123 \t 3.8369747015618754\n",
      "72     \t [0.30833887 0.50780318 0.87189821]. \t  3.742503876868913 \t 3.8369747015618754\n",
      "73     \t [0.47435515 0.48014884 0.881544  ]. \t  3.569218790854325 \t 3.8369747015618754\n",
      "74     \t [0.36679962 0.57648386 0.82818903]. \t  3.777893355781951 \t 3.8369747015618754\n",
      "75     \t [0.41148259 0.54683101 0.85695995]. \t  \u001b[92m3.851987105943082\u001b[0m \t 3.851987105943082\n",
      "76     \t [0.45203417 0.53567756 0.8016389 ]. \t  3.5944493182566726 \t 3.851987105943082\n",
      "77     \t [0.08338416 0.55633864 0.78803777]. \t  3.4936421793865207 \t 3.851987105943082\n",
      "78     \t [0.31708664 0.59040173 0.839278  ]. \t  3.7960391732963865 \t 3.851987105943082\n",
      "79     \t [0.40492172 0.60526548 0.79235256]. \t  3.4327262873751647 \t 3.851987105943082\n",
      "80     \t [0.54544629 0.49027786 0.93236976]. \t  3.084989589259835 \t 3.851987105943082\n",
      "81     \t [0.30747649 0.55663448 0.86805387]. \t  3.839147430840564 \t 3.851987105943082\n",
      "82     \t [0.33132233 0.5097567  0.82340787]. \t  3.71970439598168 \t 3.851987105943082\n",
      "83     \t [0.23977732 0.58550407 0.83966773]. \t  3.8099931620934813 \t 3.851987105943082\n",
      "84     \t [0.72757761 0.56533101 0.84228593]. \t  3.7527504284876114 \t 3.851987105943082\n",
      "85     \t [0.37562798 0.52557347 0.84284278]. \t  3.821172373145294 \t 3.851987105943082\n",
      "86     \t [0.61485533 0.59733862 0.78630001]. \t  3.297435206940757 \t 3.851987105943082\n",
      "87     \t [0.21953518 0.53655538 0.84337062]. \t  3.8392266127169483 \t 3.851987105943082\n",
      "88     \t [0.03861899 0.4682362  0.91575414]. \t  3.163239956607132 \t 3.851987105943082\n",
      "89     \t [0.33554709 0.58795255 0.791631  ]. \t  3.493034033110571 \t 3.851987105943082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.07221495 0.56387775 0.8083205 ]. \t  3.6594399407679985 \t 3.851987105943082\n",
      "91     \t [0.19183264 0.46040077 0.86313015]. \t  3.5330100500028396 \t 3.851987105943082\n",
      "92     \t [0.23136974 0.61210155 0.82572952]. \t  3.679649015095695 \t 3.851987105943082\n",
      "93     \t [0.0368064  0.57418365 0.87739794]. \t  3.752782320177969 \t 3.851987105943082\n",
      "94     \t [0.35697267 0.49298132 0.81787027]. \t  3.6338928690176866 \t 3.851987105943082\n",
      "95     \t [0.17577335 0.54394693 0.85257148]. \t  3.8488080252149848 \t 3.851987105943082\n",
      "96     \t [0.31736953 0.61633474 0.74698496]. \t  2.970275328203936 \t 3.851987105943082\n",
      "97     \t [0.36506717 0.52646899 0.82077662]. \t  3.739446618795691 \t 3.851987105943082\n",
      "98     \t [0.0485262  0.62554319 0.82240134]. \t  3.581926022792689 \t 3.851987105943082\n",
      "99     \t [0.05970461 0.53394763 0.87106289]. \t  3.7765887954417665 \t 3.851987105943082\n",
      "100    \t [0.40073814 0.47767456 0.81693613]. \t  3.5613643399533594 \t 3.851987105943082\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.67986874]. \t  \u001b[92m1.7052904164039935\u001b[0m \t 1.7052904164039935\n",
      "2      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.7052904164039935\n",
      "3      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.7052904164039935\n",
      "4      \t [0.00000000e+00 2.77555756e-17 0.00000000e+00]. \t  0.06797411659013229 \t 1.7052904164039935\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.7052904164039935\n",
      "6      \t [0.         0.53221739 1.        ]. \t  \u001b[92m2.0326895086222243\u001b[0m \t 2.0326895086222243\n",
      "7      \t [0.20190965 1.         1.        ]. \t  0.3339855274281683 \t 2.0326895086222243\n",
      "8      \t [0.         0.         0.65041229]. \t  0.14935203607799769 \t 2.0326895086222243\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.0326895086222243\n",
      "10     \t [1.         0.54137861 1.        ]. \t  1.9956949644306317 \t 2.0326895086222243\n",
      "11     \t [1.         1.         0.60912288]. \t  0.27832488592576665 \t 2.0326895086222243\n",
      "12     \t [0.5019665 0.        0.       ]. \t  0.09695698608720245 \t 2.0326895086222243\n",
      "13     \t [0.        0.5047207 0.       ]. \t  0.017408203563840135 \t 2.0326895086222243\n",
      "14     \t [0.4714014 1.        0.       ]. \t  0.00021550734736182074 \t 2.0326895086222243\n",
      "15     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.0326895086222243\n",
      "16     \t [1.         0.         0.52491509]. \t  0.0677723012809888 \t 2.0326895086222243\n",
      "17     \t [1.         0.46314745 0.        ]. \t  0.010715666437472751 \t 2.0326895086222243\n",
      "18     \t [0.25135036 1.         0.42455679]. \t  1.4481646925414022 \t 2.0326895086222243\n",
      "19     \t [0.52799957 0.51551688 1.        ]. \t  2.028923176387227 \t 2.0326895086222243\n",
      "20     \t [0.         0.69140427 0.56952599]. \t  \u001b[92m2.3624323980611797\u001b[0m \t 2.3624323980611797\n",
      "21     \t [1.         0.32333552 0.74847973]. \t  1.898277406704096 \t 2.3624323980611797\n",
      "22     \t [0.         0.2829537  0.35021081]. \t  0.43982707516608344 \t 2.3624323980611797\n",
      "23     \t [0.         0.34856398 0.80353598]. \t  \u001b[92m2.517235462233631\u001b[0m \t 2.517235462233631\n",
      "24     \t [0.7962529  0.25909137 1.        ]. \t  0.8116265619146352 \t 2.517235462233631\n",
      "25     \t [0.32047606 0.66222338 0.        ]. \t  0.006121654355110202 \t 2.517235462233631\n",
      "26     \t [1.         0.66669618 0.63199608]. \t  0.9011409827186652 \t 2.517235462233631\n",
      "27     \t [1.         0.24760046 0.27220926]. \t  0.25589818679582155 \t 2.517235462233631\n",
      "28     \t [0.71816424 0.26963111 0.        ]. \t  0.0644182368455749 \t 2.517235462233631\n",
      "29     \t [0.20841841 0.50599474 0.84964312]. \t  \u001b[92m3.773508179552057\u001b[0m \t 3.773508179552057\n",
      "30     \t [0.19620842 0.6515001  0.83359141]. \t  3.516857612961007 \t 3.773508179552057\n",
      "31     \t [0.18233819 0.43184553 0.93057687]. \t  2.7739630361810104 \t 3.773508179552057\n",
      "32     \t [0.20305291 0.19751013 0.        ]. \t  0.10118711705821441 \t 3.773508179552057\n",
      "33     \t [0. 1. 1.]. \t  0.330219860606422 \t 3.773508179552057\n",
      "34     \t [1. 1. 1.]. \t  0.31688363421277377 \t 3.773508179552057\n",
      "35     \t [0.38134386 0.         1.        ]. \t  0.09173486821895892 \t 3.773508179552057\n",
      "36     \t [0.1599027  0.56536309 0.80637937]. \t  3.661029352151215 \t 3.773508179552057\n",
      "37     \t [0.8050414  1.         0.21546816]. \t  0.01913725021169901 \t 3.773508179552057\n",
      "38     \t [0.         0.99999998 0.31899483]. \t  0.4658688023839757 \t 3.773508179552057\n",
      "39     \t [0.14550581 0.57408993 0.81685099]. \t  3.7217331602678296 \t 3.773508179552057\n",
      "40     \t [0.25545533 0.         0.29954702]. \t  0.8136208762736997 \t 3.773508179552057\n",
      "41     \t [0.09134118 0.61428493 0.77213536]. \t  3.255576528050569 \t 3.773508179552057\n",
      "42     \t [0.99999999 0.25573197 0.99999998]. \t  0.7790595081884715 \t 3.773508179552057\n",
      "43     \t [0.75462434 0.         0.26308496]. \t  0.5578707504539464 \t 3.773508179552057\n",
      "44     \t [9.00442718e-01 8.54070827e-01 2.17204754e-08]. \t  0.00026432111710891553 \t 3.773508179552057\n",
      "45     \t [0.07987247 0.54829339 0.78501249]. \t  3.462370214682161 \t 3.773508179552057\n",
      "46     \t [0.79040666 0.57186565 0.86317863]. \t  3.7420277271077866 \t 3.773508179552057\n",
      "47     \t [0.20841862 0.59175042 0.82559742]. \t  3.742111815834075 \t 3.773508179552057\n",
      "48     \t [0.86105066 0.61344115 0.84964845]. \t  3.5861888364813237 \t 3.773508179552057\n",
      "49     \t [0.04314298 0.62005446 0.84302815]. \t  3.67424958555168 \t 3.773508179552057\n",
      "50     \t [0.7366691  0.57516646 0.88034314]. \t  3.7056263919657937 \t 3.773508179552057\n",
      "51     \t [0.91573602 0.51925715 0.85425906]. \t  3.6774792640819167 \t 3.773508179552057\n",
      "52     \t [0.85869587 0.52070533 0.89075893]. \t  3.5740620705493136 \t 3.773508179552057\n",
      "53     \t [0.86981457 0.62426194 0.92194693]. \t  3.1799721023694074 \t 3.773508179552057\n",
      "54     \t [0.10553468 0.5530628  0.83221418]. \t  \u001b[92m3.8033117255501816\u001b[0m \t 3.8033117255501816\n",
      "55     \t [0.6891383  0.45728796 0.86127606]. \t  3.484784817201935 \t 3.8033117255501816\n",
      "56     \t [0.04867539 0.69766464 0.18471998]. \t  0.05647915383250816 \t 3.8033117255501816\n",
      "57     \t [0.20357473 0.55628301 0.86840925]. \t  \u001b[92m3.8319440262041162\u001b[0m \t 3.8319440262041162\n",
      "58     \t [0.76996198 0.57927203 0.80696554]. \t  3.4878133803778955 \t 3.8319440262041162\n",
      "59     \t [0.90211561 0.50686233 0.83913216]. \t  3.629525836952652 \t 3.8319440262041162\n",
      "60     \t [0.8650895  0.53371127 0.85200708]. \t  3.720289991324008 \t 3.8319440262041162\n",
      "61     \t [0.00452646 0.64257718 0.82465819]. \t  3.494309256308317 \t 3.8319440262041162\n",
      "62     \t [0.82490313 0.5189776  0.83829238]. \t  3.685557661253444 \t 3.8319440262041162\n",
      "63     \t [0.83802217 0.50693379 0.81521092]. \t  3.5332856269603314 \t 3.8319440262041162\n",
      "64     \t [0.04408795 0.57798457 0.84008517]. \t  3.7921113395757735 \t 3.8319440262041162\n",
      "65     \t [0.04043466 0.51066878 0.84203392]. \t  3.748158641733193 \t 3.8319440262041162\n",
      "66     \t [0.15400563 0.52530417 0.81711883]. \t  3.713095441084274 \t 3.8319440262041162\n",
      "67     \t [0.20063055 0.68382365 0.85939043]. \t  3.321303335927947 \t 3.8319440262041162\n",
      "68     \t [0.81281975 0.54964765 0.86170043]. \t  3.7493539914105813 \t 3.8319440262041162\n",
      "69     \t [0.09530983 0.65148108 0.78595269]. \t  3.2357713741541145 \t 3.8319440262041162\n",
      "70     \t [0.02002885 0.5032518  0.8399737 ]. \t  3.714750671918706 \t 3.8319440262041162\n",
      "71     \t [0.04050404 0.5866997  0.80101391]. \t  3.5644127084400448 \t 3.8319440262041162\n",
      "72     \t [0.78614817 0.5556853  0.8457112 ]. \t  3.748075765226866 \t 3.8319440262041162\n",
      "73     \t [0.24029262 0.5295675  0.84033935]. \t  3.8248574561523787 \t 3.8319440262041162\n",
      "74     \t [0.06671213 0.65452017 0.79550893]. \t  3.2827850825885525 \t 3.8319440262041162\n",
      "75     \t [0.15502804 0.56184526 0.81824017]. \t  3.7428704231912895 \t 3.8319440262041162\n",
      "76     \t [0.14094586 0.61848824 0.82370664]. \t  3.6387656188889794 \t 3.8319440262041162\n",
      "77     \t [0.62233579 0.51700422 0.80870554]. \t  3.581264816264473 \t 3.8319440262041162\n",
      "78     \t [0.24776631 0.64592915 0.77972263]. \t  3.2106479779818433 \t 3.8319440262041162\n",
      "79     \t [0.48747186 0.51787539 0.88924274]. \t  3.6660861110604395 \t 3.8319440262041162\n",
      "80     \t [0.38553756 0.58379017 0.90188919]. \t  3.6038980689696194 \t 3.8319440262041162\n",
      "81     \t [0.69942706 0.5680039  0.86485202]. \t  3.775253875561175 \t 3.8319440262041162\n",
      "82     \t [0.02893653 0.51184229 0.79765146]. \t  3.513928410053522 \t 3.8319440262041162\n",
      "83     \t [0.51757397 0.53352683 0.83084111]. \t  3.7737793673120934 \t 3.8319440262041162\n",
      "84     \t [0.70430162 0.56449687 0.83744248]. \t  3.7453379035534673 \t 3.8319440262041162\n",
      "85     \t [0.75912531 0.00118043 0.87291782]. \t  0.2185153578704277 \t 3.8319440262041162\n",
      "86     \t [0.62298929 0.62852473 0.82275675]. \t  3.4924371535722267 \t 3.8319440262041162\n",
      "87     \t [0.48501877 0.58256835 0.83946526]. \t  3.7885323471061048 \t 3.8319440262041162\n",
      "88     \t [0.25779046 0.47917257 0.87196063]. \t  3.6161392604750966 \t 3.8319440262041162\n",
      "89     \t [0.01145243 0.64330293 0.8026184 ]. \t  3.3690139918975 \t 3.8319440262041162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.24555343 0.49133646 0.80079745]. \t  3.51318527993351 \t 3.8319440262041162\n",
      "91     \t [0.80246214 0.43164669 0.85000747]. \t  3.295700619010151 \t 3.8319440262041162\n",
      "92     \t [0.96291128 0.6932867  0.12965136]. \t  0.008177044391513563 \t 3.8319440262041162\n",
      "93     \t [0.73955287 0.57672236 0.8312487 ]. \t  3.684553607776505 \t 3.8319440262041162\n",
      "94     \t [0.5110653  0.4925732  0.82948548]. \t  3.668783324154873 \t 3.8319440262041162\n",
      "95     \t [0.72370138 0.51540631 0.8882946 ]. \t  3.6200732082251488 \t 3.8319440262041162\n",
      "96     \t [0.65458974 0.57688882 0.87168833]. \t  3.7606725368931455 \t 3.8319440262041162\n",
      "97     \t [0.81409674 0.59396796 0.82395496]. \t  3.563915285825249 \t 3.8319440262041162\n",
      "98     \t [0.00307978 0.51657468 0.82973857]. \t  3.7185668224979835 \t 3.8319440262041162\n",
      "99     \t [0.6178117  0.48014489 0.86567363]. \t  3.6175428527405247 \t 3.8319440262041162\n",
      "100    \t [0.60735979 0.57922128 0.83455977]. \t  3.7429202831575448 \t 3.8319440262041162\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.53601501 0.         1.        ]. \t  0.09156501092033291 \t 2.5106636917702634\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.5106636917702634\n",
      "3      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.5106636917702634\n",
      "4      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.5106636917702634\n",
      "5      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.5106636917702634\n",
      "6      \t [0.         0.41246066 1.        ]. \t  1.625711270978191 \t 2.5106636917702634\n",
      "7      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.5106636917702634\n",
      "8      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.5106636917702634\n",
      "9      \t [0.64435151 0.5473125  1.        ]. \t  2.0649795023193755 \t 2.5106636917702634\n",
      "10     \t [1.         0.59755969 0.51016004]. \t  0.27837544851892626 \t 2.5106636917702634\n",
      "11     \t [1.         0.         0.56314952]. \t  0.07459155465268118 \t 2.5106636917702634\n",
      "12     \t [0.         0.61076049 0.5653088 ]. \t  1.8192322839896125 \t 2.5106636917702634\n",
      "13     \t [0.         0.47444666 0.        ]. \t  0.021780018876785916 \t 2.5106636917702634\n",
      "14     \t [0.50073596 0.         0.        ]. \t  0.09705184884417516 \t 2.5106636917702634\n",
      "15     \t [0.82663463 1.         0.51226002]. \t  0.5280567422580106 \t 2.5106636917702634\n",
      "16     \t [0.        1.        0.5494455]. \t  2.4861726219254616 \t 2.5106636917702634\n",
      "17     \t [0.53540442 1.         0.        ]. \t  0.0001894363574828558 \t 2.5106636917702634\n",
      "18     \t [1.         0.44784798 0.        ]. \t  0.011883917143876375 \t 2.5106636917702634\n",
      "19     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.5106636917702634\n",
      "20     \t [0.50779139 1.         1.        ]. \t  0.3321817831579242 \t 2.5106636917702634\n",
      "21     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.5106636917702634\n",
      "22     \t [0.31993903 0.32753889 0.98032841]. \t  1.4052495862447525 \t 2.5106636917702634\n",
      "23     \t [0.65296032 0.62316263 0.        ]. \t  0.0071655160146461795 \t 2.5106636917702634\n",
      "24     \t [0.29103566 0.27511833 0.        ]. \t  0.08967185670094967 \t 2.5106636917702634\n",
      "25     \t [1.      0.57013 1.     ]. \t  2.008665011061135 \t 2.5106636917702634\n",
      "26     \t [0.78146219 0.17207413 0.        ]. \t  0.06826044395448523 \t 2.5106636917702634\n",
      "27     \t [0.20419763 1.         0.24134748]. \t  0.1310593796289498 \t 2.5106636917702634\n",
      "28     \t [5.29864663e-01 1.67219150e-08 7.32206335e-01]. \t  0.22863610706704182 \t 2.5106636917702634\n",
      "29     \t [0.29764889 1.         0.7854939 ]. \t  0.8880309702533791 \t 2.5106636917702634\n",
      "30     \t [0.98907161 0.72854665 0.80373053]. \t  2.440741021003541 \t 2.5106636917702634\n",
      "31     \t [0.         0.         0.29602878]. \t  0.565904602195855 \t 2.5106636917702634\n",
      "32     \t [0.83004291 0.81837291 0.87911296]. \t  1.90587515655922 \t 2.5106636917702634\n",
      "33     \t [0.01272382 0.36212845 0.46347172]. \t  0.35570428585661606 \t 2.5106636917702634\n",
      "34     \t [1.         1.         0.72794407]. \t  0.36256658374632345 \t 2.5106636917702634\n",
      "35     \t [0.         0.72763956 1.        ]. \t  1.6018090714225302 \t 2.5106636917702634\n",
      "36     \t [0.01454475 0.82227581 0.27325533]. \t  0.269199093348553 \t 2.5106636917702634\n",
      "37     \t [8.04693708e-01 9.78823464e-09 2.75308708e-01]. \t  0.4924565159702439 \t 2.5106636917702634\n",
      "38     \t [0.28971091 0.00231402 0.23979764]. \t  0.8412025298758684 \t 2.5106636917702634\n",
      "39     \t [0.99999999 1.         0.30826962]. \t  0.03837491585198154 \t 2.5106636917702634\n",
      "40     \t [0.13159236 0.75357327 0.        ]. \t  0.0019756523259279964 \t 2.5106636917702634\n",
      "41     \t [2.08190654e-07 7.45882799e-01 8.05205344e-01]. \t  \u001b[92m2.6835462660472404\u001b[0m \t 2.6835462660472404\n",
      "42     \t [0.15816274 0.57191773 0.88869623]. \t  \u001b[92m3.7152302472387118\u001b[0m \t 3.7152302472387118\n",
      "43     \t [0.2187376  0.64488194 0.93064007]. \t  3.0669395302354645 \t 3.7152302472387118\n",
      "44     \t [0.06757625 0.5389819  0.8337767 ]. \t  \u001b[92m3.7917742162508574\u001b[0m \t 3.7917742162508574\n",
      "45     \t [0.18886542 0.54008316 0.77851267]. \t  3.4070414689218564 \t 3.7917742162508574\n",
      "46     \t [0.11584922 0.52061422 0.89680272]. \t  3.597264091803599 \t 3.7917742162508574\n",
      "47     \t [0.15336841 0.54256329 0.86223061]. \t  \u001b[92m3.8339088911501813\u001b[0m \t 3.8339088911501813\n",
      "48     \t [0.1227527  0.55943112 0.82877154]. \t  3.7923153889175403 \t 3.8339088911501813\n",
      "49     \t [0.98780997 0.15549297 0.23760471]. \t  0.3042372643300185 \t 3.8339088911501813\n",
      "50     \t [0.18832902 0.58038744 0.86380664]. \t  3.8222273185079816 \t 3.8339088911501813\n",
      "51     \t [0.11705109 0.60644746 0.8098086 ]. \t  3.5981732708484246 \t 3.8339088911501813\n",
      "52     \t [0.04444451 0.54073921 0.82721758]. \t  3.7613809180932867 \t 3.8339088911501813\n",
      "53     \t [0.08858863 0.5327802  0.88519389]. \t  3.708204961410998 \t 3.8339088911501813\n",
      "54     \t [0.16184387 0.58901879 0.88645656]. \t  3.7052065499523277 \t 3.8339088911501813\n",
      "55     \t [0.2764371  0.49875081 0.81040094]. \t  3.610444500719102 \t 3.8339088911501813\n",
      "56     \t [0.06028896 0.50725533 0.7995812 ]. \t  3.5274702545999874 \t 3.8339088911501813\n",
      "57     \t [0.07202099 0.56991988 0.78711418]. \t  3.4755743417972425 \t 3.8339088911501813\n",
      "58     \t [0.00668549 0.48718495 0.84904449]. \t  3.6560653766697033 \t 3.8339088911501813\n",
      "59     \t [0.7210959  0.53680721 0.86749705]. \t  3.7618323690603557 \t 3.8339088911501813\n",
      "60     \t [0.88385802 0.50923273 0.91741684]. \t  3.276611274136462 \t 3.8339088911501813\n",
      "61     \t [0.25291047 0.50134377 0.87877027]. \t  3.68314006396718 \t 3.8339088911501813\n",
      "62     \t [0.97867971 0.53592481 0.84564993]. \t  3.663710529817563 \t 3.8339088911501813\n",
      "63     \t [0.12963653 0.55076981 0.85642229]. \t  \u001b[92m3.84326727074133\u001b[0m \t 3.84326727074133\n",
      "64     \t [0.05845586 0.58597192 0.81668265]. \t  3.6816958262969965 \t 3.84326727074133\n",
      "65     \t [0.77374429 0.5678907  0.80133869]. \t  3.454951825395743 \t 3.84326727074133\n",
      "66     \t [0.20032656 0.51385866 0.90228229]. \t  3.540166562986497 \t 3.84326727074133\n",
      "67     \t [0.3801684  0.61373906 0.8667593 ]. \t  3.722747463665948 \t 3.84326727074133\n",
      "68     \t [0.3332898  0.54664755 0.83139159]. \t  3.8145753284556596 \t 3.84326727074133\n",
      "69     \t [0.36683769 0.55194795 0.86412241]. \t  \u001b[92m3.847284475685067\u001b[0m \t 3.847284475685067\n",
      "70     \t [0.74088258 0.6183853  0.86747352]. \t  3.6167940324722507 \t 3.847284475685067\n",
      "71     \t [0.04898934 0.4571185  0.83057134]. \t  3.4813625793370218 \t 3.847284475685067\n",
      "72     \t [0.12385677 0.49877271 0.87104149]. \t  3.692283922993142 \t 3.847284475685067\n",
      "73     \t [0.28666939 0.48010564 0.8620845 ]. \t  3.65582150501466 \t 3.847284475685067\n",
      "74     \t [0.18871211 0.56982644 0.82247719]. \t  3.764083858012199 \t 3.847284475685067\n",
      "75     \t [0.99999343 0.47699886 0.85771318]. \t  3.4928267483773894 \t 3.847284475685067\n",
      "76     \t [0.38994458 0.53083786 0.86894035]. \t  3.8116266480656713 \t 3.847284475685067\n",
      "77     \t [0.5277165  0.65386089 0.88897478]. \t  3.4045380771848577 \t 3.847284475685067\n",
      "78     \t [0.90288435 0.53907948 0.82483756]. \t  3.612413998663668 \t 3.847284475685067\n",
      "79     \t [0.25753667 0.58244486 0.82938498]. \t  3.7810964644350458 \t 3.847284475685067\n",
      "80     \t [0.2190721  0.59122499 0.87634319]. \t  3.7626500762580988 \t 3.847284475685067\n",
      "81     \t [0.97922896 0.52095114 0.87261392]. \t  3.623365236884848 \t 3.847284475685067\n",
      "82     \t [0.29387986 0.48991583 0.82218998]. \t  3.6475021700286714 \t 3.847284475685067\n",
      "83     \t [0.86673762 0.50342536 0.77982485]. \t  3.1714789005108046 \t 3.847284475685067\n",
      "84     \t [0.72274968 0.60418878 0.84785165]. \t  3.674357254380366 \t 3.847284475685067\n",
      "85     \t [0.22615944 0.5609198  0.90931301]. \t  3.542721984148122 \t 3.847284475685067\n",
      "86     \t [0.06135312 0.57958481 0.82363923]. \t  3.733048858519077 \t 3.847284475685067\n",
      "87     \t [0.07161124 0.57118638 0.88215564]. \t  3.7401337191350272 \t 3.847284475685067\n",
      "88     \t [0.1608927  0.53846981 0.91413318]. \t  3.4626139637265574 \t 3.847284475685067\n",
      "89     \t [0.03102986 0.48376222 0.82813804]. \t  3.607785925622363 \t 3.847284475685067\n",
      "90     \t [0.24957294 0.5080365  0.78131799]. \t  3.383918972588327 \t 3.847284475685067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.93236601 0.83906466 0.19249369]. \t  0.009360530223960538 \t 3.847284475685067\n",
      "92     \t [0.83662355 0.51504781 0.82231952]. \t  3.5981030499802626 \t 3.847284475685067\n",
      "93     \t [0.4892291  0.56872635 0.83990144]. \t  3.812130597036334 \t 3.847284475685067\n",
      "94     \t [0.53582519 0.6125249  0.90705601]. \t  3.464869953765432 \t 3.847284475685067\n",
      "95     \t [0.00883835 0.6013543  0.83189179]. \t  3.7006348348705727 \t 3.847284475685067\n",
      "96     \t [0.60226808 0.56527437 0.90002825]. \t  3.6142165519154896 \t 3.847284475685067\n",
      "97     \t [0.28897352 0.46675652 0.88104347]. \t  3.5019043779108125 \t 3.847284475685067\n",
      "98     \t [0.04275802 0.55625307 0.83663759]. \t  3.801224806991899 \t 3.847284475685067\n",
      "99     \t [0.76586006 0.55485107 0.85431554]. \t  3.7679018648531404 \t 3.847284475685067\n",
      "100    \t [0.3778065  0.51867278 0.8502434 ]. \t  3.8140234881610064 \t 3.847284475685067\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0.00000000e+00 5.55111512e-17 1.00000000e+00]. \t  0.0902894676548261 \t 1.6237282255098657\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
      "3      \t [0.00000000e+00 1.00000000e+00 1.11022302e-16]. \t  0.0002735367680454468 \t 1.6237282255098657\n",
      "4      \t [-5.55111512e-17  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 1.6237282255098657\n",
      "5      \t [1.         0.         0.72242562]. \t  0.21355346368311623 \t 1.6237282255098657\n",
      "6      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.6237282255098657\n",
      "7      \t [1.         0.27260468 0.        ]. \t  0.027863381593214852 \t 1.6237282255098657\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.6237282255098657\n",
      "9      \t [0.         0.49622216 0.61809475]. \t  1.513728768406865 \t 1.6237282255098657\n",
      "10     \t [0.2945484  0.         0.58483267]. \t  0.11777147521127185 \t 1.6237282255098657\n",
      "11     \t [0.12642012 1.         0.52128492]. \t  \u001b[92m2.4820037171440976\u001b[0m \t 2.4820037171440976\n",
      "12     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4820037171440976\n",
      "13     \t [1.         1.         0.52614329]. \t  0.24538504060047808 \t 2.4820037171440976\n",
      "14     \t [1.         0.51025569 1.        ]. \t  1.9458767819291134 \t 2.4820037171440976\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4820037171440976\n",
      "16     \t [ 4.47512925e-01  1.00000000e+00 -3.46944695e-18]. \t  0.00022481110816437966 \t 2.4820037171440976\n",
      "17     \t [0.10187928 0.53154089 1.        ]. \t  2.045712035171504 \t 2.4820037171440976\n",
      "18     \t [0.         0.         0.35942747]. \t  0.4507801685896101 \t 2.4820037171440976\n",
      "19     \t [1.         0.27737435 0.37261965]. \t  0.1775731727152571 \t 2.4820037171440976\n",
      "20     \t [0.55236865 1.         0.60186056]. \t  1.3827862045300217 \t 2.4820037171440976\n",
      "21     \t [0.37280588 0.52373687 0.66585975]. \t  1.983675702814709 \t 2.4820037171440976\n",
      "22     \t [0.31748885 0.         0.        ]. \t  0.10143908864575243 \t 2.4820037171440976\n",
      "23     \t [0.         0.76141946 0.29463711]. \t  0.3464247291030922 \t 2.4820037171440976\n",
      "24     \t [0.         1.         0.67762455]. \t  1.7269354967590687 \t 2.4820037171440976\n",
      "25     \t [1.         0.68700697 0.74887681]. \t  2.140842725366009 \t 2.4820037171440976\n",
      "26     \t [0.35964981 0.         1.        ]. \t  0.09172361030866297 \t 2.4820037171440976\n",
      "27     \t [0.24055623 1.         0.82404174]. \t  0.7788550797134279 \t 2.4820037171440976\n",
      "28     \t [0.         1.         0.27658671]. \t  0.24242366377255908 \t 2.4820037171440976\n",
      "29     \t [0.         0.70893547 1.        ]. \t  1.6927479667132743 \t 2.4820037171440976\n",
      "30     \t [0.78853196 0.         0.34235347]. \t  0.4349033000960275 \t 2.4820037171440976\n",
      "31     \t [0.77056944 0.76347283 1.        ]. \t  1.408492740788999 \t 2.4820037171440976\n",
      "32     \t [1.24507434e-11 5.36979071e-01 0.00000000e+00]. \t  0.01345058871992107 \t 2.4820037171440976\n",
      "33     \t [0.35246706 0.68115819 0.        ]. \t  0.005028646874098948 \t 2.4820037171440976\n",
      "34     \t [1.00000000e+00 7.24281736e-01 1.76472981e-09]. \t  0.0009100259557326801 \t 2.4820037171440976\n",
      "35     \t [0.         0.         0.69670833]. \t  0.1945340599545555 \t 2.4820037171440976\n",
      "36     \t [0.33015411 0.85319326 0.49716915]. \t  2.3698079009628965 \t 2.4820037171440976\n",
      "37     \t [0.88786658 0.27226877 0.81374942]. \t  1.8267315011472536 \t 2.4820037171440976\n",
      "38     \t [2.50550532e-08 2.03553813e-01 9.99999995e-01]. \t  0.5652424868093312 \t 2.4820037171440976\n",
      "39     \t [7.61766963e-01 8.64662660e-01 3.46934405e-18]. \t  0.00035745067778128237 \t 2.4820037171440976\n",
      "40     \t [0.72109778 0.         0.85852682]. \t  0.2271406917332125 \t 2.4820037171440976\n",
      "41     \t [0.58429607 1.         1.        ]. \t  0.3306100046039748 \t 2.4820037171440976\n",
      "42     \t [0.7754267 0.        0.       ]. \t  0.06227719553783536 \t 2.4820037171440976\n",
      "43     \t [0.64084234 0.3030856  0.48540896]. \t  0.2759022957314997 \t 2.4820037171440976\n",
      "44     \t [0.8667268  1.         0.80970845]. \t  0.5322947634128489 \t 2.4820037171440976\n",
      "45     \t [0.11253876 0.7530437  0.76507134]. \t  \u001b[92m2.545375490056384\u001b[0m \t 2.545375490056384\n",
      "46     \t [0.76427721 0.69039125 0.62810605]. \t  1.1741164399447304 \t 2.545375490056384\n",
      "47     \t [1.         0.18123672 1.        ]. \t  0.4702009912061038 \t 2.545375490056384\n",
      "48     \t [0.20562486 0.25244498 0.84408948]. \t  1.6850319690015798 \t 2.545375490056384\n",
      "49     \t [2.52635594e-01 4.25971955e-09 1.89715214e-01]. \t  0.699084788482663 \t 2.545375490056384\n",
      "50     \t [0.75708588 0.49723573 0.00733365]. \t  0.019761449606713617 \t 2.545375490056384\n",
      "51     \t [0.38409477 0.99254555 0.31946578]. \t  0.3947939915768515 \t 2.545375490056384\n",
      "52     \t [0.99999968 0.         0.20594664]. \t  0.2358317065816075 \t 2.545375490056384\n",
      "53     \t [0.12101932 0.20436221 0.49532204]. \t  0.2715226359825007 \t 2.545375490056384\n",
      "54     \t [1.         0.79928855 1.        ]. \t  1.182357515246301 \t 2.545375490056384\n",
      "55     \t [0.91900018 0.98870934 0.23242004]. \t  0.016468442067796043 \t 2.545375490056384\n",
      "56     \t [0.16033216 0.84483856 0.52150084]. \t  \u001b[92m2.911630266913285\u001b[0m \t 2.911630266913285\n",
      "57     \t [0.04904669 0.75873304 0.60921363]. \t  2.763424976294205 \t 2.911630266913285\n",
      "58     \t [0.29173139 0.69664516 0.97372331]. \t  2.173262562198805 \t 2.911630266913285\n",
      "59     \t [0.06425837 0.78510516 0.63539664]. \t  2.7381494962966944 \t 2.911630266913285\n",
      "60     \t [0.18048162 0.94281361 0.00304026]. \t  0.0004258878379638619 \t 2.911630266913285\n",
      "61     \t [0.29199105 0.70373976 0.73465665]. \t  2.614605271067986 \t 2.911630266913285\n",
      "62     \t [0.99458018 0.46850079 0.72204817]. \t  2.2715938685534187 \t 2.911630266913285\n",
      "63     \t [0.27339143 0.72029125 0.62819227]. \t  2.459957575449253 \t 2.911630266913285\n",
      "64     \t [0.00400112 0.88825482 0.55232926]. \t  \u001b[92m2.9623720118622177\u001b[0m \t 2.9623720118622177\n",
      "65     \t [0.08692128 0.86926184 0.64586882]. \t  2.6499530838307455 \t 2.9623720118622177\n",
      "66     \t [0.83998708 0.45524194 0.91698858]. \t  \u001b[92m3.0416034926370172\u001b[0m \t 3.0416034926370172\n",
      "67     \t [0.86608666 0.56331172 0.89871057]. \t  \u001b[92m3.5521157238936993\u001b[0m \t 3.5521157238936993\n",
      "68     \t [0.97271607 0.52364599 0.86063424]. \t  \u001b[92m3.6587375366508113\u001b[0m \t 3.6587375366508113\n",
      "69     \t [0.71842608 0.56866578 0.85959434]. \t  \u001b[92m3.773985313872086\u001b[0m \t 3.773985313872086\n",
      "70     \t [0.06553614 0.86541518 0.57227386]. \t  3.0659479126964952 \t 3.773985313872086\n",
      "71     \t [0.67681719 0.52661603 0.85870288]. \t  \u001b[92m3.7745252416454917\u001b[0m \t 3.7745252416454917\n",
      "72     \t [0.89434076 0.50905017 0.83299705]. \t  3.616643229768447 \t 3.7745252416454917\n",
      "73     \t [0.93997681 0.56911958 0.81289663]. \t  3.4841658852603112 \t 3.7745252416454917\n",
      "74     \t [0.74963805 0.53001401 0.897903  ]. \t  3.572428581893354 \t 3.7745252416454917\n",
      "75     \t [0.57518722 0.43545297 0.83292191]. \t  3.357956095640593 \t 3.7745252416454917\n",
      "76     \t [0.71140475 0.47609772 0.8409448 ]. \t  3.5850207350519465 \t 3.7745252416454917\n",
      "77     \t [0.86894451 0.58416537 0.85991656]. \t  3.6907852128344443 \t 3.7745252416454917\n",
      "78     \t [0.79234105 0.53343616 0.83973691]. \t  3.7235932735871 \t 3.7745252416454917\n",
      "79     \t [0.55666932 0.60737495 0.93191654]. \t  3.183902487462192 \t 3.7745252416454917\n",
      "80     \t [0.84006578 0.44690442 0.92718851]. \t  2.8675632175916403 \t 3.7745252416454917\n",
      "81     \t [0.68086663 0.60716435 0.90490368]. \t  3.473922768139772 \t 3.7745252416454917\n",
      "82     \t [0.67727952 0.54831169 0.90166368]. \t  3.5810933739950856 \t 3.7745252416454917\n",
      "83     \t [0.63055192 0.60691467 0.75545619]. \t  2.878362776019965 \t 3.7745252416454917\n",
      "84     \t [0.82690654 0.54941981 0.88848604]. \t  3.6421841929092578 \t 3.7745252416454917\n",
      "85     \t [0.61002647 0.57701745 0.76927411]. \t  3.147197184876285 \t 3.7745252416454917\n",
      "86     \t [0.5822753  0.66456719 0.92699188]. \t  2.9906348190677066 \t 3.7745252416454917\n",
      "87     \t [0.74668248 0.48756095 0.88059323]. \t  3.55902279984581 \t 3.7745252416454917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.6540553  0.54640565 0.79464782]. \t  3.4599575936105356 \t 3.7745252416454917\n",
      "89     \t [0.8841064  0.57530021 0.83557712]. \t  3.648607244943269 \t 3.7745252416454917\n",
      "90     \t [0.67186964 0.52074167 0.92586305]. \t  3.2595823482385162 \t 3.7745252416454917\n",
      "91     \t [0.89238957 0.51104164 0.8549311 ]. \t  3.667480842491953 \t 3.7745252416454917\n",
      "92     \t [0.66885226 0.57050478 0.88276053]. \t  3.7187650958225165 \t 3.7745252416454917\n",
      "93     \t [0.90035736 0.60478315 0.79057308]. \t  3.1700194908248287 \t 3.7745252416454917\n",
      "94     \t [0.60109558 0.56285846 0.91295849]. \t  3.4840978184976024 \t 3.7745252416454917\n",
      "95     \t [0.59559431 0.60536316 0.87923851]. \t  3.675957651663423 \t 3.7745252416454917\n",
      "96     \t [0.80863706 0.59645281 0.87176446]. \t  3.665503352526258 \t 3.7745252416454917\n",
      "97     \t [0.64304536 0.57858793 0.92807557]. \t  3.277951871253468 \t 3.7745252416454917\n",
      "98     \t [0.79655235 0.52445673 0.79255321]. \t  3.3723694054037994 \t 3.7745252416454917\n",
      "99     \t [0.80998165 0.55362968 0.78586864]. \t  3.2872957730046752 \t 3.7745252416454917\n",
      "100    \t [0.62027089 0.61215024 0.86447579]. \t  3.685502546991401 \t 3.7745252416454917\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.8830091449513892\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.8830091449513892\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.8830091449513892\n",
      "4      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.8830091449513892\n",
      "5      \t [0.00000000e+00 1.00000000e+00 5.55111512e-17]. \t  0.0002735367680454459 \t 0.8830091449513892\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 0.8830091449513892\n",
      "7      \t [0.         0.40278726 1.        ]. \t  \u001b[92m1.5772680091587468\u001b[0m \t 1.5772680091587468\n",
      "8      \t [0.5268609  0.56127941 1.        ]. \t  \u001b[92m2.0826736999527293\u001b[0m \t 2.0826736999527293\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.0826736999527293\n",
      "10     \t [0.3298191 0.        1.       ]. \t  0.09169403483553269 \t 2.0826736999527293\n",
      "11     \t [ 0.00000000e+00  5.08030544e-01 -2.77555756e-17]. \t  0.016968882546729392 \t 2.0826736999527293\n",
      "12     \t [1.         0.51313298 1.        ]. \t  1.951982894756909 \t 2.0826736999527293\n",
      "13     \t [1.         0.50261923 0.        ]. \t  0.008030532699659365 \t 2.0826736999527293\n",
      "14     \t [1.         1.         0.53149658]. \t  0.24891067621563612 \t 2.0826736999527293\n",
      "15     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.0826736999527293\n",
      "16     \t [0.         1.         0.55728117]. \t  \u001b[92m2.4912414452415304\u001b[0m \t 2.4912414452415304\n",
      "17     \t [0.         0.69602716 0.67204512]. \t  2.4712297309931928 \t 2.4912414452415304\n",
      "18     \t [0.47925655 0.         0.        ]. \t  0.09857836787552751 \t 2.4912414452415304\n",
      "19     \t [0.55419004 1.         0.73356264]. \t  0.8299382694974483 \t 2.4912414452415304\n",
      "20     \t [0.6953475  0.34700554 0.79525455]. \t  2.4690713626740366 \t 2.4912414452415304\n",
      "21     \t [0.76134018 0.27122906 1.        ]. \t  0.8740604846617552 \t 2.4912414452415304\n",
      "22     \t [0.26704729 0.47078547 0.79990309]. \t  \u001b[92m3.420626040733924\u001b[0m \t 3.420626040733924\n",
      "23     \t [1.         0.         0.60826808]. \t  0.10314677162218962 \t 3.420626040733924\n",
      "24     \t [1.         0.59629133 0.72344649]. \t  2.207693166351705 \t 3.420626040733924\n",
      "25     \t [ 4.05997130e-01  1.00000000e+00 -3.46944695e-18]. \t  0.0002401138206310765 \t 3.420626040733924\n",
      "26     \t [0.20791697 0.80216963 0.8090762 ]. \t  2.2448529897145453 \t 3.420626040733924\n",
      "27     \t [1.         0.27969939 0.79196805]. \t  1.8144484476282354 \t 3.420626040733924\n",
      "28     \t [0.         0.32265509 0.7566934 ]. \t  2.0235728621964206 \t 3.420626040733924\n",
      "29     \t [0.74638994 1.         0.2527126 ]. \t  0.04925010507290087 \t 3.420626040733924\n",
      "30     \t [0.24287083 0.24893061 0.        ]. \t  0.09393752153718593 \t 3.420626040733924\n",
      "31     \t [0.02109294 0.31213142 0.31455571]. \t  0.4683325451959228 \t 3.420626040733924\n",
      "32     \t [0.43200225 1.         1.        ]. \t  0.33335433076672005 \t 3.420626040733924\n",
      "33     \t [0.68971825 0.38412976 0.        ]. \t  0.04218869563601234 \t 3.420626040733924\n",
      "34     \t [0.34797855 0.23208253 0.62295725]. \t  0.6053961745273174 \t 3.420626040733924\n",
      "35     \t [1.         0.82778976 0.23156161]. \t  0.013734525366328083 \t 3.420626040733924\n",
      "36     \t [1.20100998e-09 1.00000000e+00 2.67725836e-01]. \t  0.20864708436823212 \t 3.420626040733924\n",
      "37     \t [0.73529885 0.57596285 0.64782515]. \t  1.4529102355171262 \t 3.420626040733924\n",
      "38     \t [8.75315024e-09 0.00000000e+00 2.92489050e-01]. \t  0.5691271414778348 \t 3.420626040733924\n",
      "39     \t [0.73050705 0.         0.80686857]. \t  0.24905382812821508 \t 3.420626040733924\n",
      "40     \t [0.23607229 0.37844857 0.95469295]. \t  2.072222788765622 \t 3.420626040733924\n",
      "41     \t [0.81129252 0.74482251 0.95613756]. \t  2.050820544203273 \t 3.420626040733924\n",
      "42     \t [1.32308483e-09 9.99999956e-01 7.53773944e-01]. \t  1.0884424089748705 \t 3.420626040733924\n",
      "43     \t [0.16327193 0.53093577 0.6763547 ]. \t  2.2122821618096493 \t 3.420626040733924\n",
      "44     \t [5.90699137e-01 1.36190592e-07 2.99654310e-01]. \t  0.7294115471199866 \t 3.420626040733924\n",
      "45     \t [0.99999993 0.         0.155202  ]. \t  0.1810975378845152 \t 3.420626040733924\n",
      "46     \t [0.        0.7610155 1.       ]. \t  1.4269633322036994 \t 3.420626040733924\n",
      "47     \t [0.16521725 0.85547041 0.00303941]. \t  0.0007863552504010874 \t 3.420626040733924\n",
      "48     \t [0.48719898 0.6956305  0.83133868]. \t  3.141577048687215 \t 3.420626040733924\n",
      "49     \t [0.79892171 0.         0.02584488]. \t  0.08710313854967074 \t 3.420626040733924\n",
      "50     \t [0.30229938 0.         0.09787786]. \t  0.36374374120742403 \t 3.420626040733924\n",
      "51     \t [1.         0.18900269 0.01715273]. \t  0.043985069514993494 \t 3.420626040733924\n",
      "52     \t [0.3005716  0.99721858 0.35211055]. \t  0.6732986741291188 \t 3.420626040733924\n",
      "53     \t [0.9654361  0.79748553 0.79583719]. \t  1.7943912146292629 \t 3.420626040733924\n",
      "54     \t [1.         0.80954628 0.99999992]. \t  1.1269926180324774 \t 3.420626040733924\n",
      "55     \t [0.73639967 0.56639845 0.87950313]. \t  \u001b[92m3.719363340464829\u001b[0m \t 3.719363340464829\n",
      "56     \t [0.51866603 0.54593488 0.83646783]. \t  \u001b[92m3.8045473121396887\u001b[0m \t 3.8045473121396887\n",
      "57     \t [0.55480589 0.49092082 0.88741311]. \t  3.5729652953829993 \t 3.8045473121396887\n",
      "58     \t [0.50930287 0.52151501 0.87977252]. \t  3.7330412128572767 \t 3.8045473121396887\n",
      "59     \t [0.50560889 0.62006241 0.77733469]. \t  3.1790051455101325 \t 3.8045473121396887\n",
      "60     \t [0.64281667 0.45634269 0.90579636]. \t  3.2136133808280314 \t 3.8045473121396887\n",
      "61     \t [0.92886572 0.56500471 0.83014894]. \t  3.618100670484024 \t 3.8045473121396887\n",
      "62     \t [0.87278992 0.44551122 0.89059125]. \t  3.2168431665078487 \t 3.8045473121396887\n",
      "63     \t [0.69469631 0.53422162 0.90911349]. \t  3.485759003063221 \t 3.8045473121396887\n",
      "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.8045473121396887\n",
      "65     \t [0.47653011 0.509539   0.81471931]. \t  3.648869132945113 \t 3.8045473121396887\n",
      "66     \t [0.45603655 0.41528063 0.80267407]. \t  3.0951814259497166 \t 3.8045473121396887\n",
      "67     \t [0.48432799 0.55004742 0.79642113]. \t  3.540623612775474 \t 3.8045473121396887\n",
      "68     \t [0.87179845 0.43865121 0.90859776]. \t  3.01423329348366 \t 3.8045473121396887\n",
      "69     \t [0.54194997 0.56165652 0.83061741]. \t  3.770231389700418 \t 3.8045473121396887\n",
      "70     \t [0.92207377 0.52334432 0.88189628]. \t  3.6117070130139544 \t 3.8045473121396887\n",
      "71     \t [0.37065236 0.56154372 0.95393641]. \t  2.9142596336441584 \t 3.8045473121396887\n",
      "72     \t [0.29206545 0.52580935 0.83811717]. \t  \u001b[92m3.8144933172141657\u001b[0m \t 3.8144933172141657\n",
      "73     \t [0.65374409 0.48195269 0.86115928]. \t  3.6290187462189936 \t 3.8144933172141657\n",
      "74     \t [0.33566481 0.55382204 0.92684717]. \t  3.329553033547877 \t 3.8144933172141657\n",
      "75     \t [0.37212223 0.55754695 0.87933418]. \t  3.7917278305402933 \t 3.8144933172141657\n",
      "76     \t [0.47194388 0.63602989 0.90489887]. \t  3.3944192777962265 \t 3.8144933172141657\n",
      "77     \t [0.74005154 0.53998419 0.88628843]. \t  3.676958014500469 \t 3.8144933172141657\n",
      "78     \t [0.92385847 0.53814442 0.86506586]. \t  3.6942871588511568 \t 3.8144933172141657\n",
      "79     \t [0.35311964 0.55108644 0.84034931]. \t  \u001b[92m3.8434495197755814\u001b[0m \t 3.8434495197755814\n",
      "80     \t [0.48449423 0.50569442 0.87314253]. \t  3.721354955441691 \t 3.8434495197755814\n",
      "81     \t [0.93192979 0.51147782 0.83448017]. \t  3.6124824065061665 \t 3.8434495197755814\n",
      "82     \t [0.40943426 0.49718776 0.86570501]. \t  3.7228340813492276 \t 3.8434495197755814\n",
      "83     \t [0.87306617 0.48154108 0.83261162]. \t  3.5320706967237703 \t 3.8434495197755814\n",
      "84     \t [0.79095226 0.4391966  0.84686793]. \t  3.352313943857798 \t 3.8434495197755814\n",
      "85     \t [0.44267905 0.5070677  0.88149524]. \t  3.6880469138531016 \t 3.8434495197755814\n",
      "86     \t [0.50079549 0.57211678 0.82560818]. \t  3.743150773586672 \t 3.8434495197755814\n",
      "87     \t [0.39165263 0.5831688  0.84318725]. \t  3.814531929588499 \t 3.8434495197755814\n",
      "88     \t [0.37318809 0.54726857 0.81816116]. \t  3.7418744815514593 \t 3.8434495197755814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.33670759 0.53928548 0.8320637 ]. \t  3.8122748603799232 \t 3.8434495197755814\n",
      "90     \t [0.27743361 0.62513881 0.7841676 ]. \t  3.3276200901643986 \t 3.8434495197755814\n",
      "91     \t [0.41227442 0.57998277 0.90276412]. \t  3.600077969092219 \t 3.8434495197755814\n",
      "92     \t [0.55721493 0.58055509 0.85675384]. \t  3.8023953995610937 \t 3.8434495197755814\n",
      "93     \t [0.20705158 0.51730406 0.83333512]. \t  3.77795985060964 \t 3.8434495197755814\n",
      "94     \t [0.72649759 0.5268573  0.82643319]. \t  3.682157215414143 \t 3.8434495197755814\n",
      "95     \t [0.24505424 0.62794901 0.92736783]. \t  3.1864252583884465 \t 3.8434495197755814\n",
      "96     \t [0.44318231 0.50795291 0.79750703]. \t  3.515437500707135 \t 3.8434495197755814\n",
      "97     \t [0.89015511 0.47965097 0.81728186]. \t  3.441469807065606 \t 3.8434495197755814\n",
      "98     \t [0.3865261  0.57123201 0.89047821]. \t  3.7150155424359044 \t 3.8434495197755814\n",
      "99     \t [0.17548675 0.56077158 0.84874857]. \t  \u001b[92m3.8509526525165247\u001b[0m \t 3.8509526525165247\n",
      "100    \t [0.35474914 0.48793582 0.8946866 ]. \t  3.518024571783889 \t 3.8509526525165247\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
      "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
      "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
      "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
      "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
      "1      \t [0.63933257 1.         0.        ]. \t  0.0001463735508754916 \t 3.595021899183128\n",
      "2      \t [0.82557595 1.         1.        ]. \t  0.3236014120792165 \t 3.595021899183128\n",
      "3      \t [0.22886922 0.19756527 0.        ]. \t  0.10360353060833467 \t 3.595021899183128\n",
      "4      \t [1.         0.08544199 1.        ]. \t  0.2101670794748412 \t 3.595021899183128\n",
      "5      \t [0.45669796 0.59034805 0.57702355]. \t  1.3977952539931047 \t 3.595021899183128\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.595021899183128\n",
      "7      \t [1.         0.56591371 1.        ]. \t  2.0087955197706924 \t 3.595021899183128\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.595021899183128\n",
      "9      \t [1.00000000e+00 1.00000000e+00 5.55111512e-17]. \t  3.7727185179443916e-05 \t 3.595021899183128\n",
      "10     \t [0.         1.         0.60738852]. \t  2.329497683436387 \t 3.595021899183128\n",
      "11     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
      "12     \t [0.67820239 0.42084684 1.        ]. \t  1.6768714825167574 \t 3.595021899183128\n",
      "13     \t [1.         0.         0.60712556]. \t  0.10219884375206649 \t 3.595021899183128\n",
      "14     \t [1.        1.        0.6666898]. \t  0.3000518866408759 \t 3.595021899183128\n",
      "15     \t [0.         0.25377969 0.6943709 ]. \t  1.1197076094275862 \t 3.595021899183128\n",
      "16     \t [1.         0.46983288 0.        ]. \t  0.010226957954748223 \t 3.595021899183128\n",
      "17     \t [0.         0.46361863 0.        ]. \t  0.023495952464086072 \t 3.595021899183128\n",
      "18     \t [1.         0.4153559  0.79591026]. \t  2.909863555511234 \t 3.595021899183128\n",
      "19     \t [0.49955818 0.77582225 0.81584252]. \t  2.369267263158768 \t 3.595021899183128\n",
      "20     \t [0.39994039 0.34988693 0.80153319]. \t  2.557904611889687 \t 3.595021899183128\n",
      "21     \t [0.       0.260746 1.      ]. \t  0.8207747320420903 \t 3.595021899183128\n",
      "22     \t [0.65993223 0.         0.        ]. \t  0.07930395747397893 \t 3.595021899183128\n",
      "23     \t [1.         0.72118468 0.84959093]. \t  2.7677178961717606 \t 3.595021899183128\n",
      "24     \t [0.         0.         0.93924404]. \t  0.15124382140066256 \t 3.595021899183128\n",
      "25     \t [0.         1.         0.34684475]. \t  0.674792206929132 \t 3.595021899183128\n",
      "26     \t [0.64233056 0.57211545 0.        ]. \t  0.011857485152041524 \t 3.595021899183128\n",
      "27     \t [0.00727597 0.57471633 0.67511069]. \t  2.284843640981676 \t 3.595021899183128\n",
      "28     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.595021899183128\n",
      "29     \t [0.1075305  0.         0.33753661]. \t  0.6135803259112198 \t 3.595021899183128\n",
      "30     \t [0. 1. 1.]. \t  0.330219860606422 \t 3.595021899183128\n",
      "31     \t [0.20838934 0.79751412 0.        ]. \t  0.0013270710831160904 \t 3.595021899183128\n",
      "32     \t [0.79932188 0.59312419 0.82756939]. \t  \u001b[92m3.5963339102092\u001b[0m \t 3.5963339102092\n",
      "33     \t [0.83889585 0.18824892 0.2391207 ]. \t  0.4787728750961279 \t 3.5963339102092\n",
      "34     \t [0.83845094 0.21693386 0.        ]. \t  0.05476160369263673 \t 3.5963339102092\n",
      "35     \t [1.32796758e-07 2.81451859e-01 2.68538739e-01]. \t  0.5150369691639923 \t 3.5963339102092\n",
      "36     \t [0.16228708 0.99256551 0.95937238]. \t  0.481364135381905 \t 3.5963339102092\n",
      "37     \t [0.96167313 0.73691442 0.35762949]. \t  0.09617120977454739 \t 3.5963339102092\n",
      "38     \t [0.         0.73727719 0.29710277]. \t  0.3429525774998087 \t 3.5963339102092\n",
      "39     \t [7.22126683e-01 7.69371209e-08 6.27415626e-01]. \t  0.13007340811683 \t 3.5963339102092\n",
      "40     \t [0.8500531  0.54994103 0.76837546]. \t  3.0481215575840226 \t 3.5963339102092\n",
      "41     \t [0.47236768 1.         0.54796686]. \t  1.7402029867421622 \t 3.5963339102092\n",
      "42     \t [0.19791624 0.59386593 0.79848134]. \t  3.5563435330812987 \t 3.5963339102092\n",
      "43     \t [0.80348273 0.91991991 0.23694835]. \t  0.034305334343870814 \t 3.5963339102092\n",
      "44     \t [9.08354475e-01 8.17306848e-01 1.47066703e-08]. \t  0.0004050267345025078 \t 3.5963339102092\n",
      "45     \t [0.16391711 0.74215101 0.98799703]. \t  1.7022572189406648 \t 3.5963339102092\n",
      "46     \t [0.13334748 0.88118624 0.68207024]. \t  2.2867511109109593 \t 3.5963339102092\n",
      "47     \t [0.89841385 0.2074802  0.84245765]. \t  1.2685822114434213 \t 3.5963339102092\n",
      "48     \t [0.         0.         0.37922951]. \t  0.40020492980075967 \t 3.5963339102092\n",
      "49     \t [0.73558926 0.0300209  0.95588724]. \t  0.1842052122949425 \t 3.5963339102092\n",
      "50     \t [0.32934091 0.98500759 0.16266349]. \t  0.02314466590494734 \t 3.5963339102092\n",
      "51     \t [0.91386188 0.60285578 0.80972943]. \t  3.3717285564811856 \t 3.5963339102092\n",
      "52     \t [9.99999871e-01 6.10451388e-07 8.73867872e-01]. \t  0.2104248034778657 \t 3.5963339102092\n",
      "53     \t [9.33117093e-01 5.77498877e-08 2.44328839e-01]. \t  0.3303390777408542 \t 3.5963339102092\n",
      "54     \t [0.72898567 0.99279119 0.82386093]. \t  0.6242458323862857 \t 3.5963339102092\n",
      "55     \t [0.78356704 0.51428556 0.80507654]. \t  3.487912292250734 \t 3.5963339102092\n",
      "56     \t [0.86859572 0.61560732 0.83685212]. \t  3.5293642831300764 \t 3.5963339102092\n",
      "57     \t [0.87095552 0.55606865 0.88069051]. \t  \u001b[92m3.671598841611249\u001b[0m \t 3.671598841611249\n",
      "58     \t [0.83856288 0.54880025 0.86012893]. \t  \u001b[92m3.7408435272176157\u001b[0m \t 3.7408435272176157\n",
      "59     \t [0.91450382 0.59457429 0.90162723]. \t  3.464201738811917 \t 3.7408435272176157\n",
      "60     \t [0.7141952  0.60631736 0.91901071]. \t  3.3208066853988787 \t 3.7408435272176157\n",
      "61     \t [0.88049889 0.55516944 0.81587192]. \t  3.554825997585717 \t 3.7408435272176157\n",
      "62     \t [0.83880556 0.61919119 0.90711096]. \t  3.3643478955601767 \t 3.7408435272176157\n",
      "63     \t [0.26960138 0.53425636 0.72107453]. \t  2.7087399417288562 \t 3.7408435272176157\n",
      "64     \t [0.70725276 0.57121032 0.87027555]. \t  \u001b[92m3.757392239494667\u001b[0m \t 3.757392239494667\n",
      "65     \t [0.58124656 0.57974487 0.90268774]. \t  3.5801658652278263 \t 3.757392239494667\n",
      "66     \t [0.61044761 0.49195385 0.77884557]. \t  3.2344668033359922 \t 3.757392239494667\n",
      "67     \t [0.83451629 0.55863738 0.75434904]. \t  2.848498852449547 \t 3.757392239494667\n",
      "68     \t [0.49265462 0.54289676 0.85190865]. \t  \u001b[92m3.838836456729617\u001b[0m \t 3.838836456729617\n",
      "69     \t [0.48737211 0.01042265 0.19748335]. \t  0.7394231684160854 \t 3.838836456729617\n",
      "70     \t [0.71546134 0.65600077 0.85788176]. \t  3.419583226043118 \t 3.838836456729617\n",
      "71     \t [0.82240087 0.64670274 0.8592813 ]. \t  3.439625769398138 \t 3.838836456729617\n",
      "72     \t [0.51982837 0.49132132 0.88432192]. \t  3.600376688012345 \t 3.838836456729617\n",
      "73     \t [0.97651521 0.51854114 0.83662068]. \t  3.6153009892157164 \t 3.838836456729617\n",
      "74     \t [0.30936578 0.52824833 0.87581893]. \t  3.7806163699640045 \t 3.838836456729617\n",
      "75     \t [0.54204297 0.53992512 0.94088349]. \t  3.0997835869734587 \t 3.838836456729617\n",
      "76     \t [0.94966605 0.6107818  0.81408009]. \t  3.360543894901361 \t 3.838836456729617\n",
      "77     \t [0.761864   0.60707749 0.81926702]. \t  3.5059262058324276 \t 3.838836456729617\n",
      "78     \t [0.40354378 0.59055696 0.8811553 ]. \t  3.7409266547955955 \t 3.838836456729617\n",
      "79     \t [0.37409206 0.59439558 0.82485044]. \t  3.7197502427248272 \t 3.838836456729617\n",
      "80     \t [0.55347369 0.56568787 0.8204301 ]. \t  3.704858272070446 \t 3.838836456729617\n",
      "81     \t [0.28492441 0.60088021 0.85594906]. \t  3.7878149282481894 \t 3.838836456729617\n",
      "82     \t [0.72756456 0.61578316 0.86521054]. \t  3.6357957097530953 \t 3.838836456729617\n",
      "83     \t [0.83738181 0.59421967 0.87827118]. \t  3.6400817139260493 \t 3.838836456729617\n",
      "84     \t [0.3536109  0.58270205 0.90982784]. \t  3.5263728863776165 \t 3.838836456729617\n",
      "85     \t [0.26626138 0.59676413 0.80477505]. \t  3.596456532816174 \t 3.838836456729617\n",
      "86     \t [0.4386914  0.54778103 0.82256172]. \t  3.75726378697053 \t 3.838836456729617\n",
      "87     \t [0.89963684 0.49336545 0.80722905]. \t  3.4120726699131882 \t 3.838836456729617\n",
      "88     \t [0.41069932 0.46665412 0.86287555]. \t  3.5795679536133056 \t 3.838836456729617\n",
      "89     \t [0.46903322 0.63313148 0.9013475 ]. \t  3.4400304274656435 \t 3.838836456729617\n",
      "90     \t [0.37091895 0.58426609 0.83262212]. \t  3.7819918900264238 \t 3.838836456729617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.17866222 0.69459712 0.83812824]. \t  3.2268185444431587 \t 3.838836456729617\n",
      "92     \t [0.75606498 0.51797366 0.88539399]. \t  3.6368888203378806 \t 3.838836456729617\n",
      "93     \t [0.40110135 0.57232548 0.88431382]. \t  3.754170892017967 \t 3.838836456729617\n",
      "94     \t [0.5180887  0.63460569 0.84094922]. \t  3.584888962683578 \t 3.838836456729617\n",
      "95     \t [0.71724325 0.62663052 0.87273305]. \t  3.5753347574008187 \t 3.838836456729617\n",
      "96     \t [0.50782101 0.49603556 0.81465066]. \t  3.6025319397577764 \t 3.838836456729617\n",
      "97     \t [0.88575861 0.51838884 0.89410955]. \t  3.5333532505074148 \t 3.838836456729617\n",
      "98     \t [0.50772457 0.53934594 0.88284166]. \t  3.7486082055621224 \t 3.838836456729617\n",
      "99     \t [0.3386847  0.58515066 0.84729569]. \t  3.8231845652594227 \t 3.838836456729617\n",
      "100    \t [0.57165835 0.65089202 0.89220416]. \t  3.394374575159 \t 3.838836456729617\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1029187088185965\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1029187088185965\n",
      "3      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1029187088185965\n",
      "4      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.1029187088185965\n",
      "5      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.1029187088185965\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.1029187088185965\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.1029187088185965\n",
      "8      \t [0.         1.         0.51362551]. \t  \u001b[92m2.3550611123300182\u001b[0m \t 2.3550611123300182\n",
      "9      \t [1.         0.         0.61911635]. \t  0.11260661404582735 \t 2.3550611123300182\n",
      "10     \t [1.         1.         0.51260861]. \t  0.23540695639605855 \t 2.3550611123300182\n",
      "11     \t [0.         0.51230792 1.        ]. \t  1.9969644544961234 \t 2.3550611123300182\n",
      "12     \t [1.         0.49091136 1.        ]. \t  1.8973195098828937 \t 2.3550611123300182\n",
      "13     \t [0.51679862 0.74385124 1.        ]. \t  1.5358708014302163 \t 2.3550611123300182\n",
      "14     \t [1.         0.51897994 0.        ]. \t  0.007061339003777718 \t 2.3550611123300182\n",
      "15     \t [0.50676531 0.         0.        ]. \t  0.09657954596298055 \t 2.3550611123300182\n",
      "16     \t [0.         0.         0.48245654]. \t  0.16100610238589214 \t 2.3550611123300182\n",
      "17     \t [0.         0.7045969  0.69663247]. \t  \u001b[92m2.5113400775835277\u001b[0m \t 2.5113400775835277\n",
      "18     \t [0.37851697 1.         0.72730985]. \t  1.1153555550876064 \t 2.5113400775835277\n",
      "19     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.5113400775835277\n",
      "20     \t [1.         0.37066285 0.35181021]. \t  0.13967459366270257 \t 2.5113400775835277\n",
      "21     \t [1.         0.69688406 0.75796728]. \t  2.191365103930755 \t 2.5113400775835277\n",
      "22     \t [0.54220801 1.         0.        ]. \t  0.00018660694550740365 \t 2.5113400775835277\n",
      "23     \t [0.         0.3077094  0.76495325]. \t  1.9622931654971816 \t 2.5113400775835277\n",
      "24     \t [0.32989931 0.50731873 0.        ]. \t  0.02550413050664012 \t 2.5113400775835277\n",
      "25     \t [0.74712199 0.75648841 0.74383168]. \t  1.8500394415763641 \t 2.5113400775835277\n",
      "26     \t [0.        0.6145774 0.       ]. \t  0.006700001795097591 \t 2.5113400775835277\n",
      "27     \t [0.28144181 0.40483985 1.        ]. \t  1.6098582143026692 \t 2.5113400775835277\n",
      "28     \t [0.74255535 0.         0.29495822]. \t  0.5608247999939653 \t 2.5113400775835277\n",
      "29     \t [0.74396966 0.27808762 0.        ]. \t  0.05931104335054071 \t 2.5113400775835277\n",
      "30     \t [0.6323 1.     1.    ]. \t  0.32944801091651077 \t 2.5113400775835277\n",
      "31     \t [0.85781581 0.78390103 0.06073466]. \t  0.0019300911177467263 \t 2.5113400775835277\n",
      "32     \t [0.55123651 0.46400316 0.65417756]. \t  1.5703175616426037 \t 2.5113400775835277\n",
      "33     \t [0.         0.79450429 0.35024503]. \t  0.7833297037354713 \t 2.5113400775835277\n",
      "34     \t [0.48415005 0.         0.81272842]. \t  0.25025133436462055 \t 2.5113400775835277\n",
      "35     \t [0.77959597 0.60329956 0.99633187]. \t  2.0931560539927525 \t 2.5113400775835277\n",
      "36     \t [2.59549844e-01 1.46282188e-04 1.05044181e-01]. \t  0.3820437214118653 \t 2.5113400775835277\n",
      "37     \t [0.22394973 0.66183237 0.74797921]. \t  \u001b[92m2.891440526562337\u001b[0m \t 2.891440526562337\n",
      "38     \t [0.15373256 0.99999998 0.31838765]. \t  0.47589006396272726 \t 2.891440526562337\n",
      "39     \t [0.04110872 0.9501836  0.74101011]. \t  1.4512617774182737 \t 2.891440526562337\n",
      "40     \t [0.80626388 0.99999993 0.21797631]. \t  0.020024228673921264 \t 2.891440526562337\n",
      "41     \t [0.         0.19899847 0.        ]. \t  0.07288033169160324 \t 2.891440526562337\n",
      "42     \t [0.14075987 0.49347147 0.57009587]. \t  1.198029770781555 \t 2.891440526562337\n",
      "43     \t [0.48141595 0.56262071 0.87521294]. \t  \u001b[92m3.7995131594612284\u001b[0m \t 3.7995131594612284\n",
      "44     \t [0.54822125 0.5571453  0.85711023]. \t  \u001b[92m3.830760349671342\u001b[0m \t 3.830760349671342\n",
      "45     \t [0.43693121 0.55502377 0.86318614]. \t  \u001b[92m3.843360806980689\u001b[0m \t 3.843360806980689\n",
      "46     \t [0.36636719 0.52422056 0.8475421 ]. \t  3.8255282161504183 \t 3.843360806980689\n",
      "47     \t [1.         1.         0.80818068]. \t  0.49204356299291024 \t 3.843360806980689\n",
      "48     \t [0.50540553 0.5872696  0.82083823]. \t  3.68244070145019 \t 3.843360806980689\n",
      "49     \t [0.34990493 0.50129603 0.85532481]. \t  3.7606883508776194 \t 3.843360806980689\n",
      "50     \t [0.40844279 0.64156256 0.82201056]. \t  3.497144696089416 \t 3.843360806980689\n",
      "51     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.843360806980689\n",
      "52     \t [0.56951969 0.59930017 0.84925982]. \t  3.744359587586567 \t 3.843360806980689\n",
      "53     \t [9.99999995e-01 1.11783989e-07 2.21859782e-01]. \t  0.24817233446402426 \t 3.843360806980689\n",
      "54     \t [0.44072667 0.53041069 0.84547896]. \t  3.826849029735765 \t 3.843360806980689\n",
      "55     \t [0.51884018 0.57083199 0.85833946]. \t  3.825950075211796 \t 3.843360806980689\n",
      "56     \t [0.35476343 0.61825356 0.89838584]. \t  3.5390199643249427 \t 3.843360806980689\n",
      "57     \t [0.40433719 0.57351582 0.85427112]. \t  3.8422370988583694 \t 3.843360806980689\n",
      "58     \t [0.2284644  0.58820764 0.82333727]. \t  3.7399428148299823 \t 3.843360806980689\n",
      "59     \t [0.49335984 0.51853177 0.90593362]. \t  3.5192044277487864 \t 3.843360806980689\n",
      "60     \t [0.99285117 0.47723638 0.77648724]. \t  3.018454478064907 \t 3.843360806980689\n",
      "61     \t [0.01783992 0.58648547 0.86215752]. \t  3.7762822793961615 \t 3.843360806980689\n",
      "62     \t [0.41558576 0.64010523 0.88541009]. \t  3.52166767484844 \t 3.843360806980689\n",
      "63     \t [0.37482693 0.52833828 0.88286578]. \t  3.742140583547997 \t 3.843360806980689\n",
      "64     \t [0.         0.         0.78313019]. \t  0.24645340947144168 \t 3.843360806980689\n",
      "65     \t [0.77183179 0.51562576 0.84182933]. \t  3.708119751559354 \t 3.843360806980689\n",
      "66     \t [0.26456337 0.41164979 0.83452644]. \t  3.199631139975319 \t 3.843360806980689\n",
      "67     \t [0.52199805 0.56106496 0.83557107]. \t  3.797185090980304 \t 3.843360806980689\n",
      "68     \t [0.49336126 0.45819474 0.85746153]. \t  3.5327562987278145 \t 3.843360806980689\n",
      "69     \t [0.31550114 0.5718902  0.83286249]. \t  3.810185662781201 \t 3.843360806980689\n",
      "70     \t [0.35171343 0.48156892 0.87819285]. \t  3.6015197319732932 \t 3.843360806980689\n",
      "71     \t [0.36689723 0.46891745 0.80815362]. \t  3.4681345143155995 \t 3.843360806980689\n",
      "72     \t [0.48175961 0.4436549  0.83475404]. \t  3.431951571824609 \t 3.843360806980689\n",
      "73     \t [0.32078452 0.5653035  0.91972557]. \t  3.4249850246036213 \t 3.843360806980689\n",
      "74     \t [0.48819988 0.55462243 0.86625151]. \t  3.829755383114037 \t 3.843360806980689\n",
      "75     \t [0.43682826 0.57562656 0.86610942]. \t  3.822216438397125 \t 3.843360806980689\n",
      "76     \t [0.23540693 0.57886848 0.83734515]. \t  3.8161850011663754 \t 3.843360806980689\n",
      "77     \t [0.40128301 0.56374296 0.80830717]. \t  3.6609003538871 \t 3.843360806980689\n",
      "78     \t [0.37233617 0.60712217 0.8870054 ]. \t  3.662042686639273 \t 3.843360806980689\n",
      "79     \t [0.04576661 0.54262948 0.83036621]. \t  3.776456980229515 \t 3.843360806980689\n",
      "80     \t [0.19079304 0.5492814  0.85276274]. \t  \u001b[92m3.8540176131000927\u001b[0m \t 3.8540176131000927\n",
      "81     \t [0.08117898 0.5604155  0.87319674]. \t  3.7924279838519777 \t 3.8540176131000927\n",
      "82     \t [0.15421961 0.58138463 0.86315443]. \t  3.8170253126461455 \t 3.8540176131000927\n",
      "83     \t [0.37579014 0.56459625 0.89313639]. \t  3.6997044489529882 \t 3.8540176131000927\n",
      "84     \t [0.09375537 0.58456816 0.88210302]. \t  3.727469490892513 \t 3.8540176131000927\n",
      "85     \t [0.03127939 0.60986704 0.8462831 ]. \t  3.7158909199603967 \t 3.8540176131000927\n",
      "86     \t [0.03326941 0.44343024 0.84275344]. \t  3.4110461466072315 \t 3.8540176131000927\n",
      "87     \t [0.26918102 0.53514104 0.85578544]. \t  3.8460883962190495 \t 3.8540176131000927\n",
      "88     \t [0.69229404 0.54461999 0.80891656]. \t  3.5812292697255392 \t 3.8540176131000927\n",
      "89     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.8540176131000927\n",
      "90     \t [0.06567332 0.55762304 0.79765585]. \t  3.575874823446394 \t 3.8540176131000927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.43252743 0.68975417 0.820368  ]. \t  3.1604136670402947 \t 3.8540176131000927\n",
      "92     \t [0.24782861 0.472361   0.83995492]. \t  3.6218761795245475 \t 3.8540176131000927\n",
      "93     \t [0.52212218 0.55121483 0.84680292]. \t  3.8310199425327123 \t 3.8540176131000927\n",
      "94     \t [0.09016665 0.56322139 0.89780758]. \t  3.6347731987007825 \t 3.8540176131000927\n",
      "95     \t [0.23260502 0.52617537 0.85247535]. \t  3.83002532604717 \t 3.8540176131000927\n",
      "96     \t [0.64688986 0.56633895 0.81046338]. \t  3.595854693074133 \t 3.8540176131000927\n",
      "97     \t [0.03456559 0.67275585 0.85746964]. \t  3.3763457087853324 \t 3.8540176131000927\n",
      "98     \t [0.11961095 0.6077818  0.86856485]. \t  3.728720244616088 \t 3.8540176131000927\n",
      "99     \t [0.24580154 0.4780867  0.83406884]. \t  3.638437935343501 \t 3.8540176131000927\n",
      "100    \t [0.27021196 0.46904299 0.91493543]. \t  3.211712971143558 \t 3.8540176131000927\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.687459437576373\n",
      "2      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.687459437576373\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.687459437576373\n",
      "4      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 0.687459437576373\n",
      "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.687459437576373\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.687459437576373\n",
      "7      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.687459437576373\n",
      "8      \t [0.39478799 1.         0.43742025]. \t  \u001b[92m1.3260517184755405\u001b[0m \t 1.3260517184755405\n",
      "9      \t [0.52468994 0.5702431  1.        ]. \t  \u001b[92m2.0834552241478264\u001b[0m \t 2.0834552241478264\n",
      "10     \t [1.11022302e-16 1.00000000e+00 0.00000000e+00]. \t  0.0002735367680454459 \t 2.0834552241478264\n",
      "11     \t [1.        0.4915805 1.       ]. \t  1.8992109167288653 \t 2.0834552241478264\n",
      "12     \t [1.         0.49476803 0.        ]. \t  0.008525741075846294 \t 2.0834552241478264\n",
      "13     \t [0.         0.50626891 1.        ]. \t  1.9832119227856564 \t 2.0834552241478264\n",
      "14     \t [1.         0.         0.49341078]. \t  0.07557071059165289 \t 2.0834552241478264\n",
      "15     \t [0.56291142 1.         0.        ]. \t  0.00017797234495555553 \t 2.0834552241478264\n",
      "16     \t [0.         1.         0.53966686]. \t  \u001b[92m2.4677258914674196\u001b[0m \t 2.4677258914674196\n",
      "17     \t [0.52131508 0.         0.        ]. \t  0.09536354920403907 \t 2.4677258914674196\n",
      "18     \t [0.4363055 0.        1.       ]. \t  0.09172472665611917 \t 2.4677258914674196\n",
      "19     \t [0.19243867 0.7583923  0.70754565]. \t  2.459074468345434 \t 2.4677258914674196\n",
      "20     \t [0.77126086 0.33355066 0.80158317]. \t  2.3677291475877222 \t 2.4677258914674196\n",
      "21     \t [0.57602852 1.         0.82550753]. \t  0.6525782604330614 \t 2.4677258914674196\n",
      "22     \t [1.         0.34149774 0.39414338]. \t  0.13445893442327006 \t 2.4677258914674196\n",
      "23     \t [0.         0.         0.52832324]. \t  0.1100302215844237 \t 2.4677258914674196\n",
      "24     \t [0.68592634 0.69882039 0.26929558]. \t  0.09610422547563505 \t 2.4677258914674196\n",
      "25     \t [0.71874485 0.27373146 0.        ]. \t  0.06352900637271494 \t 2.4677258914674196\n",
      "26     \t [0.         0.74826733 0.76211406]. \t  \u001b[92m2.527519788193467\u001b[0m \t 2.527519788193467\n",
      "27     \t [0.         0.63148899 0.        ]. \t  0.005681709159008178 \t 2.527519788193467\n",
      "28     \t [0.70446661 0.26858987 1.        ]. \t  0.8641740615149476 \t 2.527519788193467\n",
      "29     \t [0.84212492 0.74523301 1.        ]. \t  1.4967626825032319 \t 2.527519788193467\n",
      "30     \t [1.         0.2270596  0.75731674]. \t  1.2756838963319344 \t 2.527519788193467\n",
      "31     \t [0.712164   0.         0.79375175]. \t  0.25012260354571497 \t 2.527519788193467\n",
      "32     \t [0.53843665 0.6293678  0.81316797]. \t  \u001b[92m3.457120400414187\u001b[0m \t 3.457120400414187\n",
      "33     \t [0.78190189 1.         0.44481681]. \t  0.4601159765607807 \t 3.457120400414187\n",
      "34     \t [0.         0.20209976 0.00251504]. \t  0.0754738995033921 \t 3.457120400414187\n",
      "35     \t [0.19806575 0.         0.79478881]. \t  0.2507839622554766 \t 3.457120400414187\n",
      "36     \t [0.71585254 0.61703637 0.73280595]. \t  2.4747019077029457 \t 3.457120400414187\n",
      "37     \t [0.80923436 0.         0.23185548]. \t  0.4694484392890072 \t 3.457120400414187\n",
      "38     \t [0.         1.         0.80851434]. \t  0.8254079247552304 \t 3.457120400414187\n",
      "39     \t [0.11313211 1.         0.17503857]. \t  0.03408896852853891 \t 3.457120400414187\n",
      "40     \t [2.15698100e-01 8.26748048e-09 1.50564545e-01]. \t  0.5400483232399516 \t 3.457120400414187\n",
      "41     \t [6.09958072e-09 7.40102439e-01 9.99999989e-01]. \t  1.5380717221855489 \t 3.457120400414187\n",
      "42     \t [1.        1.        0.2854068]. \t  0.026967619069433423 \t 3.457120400414187\n",
      "43     \t [0.43770016 0.47151935 0.85509414]. \t  \u001b[92m3.6194184544354924\u001b[0m \t 3.6194184544354924\n",
      "44     \t [0.23122414 0.2711026  0.99695155]. \t  0.9083758148895704 \t 3.6194184544354924\n",
      "45     \t [0.35463209 1.         0.99999992]. \t  0.33409714865494883 \t 3.6194184544354924\n",
      "46     \t [0.48293347 0.5164433  0.85728319]. \t  \u001b[92m3.7952227231590583\u001b[0m \t 3.7952227231590583\n",
      "47     \t [0.44925121 0.56714943 0.80028509]. \t  3.5763107181114884 \t 3.7952227231590583\n",
      "48     \t [4.80940202e-01 5.86919763e-01 5.61564645e-08]. \t  0.012493931869040987 \t 3.7952227231590583\n",
      "49     \t [0.51469034 0.57988892 0.76506772]. \t  3.143053600686706 \t 3.7952227231590583\n",
      "50     \t [1.         0.57370808 0.85394869]. \t  3.6505361710012902 \t 3.7952227231590583\n",
      "51     \t [0.52703608 0.47789413 0.83630641]. \t  3.6258419932046992 \t 3.7952227231590583\n",
      "52     \t [0.29361748 0.64257462 0.83406972]. \t  3.5670620406901126 \t 3.7952227231590583\n",
      "53     \t [0.48075247 0.4868558  0.84329412]. \t  3.6882293140094626 \t 3.7952227231590583\n",
      "54     \t [0.41981522 0.53971778 0.86767695]. \t  \u001b[92m3.826093216928717\u001b[0m \t 3.826093216928717\n",
      "55     \t [0.40823651 0.54556592 0.7581161 ]. \t  3.133904739036576 \t 3.826093216928717\n",
      "56     \t [0.38712041 0.44170957 0.85740703]. \t  3.430500456537292 \t 3.826093216928717\n",
      "57     \t [0.99907333 0.21383615 0.09469339]. \t  0.11317035466835992 \t 3.826093216928717\n",
      "58     \t [0.48944405 0.6559092  0.83877525]. \t  3.459362722987617 \t 3.826093216928717\n",
      "59     \t [0.36060133 0.57182717 0.81018529]. \t  3.6746579538437913 \t 3.826093216928717\n",
      "60     \t [0.99999987 0.69717743 0.99999991]. \t  1.7015627369623658 \t 3.826093216928717\n",
      "61     \t [0.36926517 0.58866322 0.82623097]. \t  3.7428470983377164 \t 3.826093216928717\n",
      "62     \t [0.3212692  0.56949411 0.90355774]. \t  3.6052269789702023 \t 3.826093216928717\n",
      "63     \t [0.17943106 0.63742331 0.81877806]. \t  3.531914664775532 \t 3.826093216928717\n",
      "64     \t [0.40226489 0.5650331  0.83339462]. \t  3.8100059316328183 \t 3.826093216928717\n",
      "65     \t [0.87755228 0.50984694 0.84819322]. \t  3.666385286401634 \t 3.826093216928717\n",
      "66     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
      "67     \t [0.28671926 0.50649644 0.89419523]. \t  3.5971502588572193 \t 3.832003952745011\n",
      "68     \t [0.46530289 0.56618438 0.82141748]. \t  3.7356284054236553 \t 3.832003952745011\n",
      "69     \t [0.44287795 0.56525754 0.81561841]. \t  3.704077081131519 \t 3.832003952745011\n",
      "70     \t [0.24643853 0.57801793 0.87130892]. \t  3.8102488871675555 \t 3.832003952745011\n",
      "71     \t [0.16028985 0.57018088 0.79471086]. \t  3.561052023701031 \t 3.832003952745011\n",
      "72     \t [0.97818867 0.45233578 0.88390651]. \t  3.268710931535794 \t 3.832003952745011\n",
      "73     \t [0.31687796 0.53778831 0.93378636]. \t  3.2133364677997203 \t 3.832003952745011\n",
      "74     \t [0.29175317 0.5587257  0.88714593]. \t  3.745084287930277 \t 3.832003952745011\n",
      "75     \t [0.41443338 0.60581294 0.863799  ]. \t  3.7542537274948486 \t 3.832003952745011\n",
      "76     \t [0.3815899  0.59862108 0.87130661]. \t  3.76291945326389 \t 3.832003952745011\n",
      "77     \t [0.23359172 0.5771362  0.76057457]. \t  3.205916953252994 \t 3.832003952745011\n",
      "78     \t [0.37441595 0.50152922 0.89357223]. \t  3.585464395890391 \t 3.832003952745011\n",
      "79     \t [0.44004829 0.57021048 0.83858703]. \t  3.8162505296339466 \t 3.832003952745011\n",
      "80     \t [0.3353267  0.4861297  0.82736568]. \t  3.6542625372873077 \t 3.832003952745011\n",
      "81     \t [0.96110632 0.50696518 0.80713948]. \t  3.418348500937991 \t 3.832003952745011\n",
      "82     \t [0.3768818  0.58418398 0.82541667]. \t  3.747688861624937 \t 3.832003952745011\n",
      "83     \t [0.72992343 0.53146798 0.82321234]. \t  3.667309886685542 \t 3.832003952745011\n",
      "84     \t [0.3025782  0.57533147 0.92981912]. \t  3.284296438449202 \t 3.832003952745011\n",
      "85     \t [0.41301166 0.54243128 0.91044301]. \t  3.5237623779308946 \t 3.832003952745011\n",
      "86     \t [0.35089107 0.60967689 0.88760602]. \t  3.6504047070968335 \t 3.832003952745011\n",
      "87     \t [0.33710522 0.47970266 0.79148273]. \t  3.38565793777947 \t 3.832003952745011\n",
      "88     \t [0.33438085 0.56046464 0.88776229]. \t  3.7413806662366227 \t 3.832003952745011\n",
      "89     \t [0.50940053 0.57562184 0.85740423]. \t  3.821596768146967 \t 3.832003952745011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.90038219 0.44731922 0.8766767 ]. \t  3.3042847252436847 \t 3.832003952745011\n",
      "91     \t [0.50664781 0.49179029 0.86479967]. \t  3.693097341444991 \t 3.832003952745011\n",
      "92     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
      "93     \t [0.43280195 0.49019981 0.83056748]. \t  3.6758388635559784 \t 3.8511165079641128\n",
      "94     \t [0.47540178 0.5024734  0.84724177]. \t  3.7544441307385092 \t 3.8511165079641128\n",
      "95     \t [0.03705244 0.16683767 0.87640229]. \t  0.9141787320550978 \t 3.8511165079641128\n",
      "96     \t [0.27678647 0.47274801 0.90379937]. \t  3.353292206748522 \t 3.8511165079641128\n",
      "97     \t [0.19024373 0.57825008 0.79120571]. \t  3.522053195912587 \t 3.8511165079641128\n",
      "98     \t [0.39259916 0.51814806 0.78806564]. \t  3.4567027216635013 \t 3.8511165079641128\n",
      "99     \t [0.80699925 0.58395139 0.86000123]. \t  3.7158266575849224 \t 3.8511165079641128\n",
      "100    \t [0.27505784 0.63490525 0.87654237]. \t  3.5990436497783254 \t 3.8511165079641128\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.6482992955272024\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.6482992955272024\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6482992955272024\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.6482992955272024\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.6482992955272024\n",
      "6      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.6482992955272024\n",
      "7      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.6482992955272024\n",
      "8      \t [0.43857939 0.45254531 0.        ]. \t  0.03751605718694749 \t 1.6482992955272024\n",
      "9      \t [0.44320695 0.         0.57929151]. \t  0.11741776830582973 \t 1.6482992955272024\n",
      "10     \t [0.         0.50627112 1.        ]. \t  \u001b[92m1.9832172103940562\u001b[0m \t 1.9832172103940562\n",
      "11     \t [0.59070646 0.35795692 1.        ]. \t  1.3533726990403852 \t 1.9832172103940562\n",
      "12     \t [0.53952329 1.         0.        ]. \t  0.00018772413476122242 \t 1.9832172103940562\n",
      "13     \t [1.         0.56662296 1.        ]. \t  \u001b[92m2.008822606852714\u001b[0m \t 2.008822606852714\n",
      "14     \t [0.         0.         0.49557455]. \t  0.14205828482435554 \t 2.008822606852714\n",
      "15     \t [0.         1.         0.55391412]. \t  \u001b[92m2.4901236947752974\u001b[0m \t 2.4901236947752974\n",
      "16     \t [0.50196268 0.         0.        ]. \t  0.09695728166716083 \t 2.4901236947752974\n",
      "17     \t [1.         0.54931568 0.        ]. \t  0.0054859137401361 \t 2.4901236947752974\n",
      "18     \t [1.         1.         0.60646431]. \t  0.277681940881183 \t 2.4901236947752974\n",
      "19     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.4901236947752974\n",
      "20     \t [0.         0.72458127 0.73199457]. \t  \u001b[92m2.5534995468711372\u001b[0m \t 2.5534995468711372\n",
      "21     \t [0.5672644  1.         0.79135647]. \t  0.6901063163229226 \t 2.5534995468711372\n",
      "22     \t [1.         0.         0.54687538]. \t  0.06955083575098915 \t 2.5534995468711372\n",
      "23     \t [0.17173354 0.33715576 0.81089545]. \t  2.468494655567339 \t 2.5534995468711372\n",
      "24     \t [0.         0.38918082 0.        ]. \t  0.03718569152607339 \t 2.5534995468711372\n",
      "25     \t [1.         0.70595212 0.77216954]. \t  2.2967439139176373 \t 2.5534995468711372\n",
      "26     \t [0.76037883 0.76048223 1.        ]. \t  1.4259054277793088 \t 2.5534995468711372\n",
      "27     \t [0.59724053 0.         1.        ]. \t  0.09137701936324144 \t 2.5534995468711372\n",
      "28     \t [0.76788358 0.         0.75040801]. \t  0.23719626005370842 \t 2.5534995468711372\n",
      "29     \t [0.23128904 1.         0.71146459]. \t  1.4040753980371845 \t 2.5534995468711372\n",
      "30     \t [0.         0.38661995 0.59736599]. \t  0.9189086121507378 \t 2.5534995468711372\n",
      "31     \t [0.18146267 1.         0.24794359]. \t  0.14981394175714685 \t 2.5534995468711372\n",
      "32     \t [0.15518856 0.80530016 0.00408246]. \t  0.0012750160070564827 \t 2.5534995468711372\n",
      "33     \t [0.20531488 0.06269165 0.18880888]. \t  0.7449086055159259 \t 2.5534995468711372\n",
      "34     \t [8.70611854e-01 1.12570544e-08 1.68381924e-01]. \t  0.30557386778433243 \t 2.5534995468711372\n",
      "35     \t [0.19762723 0.29299143 0.98438743]. \t  1.1416891721316016 \t 2.5534995468711372\n",
      "36     \t [0.35646863 0.58472403 0.81879324]. \t  \u001b[92m3.711904404183571\u001b[0m \t 3.711904404183571\n",
      "37     \t [0.5190957  0.57467459 0.83104749]. \t  \u001b[92m3.762051481188146\u001b[0m \t 3.762051481188146\n",
      "38     \t [0.76837105 0.58217159 0.83071408]. \t  3.658049332764312 \t 3.762051481188146\n",
      "39     \t [0.7227652  0.59916014 0.82533954]. \t  3.594666471397612 \t 3.762051481188146\n",
      "40     \t [0.25811437 0.57606859 0.8276642 ]. \t  \u001b[92m3.784483779109948\u001b[0m \t 3.784483779109948\n",
      "41     \t [0.31028324 0.63169194 0.7819008 ]. \t  3.2726029479883523 \t 3.784483779109948\n",
      "42     \t [0.51230731 0.5711934  0.80378544]. \t  3.581442473362764 \t 3.784483779109948\n",
      "43     \t [0.         0.77118412 0.12675497]. \t  0.01581479913609059 \t 3.784483779109948\n",
      "44     \t [0.26102391 0.5546019  0.85060728]. \t  \u001b[92m3.860418791506413\u001b[0m \t 3.860418791506413\n",
      "45     \t [0.26613627 0.59189361 0.86882046]. \t  3.791800636095747 \t 3.860418791506413\n",
      "46     \t [0.33551091 0.59923919 0.84463619]. \t  3.782094179584197 \t 3.860418791506413\n",
      "47     \t [0.99661524 0.4065225  0.86225617]. \t  3.018505867767027 \t 3.860418791506413\n",
      "48     \t [0.83686711 0.52107438 0.85214743]. \t  3.7118463483990243 \t 3.860418791506413\n",
      "49     \t [0.39200054 0.57628779 0.87842639]. \t  3.7820129742252773 \t 3.860418791506413\n",
      "50     \t [0.2033146  0.58653128 0.79902923]. \t  3.5767822281129122 \t 3.860418791506413\n",
      "51     \t [0.45747203 0.56950091 0.9014324 ]. \t  3.620159751618462 \t 3.860418791506413\n",
      "52     \t [0.24405708 0.53906066 0.8494995 ]. \t  3.85043720781131 \t 3.860418791506413\n",
      "53     \t [0.75111783 0.56584911 0.87308615]. \t  3.740605990940058 \t 3.860418791506413\n",
      "54     \t [0.42354306 0.51816943 0.80389455]. \t  3.597003742357078 \t 3.860418791506413\n",
      "55     \t [0.23661923 0.52490883 0.81156627]. \t  3.684222600028257 \t 3.860418791506413\n",
      "56     \t [0.22414007 0.61159123 0.82152408]. \t  3.660521695567693 \t 3.860418791506413\n",
      "57     \t [0.80605406 0.56497583 0.89545716]. \t  3.5984635464926775 \t 3.860418791506413\n",
      "58     \t [0.77082037 0.68232051 0.        ]. \t  0.0030107382072559847 \t 3.860418791506413\n",
      "59     \t [0.51688836 0.56900757 0.83937028]. \t  3.8037326865716565 \t 3.860418791506413\n",
      "60     \t [0.60324661 0.52437432 0.89685261]. \t  3.604416165798052 \t 3.860418791506413\n",
      "61     \t [0.32065486 0.59586172 0.83741202]. \t  3.7755992713714672 \t 3.860418791506413\n",
      "62     \t [3.32593546e-07 9.99999924e-01 7.63705961e-01]. \t  1.0279963065827649 \t 3.860418791506413\n",
      "63     \t [0.36287528 0.56362977 0.86683125]. \t  3.839211669326295 \t 3.860418791506413\n",
      "64     \t [0.35601951 0.60182613 0.83915747]. \t  3.7587113983393987 \t 3.860418791506413\n",
      "65     \t [0.27283442 0.57237229 0.86144053]. \t  3.8442345038125856 \t 3.860418791506413\n",
      "66     \t [0.26103967 0.56434181 0.91202007]. \t  3.51502008404899 \t 3.860418791506413\n",
      "67     \t [0.81969283 0.57490996 0.83490177]. \t  3.6738445229005485 \t 3.860418791506413\n",
      "68     \t [0.18426827 0.57309721 0.90597594]. \t  3.568037089007756 \t 3.860418791506413\n",
      "69     \t [8.66243325e-01 2.84393549e-01 4.60123490e-05]. \t  0.0422158764365998 \t 3.860418791506413\n",
      "70     \t [0.3173907  0.6049759  0.79598172]. \t  3.4925804576197246 \t 3.860418791506413\n",
      "71     \t [2.76721950e-01 7.05824632e-07 0.00000000e+00]. \t  0.0996734338923421 \t 3.860418791506413\n",
      "72     \t [0.43690339 0.54231661 0.86322409]. \t  3.8377949543612395 \t 3.860418791506413\n",
      "73     \t [0.30911371 0.61021159 0.84350478]. \t  3.744171172856882 \t 3.860418791506413\n",
      "74     \t [0.8435284  0.52420099 0.83799086]. \t  3.686411563113091 \t 3.860418791506413\n",
      "75     \t [0.54119408 0.60390922 0.81667899]. \t  3.5911190972000497 \t 3.860418791506413\n",
      "76     \t [0.4539126  0.53798135 0.75583519]. \t  3.082710386153611 \t 3.860418791506413\n",
      "77     \t [0.79601336 0.53598043 0.84987784]. \t  3.7469806915840733 \t 3.860418791506413\n",
      "78     \t [0.38289288 0.53742998 0.81764235]. \t  3.731833190479916 \t 3.860418791506413\n",
      "79     \t [0.81348977 0.98707455 0.41300576]. \t  0.32725351055635504 \t 3.860418791506413\n",
      "80     \t [0.28539533 0.5976575  0.85838615]. \t  3.7961476404798056 \t 3.860418791506413\n",
      "81     \t [0.43750944 0.61952838 0.88054085]. \t  3.64434155707522 \t 3.860418791506413\n",
      "82     \t [0.78646892 0.6260841  0.79742051]. \t  3.209726615183765 \t 3.860418791506413\n",
      "83     \t [0.21083908 0.51134112 0.80927357]. \t  3.637646128551391 \t 3.860418791506413\n",
      "84     \t [0.36376913 0.99999867 0.99999844]. \t  0.33404344464428043 \t 3.860418791506413\n",
      "85     \t [0.25201035 0.64799033 0.80320333]. \t  3.380568911666403 \t 3.860418791506413\n",
      "86     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.860418791506413\n",
      "87     \t [0.70108693 0.57325593 0.87006523]. \t  3.7571656181765887 \t 3.860418791506413\n",
      "88     \t [0.43448546 0.6082282  0.85670849]. \t  3.7488352336689683 \t 3.860418791506413\n",
      "89     \t [0.32432703 0.52778817 0.87755112]. \t  3.771366313912431 \t 3.860418791506413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.7364414  0.57098826 0.82546401]. \t  3.6633956235665446 \t 3.860418791506413\n",
      "91     \t [0.66546094 0.55096428 0.89342967]. \t  3.6555229757209715 \t 3.860418791506413\n",
      "92     \t [0.40862357 0.60480596 0.86182646]. \t  3.7613317739438994 \t 3.860418791506413\n",
      "93     \t [0.59580934 0.54706878 0.89548471]. \t  3.6520389622171043 \t 3.860418791506413\n",
      "94     \t [0.36016406 0.56044179 0.87076428]. \t  3.828787703864702 \t 3.860418791506413\n",
      "95     \t [0.4525258  0.61035657 0.92169845]. \t  3.319879560769567 \t 3.860418791506413\n",
      "96     \t [0.12726096 0.52402881 0.88223293]. \t  3.716931585940782 \t 3.860418791506413\n",
      "97     \t [0.79397038 0.46725429 0.86642632]. \t  3.501720129624022 \t 3.860418791506413\n",
      "98     \t [0.33723774 0.58684078 0.84895181]. \t  3.8214814789930904 \t 3.860418791506413\n",
      "99     \t [0.34608303 0.4819577  0.83666806]. \t  3.6652180373579744 \t 3.860418791506413\n",
      "100    \t [0.39414903 0.57429471 0.81947017]. \t  3.728661995275446 \t 3.860418791506413\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [1. 1. 1.]. \t  0.3168836207041561 \t 2.6697919207500047\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.6697919207500047\n",
      "3      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.6697919207500047\n",
      "4      \t [ 1.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  3.7727185179443895e-05 \t 2.6697919207500047\n",
      "5      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 2.6697919207500047\n",
      "6      \t [0.57078593 1.         0.64894785]. \t  1.145866805777378 \t 2.6697919207500047\n",
      "7      \t [0.         0.         0.53528064]. \t  0.10601169554281045 \t 2.6697919207500047\n",
      "8      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.6697919207500047\n",
      "9      \t [0.49349099 0.68699605 1.        ]. \t  1.8129105470274136 \t 2.6697919207500047\n",
      "10     \t [0.         0.58154525 0.59504107]. \t  1.78095470587678 \t 2.6697919207500047\n",
      "11     \t [0.54298016 0.39684153 0.        ]. \t  0.048940496743189135 \t 2.6697919207500047\n",
      "12     \t [1.         0.51690815 0.        ]. \t  0.007179389041648971 \t 2.6697919207500047\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.6697919207500047\n",
      "14     \t [1.         0.51017022 1.        ]. \t  1.9456908563842734 \t 2.6697919207500047\n",
      "15     \t [1.         0.81346571 0.58513118]. \t  0.45309401921017256 \t 2.6697919207500047\n",
      "16     \t [0.         1.         0.54133871]. \t  2.471828871953486 \t 2.6697919207500047\n",
      "17     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.6697919207500047\n",
      "18     \t [0.48017491 0.         0.        ]. \t  0.0985181930121385 \t 2.6697919207500047\n",
      "19     \t [1.         0.         0.51563227]. \t  0.06880456746835072 \t 2.6697919207500047\n",
      "20     \t [0.51146541 1.         0.        ]. \t  0.00019932858429175867 \t 2.6697919207500047\n",
      "21     \t [0.         0.61150431 1.        ]. \t  2.0197727156611918 \t 2.6697919207500047\n",
      "22     \t [1.         0.27356736 0.28170139]. \t  0.2367316573372539 \t 2.6697919207500047\n",
      "23     \t [1.         0.28300794 0.78624713]. \t  1.8205841351360754 \t 2.6697919207500047\n",
      "24     \t [0.         0.50502654 0.        ]. \t  0.017367287604181728 \t 2.6697919207500047\n",
      "25     \t [0.20221951 0.18369589 0.2426028 ]. \t  0.8654862976160563 \t 2.6697919207500047\n",
      "26     \t [0.33670136 1.         1.        ]. \t  0.33419693447326493 \t 2.6697919207500047\n",
      "27     \t [0.77247207 0.28019207 1.        ]. \t  0.9189261921595916 \t 2.6697919207500047\n",
      "28     \t [1.         1.         0.34405791]. \t  0.06268193563171146 \t 2.6697919207500047\n",
      "29     \t [0.7238703  0.         0.21494266]. \t  0.5504066820424617 \t 2.6697919207500047\n",
      "30     \t [0.72286979 0.76656128 0.20150794]. \t  0.029377537956422367 \t 2.6697919207500047\n",
      "31     \t [0.34575559 0.         0.56926841]. \t  0.11936827202110599 \t 2.6697919207500047\n",
      "32     \t [0.17282828 0.6203404  1.        ]. \t  2.023830519265349 \t 2.6697919207500047\n",
      "33     \t [3.60493775e-04 8.61152080e-01 7.59471762e-01]. \t  1.8311737304252347 \t 2.6697919207500047\n",
      "34     \t [0.31687969 1.         0.37401116]. \t  0.8434468543816769 \t 2.6697919207500047\n",
      "35     \t [0.35488214 0.70039674 0.        ]. \t  0.004078768125463712 \t 2.6697919207500047\n",
      "36     \t [4.43756467e-11 2.88284360e-01 1.00000000e+00]. \t  0.9604361388611361 \t 2.6697919207500047\n",
      "37     \t [1.38728413e-09 8.54552485e-01 2.41643967e-01]. \t  0.1553410692182965 \t 2.6697919207500047\n",
      "38     \t [0.42412713 0.00681095 0.99999996]. \t  0.09878169585800814 \t 2.6697919207500047\n",
      "39     \t [0.79079871 0.82239012 0.92640158]. \t  1.690775661723458 \t 2.6697919207500047\n",
      "40     \t [0.33157175 0.54557844 0.75936507]. \t  \u001b[92m3.1759547181102503\u001b[0m \t 3.1759547181102503\n",
      "41     \t [8.48306658e-01 1.66460142e-08 8.95320472e-01]. \t  0.19499839636567956 \t 3.1759547181102503\n",
      "42     \t [4.15011521e-09 2.04879343e-01 4.90692528e-07]. \t  0.0721567058516778 \t 3.1759547181102503\n",
      "43     \t [0.99999995 0.99999994 0.76412842]. \t  0.42000306560354483 \t 3.1759547181102503\n",
      "44     \t [0.87034731 0.13090643 0.00131332]. \t  0.056205220708913546 \t 3.1759547181102503\n",
      "45     \t [2.44550008e-09 0.00000000e+00 2.04043295e-01]. \t  0.5142147994838282 \t 3.1759547181102503\n",
      "46     \t [0.25654361 0.47437194 0.57368656]. \t  1.0969024511581624 \t 3.1759547181102503\n",
      "47     \t [0.55443326 0.61927208 0.80592639]. \t  \u001b[92m3.4395324133151317\u001b[0m \t 3.4395324133151317\n",
      "48     \t [0.84815626 0.56781289 0.81937745]. \t  \u001b[92m3.5802289384838684\u001b[0m \t 3.5802289384838684\n",
      "49     \t [0.83020229 0.54032427 0.81583954]. \t  \u001b[92m3.5817836808692127\u001b[0m \t 3.5817836808692127\n",
      "50     \t [0.71836835 0.58474735 0.88834793]. \t  \u001b[92m3.650951430753442\u001b[0m \t 3.650951430753442\n",
      "51     \t [0.61940139 0.58288995 0.85798219]. \t  \u001b[92m3.781126641644223\u001b[0m \t 3.781126641644223\n",
      "52     \t [0.7278839  0.59730401 0.83369632]. \t  3.6460408404160263 \t 3.781126641644223\n",
      "53     \t [0.21780818 0.99524318 0.77911083]. \t  0.9750482896602424 \t 3.781126641644223\n",
      "54     \t [0.94379675 0.52880827 0.84094736]. \t  3.660190775154853 \t 3.781126641644223\n",
      "55     \t [0.741104   0.6653139  0.85499743]. \t  3.3432066614327587 \t 3.781126641644223\n",
      "56     \t [0.54421388 0.63544997 0.72933377]. \t  2.5407657833640505 \t 3.781126641644223\n",
      "57     \t [0.22925913 0.         0.06448727]. \t  0.23945867281543412 \t 3.781126641644223\n",
      "58     \t [0.92501915 0.58775145 0.86954932]. \t  3.6477188108978607 \t 3.781126641644223\n",
      "59     \t [0.70292413 0.45357459 0.8276676 ]. \t  3.4286936150198537 \t 3.781126641644223\n",
      "60     \t [0.79598951 0.57660656 0.85055307]. \t  3.7291451611160507 \t 3.781126641644223\n",
      "61     \t [0.55944476 0.5371355  0.82596409]. \t  3.7427196510537337 \t 3.781126641644223\n",
      "62     \t [0.75654084 0.59625916 0.8461442 ]. \t  3.6849978470308056 \t 3.781126641644223\n",
      "63     \t [0.55203663 0.50447308 0.8642504 ]. \t  3.73636710297889 \t 3.781126641644223\n",
      "64     \t [0.75726854 0.54330057 0.8982174 ]. \t  3.5864410275999807 \t 3.781126641644223\n",
      "65     \t [0.8267356  0.52087013 0.78203712]. \t  3.237094887612937 \t 3.781126641644223\n",
      "66     \t [0.34588605 0.53127501 0.98414561]. \t  2.349219227813843 \t 3.781126641644223\n",
      "67     \t [0.74413989 0.5001121  0.86989579]. \t  3.65890588839635 \t 3.781126641644223\n",
      "68     \t [0.72504309 0.55173407 0.94721516]. \t  2.982308982921724 \t 3.781126641644223\n",
      "69     \t [0.57100481 0.54355358 0.88237556]. \t  3.7452300730552173 \t 3.781126641644223\n",
      "70     \t [0.73999113 0.51658583 0.89231812]. \t  3.5895994430253797 \t 3.781126641644223\n",
      "71     \t [0.53083421 0.56961007 0.85009631]. \t  \u001b[92m3.8225969778860365\u001b[0m \t 3.8225969778860365\n",
      "72     \t [0.60949427 0.57545531 0.83868009]. \t  3.7657488295516686 \t 3.8225969778860365\n",
      "73     \t [0.75470552 0.57341082 0.84398362]. \t  3.73620072137577 \t 3.8225969778860365\n",
      "74     \t [0.58212183 0.58224386 0.88011211]. \t  3.7362875572513397 \t 3.8225969778860365\n",
      "75     \t [0.84183447 0.59356868 0.85404742]. \t  3.6742657043199305 \t 3.8225969778860365\n",
      "76     \t [0.45611373 0.59907634 0.79962507]. \t  3.500251720386479 \t 3.8225969778860365\n",
      "77     \t [0.6311921  0.54410717 0.88932913]. \t  3.689142636381173 \t 3.8225969778860365\n",
      "78     \t [0.75167395 0.65688837 0.85529144]. \t  3.397965930403134 \t 3.8225969778860365\n",
      "79     \t [0.95179518 0.51002234 0.91630718]. \t  3.2680083050691007 \t 3.8225969778860365\n",
      "80     \t [0.68939258 0.52375207 0.88165166]. \t  3.691592425297216 \t 3.8225969778860365\n",
      "81     \t [0.77188742 0.58862194 0.89803571]. \t  3.557402551457265 \t 3.8225969778860365\n",
      "82     \t [0.62950017 0.54667585 0.86157548]. \t  3.80675748814033 \t 3.8225969778860365\n",
      "83     \t [0.45094296 0.55922117 0.79823659]. \t  3.564280359421383 \t 3.8225969778860365\n",
      "84     \t [0.80984916 0.55964952 0.79882561]. \t  3.425194588510967 \t 3.8225969778860365\n",
      "85     \t [0.65048937 0.59658293 0.87813901]. \t  3.6944486474659373 \t 3.8225969778860365\n",
      "86     \t [0.48823953 0.56184459 0.90186789]. \t  3.616290276594527 \t 3.8225969778860365\n",
      "87     \t [0.73349387 0.60263227 0.87273575]. \t  3.669444763056104 \t 3.8225969778860365\n",
      "88     \t [0.66240496 0.60063145 0.854831  ]. \t  3.7181474150796885 \t 3.8225969778860365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.83708022 0.54399847 0.88881783]. \t  3.632970735674914 \t 3.8225969778860365\n",
      "90     \t [0.80233167 0.48814811 0.90335908]. \t  3.3723543906826103 \t 3.8225969778860365\n",
      "91     \t [0.53765772 0.55259465 0.85440826]. \t  \u001b[92m3.8341023124015146\u001b[0m \t 3.8341023124015146\n",
      "92     \t [0.76393116 0.43812402 0.84668039]. \t  3.3526424371430634 \t 3.8341023124015146\n",
      "93     \t [0.61153534 0.54534402 0.85233832]. \t  3.81412469644216 \t 3.8341023124015146\n",
      "94     \t [0.91989219 0.50565669 0.85647217]. \t  3.640491256681463 \t 3.8341023124015146\n",
      "95     \t [0.48905214 0.57651956 0.85369718]. \t  3.8242036172219303 \t 3.8341023124015146\n",
      "96     \t [0.64835443 0.59681207 0.84264735]. \t  3.712611342931939 \t 3.8341023124015146\n",
      "97     \t [0.65685007 0.62741438 0.88488034]. \t  3.542806741698026 \t 3.8341023124015146\n",
      "98     \t [0.55670654 0.55550109 0.94893878]. \t  2.9843173596562775 \t 3.8341023124015146\n",
      "99     \t [0.79144612 0.56616938 0.89651198]. \t  3.594204634488944 \t 3.8341023124015146\n",
      "100    \t [0.3639437  0.49296533 0.84844836]. \t  3.7294242837998492 \t 3.8341023124015146\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.610000357863649\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.610000357863649\n",
      "5      \t [1.         1.         0.39403355]. \t  0.1096929324684661 \t 2.610000357863649\n",
      "6      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.610000357863649\n",
      "8      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.610000357863649\n",
      "9      \t [1.         0.55241331 0.        ]. \t  0.0053408663793728115 \t 2.610000357863649\n",
      "10     \t [0.         0.5333334  0.57188989]. \t  1.3833440227595115 \t 2.610000357863649\n",
      "11     \t [0.         1.         0.52562148]. \t  2.41809324875465 \t 2.610000357863649\n",
      "12     \t [0.         0.50823609 0.        ]. \t  0.01694185175082114 \t 2.610000357863649\n",
      "13     \t [1.         0.51970668 1.        ]. \t  1.9648099276043962 \t 2.610000357863649\n",
      "14     \t [1.         0.         0.47505667]. \t  0.08556608903734182 \t 2.610000357863649\n",
      "15     \t [0.         0.         0.47320696]. \t  0.17641075535713294 \t 2.610000357863649\n",
      "16     \t [0.49012536 1.         1.        ]. \t  0.33249134765120086 \t 2.610000357863649\n",
      "17     \t [0.43177669 0.         0.        ]. \t  0.10104108216608969 \t 2.610000357863649\n",
      "18     \t [0.50417313 0.         1.        ]. \t  0.09163578301551506 \t 2.610000357863649\n",
      "19     \t [0.18356768 0.58288561 1.        ]. \t  2.075668596416102 \t 2.610000357863649\n",
      "20     \t [0.29745426 1.         0.5689374 ]. \t  2.3161342309382214 \t 2.610000357863649\n",
      "21     \t [1.         0.47343664 0.43209113]. \t  0.11147589295064456 \t 2.610000357863649\n",
      "22     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.610000357863649\n",
      "23     \t [0.        0.6581611 1.       ]. \t  1.8995957167410338 \t 2.610000357863649\n",
      "24     \t [0.12372483 0.82983266 0.77752852]. \t  2.0345708521057198 \t 2.610000357863649\n",
      "25     \t [0.72634237 0.68208665 1.        ]. \t  1.8126005859949927 \t 2.610000357863649\n",
      "26     \t [0.20614615 0.23051953 0.20908096]. \t  0.7347678264100443 \t 2.610000357863649\n",
      "27     \t [0.86229243 0.25339854 0.85131316]. \t  1.652047873004636 \t 2.610000357863649\n",
      "28     \t [ 6.01336754e-01 -1.11022302e-16  4.07031408e-01]. \t  0.4163022283121983 \t 2.610000357863649\n",
      "29     \t [0.75046111 1.         0.69688868]. \t  0.6059143122267022 \t 2.610000357863649\n",
      "30     \t [0.23924503 0.         0.75569655]. \t  0.24183429387802186 \t 2.610000357863649\n",
      "31     \t [0.         0.29306014 1.        ]. \t  0.9855009569683016 \t 2.610000357863649\n",
      "32     \t [0.14371642 1.         0.26795397]. \t  0.21637331886645472 \t 2.610000357863649\n",
      "33     \t [1.         0.99321872 0.7709804 ]. \t  0.4556350662693879 \t 2.610000357863649\n",
      "34     \t [0.69274655 1.         0.        ]. \t  0.00012525684616114044 \t 2.610000357863649\n",
      "35     \t [1.         0.19928428 0.        ]. \t  0.03317235043573726 \t 2.610000357863649\n",
      "36     \t [0.60940496 0.45075799 0.        ]. \t  0.032379459593107215 \t 2.610000357863649\n",
      "37     \t [1.         0.00847091 0.75590592]. \t  0.25436067431170206 \t 2.610000357863649\n",
      "38     \t [0.1712243  0.         0.30545502]. \t  0.7428706908994156 \t 2.610000357863649\n",
      "39     \t [0.48933515 0.37689929 1.        ]. \t  1.4618554215038628 \t 2.610000357863649\n",
      "40     \t [0.64264432 0.87141022 0.25919486]. \t  0.09573896317164753 \t 2.610000357863649\n",
      "41     \t [0.         0.2065082  0.23055391]. \t  0.5907743205209249 \t 2.610000357863649\n",
      "42     \t [0.8076751 0.        0.       ]. \t  0.057386920679900325 \t 2.610000357863649\n",
      "43     \t [0.         0.65247558 0.12187195]. \t  0.0264844459208941 \t 2.610000357863649\n",
      "44     \t [0.3855109  0.18699989 0.60930021]. \t  0.42795528746208156 \t 2.610000357863649\n",
      "45     \t [0.43942798 0.75689951 0.72493035]. \t  2.178460752144381 \t 2.610000357863649\n",
      "46     \t [1.         0.19552052 1.        ]. \t  0.5222910920160517 \t 2.610000357863649\n",
      "47     \t [0.69694431 0.00184339 0.87573861]. \t  0.21851778196850172 \t 2.610000357863649\n",
      "48     \t [0.83941133 0.81676173 0.        ]. \t  0.0005083716850679671 \t 2.610000357863649\n",
      "49     \t [1.         0.77555152 1.        ]. \t  1.310634505007536 \t 2.610000357863649\n",
      "50     \t [5.15232885e-11 8.32805316e-01 6.54792855e-01]. \t  2.555140227512654 \t 2.610000357863649\n",
      "51     \t [0.16709745 0.26238782 0.        ]. \t  0.08399948511895144 \t 2.610000357863649\n",
      "52     \t [0.23037359 0.50640647 0.77467159]. \t  \u001b[92m3.3097202234761767\u001b[0m \t 3.3097202234761767\n",
      "53     \t [0.17026579 0.67504728 0.62745865]. \t  2.4162124455656624 \t 3.3097202234761767\n",
      "54     \t [0.3898643  0.52899715 0.81242799]. \t  \u001b[92m3.6863644506853372\u001b[0m \t 3.6863644506853372\n",
      "55     \t [0.50067365 0.55814314 0.89026591]. \t  \u001b[92m3.7120369714138044\u001b[0m \t 3.7120369714138044\n",
      "56     \t [0.9504061  0.51448599 0.88196542]. \t  3.578182531744687 \t 3.7120369714138044\n",
      "57     \t [0.45722529 0.61733784 0.82283267]. \t  3.606682167011595 \t 3.7120369714138044\n",
      "58     \t [0.44660966 0.59128207 0.79722321]. \t  3.5035688581845132 \t 3.7120369714138044\n",
      "59     \t [0.97886458 0.51205215 0.81735679]. \t  3.4992359860678026 \t 3.7120369714138044\n",
      "60     \t [0.94799715 0.52599815 0.8428069 ]. \t  3.6597058158729014 \t 3.7120369714138044\n",
      "61     \t [0.30621125 0.45917391 0.87283353]. \t  3.4993042294920005 \t 3.7120369714138044\n",
      "62     \t [0.55469882 0.54897763 0.85721779]. \t  \u001b[92m3.8293666595462295\u001b[0m \t 3.8293666595462295\n",
      "63     \t [0.99673217 0.59663525 0.85441138]. \t  3.5965665351648743 \t 3.8293666595462295\n",
      "64     \t [0.94720616 0.47242257 0.90623821]. \t  3.219553890971289 \t 3.8293666595462295\n",
      "65     \t [0.44354707 0.53893244 0.83841449]. \t  3.8212533833598075 \t 3.8293666595462295\n",
      "66     \t [0.47594705 0.57925967 0.8076725 ]. \t  3.6130052734315745 \t 3.8293666595462295\n",
      "67     \t [0.53487077 0.61937154 0.89179696]. \t  3.565098971427934 \t 3.8293666595462295\n",
      "68     \t [0.50816212 0.4928744  0.78040692]. \t  3.2870861646763965 \t 3.8293666595462295\n",
      "69     \t [0.24907094 0.6030287  0.88781508]. \t  3.6696272680487736 \t 3.8293666595462295\n",
      "70     \t [0.50036184 0.38998395 0.8641784 ]. \t  2.982623997961752 \t 3.8293666595462295\n",
      "71     \t [0.38388103 0.44650792 0.84355201]. \t  3.471666888811298 \t 3.8293666595462295\n",
      "72     \t [0.38493687 0.54553008 0.82933352]. \t  3.800234806809863 \t 3.8293666595462295\n",
      "73     \t [0.28694735 0.57115189 0.74554504]. \t  3.024369239667151 \t 3.8293666595462295\n",
      "74     \t [0.60520309 0.55790865 0.83814424]. \t  3.78594721202944 \t 3.8293666595462295\n",
      "75     \t [0.45506756 0.49871733 0.86237334]. \t  3.733421628631016 \t 3.8293666595462295\n",
      "76     \t [0.62685671 0.51753651 0.88366394]. \t  3.6796162690190855 \t 3.8293666595462295\n",
      "77     \t [0.4245507  0.63966153 0.76985892]. \t  3.070004046284165 \t 3.8293666595462295\n",
      "78     \t [0.46056979 0.51504639 0.82864735]. \t  3.7433740402722933 \t 3.8293666595462295\n",
      "79     \t [0.2479777  0.56996802 0.79525282]. \t  3.5677035511327047 \t 3.8293666595462295\n",
      "80     \t [0.97528777 0.61101149 0.88199524]. \t  3.5132640977745324 \t 3.8293666595462295\n",
      "81     \t [0.76776077 0.5156059  0.89858277]. \t  3.5274665214557146 \t 3.8293666595462295\n",
      "82     \t [0.99807669 0.53227432 0.89848693]. \t  3.4819555325225555 \t 3.8293666595462295\n",
      "83     \t [0.01442136 0.18544626 0.79195273]. \t  1.1029869805388102 \t 3.8293666595462295\n",
      "84     \t [0.72264673 0.54185131 0.85937662]. \t  3.7792188593199314 \t 3.8293666595462295\n",
      "85     \t [0.98273638 0.5275688  0.85091083]. \t  3.660753933371679 \t 3.8293666595462295\n",
      "86     \t [0.41553855 0.48065549 0.77676844]. \t  3.230769578193761 \t 3.8293666595462295\n",
      "87     \t [0.2491679  0.56829007 0.86205731]. \t  \u001b[92m3.8463719539915955\u001b[0m \t 3.8463719539915955\n",
      "88     \t [0.41282724 0.4716096  0.84402316]. \t  3.6222279167073532 \t 3.8463719539915955\n",
      "89     \t [0.51316516 0.42935668 0.89228942]. \t  3.1619789020259117 \t 3.8463719539915955\n",
      "90     \t [0.14941374 0.52339629 0.84834939]. \t  3.8130408855223177 \t 3.8463719539915955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.42561506 0.54511276 0.81686928]. \t  3.7238911163332777 \t 3.8463719539915955\n",
      "92     \t [0.58915779 0.61827511 0.85944557]. \t  3.67202858198469 \t 3.8463719539915955\n",
      "93     \t [0.46892992 0.62611875 0.82729685]. \t  3.5874853718221016 \t 3.8463719539915955\n",
      "94     \t [0.907669   0.46177227 0.85743066]. \t  3.450561405927408 \t 3.8463719539915955\n",
      "95     \t [0.59996432 0.4988854  0.8278061 ]. \t  3.6640349961844114 \t 3.8463719539915955\n",
      "96     \t [0.07099975 0.9922262  0.65040962]. \t  2.092759258100665 \t 3.8463719539915955\n",
      "97     \t [0.32523804 0.52599822 0.84823202]. \t  3.831315180540739 \t 3.8463719539915955\n",
      "98     \t [0.54013664 0.55060357 0.76342933]. \t  3.140203000107105 \t 3.8463719539915955\n",
      "99     \t [0.4061525  0.61296701 0.89524398]. \t  3.5816071816864943 \t 3.8463719539915955\n",
      "100    \t [0.65785439 0.50215252 0.83164283]. \t  3.6752325666209935 \t 3.8463719539915955\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.540625560354162\n",
      "2      \t [ 1.00000000e+00 -5.55111512e-17  1.00000000e+00]. \t  0.08848201872702738 \t 1.540625560354162\n",
      "3      \t [0.08482078 0.         1.        ]. \t  0.09083736188881222 \t 1.540625560354162\n",
      "4      \t [5.55111512e-17 1.00000000e+00 0.00000000e+00]. \t  0.0002735367680454459 \t 1.540625560354162\n",
      "5      \t [0.73209513 1.         1.        ]. \t  0.32664904504033143 \t 1.540625560354162\n",
      "6      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.540625560354162\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.540625560354162\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.540625560354162\n",
      "9      \t [0.41924569 0.48144949 1.        ]. \t  \u001b[92m1.9415429776491033\u001b[0m \t 1.9415429776491033\n",
      "10     \t [ 1.00000000e+00 -2.77555756e-17  5.03934980e-01]. \t  0.0716179554856487 \t 1.9415429776491033\n",
      "11     \t [0.         0.49548781 1.        ]. \t  \u001b[92m1.9554269293156434\u001b[0m \t 1.9554269293156434\n",
      "12     \t [1.         0.56227701 1.        ]. \t  \u001b[92m2.0083451068282767\u001b[0m \t 2.0083451068282767\n",
      "13     \t [0.49972683 1.         0.        ]. \t  0.00020412808950374386 \t 2.0083451068282767\n",
      "14     \t [0.         1.         0.52613344]. \t  \u001b[92m2.4203697048947026\u001b[0m \t 2.4203697048947026\n",
      "15     \t [0.         0.         0.54177425]. \t  0.10311247586868966 \t 2.4203697048947026\n",
      "16     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.4203697048947026\n",
      "17     \t [0.         0.7113602  0.68783735]. \t  \u001b[92m2.4938165937811587\u001b[0m \t 2.4938165937811587\n",
      "18     \t [0.46804375 0.         0.        ]. \t  0.0992755301773136 \t 2.4938165937811587\n",
      "19     \t [0.60496497 0.         0.78112127]. \t  0.25017184124928493 \t 2.4938165937811587\n",
      "20     \t [1.         0.31899533 0.75834019]. \t  1.9435058473177484 \t 2.4938165937811587\n",
      "21     \t [0.25125032 1.         0.73372587]. \t  1.2096482448048103 \t 2.4938165937811587\n",
      "22     \t [0.2147754  0.50402304 0.        ]. \t  0.024486462641436127 \t 2.4938165937811587\n",
      "23     \t [0.79254586 0.30970847 1.        ]. \t  1.0730603111784776 \t 2.4938165937811587\n",
      "24     \t [0.28201494 0.37698132 0.69629657]. \t  1.823204742197367 \t 2.4938165937811587\n",
      "25     \t [1.         0.41625534 0.        ]. \t  0.014499438547233156 \t 2.4938165937811587\n",
      "26     \t [1.        1.        0.3422746]. \t  0.06127694015975667 \t 2.4938165937811587\n",
      "27     \t [0.24780765 0.         0.29349873]. \t  0.8178365551670035 \t 2.4938165937811587\n",
      "28     \t [1.         0.42607446 0.38060298]. \t  0.10534447487242796 \t 2.4938165937811587\n",
      "29     \t [0.00089592 0.8016897  0.26293237]. \t  0.2202769521987189 \t 2.4938165937811587\n",
      "30     \t [0.69041222 1.         0.43252103]. \t  0.5944484663020517 \t 2.4938165937811587\n",
      "31     \t [0.66378895 0.65751659 0.9081307 ]. \t  \u001b[92m3.2124952176885024\u001b[0m \t 3.2124952176885024\n",
      "32     \t [0.39326856 0.74830187 1.        ]. \t  1.5159197098821124 \t 3.2124952176885024\n",
      "33     \t [0.18305707 1.         0.09650552]. \t  0.004776856421190804 \t 3.2124952176885024\n",
      "34     \t [0.57813143 0.54762861 0.73553656]. \t  2.7428128046132043 \t 3.2124952176885024\n",
      "35     \t [1.         0.9714251  0.77709163]. \t  0.5535573563575411 \t 3.2124952176885024\n",
      "36     \t [3.83306128e-10 3.47518718e-01 7.06026485e-10]. \t  0.04583356336069581 \t 3.2124952176885024\n",
      "37     \t [0.22538682 0.10035801 0.        ]. \t  0.10991699294878234 \t 3.2124952176885024\n",
      "38     \t [0.         0.25174622 0.89646874]. \t  1.487495433025562 \t 3.2124952176885024\n",
      "39     \t [0.0010987  0.9834693  0.80200157]. \t  0.9343346387181919 \t 3.2124952176885024\n",
      "40     \t [0.82175492 0.21994117 0.        ]. \t  0.057002253620212814 \t 3.2124952176885024\n",
      "41     \t [6.28914898e-01 2.11736278e-10 1.00000000e+00]. \t  0.0912530727288492 \t 3.2124952176885024\n",
      "42     \t [0.78637733 0.66423455 1.        ]. \t  1.872440910026699 \t 3.2124952176885024\n",
      "43     \t [0.75597316 0.         0.13102965]. \t  0.3187181082063751 \t 3.2124952176885024\n",
      "44     \t [5.90237724e-01 7.05994454e-01 6.94811295e-08]. \t  0.003263369979172051 \t 3.2124952176885024\n",
      "45     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.2124952176885024\n",
      "46     \t [0.9869975  0.7113958  0.85013582]. \t  2.861826611874696 \t 3.2124952176885024\n",
      "47     \t [0.99953162 0.54951963 0.83996966]. \t  \u001b[92m3.641737635356061\u001b[0m \t 3.641737635356061\n",
      "48     \t [0.46070635 0.6571576  0.81709181]. \t  3.358545346945025 \t 3.641737635356061\n",
      "49     \t [0.12807809 0.71043487 0.78441767]. \t  2.8944366251963025 \t 3.641737635356061\n",
      "50     \t [4.80382039e-08 9.99999968e-01 2.95528032e-01]. \t  0.32888760669653466 \t 3.641737635356061\n",
      "51     \t [0.77466552 0.51472641 0.82920059]. \t  \u001b[92m3.658761375513091\u001b[0m \t 3.658761375513091\n",
      "52     \t [0.87778514 0.56759652 0.88137217]. \t  \u001b[92m3.6598277213345844\u001b[0m \t 3.6598277213345844\n",
      "53     \t [0.79489101 0.65541537 0.81452018]. \t  3.194884273275409 \t 3.6598277213345844\n",
      "54     \t [0.80988271 0.57396563 0.87442142]. \t  \u001b[92m3.7066777756857445\u001b[0m \t 3.7066777756857445\n",
      "55     \t [0.78831681 0.6133951  0.8261288 ]. \t  3.515218565020726 \t 3.7066777756857445\n",
      "56     \t [0.70542628 0.48160683 0.89005918]. \t  3.4804528353765574 \t 3.7066777756857445\n",
      "57     \t [0.54134615 0.55111158 0.84553353]. \t  \u001b[92m3.8245307409040867\u001b[0m \t 3.8245307409040867\n",
      "58     \t [0.90232639 0.52957889 0.86519088]. \t  3.6923795748979615 \t 3.8245307409040867\n",
      "59     \t [0.79536176 0.6383282  0.8702079 ]. \t  3.492262817819598 \t 3.8245307409040867\n",
      "60     \t [0.26145564 0.56450224 0.78986231]. \t  3.521291491894814 \t 3.8245307409040867\n",
      "61     \t [0.83904742 0.59077945 0.88926576]. \t  3.5925667377136903 \t 3.8245307409040867\n",
      "62     \t [0.68446677 0.51279333 0.81229842]. \t  3.580839860297626 \t 3.8245307409040867\n",
      "63     \t [0.52567128 0.62899831 0.77319968]. \t  3.085241520961751 \t 3.8245307409040867\n",
      "64     \t [0.82466859 0.55686279 0.87443411]. \t  3.7152588818311596 \t 3.8245307409040867\n",
      "65     \t [0.50210875 0.60926231 0.86352957]. \t  3.7272032956928065 \t 3.8245307409040867\n",
      "66     \t [0.06418233 0.50351305 0.73509287]. \t  2.816985152498666 \t 3.8245307409040867\n",
      "67     \t [0.4776729  0.63141189 0.83429465]. \t  3.590361715172583 \t 3.8245307409040867\n",
      "68     \t [0.62446495 0.54791673 0.88388215]. \t  3.728567651381497 \t 3.8245307409040867\n",
      "69     \t [0.57665216 0.5568635  0.88157792]. \t  3.754171181933457 \t 3.8245307409040867\n",
      "70     \t [0.65347555 0.52821071 0.88467281]. \t  3.692109724830107 \t 3.8245307409040867\n",
      "71     \t [0.99777814 0.03174365 0.14600016]. \t  0.18258370078103925 \t 3.8245307409040867\n",
      "72     \t [0.29494063 0.61369802 0.87519607]. \t  3.700815701671898 \t 3.8245307409040867\n",
      "73     \t [0.66033082 0.57815573 0.86067891]. \t  3.7773852837030812 \t 3.8245307409040867\n",
      "74     \t [0.60767731 0.52111827 0.86551878]. \t  3.7698027690313385 \t 3.8245307409040867\n",
      "75     \t [0.4871254  0.52984346 0.82683505]. \t  3.7571558209719513 \t 3.8245307409040867\n",
      "76     \t [0.78033333 0.56435585 0.91676593]. \t  3.3977313459139586 \t 3.8245307409040867\n",
      "77     \t [0.65727694 0.55433864 0.84811928]. \t  3.7973493248699297 \t 3.8245307409040867\n",
      "78     \t [0.86376683 0.55275623 0.86425874]. \t  3.7267047572284535 \t 3.8245307409040867\n",
      "79     \t [0.23971755 0.57924837 0.8705885 ]. \t  3.810420017853351 \t 3.8245307409040867\n",
      "80     \t [0.94523047 0.58616691 0.82232188]. \t  3.516185661484331 \t 3.8245307409040867\n",
      "81     \t [0.73374073 0.59757252 0.81266867]. \t  3.501540514981656 \t 3.8245307409040867\n",
      "82     \t [0.69390742 0.5783171  0.83294935]. \t  3.7072361538013103 \t 3.8245307409040867\n",
      "83     \t [0.86849217 0.5815052  0.86750985]. \t  3.6892206450215186 \t 3.8245307409040867\n",
      "84     \t [0.45554388 0.56453606 0.87103391]. \t  3.8176385555315235 \t 3.8245307409040867\n",
      "85     \t [0.31987897 0.57677953 0.8572652 ]. \t  \u001b[92m3.8429444041621084\u001b[0m \t 3.8429444041621084\n",
      "86     \t [0.56965328 0.53517017 0.84099421]. \t  3.7989218302072594 \t 3.8429444041621084\n",
      "87     \t [0.69616726 0.5219273  0.84847024]. \t  3.758316522020307 \t 3.8429444041621084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.19333861 0.52943629 0.84441475]. \t  3.82755004937123 \t 3.8429444041621084\n",
      "89     \t [0.22523989 0.46307685 0.86479607]. \t  3.5485143101141867 \t 3.8429444041621084\n",
      "90     \t [0.317162   0.62590689 0.86344756]. \t  3.680570344511672 \t 3.8429444041621084\n",
      "91     \t [0.99283541 0.49417818 0.87946025]. \t  3.5052187616548487 \t 3.8429444041621084\n",
      "92     \t [5.4992402e-07 5.0967735e-07 1.7201110e-01]. \t  0.44153179842305645 \t 3.8429444041621084\n",
      "93     \t [0.37450194 0.61637475 0.90432365]. \t  3.4930345681493598 \t 3.8429444041621084\n",
      "94     \t [0.15838102 0.53702434 0.90588317]. \t  3.551937728533294 \t 3.8429444041621084\n",
      "95     \t [0.49519851 0.5715585  0.88832895]. \t  3.7194618274105085 \t 3.8429444041621084\n",
      "96     \t [0.28886814 0.54908483 0.76346403]. \t  3.236540958345147 \t 3.8429444041621084\n",
      "97     \t [0.45522576 0.60972918 0.80957742]. \t  3.547894852850064 \t 3.8429444041621084\n",
      "98     \t [0.72665915 0.48520817 0.85434783]. \t  3.6328765874134885 \t 3.8429444041621084\n",
      "99     \t [0.61987901 0.50286908 0.85537619]. \t  3.729787091818289 \t 3.8429444041621084\n",
      "100    \t [0.7484816  0.568986   0.88041113]. \t  3.708972976074263 \t 3.8429444041621084\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.06352555 0.         1.        ]. \t  0.09071177709658772 \t 3.8084053754826726\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.8084053754826726\n",
      "3      \t [0.         0.14021735 0.        ]. \t  0.07752818097496464 \t 3.8084053754826726\n",
      "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "5      \t [0.18654879 0.444685   0.65232415]. \t  1.6459459352536283 \t 3.8084053754826726\n",
      "6      \t [0.         0.50763301 1.        ]. \t  1.9864338663660417 \t 3.8084053754826726\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "8      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.8084053754826726\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "10     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.8084053754826726\n",
      "11     \t [0.         0.95849517 0.62420067]. \t  2.4443899113859606 \t 3.8084053754826726\n",
      "12     \t [0.90027582 0.51275001 0.        ]. \t  0.010508481705602875 \t 3.8084053754826726\n",
      "13     \t [ 5.13152243e-01 -5.55111512e-17  0.00000000e+00]. \t  0.0960588833131983 \t 3.8084053754826726\n",
      "14     \t [0.50733405 1.         0.87688163]. \t  0.6200518202621987 \t 3.8084053754826726\n",
      "15     \t [1.         0.         0.50502311]. \t  0.07128555695935472 \t 3.8084053754826726\n",
      "16     \t [0.         0.73327556 0.7828669 ]. \t  2.697380769858543 \t 3.8084053754826726\n",
      "17     \t [0.53180788 1.         0.        ]. \t  0.0001909294755564038 \t 3.8084053754826726\n",
      "18     \t [1.         1.         0.47217143]. \t  0.1974738909456483 \t 3.8084053754826726\n",
      "19     \t [1.         0.50658314 1.        ]. \t  1.9376544148809454 \t 3.8084053754826726\n",
      "20     \t [0.         0.         0.53699019]. \t  0.10516947088170732 \t 3.8084053754826726\n",
      "21     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "22     \t [0.56478402 0.32579315 1.        ]. \t  1.1759234697161705 \t 3.8084053754826726\n",
      "23     \t [0.16713479 1.         0.74337814]. \t  1.183463788059563 \t 3.8084053754826726\n",
      "24     \t [0.         0.26656022 0.82706883]. \t  1.8026198886986013 \t 3.8084053754826726\n",
      "25     \t [0.22940699 0.60422526 1.        ]. \t  2.056651302308545 \t 3.8084053754826726\n",
      "26     \t [0.36839592 0.         0.77099835]. \t  0.2485193456811489 \t 3.8084053754826726\n",
      "27     \t [0.74613893 0.69714154 0.78514956]. \t  2.657543279379188 \t 3.8084053754826726\n",
      "28     \t [0.75418164 1.         0.31225901]. \t  0.1260177958199433 \t 3.8084053754826726\n",
      "29     \t [0.49638112 0.54196914 0.        ]. \t  0.018410828055284524 \t 3.8084053754826726\n",
      "30     \t [0.00000000e+00 6.59937556e-01 8.77330152e-09]. \t  0.004268584322023821 \t 3.8084053754826726\n",
      "31     \t [0.98782926 0.57829216 0.28314122]. \t  0.0515494627920054 \t 3.8084053754826726\n",
      "32     \t [0.24378626 0.45575575 0.86451497]. \t  3.505052040401479 \t 3.8084053754826726\n",
      "33     \t [0.77235147 0.         0.        ]. \t  0.06274431293966148 \t 3.8084053754826726\n",
      "34     \t [0.15413984 1.         0.22263127]. \t  0.09323005927710579 \t 3.8084053754826726\n",
      "35     \t [0.16816629 0.         0.20455069]. \t  0.6867159976306685 \t 3.8084053754826726\n",
      "36     \t [1.         0.63875002 0.80149992]. \t  3.074712152152129 \t 3.8084053754826726\n",
      "37     \t [0.99873096 1.         0.79597263]. \t  0.47374245963462147 \t 3.8084053754826726\n",
      "38     \t [0.43434895 0.53606736 0.84974481]. \t  \u001b[92m3.8400563593923587\u001b[0m \t 3.8400563593923587\n",
      "39     \t [0.34999991 0.53619281 0.8618512 ]. \t  3.839974467386482 \t 3.8400563593923587\n",
      "40     \t [0.43797846 0.54156725 0.85238817]. \t  \u001b[92m3.8463849082947608\u001b[0m \t 3.8463849082947608\n",
      "41     \t [0.36448208 0.58023483 0.83740726]. \t  3.808295097781509 \t 3.8463849082947608\n",
      "42     \t [0.45043281 0.55073915 0.88117071]. \t  3.774672168195389 \t 3.8463849082947608\n",
      "43     \t [0.45164502 0.56499692 0.88088795]. \t  3.775037807418519 \t 3.8463849082947608\n",
      "44     \t [0.93999507 0.18466744 0.        ]. \t  0.042099958468913505 \t 3.8463849082947608\n",
      "45     \t [0.64623342 0.0029362  0.38829282]. \t  0.45160671643742517 \t 3.8463849082947608\n",
      "46     \t [0.39030874 0.59975527 0.84104261]. \t  3.7657515575380858 \t 3.8463849082947608\n",
      "47     \t [0.37300742 0.51149299 0.81639557]. \t  3.681244429072194 \t 3.8463849082947608\n",
      "48     \t [0.01345362 0.58814985 0.23947958]. \t  0.13849273106635251 \t 3.8463849082947608\n",
      "49     \t [0.39114293 0.5877849  0.82687739]. \t  3.744438083979967 \t 3.8463849082947608\n",
      "50     \t [0.38921603 0.56649402 0.85262373]. \t  \u001b[92m3.8515460263452965\u001b[0m \t 3.8515460263452965\n",
      "51     \t [0.22849129 0.53605699 0.87616614]. \t  3.788098655856543 \t 3.8515460263452965\n",
      "52     \t [0.32723912 0.49038783 0.8155521 ]. \t  3.612290123440908 \t 3.8515460263452965\n",
      "53     \t [0.60344098 0.         1.        ]. \t  0.09135418492405623 \t 3.8515460263452965\n",
      "54     \t [0.99999995 0.         0.83119862]. \t  0.23655691551314584 \t 3.8515460263452965\n",
      "55     \t [0.29071062 0.54478279 0.81249008]. \t  3.711795012373986 \t 3.8515460263452965\n",
      "56     \t [0.50735693 0.55230485 0.89256652]. \t  3.692876816873601 \t 3.8515460263452965\n",
      "57     \t [0.26595578 0.52786881 0.8360593 ]. \t  3.8121885887562774 \t 3.8515460263452965\n",
      "58     \t [0.2158844  0.61505546 0.86026349]. \t  3.7314419279614244 \t 3.8515460263452965\n",
      "59     \t [0.00261856 0.89420642 0.27383627]. \t  0.2724576311615674 \t 3.8515460263452965\n",
      "60     \t [0.62722165 0.5838961  0.89331556]. \t  3.6423634664211435 \t 3.8515460263452965\n",
      "61     \t [0.36555617 0.58900975 0.80997517]. \t  3.639910136989284 \t 3.8515460263452965\n",
      "62     \t [0.36749204 0.53530764 0.87486016]. \t  3.797084062988681 \t 3.8515460263452965\n",
      "63     \t [0.71264074 0.69199297 0.98113405]. \t  2.0597213675025516 \t 3.8515460263452965\n",
      "64     \t [0.42456505 0.55575072 0.86853279]. \t  3.8319418327586066 \t 3.8515460263452965\n",
      "65     \t [0.31829415 0.51272439 0.83250722]. \t  3.767160535122776 \t 3.8515460263452965\n",
      "66     \t [0.06259595 0.48840221 0.85312274]. \t  3.6763291813884367 \t 3.8515460263452965\n",
      "67     \t [0.26065689 0.5060002  0.87759128]. \t  3.7073115983773315 \t 3.8515460263452965\n",
      "68     \t [0.31158879 0.486199   0.88911618]. \t  3.5533311119250617 \t 3.8515460263452965\n",
      "69     \t [0.42266243 0.50011355 0.77898818]. \t  3.3141768775424447 \t 3.8515460263452965\n",
      "70     \t [0.2379653  0.56620641 0.84659914]. \t  3.8514974443651546 \t 3.8515460263452965\n",
      "71     \t [0.98213572 0.43119343 0.86274744]. \t  3.2131942082154197 \t 3.8515460263452965\n",
      "72     \t [0.25025865 0.49817968 0.80930379]. \t  3.6007228933938826 \t 3.8515460263452965\n",
      "73     \t [0.52718508 0.5396232  0.91970199]. \t  3.4003327048727474 \t 3.8515460263452965\n",
      "74     \t [0.05736842 0.4879553  0.82102697]. \t  3.602568948532407 \t 3.8515460263452965\n",
      "75     \t [0.40831855 0.54854732 0.85296995]. \t  \u001b[92m3.8542205556490714\u001b[0m \t 3.8542205556490714\n",
      "76     \t [0.29103228 0.54179133 0.90688619]. \t  3.5617358348931116 \t 3.8542205556490714\n",
      "77     \t [0.88721719 0.58590453 0.81744956]. \t  3.5074431251266347 \t 3.8542205556490714\n",
      "78     \t [0.31570428 0.5866649  0.8868041 ]. \t  3.7196741830937388 \t 3.8542205556490714\n",
      "79     \t [0.49254834 0.57721608 0.81827002]. \t  3.69147813877718 \t 3.8542205556490714\n",
      "80     \t [0.13687621 0.47143308 0.81992687]. \t  3.5387665080059554 \t 3.8542205556490714\n",
      "81     \t [0.45477177 0.60124668 0.8666944 ]. \t  3.7584034183741943 \t 3.8542205556490714\n",
      "82     \t [0.45538391 0.54676818 0.85461998]. \t  3.8474941743476574 \t 3.8542205556490714\n",
      "83     \t [0.66839054 0.55267888 0.84459152]. \t  3.787407413649402 \t 3.8542205556490714\n",
      "84     \t [0.65851214 0.43161574 0.8406884 ]. \t  3.3284953464353464 \t 3.8542205556490714\n",
      "85     \t [0.54000201 0.60700169 0.78395231]. \t  3.2795780915070156 \t 3.8542205556490714\n",
      "86     \t [0.73550378 0.57306883 0.88608319]. \t  3.678165752837509 \t 3.8542205556490714\n",
      "87     \t [0.32014186 0.4927334  0.85450152]. \t  3.7278381942572056 \t 3.8542205556490714\n",
      "88     \t [0.1081828  0.50494682 0.86530904]. \t  3.732162560398498 \t 3.8542205556490714\n",
      "89     \t [0.6791448  0.5123382  0.85483635]. \t  3.7437580933800176 \t 3.8542205556490714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.59647757 0.46444458 0.85153307]. \t  3.5597149620177264 \t 3.8542205556490714\n",
      "91     \t [0.31402878 0.46534755 0.84569053]. \t  3.59167572041799 \t 3.8542205556490714\n",
      "92     \t [0.40161853 0.51766284 0.84997655]. \t  3.8096428677929706 \t 3.8542205556490714\n",
      "93     \t [0.21066949 0.56809281 0.79998352]. \t  3.611303020689033 \t 3.8542205556490714\n",
      "94     \t [0.35389819 0.53837774 0.87350453]. \t  3.807640544808489 \t 3.8542205556490714\n",
      "95     \t [0.         1.         0.76364248]. \t  1.0283628825945323 \t 3.8542205556490714\n",
      "96     \t [0.16126629 0.53280991 0.91131986]. \t  3.4857788657651545 \t 3.8542205556490714\n",
      "97     \t [0.43609597 0.57308102 0.86206526]. \t  3.8333417597928894 \t 3.8542205556490714\n",
      "98     \t [0.60075091 0.60321164 0.8104533 ]. \t  3.521721790679373 \t 3.8542205556490714\n",
      "99     \t [0.56503059 0.58827295 0.82591889]. \t  3.6915939334582544 \t 3.8542205556490714\n",
      "100    \t [0.21354019 0.59635179 0.86288733]. \t  3.7909788095095998 \t 3.8542205556490714\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.         0.12199007 1.        ]. \t  0.29793431076884563 \t 3.1179188940604616\n",
      "2      \t [1.         0.77440048 1.        ]. \t  1.3168241361541009 \t 3.1179188940604616\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.1179188940604616\n",
      "4      \t [0.       0.822979 1.      ]. \t  1.0866069844895994 \t 3.1179188940604616\n",
      "5      \t [1.        0.        0.1657394]. \t  0.19375236508676796 \t 3.1179188940604616\n",
      "6      \t [0.4584403  0.         0.55352536]. \t  0.12381636364343061 \t 3.1179188940604616\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1179188940604616\n",
      "8      \t [0.50302611 0.68879544 1.        ]. \t  1.8047484587062614 \t 3.1179188940604616\n",
      "9      \t [0.         0.59056267 0.53100651]. \t  1.5126765516256617 \t 3.1179188940604616\n",
      "10     \t [1.         0.43884828 0.6445568 ]. \t  1.1941935125605974 \t 3.1179188940604616\n",
      "11     \t [1.         1.         0.70130755]. \t  0.3291487481313118 \t 3.1179188940604616\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.1179188940604616\n",
      "13     \t [0.64950176 0.         0.        ]. \t  0.08073524962603351 \t 3.1179188940604616\n",
      "14     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.1179188940604616\n",
      "15     \t [0.         1.         0.62861296]. \t  2.176676976507836 \t 3.1179188940604616\n",
      "16     \t [ 1.00000000e+00  3.09285043e-01 -6.93889390e-18]. \t  0.02452580384892804 \t 3.1179188940604616\n",
      "17     \t [0.27874831 0.50335336 0.79112514]. \t  \u001b[92m3.4673085441480636\u001b[0m \t 3.4673085441480636\n",
      "18     \t [0.44567293 0.         1.        ]. \t  0.09171747085392465 \t 3.4673085441480636\n",
      "19     \t [0.4880584 1.        0.       ]. \t  0.00020885430471876443 \t 3.4673085441480636\n",
      "20     \t [0.60671513 0.35740111 0.81239687]. \t  2.651131371188268 \t 3.4673085441480636\n",
      "21     \t [0.         0.         0.61755673]. \t  0.1210997182155923 \t 3.4673085441480636\n",
      "22     \t [1.         0.40613204 1.        ]. \t  1.5581018321797149 \t 3.4673085441480636\n",
      "23     \t [0.         0.56921754 0.        ]. \t  0.010197159918968376 \t 3.4673085441480636\n",
      "24     \t [0.12833211 1.         1.        ]. \t  0.3331023619621045 \t 3.4673085441480636\n",
      "25     \t [0.19808021 0.79008914 0.7022743 ]. \t  2.3855366811584338 \t 3.4673085441480636\n",
      "26     \t [0.         0.39589743 0.8036652 ]. \t  2.9188342607162943 \t 3.4673085441480636\n",
      "27     \t [0.74411266 1.         1.        ]. \t  0.32627950864193117 \t 3.4673085441480636\n",
      "28     \t [1.         0.         0.70267346]. \t  0.19647595530798304 \t 3.4673085441480636\n",
      "29     \t [0.78576103 0.1670357  0.24564782]. \t  0.5713511177661967 \t 3.4673085441480636\n",
      "30     \t [0.22973115 0.18662734 0.83086352]. \t  1.143111234892447 \t 3.4673085441480636\n",
      "31     \t [0.48079389 0.49504438 0.6332868 ]. \t  1.477292651078302 \t 3.4673085441480636\n",
      "32     \t [0.13307829 0.52265848 0.90600451]. \t  \u001b[92m3.5168934621306893\u001b[0m \t 3.5168934621306893\n",
      "33     \t [0.78415907 1.         0.18572116]. \t  0.011159950207454446 \t 3.5168934621306893\n",
      "34     \t [8.15973676e-09 7.87967390e-11 2.27672289e-01]. \t  0.5531454411621803 \t 3.5168934621306893\n",
      "35     \t [0.14255359 1.         0.24254432]. \t  0.1372178981408944 \t 3.5168934621306893\n",
      "36     \t [1.         0.62098488 0.0247435 ]. \t  0.0041194536454237 \t 3.5168934621306893\n",
      "37     \t [2.73690000e-01 1.16298886e-09 5.71011932e-02]. \t  0.22546566870022625 \t 3.5168934621306893\n",
      "38     \t [0.77402713 0.65248735 0.87457387]. \t  3.4056785419418043 \t 3.5168934621306893\n",
      "39     \t [0.26028086 0.65140066 0.        ]. \t  0.006675875464676674 \t 3.5168934621306893\n",
      "40     \t [7.40850555e-01 1.38081939e-09 9.82478363e-01]. \t  0.10739939388720214 \t 3.5168934621306893\n",
      "41     \t [3.63393979e-07 7.09173752e-01 7.88056561e-01]. \t  2.884665401201652 \t 3.5168934621306893\n",
      "42     \t [0.27949477 0.43448665 0.99718242]. \t  1.7979968880755692 \t 3.5168934621306893\n",
      "43     \t [7.80080513e-01 7.41336813e-01 3.83984491e-08]. \t  0.0014960449556310077 \t 3.5168934621306893\n",
      "44     \t [1.         0.6166555  0.86477107]. \t  \u001b[92m3.521897058599147\u001b[0m \t 3.521897058599147\n",
      "45     \t [0.81028907 0.81028773 0.80744047]. \t  1.835052731535574 \t 3.521897058599147\n",
      "46     \t [0.85139173 0.58187118 0.87083069]. \t  \u001b[92m3.6881170516537782\u001b[0m \t 3.6881170516537782\n",
      "47     \t [0.89806448 0.50890684 0.83764743]. \t  3.6319367114064036 \t 3.6881170516537782\n",
      "48     \t [0.80410851 0.54553125 0.89696065]. \t  3.5845585819736177 \t 3.6881170516537782\n",
      "49     \t [6.22477585e-07 5.55434918e-01 9.40811259e-01]. \t  3.0805989447792874 \t 3.6881170516537782\n",
      "50     \t [0.75519356 0.54871752 0.88700575]. \t  3.67492052470175 \t 3.6881170516537782\n",
      "51     \t [0.99999925 1.         1.        ]. \t  0.3168836524589106 \t 3.6881170516537782\n",
      "52     \t [0.9866573  0.50000978 0.85886477]. \t  3.591954179136718 \t 3.6881170516537782\n",
      "53     \t [0.88521391 0.55602325 0.87532518]. \t  \u001b[92m3.6890031415651103\u001b[0m \t 3.6890031415651103\n",
      "54     \t [9.46961544e-06 5.24099619e-01 8.32012973e-01]. \t  \u001b[92m3.7424752921608517\u001b[0m \t 3.7424752921608517\n",
      "55     \t [0.54737322 0.58573611 0.86402656]. \t  \u001b[92m3.7879440931621993\u001b[0m \t 3.7879440931621993\n",
      "56     \t [0.74779361 0.5558409  0.86635397]. \t  3.7648627653219764 \t 3.7879440931621993\n",
      "57     \t [0.68280564 0.54208422 0.82677677]. \t  3.711416409231834 \t 3.7879440931621993\n",
      "58     \t [0.84553526 0.5538053  0.87108593]. \t  3.7187710067483426 \t 3.7879440931621993\n",
      "59     \t [0.87920828 0.55290691 0.85080792]. \t  3.7213652138353606 \t 3.7879440931621993\n",
      "60     \t [0.86152566 0.53652819 0.8161802 ]. \t  3.570138594637155 \t 3.7879440931621993\n",
      "61     \t [0.66178778 0.57050102 0.8903873 ]. \t  3.673707300123532 \t 3.7879440931621993\n",
      "62     \t [0.84524187 0.57755765 0.84675028]. \t  3.700040479361015 \t 3.7879440931621993\n",
      "63     \t [0.92617788 0.49126593 0.86353806]. \t  3.578042818139548 \t 3.7879440931621993\n",
      "64     \t [0.62354076 0.49594917 0.89250505]. \t  3.544833420742553 \t 3.7879440931621993\n",
      "65     \t [0.93380302 0.46964713 0.89125093]. \t  3.33887732579225 \t 3.7879440931621993\n",
      "66     \t [0.0033798  0.61768491 0.81983961]. \t  3.585962056706626 \t 3.7879440931621993\n",
      "67     \t [0.76401235 0.62586787 0.85633993]. \t  3.577157935378206 \t 3.7879440931621993\n",
      "68     \t [0.9560194  0.54045685 0.8783928 ]. \t  3.6410311218809492 \t 3.7879440931621993\n",
      "69     \t [0.09242895 0.58401918 0.81684372]. \t  3.695469783619871 \t 3.7879440931621993\n",
      "70     \t [0.03043951 0.55366249 0.84809994]. \t  \u001b[92m3.8189332017662374\u001b[0m \t 3.8189332017662374\n",
      "71     \t [0.86318653 0.55031383 0.87676778]. \t  3.691735653311485 \t 3.8189332017662374\n",
      "72     \t [0.06662543 0.5587969  0.84021711]. \t  3.8168385220107997 \t 3.8189332017662374\n",
      "73     \t [0.57956803 0.5369845  0.88866006]. \t  3.6962305777828637 \t 3.8189332017662374\n",
      "74     \t [0.1539429  0.53408856 0.88696288]. \t  3.7114602113287662 \t 3.8189332017662374\n",
      "75     \t [0.5062672  0.57877587 0.83649492]. \t  3.780950174218069 \t 3.8189332017662374\n",
      "76     \t [0.00958991 0.60052152 0.81914844]. \t  3.6435997674039506 \t 3.8189332017662374\n",
      "77     \t [0.9032212  0.54770812 0.86537325]. \t  3.708314804488275 \t 3.8189332017662374\n",
      "78     \t [0.39232599 0.54277683 0.81525315]. \t  3.718236112156063 \t 3.8189332017662374\n",
      "79     \t [0.07151047 0.63554015 0.85790916]. \t  3.6162748547168793 \t 3.8189332017662374\n",
      "80     \t [2.73362803e-08 1.00000000e+00 8.07811413e-01]. \t  0.8278191726737157 \t 3.8189332017662374\n",
      "81     \t [0.1144593  0.56449723 0.87695647]. \t  3.781897647489056 \t 3.8189332017662374\n",
      "82     \t [0.12810153 0.64319755 0.878863  ]. \t  3.530830107083479 \t 3.8189332017662374\n",
      "83     \t [0.21347095 0.58454971 0.85448151]. \t  \u001b[92m3.8278047514324443\u001b[0m \t 3.8278047514324443\n",
      "84     \t [0.96882327 0.51690723 0.84154482]. \t  3.631211876480342 \t 3.8278047514324443\n",
      "85     \t [0.83956066 0.52866438 0.86780753]. \t  3.709560600261733 \t 3.8278047514324443\n",
      "86     \t [0.66654993 0.55489542 0.87672833]. \t  3.7566125294832835 \t 3.8278047514324443\n",
      "87     \t [0.43563422 0.57535857 0.83388054]. \t  3.7929148829603063 \t 3.8278047514324443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.07414814 0.60314053 0.89546548]. \t  3.587168914461949 \t 3.8278047514324443\n",
      "89     \t [0.25595883 0.54700806 0.84137862]. \t  \u001b[92m3.8466845266829623\u001b[0m \t 3.8466845266829623\n",
      "90     \t [0.46895826 0.55730683 0.86723655]. \t  3.8299698207979755 \t 3.8466845266829623\n",
      "91     \t [0.74073999 0.55725965 0.8356339 ]. \t  3.7312670462532016 \t 3.8466845266829623\n",
      "92     \t [0.         0.         0.95011998]. \t  0.139877590675357 \t 3.8466845266829623\n",
      "93     \t [0.14313929 0.55366846 0.8096082 ]. \t  3.6869695105628173 \t 3.8466845266829623\n",
      "94     \t [0.82296281 0.55922461 0.82720824]. \t  3.6544061426186945 \t 3.8466845266829623\n",
      "95     \t [0.49798175 0.56154754 0.81747156]. \t  3.7058783073171586 \t 3.8466845266829623\n",
      "96     \t [0.73187467 0.55294263 0.83263877]. \t  3.723499850459691 \t 3.8466845266829623\n",
      "97     \t [0.69677365 0.57559725 0.82493375]. \t  3.667195326819348 \t 3.8466845266829623\n",
      "98     \t [0.0831836  0.59994967 0.85911097]. \t  3.7647971766998554 \t 3.8466845266829623\n",
      "99     \t [0.48974764 0.52558597 0.83322474]. \t  3.7783765368404603 \t 3.8466845266829623\n",
      "100    \t [0.60308337 0.56780372 0.8271877 ]. \t  3.7272049623591985 \t 3.8466845266829623\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.1210522139432408\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1210522139432408\n",
      "3      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.1210522139432408\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.1210522139432408\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1210522139432408\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.1210522139432408\n",
      "7      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.1210522139432408\n",
      "8      \t [0.5237059  0.55146191 1.        ]. \t  \u001b[92m2.0784047505327496\u001b[0m \t 2.0784047505327496\n",
      "9      \t [0.        0.5132746 1.       ]. \t  1.9990423529403933 \t 2.0784047505327496\n",
      "10     \t [1.         0.57339424 1.        ]. \t  2.008082611681817 \t 2.0784047505327496\n",
      "11     \t [0.        0.5081439 0.       ]. \t  0.01695397223385641 \t 2.0784047505327496\n",
      "12     \t [0.50263598 1.         0.        ]. \t  0.00020294250380643755 \t 2.0784047505327496\n",
      "13     \t [1.         0.49704117 0.        ]. \t  0.00838035727043413 \t 2.0784047505327496\n",
      "14     \t [0.         1.         0.50575706]. \t  \u001b[92m2.3040749849243056\u001b[0m \t 2.3040749849243056\n",
      "15     \t [0.48815835 0.         1.        ]. \t  0.09166437427833773 \t 2.3040749849243056\n",
      "16     \t [0.         0.63897603 0.6541542 ]. \t  \u001b[92m2.3100948617696266\u001b[0m \t 2.3100948617696266\n",
      "17     \t [1.         0.         0.48931263]. \t  0.07746669142445856 \t 2.3100948617696266\n",
      "18     \t [1.         1.         0.42719327]. \t  0.14680349427992223 \t 2.3100948617696266\n",
      "19     \t [0.30531881 1.         0.73003262]. \t  1.1863918625895584 \t 2.3100948617696266\n",
      "20     \t [0.         0.         0.40683803]. \t  0.3270270248113286 \t 2.3100948617696266\n",
      "21     \t [0.59003572 0.         0.21738816]. \t  0.6988769316739052 \t 2.3100948617696266\n",
      "22     \t [0.         0.75651991 0.31161795]. \t  0.44024848164318914 \t 2.3100948617696266\n",
      "23     \t [0.2663846  0.         0.72181537]. \t  0.22055079474051287 \t 2.3100948617696266\n",
      "24     \t [0.77655463 0.33103445 0.99999996]. \t  1.1906270964410715 \t 2.3100948617696266\n",
      "25     \t [0.69944629 1.         0.2587193 ]. \t  0.06514819090558409 \t 2.3100948617696266\n",
      "26     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3100948617696266\n",
      "27     \t [ 2.94755080e-01 -2.77555756e-17  1.42715644e-09]. \t  0.10057417216541248 \t 2.3100948617696266\n",
      "28     \t [0.79900027 0.70244227 0.        ]. \t  0.002238265787738284 \t 2.3100948617696266\n",
      "29     \t [0.60486583 1.         1.        ]. \t  0.3301277369191359 \t 2.3100948617696266\n",
      "30     \t [0.         0.84656018 0.78268774]. \t  1.8753177432318575 \t 2.3100948617696266\n",
      "31     \t [6.95651659e-01 8.48690917e-09 7.27409427e-01]. \t  0.22351165858431896 \t 2.3100948617696266\n",
      "32     \t [0.         0.26205909 0.83540724]. \t  1.7598304706895553 \t 2.3100948617696266\n",
      "33     \t [1.         0.3898951  0.89804287]. \t  \u001b[92m2.6876493287759815\u001b[0m \t 2.6876493287759815\n",
      "34     \t [0.99992667 0.43313267 0.55611847]. \t  0.4352024867490304 \t 2.6876493287759815\n",
      "35     \t [0.01109487 0.27180999 0.17268854]. \t  0.4107143621804036 \t 2.6876493287759815\n",
      "36     \t [7.63829504e-01 1.75679239e-12 0.00000000e+00]. \t  0.06403816711454983 \t 2.6876493287759815\n",
      "37     \t [0.19448869 0.99894104 0.2116672 ]. \t  0.07377395900985458 \t 2.6876493287759815\n",
      "38     \t [0.21405557 0.45811529 0.99728332]. \t  1.8964016376889976 \t 2.6876493287759815\n",
      "39     \t [0.99707892 0.78574346 0.71705177]. \t  1.2107249739566082 \t 2.6876493287759815\n",
      "40     \t [0.21769613 0.75555083 1.        ]. \t  1.4738657891134552 \t 2.6876493287759815\n",
      "41     \t [0.42345795 0.56048682 0.86703486]. \t  \u001b[92m3.835298640018787\u001b[0m \t 3.835298640018787\n",
      "42     \t [0.47835026 0.54125023 0.86258365]. \t  3.8326300384094263 \t 3.835298640018787\n",
      "43     \t [0.39005162 0.54092254 0.85332723]. \t  \u001b[92m3.8510656572040656\u001b[0m \t 3.8510656572040656\n",
      "44     \t [0.59828614 0.5118445  0.81675964]. \t  3.6377628266517394 \t 3.8510656572040656\n",
      "45     \t [0.49845333 0.43287121 0.83015322]. \t  3.3442020284117566 \t 3.8510656572040656\n",
      "46     \t [0.35286008 0.54446922 0.84471972]. \t  3.849876212723423 \t 3.8510656572040656\n",
      "47     \t [0.53892042 0.65035862 0.80866993]. \t  3.307814441238366 \t 3.8510656572040656\n",
      "48     \t [0.34316041 0.58149403 0.8160292 ]. \t  3.70296314923523 \t 3.8510656572040656\n",
      "49     \t [0.25320862 0.55391338 0.87933982]. \t  3.7898714070671082 \t 3.8510656572040656\n",
      "50     \t [0.23082112 0.51981109 0.85882598]. \t  3.810184033282763 \t 3.8510656572040656\n",
      "51     \t [0.56315064 0.52995123 0.84836773]. \t  3.8075377434301636 \t 3.8510656572040656\n",
      "52     \t [0.45427445 0.53833751 0.86440849]. \t  3.8291998185754013 \t 3.8510656572040656\n",
      "53     \t [1.         0.83682146 0.19236881]. \t  0.006773639874265713 \t 3.8510656572040656\n",
      "54     \t [0.43706289 0.56467967 0.87525505]. \t  3.8037874344922997 \t 3.8510656572040656\n",
      "55     \t [0.23800071 0.58577722 0.86217081]. \t  3.819724598382984 \t 3.8510656572040656\n",
      "56     \t [0.28436178 0.53660734 0.83885419]. \t  3.8328285254927414 \t 3.8510656572040656\n",
      "57     \t [0.24988166 0.53757514 0.80237969]. \t  3.6315446688282353 \t 3.8510656572040656\n",
      "58     \t [0.38149589 0.55683354 0.84962487]. \t  \u001b[92m3.8559948262445496\u001b[0m \t 3.8559948262445496\n",
      "59     \t [0.82504715 0.527561   0.86227197]. \t  3.7239660672819688 \t 3.8559948262445496\n",
      "60     \t [0.43826404 0.52544821 0.87044221]. \t  3.7922445791665154 \t 3.8559948262445496\n",
      "61     \t [0.36516558 0.4971777  0.8190694 ]. \t  3.655081827213801 \t 3.8559948262445496\n",
      "62     \t [0.56309961 0.54267387 0.83725867]. \t  3.795137788056871 \t 3.8559948262445496\n",
      "63     \t [0.72160458 0.52892569 0.86315764]. \t  3.7596745948788 \t 3.8559948262445496\n",
      "64     \t [0.70881122 0.56442739 0.89117756]. \t  3.6602487646700617 \t 3.8559948262445496\n",
      "65     \t [0.5264465  0.54894573 0.86558838]. \t  3.823645592496251 \t 3.8559948262445496\n",
      "66     \t [0.4388661  0.50269754 0.82061527]. \t  3.67289726214171 \t 3.8559948262445496\n",
      "67     \t [0.52838276 0.57252743 0.85505919]. \t  3.8224159804886435 \t 3.8559948262445496\n",
      "68     \t [0.19674161 0.60121314 0.79585788]. \t  3.5158827210756303 \t 3.8559948262445496\n",
      "69     \t [0.92195094 0.22606668 0.14742842]. \t  0.23051754004280411 \t 3.8559948262445496\n",
      "70     \t [0.41974585 0.61251371 0.8180513 ]. \t  3.6081818559991077 \t 3.8559948262445496\n",
      "71     \t [0.02625208 0.55786585 0.8464578 ]. \t  3.815789639245332 \t 3.8559948262445496\n",
      "72     \t [0.24878524 0.56947782 0.86426663]. \t  3.840916042002484 \t 3.8559948262445496\n",
      "73     \t [0.6205178  0.46815578 0.85552278]. \t  3.573167619190373 \t 3.8559948262445496\n",
      "74     \t [0.09175818 0.5107623  0.81961765]. \t  3.6823338961985392 \t 3.8559948262445496\n",
      "75     \t [0.27416306 0.49047796 0.84535902]. \t  3.7170091249230923 \t 3.8559948262445496\n",
      "76     \t [0.21774654 0.60955027 0.83948732]. \t  3.738703500869203 \t 3.8559948262445496\n",
      "77     \t [0.39069322 0.49608983 0.8277802 ]. \t  3.6918614241565475 \t 3.8559948262445496\n",
      "78     \t [0.26065029 0.59573368 0.8698221 ]. \t  3.778615657348264 \t 3.8559948262445496\n",
      "79     \t [0.71753863 0.4992163  0.85972244]. \t  3.6872809753896827 \t 3.8559948262445496\n",
      "80     \t [0.07743487 0.47597234 0.7850551 ]. \t  3.293803015906214 \t 3.8559948262445496\n",
      "81     \t [0.28141624 0.55189009 0.81134512]. \t  3.705841149851199 \t 3.8559948262445496\n",
      "82     \t [0.60139065 0.60255591 0.80331593]. \t  3.4612346470229474 \t 3.8559948262445496\n",
      "83     \t [0.24625681 0.58182187 0.85524631]. \t  3.8349769243952587 \t 3.8559948262445496\n",
      "84     \t [0.16883855 0.53646462 0.82004888]. \t  3.7482387849161896 \t 3.8559948262445496\n",
      "85     \t [0.16730157 0.50270775 0.83847782]. \t  3.743949726876562 \t 3.8559948262445496\n",
      "86     \t [0.45192418 0.57560765 0.84663377]. \t  3.8255403675577244 \t 3.8559948262445496\n",
      "87     \t [0.55959504 0.56036878 0.86034904]. \t  3.824705320265667 \t 3.8559948262445496\n",
      "88     \t [0.56335103 0.63177929 0.83477455]. \t  3.5628077247782612 \t 3.8559948262445496\n",
      "89     \t [0.30152842 0.55281078 0.82864814]. \t  3.805872527480709 \t 3.8559948262445496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.73046525 0.52433343 0.85378131]. \t  3.755863776520758 \t 3.8559948262445496\n",
      "91     \t [0.24506026 0.54819427 0.86795361]. \t  3.8346579333349196 \t 3.8559948262445496\n",
      "92     \t [0.35572239 0.56329109 0.87598193]. \t  3.8071892337467546 \t 3.8559948262445496\n",
      "93     \t [0.35394244 0.63044841 0.86791606]. \t  3.6471533883964837 \t 3.8559948262445496\n",
      "94     \t [0.52897407 0.54925181 0.88068636]. \t  3.7659958974425827 \t 3.8559948262445496\n",
      "95     \t [0.8499885  0.49942563 0.89155455]. \t  3.505154897399043 \t 3.8559948262445496\n",
      "96     \t [0.3008616  0.60979956 0.10911653]. \t  0.044522686976103926 \t 3.8559948262445496\n",
      "97     \t [0.78750166 0.5106615  0.8288577 ]. \t  3.6434110088599025 \t 3.8559948262445496\n",
      "98     \t [0.98657018 0.15142678 0.80884914]. \t  0.8741060423802898 \t 3.8559948262445496\n",
      "99     \t [0.52393565 0.55385284 0.8618071 ]. \t  3.8319896903258073 \t 3.8559948262445496\n",
      "100    \t [0.23913699 0.53010755 0.83224022]. \t  3.802341092036368 \t 3.8559948262445496\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.524990008735946\n",
      "3      \t [0.         0.68383543 1.        ]. \t  1.8033895630848673 \t 2.524990008735946\n",
      "4      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.524990008735946\n",
      "5      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.524990008735946\n",
      "6      \t [0.53325956 0.53677212 1.        ]. \t  2.0638491205203695 \t 2.524990008735946\n",
      "7      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.524990008735946\n",
      "8      \t [1.         0.70482355 0.        ]. \t  0.00114127535376999 \t 2.524990008735946\n",
      "9      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.524990008735946\n",
      "10     \t [1.        0.5075166 1.       ]. \t  1.9397895636028852 \t 2.524990008735946\n",
      "11     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.524990008735946\n",
      "12     \t [1.         0.49430171 0.54975131]. \t  0.4219656824159397 \t 2.524990008735946\n",
      "13     \t [0.53224434 0.         0.        ]. \t  0.09438133748273987 \t 2.524990008735946\n",
      "14     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.524990008735946\n",
      "15     \t [0.         0.         0.55798474]. \t  0.09932764530579608 \t 2.524990008735946\n",
      "16     \t [0.53273678 0.         1.        ]. \t  0.09157315186067252 \t 2.524990008735946\n",
      "17     \t [0.87399698 0.         0.49496395]. \t  0.10515095618068512 \t 2.524990008735946\n",
      "18     \t [0.32046007 1.         0.68723471]. \t  1.5011849263240002 \t 2.524990008735946\n",
      "19     \t [0.         0.39844217 0.77516732]. \t  \u001b[92m2.737406695517307\u001b[0m \t 2.737406695517307\n",
      "20     \t [0.         0.50001236 0.        ]. \t  0.01804632743580917 \t 2.737406695517307\n",
      "21     \t [0.28105794 0.63107917 0.75735415]. \t  \u001b[92m3.0528723349431393\u001b[0m \t 3.0528723349431393\n",
      "22     \t [1.         1.         0.60062471]. \t  0.27627112248582736 \t 3.0528723349431393\n",
      "23     \t [0.74451502 0.73555601 0.82472109]. \t  2.646070047525537 \t 3.0528723349431393\n",
      "24     \t [0.         1.         0.67568902]. \t  1.7456257022312769 \t 3.0528723349431393\n",
      "25     \t [0.49129728 1.         0.        ]. \t  0.00020754736208783943 \t 3.0528723349431393\n",
      "26     \t [0.80645735 0.27967296 0.        ]. \t  0.05066940357274576 \t 3.0528723349431393\n",
      "27     \t [0.25863695 0.2201144  0.81701013]. \t  1.412418978165534 \t 3.0528723349431393\n",
      "28     \t [0.63176529 1.         1.        ]. \t  0.3294616475344512 \t 3.0528723349431393\n",
      "29     \t [1.         0.         0.75638963]. \t  0.2350358859545037 \t 3.0528723349431393\n",
      "30     \t [0.         0.6542366  0.65533862]. \t  2.356502692507095 \t 3.0528723349431393\n",
      "31     \t [0.         0.23405049 0.276427  ]. \t  0.5833440807236361 \t 3.0528723349431393\n",
      "32     \t [0.63028506 0.         0.78427159]. \t  0.25040881783739977 \t 3.0528723349431393\n",
      "33     \t [0.20675691 0.43280548 1.        ]. \t  1.7425850879608467 \t 3.0528723349431393\n",
      "34     \t [0.1625476  0.00224955 0.1480388 ]. \t  0.5035383808208442 \t 3.0528723349431393\n",
      "35     \t [0.28814724 0.82170955 0.9782108 ]. \t  1.3152519941685525 \t 3.0528723349431393\n",
      "36     \t [2.16124149e-01 2.00601473e-01 7.47022515e-12]. \t  0.10194091450421183 \t 3.0528723349431393\n",
      "37     \t [1.         0.28293541 0.23496908]. \t  0.22327256536030424 \t 3.0528723349431393\n",
      "38     \t [0.20295071 0.99572293 0.00444024]. \t  0.0003367741748925019 \t 3.0528723349431393\n",
      "39     \t [0.80973591 0.18013198 1.        ]. \t  0.4754984371988721 \t 3.0528723349431393\n",
      "40     \t [0.70409359 1.         0.37257819]. \t  0.3271626990009979 \t 3.0528723349431393\n",
      "41     \t [0.51046184 0.54323186 0.77776489]. \t  \u001b[92m3.3310894689584805\u001b[0m \t 3.3310894689584805\n",
      "42     \t [0.         0.99999991 0.26186303]. \t  0.18844317116316928 \t 3.3310894689584805\n",
      "43     \t [1.         0.30247646 0.87539247]. \t  2.0129566162304346 \t 3.3310894689584805\n",
      "44     \t [1.         0.71474118 0.84719932]. \t  2.818501378241514 \t 3.3310894689584805\n",
      "45     \t [0.46945604 0.59832769 0.83075945]. \t  \u001b[92m3.7162027367848123\u001b[0m \t 3.7162027367848123\n",
      "46     \t [0.57296313 0.51247058 0.85147428]. \t  \u001b[92m3.770313510530524\u001b[0m \t 3.770313510530524\n",
      "47     \t [1.22608268e-07 7.18771515e-01 8.31194750e-01]. \t  2.980099375871817 \t 3.770313510530524\n",
      "48     \t [0.47862297 0.57322488 0.8293071 ]. \t  3.7665449591084994 \t 3.770313510530524\n",
      "49     \t [0.45000596 0.5412491  0.8470876 ]. \t  \u001b[92m3.8404472957345197\u001b[0m \t 3.8404472957345197\n",
      "50     \t [0.59075996 0.55917    0.86174525]. \t  3.816419057193863 \t 3.8404472957345197\n",
      "51     \t [0.48615075 0.51500044 0.84201932]. \t  3.782507148033033 \t 3.8404472957345197\n",
      "52     \t [0.52796873 0.57391635 0.80925381]. \t  3.618373312196532 \t 3.8404472957345197\n",
      "53     \t [0.46559326 0.57329702 0.83902835]. \t  3.8083486801272017 \t 3.8404472957345197\n",
      "54     \t [0.43352018 0.56228574 0.8143855 ]. \t  3.7004154007175107 \t 3.8404472957345197\n",
      "55     \t [0.33125429 0.53246644 0.8815315 ]. \t  3.758587882028354 \t 3.8404472957345197\n",
      "56     \t [0.42475138 0.56245805 0.77423287]. \t  3.317528490814721 \t 3.8404472957345197\n",
      "57     \t [0.43952428 0.55492472 0.87988208]. \t  3.7839057652981767 \t 3.8404472957345197\n",
      "58     \t [0.49881339 0.59610827 0.88432055]. \t  3.6981130447501274 \t 3.8404472957345197\n",
      "59     \t [0.31415282 0.52555704 0.88290316]. \t  3.736257897221189 \t 3.8404472957345197\n",
      "60     \t [0.37518555 0.49890336 0.84469778]. \t  3.749210403103338 \t 3.8404472957345197\n",
      "61     \t [0.31674367 0.64629917 0.82252103]. \t  3.494794707320658 \t 3.8404472957345197\n",
      "62     \t [0.49510697 0.53890684 0.8947137 ]. \t  3.665664785609923 \t 3.8404472957345197\n",
      "63     \t [0.30714679 0.5778924  0.88562967]. \t  3.7419061915422085 \t 3.8404472957345197\n",
      "64     \t [0.35144939 0.51946758 0.87663772]. \t  3.756379241175666 \t 3.8404472957345197\n",
      "65     \t [0.80961812 0.57623615 0.88725665]. \t  3.642943675639853 \t 3.8404472957345197\n",
      "66     \t [0.16889282 0.53371454 0.83922726]. \t  3.821543429737374 \t 3.8404472957345197\n",
      "67     \t [0.29809386 0.60561631 0.85820956]. \t  3.770716830398553 \t 3.8404472957345197\n",
      "68     \t [0.39336716 0.60204926 0.82065227]. \t  3.668717936539382 \t 3.8404472957345197\n",
      "69     \t [0.2457234  0.59892131 0.81010012]. \t  3.631085954451675 \t 3.8404472957345197\n",
      "70     \t [0.53237579 0.49643363 0.88773456]. \t  3.597566160800434 \t 3.8404472957345197\n",
      "71     \t [0.20431318 0.58351669 0.88645951]. \t  3.721372261068683 \t 3.8404472957345197\n",
      "72     \t [0.0916554  0.58968394 0.82303558]. \t  3.7177878846125427 \t 3.8404472957345197\n",
      "73     \t [0.36063581 0.54424985 0.88223465]. \t  3.7700355262267764 \t 3.8404472957345197\n",
      "74     \t [0.35242693 0.55286808 0.83924449]. \t  \u001b[92m3.8407720128989036\u001b[0m \t 3.8407720128989036\n",
      "75     \t [0.43662566 0.55182072 0.83997343]. \t  3.8324967375918093 \t 3.8407720128989036\n",
      "76     \t [0.96948904 0.88356555 0.21050459]. \t  0.010173273390628521 \t 3.8407720128989036\n",
      "77     \t [0.67405208 0.51758176 0.9058413 ]. \t  3.4880254463615987 \t 3.8407720128989036\n",
      "78     \t [0.70083371 0.51246534 0.8685604 ]. \t  3.7158637776145906 \t 3.8407720128989036\n",
      "79     \t [0.66223202 0.54100328 0.90628869]. \t  3.5325252495176356 \t 3.8407720128989036\n",
      "80     \t [0.54611278 0.52262187 0.88394322]. \t  3.706006864738473 \t 3.8407720128989036\n",
      "81     \t [0.13507434 0.63465684 0.84831915]. \t  3.6338206938276767 \t 3.8407720128989036\n",
      "82     \t [0.32382865 0.52370181 0.87469713]. \t  3.77614318351931 \t 3.8407720128989036\n",
      "83     \t [0.31143515 0.574661   0.85726006]. \t  \u001b[92m3.8462367257087906\u001b[0m \t 3.8462367257087906\n",
      "84     \t [0.22552914 0.49016516 0.88695273]. \t  3.581809532434937 \t 3.8462367257087906\n",
      "85     \t [0.65906202 0.52960367 0.83542641]. \t  3.74969033841937 \t 3.8462367257087906\n",
      "86     \t [0.71990979 0.55496913 0.88775217]. \t  3.6824571440872957 \t 3.8462367257087906\n",
      "87     \t [0.50990504 0.50798049 0.84600198]. \t  3.7654845659765326 \t 3.8462367257087906\n",
      "88     \t [0.48091724 0.58901157 0.80987201]. \t  3.607018879447114 \t 3.8462367257087906\n",
      "89     \t [0.58688547 0.4315069  0.85412101]. \t  3.3411404311197046 \t 3.8462367257087906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.65301349 0.55285517 0.80905556]. \t  3.594863726551615 \t 3.8462367257087906\n",
      "91     \t [0.13725822 0.59994868 0.75882966]. \t  3.1624855804067136 \t 3.8462367257087906\n",
      "92     \t [0.81014129 0.60786318 0.86603949]. \t  3.637166809322262 \t 3.8462367257087906\n",
      "93     \t [0.25193087 0.58062807 0.85164807]. \t  3.8373946545023903 \t 3.8462367257087906\n",
      "94     \t [0.31502481 0.54233967 0.82946258]. \t  3.8053278345702353 \t 3.8462367257087906\n",
      "95     \t [0.09040439 0.58869752 0.77310347]. \t  3.3193398035283863 \t 3.8462367257087906\n",
      "96     \t [0.21353801 0.48592161 0.89846447]. \t  3.467051440081773 \t 3.8462367257087906\n",
      "97     \t [0.49480442 0.53785471 0.89627989]. \t  3.651091479371471 \t 3.8462367257087906\n",
      "98     \t [0.32693676 0.63075    0.84189239]. \t  3.6484657620895855 \t 3.8462367257087906\n",
      "99     \t [9.99999943e-01 5.80561354e-09 2.56368340e-01]. \t  0.26311649090435973 \t 3.8462367257087906\n",
      "100    \t [0.32549712 0.46916938 0.90100188]. \t  3.3625193003391223 \t 3.8462367257087906\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.675391399411646\n",
      "2      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.675391399411646\n",
      "3      \t [1. 0. 0.]. \t  0.03095471703300515 \t 0.675391399411646\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.675391399411646\n",
      "5      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.675391399411646\n",
      "6      \t [ 0.00000000e+00 -5.55111512e-17  1.00000000e+00]. \t  0.0902894676548261 \t 0.675391399411646\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.675391399411646\n",
      "8      \t [0.47857856 0.5633605  1.        ]. \t  \u001b[92m2.0856841669381945\u001b[0m \t 2.0856841669381945\n",
      "9      \t [0.54386404 0.49894381 0.        ]. \t  0.02491025008130645 \t 2.0856841669381945\n",
      "10     \t [0.510037 1.       1.      ]. \t  0.33214093057189886 \t 2.0856841669381945\n",
      "11     \t [1.        0.5023726 1.       ]. \t  1.9276440549874374 \t 2.0856841669381945\n",
      "12     \t [0.         0.49798881 1.        ]. \t  1.9622346230101537 \t 2.0856841669381945\n",
      "13     \t [0.53337684 0.         0.        ]. \t  0.0942762757155479 \t 2.0856841669381945\n",
      "14     \t [1.         0.         0.47920499]. \t  0.08297336565152812 \t 2.0856841669381945\n",
      "15     \t [0.         0.         0.42097045]. \t  0.2905212287126967 \t 2.0856841669381945\n",
      "16     \t [ 5.38400723e-01  1.00000000e+00 -5.55111512e-17]. \t  0.0001881910079509131 \t 2.0856841669381945\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.0856841669381945\n",
      "18     \t [1.         1.         0.51088542]. \t  0.23402905385889072 \t 2.0856841669381945\n",
      "19     \t [0.         1.         0.48955746]. \t  \u001b[92m2.1774918159595993\u001b[0m \t 2.1774918159595993\n",
      "20     \t [1.         0.46889044 0.17523359]. \t  0.06878843855739121 \t 2.1774918159595993\n",
      "21     \t [0.55056563 0.         1.        ]. \t  0.09152651192254913 \t 2.1774918159595993\n",
      "22     \t [0.         0.77562605 0.76942082]. \t  \u001b[92m2.371647900962836\u001b[0m \t 2.371647900962836\n",
      "23     \t [1.         0.70962842 0.77359443]. \t  2.2859972973396507 \t 2.371647900962836\n",
      "24     \t [0.29322637 1.         0.68429725]. \t  1.570236829503626 \t 2.371647900962836\n",
      "25     \t [1.         0.29113736 0.76536564]. \t  1.78247866641351 \t 2.371647900962836\n",
      "26     \t [0.34016828 0.         0.66807199]. \t  0.1719977629478357 \t 2.371647900962836\n",
      "27     \t [0.18626691 0.74924323 1.        ]. \t  1.506324752155658 \t 2.371647900962836\n",
      "28     \t [1.         0.67867234 0.        ]. \t  0.0015324659489209125 \t 2.371647900962836\n",
      "29     \t [0.         0.22810665 0.75546859]. \t  1.3047475437420832 \t 2.371647900962836\n",
      "30     \t [0.23435763 0.27882224 1.        ]. \t  0.9232016310941953 \t 2.371647900962836\n",
      "31     \t [0.7456424  0.80915599 0.90890531]. \t  1.916539673560784 \t 2.371647900962836\n",
      "32     \t [3.51896908e-09 3.31844425e-01 0.00000000e+00]. \t  0.04914336169828932 \t 2.371647900962836\n",
      "33     \t [0.77655875 0.28281038 0.90089129]. \t  1.739254872457203 \t 2.371647900962836\n",
      "34     \t [0.8847271  0.25874041 0.        ]. \t  0.043170874959164704 \t 2.371647900962836\n",
      "35     \t [2.21346114e-01 8.36695911e-01 2.75203933e-09]. \t  0.0008976610203101911 \t 2.371647900962836\n",
      "36     \t [0.59225222 0.09130019 0.32079139]. \t  0.7862393964743851 \t 2.371647900962836\n",
      "37     \t [0.85583142 0.00391653 0.77516866]. \t  0.2545097243676286 \t 2.371647900962836\n",
      "38     \t [0.30837883 0.99525652 0.28900818]. \t  0.276497511889224 \t 2.371647900962836\n",
      "39     \t [0.75634248 0.85050839 0.22270742]. \t  0.03390172362469721 \t 2.371647900962836\n",
      "40     \t [0.47292904 0.75457514 0.7896154 ]. \t  \u001b[92m2.4545235250008686\u001b[0m \t 2.4545235250008686\n",
      "41     \t [0.64313984 0.99914418 0.67483805]. \t  0.8638496956189473 \t 2.4545235250008686\n",
      "42     \t [0.13379378 0.60833599 0.77438079]. \t  \u001b[92m3.2993384799232537\u001b[0m \t 3.2993384799232537\n",
      "43     \t [0.8071482  0.         0.11540653]. \t  0.24531346837537057 \t 3.2993384799232537\n",
      "44     \t [0.19669134 0.         0.20987098]. \t  0.7227192769550226 \t 3.2993384799232537\n",
      "45     \t [0.00498289 0.53709125 0.60765623]. \t  1.6264194452916043 \t 3.2993384799232537\n",
      "46     \t [1.84134261e-07 9.99999904e-01 7.98414740e-01]. \t  0.8620993429282705 \t 3.2993384799232537\n",
      "47     \t [0.         0.75670977 0.        ]. \t  0.0015494108701906496 \t 3.2993384799232537\n",
      "48     \t [0.99999994 0.9999998  0.87026206]. \t  0.5414048407864808 \t 3.2993384799232537\n",
      "49     \t [0.38301014 0.60710968 0.81577657]. \t  \u001b[92m3.6230679726518957\u001b[0m \t 3.6230679726518957\n",
      "50     \t [0.15236476 0.55274565 0.8696545 ]. \t  \u001b[92m3.82002898698928\u001b[0m \t 3.82002898698928\n",
      "51     \t [0.71526649 0.56639787 0.82253465]. \t  3.6600519449920075 \t 3.82002898698928\n",
      "52     \t [0.21951205 0.55564684 0.8667471 ]. \t  \u001b[92m3.838512278536112\u001b[0m \t 3.838512278536112\n",
      "53     \t [0.32936158 0.62150018 0.80883819]. \t  3.5318159352442273 \t 3.838512278536112\n",
      "54     \t [0.39785477 0.54229038 0.85049191]. \t  \u001b[92m3.8508346643679277\u001b[0m \t 3.8508346643679277\n",
      "55     \t [0.38634733 0.53831737 0.80044362]. \t  3.6007205757098237 \t 3.8508346643679277\n",
      "56     \t [0.13026588 0.54698652 0.85132306]. \t  3.843337213251475 \t 3.8508346643679277\n",
      "57     \t [0.22971384 0.55831335 0.86585614]. \t  3.8417388754503374 \t 3.8508346643679277\n",
      "58     \t [0.43937511 0.56760558 0.85218189]. \t  3.84397197332594 \t 3.8508346643679277\n",
      "59     \t [0.33566021 0.57681387 0.84819008]. \t  3.8401138413354348 \t 3.8508346643679277\n",
      "60     \t [0.02384854 0.55888663 0.86491059]. \t  3.8036298266311954 \t 3.8508346643679277\n",
      "61     \t [0.23341524 0.64051597 0.81569222]. \t  3.4995668130797943 \t 3.8508346643679277\n",
      "62     \t [0.02302266 0.55759926 0.85307513]. \t  3.8182835594078135 \t 3.8508346643679277\n",
      "63     \t [0.27621591 0.64601164 0.83408706]. \t  3.5491310954671973 \t 3.8508346643679277\n",
      "64     \t [0.2228989  0.5925615  0.86417633]. \t  3.799315818059108 \t 3.8508346643679277\n",
      "65     \t [0.37211483 0.56612481 0.87560344]. \t  3.8065627287602393 \t 3.8508346643679277\n",
      "66     \t [0.21495804 0.60982321 0.84502789]. \t  3.7488135925207304 \t 3.8508346643679277\n",
      "67     \t [0.26362993 0.60575612 0.87257506]. \t  3.7389934734544736 \t 3.8508346643679277\n",
      "68     \t [0.72840082 0.57290497 0.83569913]. \t  3.717327853498455 \t 3.8508346643679277\n",
      "69     \t [0.44797249 0.51367197 0.88006093]. \t  3.7171086339940795 \t 3.8508346643679277\n",
      "70     \t [0.16058444 0.61398367 0.90177234]. \t  3.5144420137944152 \t 3.8508346643679277\n",
      "71     \t [0.88547272 0.590213   0.88291759]. \t  3.6115287063533854 \t 3.8508346643679277\n",
      "72     \t [0.48220627 0.53433226 0.85113853]. \t  3.8317134965665343 \t 3.8508346643679277\n",
      "73     \t [0.20494665 0.54585712 0.90041803]. \t  3.6240236732297015 \t 3.8508346643679277\n",
      "74     \t [0.08042055 0.5185713  0.83791382]. \t  3.771973244357548 \t 3.8508346643679277\n",
      "75     \t [0.45373321 0.58623462 0.86229639]. \t  3.8072079760902846 \t 3.8508346643679277\n",
      "76     \t [0.71616275 0.51627649 0.80424056]. \t  3.509449059046258 \t 3.8508346643679277\n",
      "77     \t [0.0552063  0.47095905 0.84563518]. \t  3.588221812140124 \t 3.8508346643679277\n",
      "78     \t [0.19715738 0.50336483 0.92125754]. \t  3.287622556071226 \t 3.8508346643679277\n",
      "79     \t [0.39725423 0.5261547  0.77963125]. \t  3.3783721916373786 \t 3.8508346643679277\n",
      "80     \t [0.24144565 0.51696176 0.83242375]. \t  3.776690395212916 \t 3.8508346643679277\n",
      "81     \t [0.15475316 0.57830241 0.84127405]. \t  3.8194383106208165 \t 3.8508346643679277\n",
      "82     \t [0.13474589 0.5529821  0.86172368]. \t  3.8378284577980186 \t 3.8508346643679277\n",
      "83     \t [0.53237117 0.59647444 0.87525164]. \t  3.7339193401194994 \t 3.8508346643679277\n",
      "84     \t [0.42804805 0.57731867 0.85816354]. \t  3.832333311735268 \t 3.8508346643679277\n",
      "85     \t [0.09180288 0.57618866 0.87801025]. \t  3.7616177541139306 \t 3.8508346643679277\n",
      "86     \t [0.12391749 0.48516798 0.85115232]. \t  3.676447831842407 \t 3.8508346643679277\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8508346643679277\n",
      "88     \t [0.30745707 0.65592443 0.78629863]. \t  3.19988468301404 \t 3.8508346643679277\n",
      "89     \t [0.28248767 0.4988328  0.79023621]. \t  3.4455982212587832 \t 3.8508346643679277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.35167873 0.47924966 0.82501181]. \t  3.612997170699589 \t 3.8508346643679277\n",
      "91     \t [0.21732874 0.55758111 0.84414664]. \t  \u001b[92m3.8510374076303444\u001b[0m \t 3.8510374076303444\n",
      "92     \t [0.9966977  0.11281528 0.1773188 ]. \t  0.24041387552247068 \t 3.8510374076303444\n",
      "93     \t [0.88018412 0.51826977 0.79802255]. \t  3.3873958799103514 \t 3.8510374076303444\n",
      "94     \t [0.25501591 0.69598662 0.88337257]. \t  3.1569814753127523 \t 3.8510374076303444\n",
      "95     \t [0.20372773 0.51560341 0.80428992]. \t  3.6105815437954325 \t 3.8510374076303444\n",
      "96     \t [0.3222957  0.50068031 0.85358552]. \t  3.7599991051472195 \t 3.8510374076303444\n",
      "97     \t [0.24995465 0.55365055 0.89694105]. \t  3.665437142884377 \t 3.8510374076303444\n",
      "98     \t [0.28094583 0.56704615 0.85348783]. \t  \u001b[92m3.8562383580084028\u001b[0m \t 3.8562383580084028\n",
      "99     \t [0.25517709 0.50622885 0.89261288]. \t  3.607172647514496 \t 3.8562383580084028\n",
      "100    \t [0.37655113 0.57882966 0.80215087]. \t  3.5958014059591705 \t 3.8562383580084028\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2205577764509106, -5.152773175058427)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3952980347555135, -4.895755980716839)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.344254090624879, -5.140888323807924)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.045118598435262, -4.528867319093919)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2175760190503504, -3.47907329017522)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4621495814352967, -4.167204050629664)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.667069349629895, -2.427527665591056)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3479486680449515, -4.437340845619253)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0258572440729457, -3.732056583522499)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -4.737286933990644)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.786336501531502, -4.451291654399481)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.719191058071372, -6.048581717455769)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9435462090920615, -3.5516358941691784)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.904931863891832, -4.1099834541932925)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.687574601537175, -3.9202771857447423)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.061350856474767, -4.760720003210089)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.6579528660276637, -4.1292172069571755)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.2682057916786316, -4.993015377261186)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1478049858514476, -4.101775646994848)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1123460797014895, -5.029567075974041)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -3.9202771857447423)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
