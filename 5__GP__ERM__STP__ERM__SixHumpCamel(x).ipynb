{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SixHumpCamel synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 3 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/camel6.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'SixHumpCamel'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'SixHumpCamel':\n",
    "    \n",
    "    # True y bounds:\n",
    "    y_lb = -1.0316\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb_x1 = -3\n",
    "    ub_x1 = +3\n",
    "    \n",
    "    lb_x2 = -2\n",
    "    ub_x2 = +2\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 2\n",
    "\n",
    "# 2-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb_x1, ub_x1]),\n",
    "             'x2_training': ('cont', [lb_x2, ub_x2])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb_x1, ub_x1, n_test)\n",
    "    x2_test = np.linspace(lb_x2, ub_x2, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
    "\n",
    "    def f_syn_polarity(x1_training, x2_training):\n",
    "        return operator * ((4 - 2.1 * x1_training ** 2 + 1 / 3 * x1_training ** 4) * x1_training ** 2 +\n",
    "                (x1_training * x2_training) + (-4 + 4 * x2_training ** 2) * x2_training ** 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.67302105 -1.32372098]. \t  -5.793449752432556 \t -0.8736935954900025\n",
      "init   \t [-0.38364588  1.07704989]. \t  -0.8736935954900025 \t -0.8736935954900025\n",
      "init   \t [-1.22804817 -1.40334817]. \t  -11.759316761133794 \t -0.8736935954900025\n",
      "init   \t [-2.86513005 -0.31910203]. \t  -76.26436708944966 \t -0.8736935954900025\n",
      "init   \t [-1.56790715 -0.64937523]. \t  -2.1371114994016214 \t -0.8736935954900025\n",
      "1      \t [-0.93715312 -0.14094362]. \t  -2.1732366761917628 \t -0.8736935954900025\n",
      "2      \t [-0.07304745  1.9144092 ]. \t  -38.94939404436368 \t -0.8736935954900025\n",
      "3      \t [-0.78050993  0.72945565]. \t  \u001b[92m-0.167569287879497\u001b[0m \t -0.167569287879497\n",
      "4      \t [0.30116645 0.52574138]. \t  \u001b[92m0.29590610486999536\u001b[0m \t 0.29590610486999536\n",
      "5      \t [-0.24800953  0.70022966]. \t  \u001b[92m0.935121440318956\u001b[0m \t 0.935121440318956\n",
      "6      \t [1.5609464  0.14143977]. \t  -2.243079860045657 \t 0.935121440318956\n",
      "7      \t [ 1.98277411 -2.        ]. \t  -47.55707867715 \t 0.935121440318956\n",
      "8      \t [3. 2.]. \t  -162.89999999999998 \t 0.935121440318956\n",
      "9      \t [ 1.04581603 -0.12582446]. \t  -2.1050148479641027 \t 0.935121440318956\n",
      "10     \t [-0.33128604  0.80795693]. \t  0.760132762399243 \t 0.935121440318956\n",
      "11     \t [-1.48211326  1.64684486]. \t  -18.319370010217696 \t 0.935121440318956\n",
      "12     \t [0.20276379 0.02789873]. \t  -0.16347208264014565 \t 0.935121440318956\n",
      "13     \t [ 1.86388429 -0.23019458]. \t  -1.8976200882948167 \t 0.935121440318956\n",
      "14     \t [ 0.26096036 -1.76606431]. \t  -26.238185894466334 \t 0.935121440318956\n",
      "15     \t [ 0.99354472 -0.93019942]. \t  -0.8323521358113928 \t 0.935121440318956\n",
      "16     \t [-0.00435298  0.50492238]. \t  0.7619174732096596 \t 0.935121440318956\n",
      "17     \t [ 0.61502719 -0.71637269]. \t  0.2092855324522761 \t 0.935121440318956\n",
      "18     \t [-2.06825954 -1.82702401]. \t  -39.77174015579872 \t 0.935121440318956\n",
      "19     \t [ 2.79836475 -0.36511083]. \t  -61.13109513906215 \t 0.935121440318956\n",
      "20     \t [ 0.08952598 -0.45795008]. \t  0.6720196307983171 \t 0.935121440318956\n",
      "21     \t [ 1.55366239 -0.32439664]. \t  -1.2269816968095526 \t 0.935121440318956\n",
      "22     \t [-0.03248066  0.71572693]. \t  \u001b[92m1.0184279216230645\u001b[0m \t 1.0184279216230645\n",
      "23     \t [-1.0331679  -0.80591207]. \t  -2.2044185478435976 \t 1.0184279216230645\n",
      "24     \t [-0.22125    -0.31741033]. \t  0.10135530706690432 \t 1.0184279216230645\n",
      "25     \t [-0.06617994  0.69130856]. \t  \u001b[92m1.026319568264805\u001b[0m \t 1.026319568264805\n",
      "26     \t [-0.12064446  0.3855229 ]. \t  0.4948853256820529 \t 1.026319568264805\n",
      "27     \t [ 1.69944873 -0.45869842]. \t  -0.6219762540829291 \t 1.026319568264805\n",
      "28     \t [-0.06057763  0.75063218]. \t  1.0147182520433624 \t 1.026319568264805\n",
      "29     \t [-1.58777894  0.64712721]. \t  -0.07714900223152943 \t 1.026319568264805\n",
      "30     \t [-1.30880562  0.71542016]. \t  -0.42957053211949625 \t 1.026319568264805\n",
      "31     \t [-2.85455508  1.61243532]. \t  -85.54204862515923 \t 1.026319568264805\n",
      "32     \t [-1.76511934 -0.15155532]. \t  -2.3365582004902055 \t 1.026319568264805\n",
      "33     \t [-1.7508726   0.38394134]. \t  -0.9552273236159621 \t 1.026319568264805\n",
      "34     \t [-0.1198851  -0.78435006]. \t  0.7958223749138317 \t 1.026319568264805\n",
      "35     \t [ 0.13030098 -0.71771591]. \t  1.0252954392152667 \t 1.026319568264805\n",
      "36     \t [-1.80852718  0.38025444]. \t  -1.0984653743738888 \t 1.026319568264805\n",
      "37     \t [-1.78809222  1.4077139 ]. \t  -7.4806184141338425 \t 1.026319568264805\n",
      "38     \t [-0.39087178 -0.64675075]. \t  0.1572014174845049 \t 1.026319568264805\n",
      "39     \t [ 0.07675711 -0.69114445]. \t  \u001b[92m1.027563848132161\u001b[0m \t 1.027563848132161\n",
      "40     \t [-2.25205148  1.19447344]. \t  -9.501171268168793 \t 1.027563848132161\n",
      "41     \t [-1.81306811  0.89572291]. \t  -0.0386096181568919 \t 1.027563848132161\n",
      "42     \t [0.11986892 1.20162661]. \t  -2.7649172983455013 \t 1.027563848132161\n",
      "43     \t [-0.18441338  0.78811711]. \t  0.9530333021585169 \t 1.027563848132161\n",
      "44     \t [-1.52807232  1.13158191]. \t  -1.8414164933370722 \t 1.027563848132161\n",
      "45     \t [1.08878427 1.92103978]. \t  -44.1520376928909 \t 1.027563848132161\n",
      "46     \t [ 0.0761291  -0.69372883]. \t  \u001b[92m1.0282960675613686\u001b[0m \t 1.0282960675613686\n",
      "47     \t [-1.77367045  1.64443934]. \t  -17.6955156113525 \t 1.0282960675613686\n",
      "48     \t [-1.95465509  0.67202311]. \t  -0.9144651821575858 \t 1.0282960675613686\n",
      "49     \t [-0.33830206 -1.5749701 ]. \t  -15.65361112403584 \t 1.0282960675613686\n",
      "50     \t [0.96700511 1.6088807 ]. \t  -20.1797643715767 \t 1.0282960675613686\n",
      "51     \t [-0.63320781  1.30818715]. \t  -5.328849189545264 \t 1.0282960675613686\n",
      "52     \t [1.78943477 1.05874623]. \t  -4.657173885277427 \t 1.0282960675613686\n",
      "53     \t [ 0.24251203 -0.5235617 ]. \t  0.6948246801657919 \t 1.0282960675613686\n",
      "54     \t [1.5095283  0.74482543]. \t  -2.2909762346483764 \t 1.0282960675613686\n",
      "55     \t [-0.76016924 -1.6482706 ]. \t  -21.584220412392074 \t 1.0282960675613686\n",
      "56     \t [-1.39263147  1.44762328]. \t  -9.458376248624448 \t 1.0282960675613686\n",
      "57     \t [-0.02252971  0.75824675]. \t  0.9925903742455113 \t 1.0282960675613686\n",
      "58     \t [1.17939302 0.48776517]. \t  -2.247908063207417 \t 1.0282960675613686\n",
      "59     \t [0.59966949 1.15925269]. \t  -3.7259669311157895 \t 1.0282960675613686\n",
      "60     \t [1.4978104  1.68247342]. \t  -25.417164401686623 \t 1.0282960675613686\n",
      "61     \t [-0.79823274 -0.23445295]. \t  -1.7617083174416392 \t 1.0282960675613686\n",
      "62     \t [0.38030497 1.23549112]. \t  -4.219773213399278 \t 1.0282960675613686\n",
      "63     \t [-1.6531405   1.26525901]. \t  -3.8071160185632826 \t 1.0282960675613686\n",
      "64     \t [-1.47517746  1.13816911]. \t  -2.0467536045476122 \t 1.0282960675613686\n",
      "65     \t [ 0.04446127 -0.78391692]. \t  0.9744903444205099 \t 1.0282960675613686\n",
      "66     \t [-0.08423858 -0.23792483]. \t  0.16529356485900776 \t 1.0282960675613686\n",
      "67     \t [ 2.14377368 -1.45496872]. \t  -12.72342743307032 \t 1.0282960675613686\n",
      "68     \t [ 0.16023755 -0.76028763]. \t  0.9961418762046389 \t 1.0282960675613686\n",
      "69     \t [-0.08627193  0.75742197]. \t  1.0139693537690022 \t 1.0282960675613686\n",
      "70     \t [-2.27538876 -1.35300516]. \t  -19.839781473580203 \t 1.0282960675613686\n",
      "71     \t [1.86722164 0.27060845]. \t  -2.7798413326718903 \t 1.0282960675613686\n",
      "72     \t [-0.12988261  0.68633578]. \t  1.0189102949372086 \t 1.0282960675613686\n",
      "73     \t [0.71379781 1.66023714]. \t  -22.087172660864255 \t 1.0282960675613686\n",
      "74     \t [-0.4026587  -1.88690108]. \t  -37.81861699002644 \t 1.0282960675613686\n",
      "75     \t [-0.36505783  0.38931278]. \t  0.15993082202549325 \t 1.0282960675613686\n",
      "76     \t [-0.76437006 -0.06979844]. \t  -1.720627974182256 \t 1.0282960675613686\n",
      "77     \t [ 0.31622755 -0.56341955]. \t  0.6655256602171276 \t 1.0282960675613686\n",
      "78     \t [1.50877989 1.95950441]. \t  -48.725209640449066 \t 1.0282960675613686\n",
      "79     \t [-0.63462875 -0.18815498]. \t  -1.2749621822997672 \t 1.0282960675613686\n",
      "80     \t [-0.75974962 -1.7818196 ]. \t  -30.64699317939762 \t 1.0282960675613686\n",
      "81     \t [ 0.97444179 -0.22800717]. \t  -1.7708006741633173 \t 1.0282960675613686\n",
      "82     \t [1.32274211 0.57888504]. \t  -2.2297831175980805 \t 1.0282960675613686\n",
      "83     \t [ 1.09004211 -1.94185986]. \t  -42.02348857257047 \t 1.0282960675613686\n",
      "84     \t [1.74306568 1.61403765]. \t  -21.65610178465473 \t 1.0282960675613686\n",
      "85     \t [-2.6672884   0.65360609]. \t  -39.47632188643165 \t 1.0282960675613686\n",
      "86     \t [ 1.08795635 -1.7951684 ]. \t  -29.043000392860524 \t 1.0282960675613686\n",
      "87     \t [-0.58486961  1.27081664]. \t  -4.365318131187895 \t 1.0282960675613686\n",
      "88     \t [-1.58461732  0.13930054]. \t  -1.7837788225475826 \t 1.0282960675613686\n",
      "89     \t [1.29854219 0.70445066]. \t  -2.2868519260290716 \t 1.0282960675613686\n",
      "90     \t [-0.05184237  0.73552739]. \t  1.0206719398153363 \t 1.0282960675613686\n",
      "91     \t [-0.04926054  0.75311248]. \t  1.009352932694486 \t 1.0282960675613686\n",
      "92     \t [ 2.15048771 -0.66737539]. \t  -4.13124619636116 \t 1.0282960675613686\n",
      "93     \t [-0.17662727  0.73279209]. \t  1.0012046919843132 \t 1.0282960675613686\n",
      "94     \t [ 0.11333463 -0.70660871]. \t  \u001b[92m1.0290480670839535\u001b[0m \t 1.0290480670839535\n",
      "95     \t [ 2.42228947 -1.65386767]. \t  -33.486234540202446 \t 1.0290480670839535\n",
      "96     \t [-2.42244289 -1.42115387]. \t  -30.19710499829729 \t 1.0290480670839535\n",
      "97     \t [-0.31163253  1.16961094]. \t  -2.018095017814412 \t 1.0290480670839535\n",
      "98     \t [ 2.52120677 -0.8352527 ]. \t  -23.23708434097477 \t 1.0290480670839535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 1.73673603 -0.96623492]. \t  -0.1807152065744506 \t 1.0290480670839535\n",
      "100    \t [-0.09590284  0.63470091]. \t  0.9865013931367435 \t 1.0290480670839535\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.11191296 -1.7043856 ]. \t  -24.175973303775706 \t -2.2733752808550527\n",
      "init   \t [ 2.36959058 -0.25402581]. \t  -14.417173913696448 \t -2.2733752808550527\n",
      "init   \t [-2.23393359  0.3034315 ]. \t  -8.078384314238003 \t -2.2733752808550527\n",
      "init   \t [ 2.04282552 -0.2595178 ]. \t  -3.564649730949853 \t -2.2733752808550527\n",
      "init   \t [1.17546336 0.73855239]. \t  -2.2733752808550527 \t -2.2733752808550527\n",
      "1      \t [ 1.15696415 -0.34291717]. \t  \u001b[92m-1.5792328485400045\u001b[0m \t -1.5792328485400045\n",
      "2      \t [1.60259527 0.08121045]. \t  -2.1721691951550346 \t -1.5792328485400045\n",
      "3      \t [0.72219245 1.74530995]. \t  -27.75337391833869 \t -1.5792328485400045\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t -1.5792328485400045\n",
      "5      \t [ 1.66202741 -0.63776504]. \t  \u001b[92m-0.026098956189019362\u001b[0m \t -0.026098956189019362\n",
      "6      \t [-0.03019684  0.24438903]. \t  \u001b[92m0.22836934045908666\u001b[0m \t 0.22836934045908666\n",
      "7      \t [0.49745879 0.37247973]. \t  -0.5736356477870388 \t 0.22836934045908666\n",
      "8      \t [-0.20022727 -0.40189988]. \t  \u001b[92m0.30425333420351636\u001b[0m \t 0.30425333420351636\n",
      "9      \t [ 1.68168007 -0.41728565]. \t  -0.7791662803424317 \t 0.30425333420351636\n",
      "10     \t [ 0.50908846 -1.18377706]. \t  -2.548353120429341 \t 0.30425333420351636\n",
      "11     \t [1.94010885 1.06633308]. \t  -5.771842917675719 \t 0.30425333420351636\n",
      "12     \t [ 0.31348502 -0.35373046]. \t  0.17563792275485451 \t 0.30425333420351636\n",
      "13     \t [-1.03446173 -0.00530849]. \t  -2.289508657660735 \t 0.30425333420351636\n",
      "14     \t [-3.          1.22224409]. \t  -108.18446248270295 \t 0.30425333420351636\n",
      "15     \t [ 1.12828656 -0.9698438 ]. \t  -1.0587830708302863 \t 0.30425333420351636\n",
      "16     \t [-0.43977002 -1.17762934]. \t  -3.3610763568667164 \t 0.30425333420351636\n",
      "17     \t [-1.84426985 -0.17329162]. \t  -2.630123444597507 \t 0.30425333420351636\n",
      "18     \t [-0.37165112 -0.02267267]. \t  -0.5196831634034867 \t 0.30425333420351636\n",
      "19     \t [-0.31633902  0.68965521]. \t  \u001b[92m0.8362021915252301\u001b[0m \t 0.8362021915252301\n",
      "20     \t [ 0.20870977 -0.72443872]. \t  \u001b[92m0.978453036569203\u001b[0m \t 0.978453036569203\n",
      "21     \t [0.0798901  0.68382019]. \t  0.915728843592086 \t 0.978453036569203\n",
      "22     \t [ 0.4705197  -0.69589917]. \t  0.5402004135791172 \t 0.978453036569203\n",
      "23     \t [-0.00315429 -0.74352158]. \t  \u001b[92m0.986453274759761\u001b[0m \t 0.986453274759761\n",
      "24     \t [-0.10009624  0.64131387]. \t  \u001b[92m0.9928440662615338\u001b[0m \t 0.9928440662615338\n",
      "25     \t [-1.69618354  0.20884572]. \t  -1.5427060895551934 \t 0.9928440662615338\n",
      "26     \t [-0.13385615  0.73528114]. \t  \u001b[92m1.0208183788548912\u001b[0m \t 1.0208183788548912\n",
      "27     \t [ 0.08037485 -0.72256931]. \t  \u001b[92m1.0303687102869248\u001b[0m \t 1.0303687102869248\n",
      "28     \t [-0.94756007  1.47131169]. \t  -10.831322363135858 \t 1.0303687102869248\n",
      "29     \t [-0.10350334  0.6945203 ]. \t  1.028029107639676 \t 1.0303687102869248\n",
      "30     \t [-0.55330569 -1.9282616 ]. \t  -42.53130635324554 \t 1.0303687102869248\n",
      "31     \t [2.2316572  0.70391909]. \t  -9.58112801346515 \t 1.0303687102869248\n",
      "32     \t [2.66224111 1.9054783 ]. \t  -84.81841961256475 \t 1.0303687102869248\n",
      "33     \t [-0.14549017  0.65040713]. \t  0.9871981755970161 \t 1.0303687102869248\n",
      "34     \t [-1.06399978 -0.70405953]. \t  -2.069777083252896 \t 1.0303687102869248\n",
      "35     \t [ 0.1235709  -0.68984284]. \t  1.0223274036498695 \t 1.0303687102869248\n",
      "36     \t [-0.00691102  0.71684901]. \t  1.003993324956122 \t 1.0303687102869248\n",
      "37     \t [-0.09001087 -0.70324145]. \t  0.9043115921918863 \t 1.0303687102869248\n",
      "38     \t [1.20821799 1.07536779]. \t  -4.423824629190195 \t 1.0303687102869248\n",
      "39     \t [-1.43663077 -1.95937682]. \t  -48.65557496698203 \t 1.0303687102869248\n",
      "40     \t [-1.18604578 -0.62546756]. \t  -2.1883461710378285 \t 1.0303687102869248\n",
      "41     \t [1.51121874 0.64860887]. \t  -2.158066466727826 \t 1.0303687102869248\n",
      "42     \t [ 0.001911   -0.77300886]. \t  0.963404307308214 \t 1.0303687102869248\n",
      "43     \t [ 0.13090245 -0.73760147]. \t  1.0208632690035335 \t 1.0303687102869248\n",
      "44     \t [-2.27546516  1.05426529]. \t  -8.778753399171318 \t 1.0303687102869248\n",
      "45     \t [-1.76746819  1.20839752]. \t  -2.7163588216432144 \t 1.0303687102869248\n",
      "46     \t [-2.57067255  0.4268469 ]. \t  -29.228832380413056 \t 1.0303687102869248\n",
      "47     \t [-2.81904323  1.97213767]. \t  -105.85075856177869 \t 1.0303687102869248\n",
      "48     \t [-1.15860497 -1.50114465]. \t  -15.42904142345882 \t 1.0303687102869248\n",
      "49     \t [-0.83549937 -1.30350278]. \t  -7.722945864915705 \t 1.0303687102869248\n",
      "50     \t [-1.89214966  0.84477724]. \t  -0.284345414908568 \t 1.0303687102869248\n",
      "51     \t [2.96379456 0.22237478]. \t  -99.4978816164872 \t 1.0303687102869248\n",
      "52     \t [ 2.47769042 -1.0686117 ]. \t  -20.533261295807936 \t 1.0303687102869248\n",
      "53     \t [2.93011755 1.64268014]. \t  -113.64611579697947 \t 1.0303687102869248\n",
      "54     \t [-0.84027301  0.36321686]. \t  -1.1313841567784735 \t 1.0303687102869248\n",
      "55     \t [-1.23207633  0.56761209]. \t  -0.82604010282297 \t 1.0303687102869248\n",
      "56     \t [1.92485735 0.45537205]. \t  -3.165357890014924 \t 1.0303687102869248\n",
      "57     \t [2.88057046 0.26111487]. \t  -79.5367356274671 \t 1.0303687102869248\n",
      "58     \t [ 0.10671594 -0.72062059]. \t  1.030131361019288 \t 1.0303687102869248\n",
      "59     \t [-1.93985613  0.33712633]. \t  -2.020391385944096 \t 1.0303687102869248\n",
      "60     \t [-0.1372666   0.77745327]. \t  0.9884677140556731 \t 1.0303687102869248\n",
      "61     \t [2.30360534 0.00581503]. \t  -11.914920370314794 \t 1.0303687102869248\n",
      "62     \t [ 0.06054849 -0.77093444]. \t  0.996442573984819 \t 1.0303687102869248\n",
      "63     \t [1.61382399 1.61781357]. \t  -21.6050231733215 \t 1.0303687102869248\n",
      "64     \t [ 0.12055112 -0.65800655]. \t  1.0036649497844463 \t 1.0303687102869248\n",
      "65     \t [-0.04232208  0.71929355]. \t  1.0220753960045983 \t 1.0303687102869248\n",
      "66     \t [ 0.1231373  -1.97281926]. \t  -44.840327152198974 \t 1.0303687102869248\n",
      "67     \t [ 0.12672596 -0.80216317]. \t  0.9556275589106191 \t 1.0303687102869248\n",
      "68     \t [ 0.05182957 -0.75818359]. \t  1.0061607500647776 \t 1.0303687102869248\n",
      "69     \t [-1.6784934  -1.01690178]. \t  -3.9028009049976027 \t 1.0303687102869248\n",
      "70     \t [ 0.40081555 -0.20565458]. \t  -0.3453450310759652 \t 1.0303687102869248\n",
      "71     \t [-2.47670402  0.2944167 ]. \t  -21.409173377901208 \t 1.0303687102869248\n",
      "72     \t [ 2.31159292 -0.81827272]. \t  -9.493471172091168 \t 1.0303687102869248\n",
      "73     \t [ 0.12626477 -0.69760157]. \t  1.0241306341459926 \t 1.0303687102869248\n",
      "74     \t [-0.09481064 -0.11165014]. \t  0.0028690320535212158 \t 1.0303687102869248\n",
      "75     \t [-2.85902183 -0.67450772]. \t  -75.36940988507637 \t 1.0303687102869248\n",
      "76     \t [2.59171689 1.09948148]. \t  -36.99881218323877 \t 1.0303687102869248\n",
      "77     \t [-0.03911843  0.71549453]. \t  1.0213034008135675 \t 1.0303687102869248\n",
      "78     \t [ 0.87500532 -0.65176966]. \t  -0.4334457811383149 \t 1.0303687102869248\n",
      "79     \t [-1.51393183  1.20427673]. \t  -2.9385972199817028 \t 1.0303687102869248\n",
      "80     \t [ 0.13567873 -0.6955568 ]. \t  1.0203971069895057 \t 1.0303687102869248\n",
      "81     \t [0.80095779 1.66342919]. \t  -22.67928971311677 \t 1.0303687102869248\n",
      "82     \t [-0.01808215 -0.73119089]. \t  0.9806711139121456 \t 1.0303687102869248\n",
      "83     \t [ 0.01580952 -0.74634041]. \t  0.9977927008092208 \t 1.0303687102869248\n",
      "84     \t [ 2.389094   -0.75934143]. \t  -13.609064836510703 \t 1.0303687102869248\n",
      "85     \t [-0.12686207 -0.68427057]. \t  0.8453203260181767 \t 1.0303687102869248\n",
      "86     \t [-0.52273784  0.83621451]. \t  0.33529392790640367 \t 1.0303687102869248\n",
      "87     \t [2.7672583  0.20580894]. \t  -57.57731382171728 \t 1.0303687102869248\n",
      "88     \t [-0.50532771 -1.40228085]. \t  -9.199869130023037 \t 1.0303687102869248\n",
      "89     \t [ 2.08784573 -0.92626063]. \t  -2.721546899008986 \t 1.0303687102869248\n",
      "90     \t [2.45266727 1.52614456]. \t  -36.757287253058195 \t 1.0303687102869248\n",
      "91     \t [-1.435657    1.76607725]. \t  -26.143691970884298 \t 1.0303687102869248\n",
      "92     \t [-0.03710924  0.61043453]. \t  0.9522562120363517 \t 1.0303687102869248\n",
      "93     \t [-0.52505796  0.39306676]. \t  -0.22121615591223176 \t 1.0303687102869248\n",
      "94     \t [-2.3539009   1.71382247]. \t  -33.11986351303693 \t 1.0303687102869248\n",
      "95     \t [ 1.89971677 -1.11108798]. \t  -1.7998449946112847 \t 1.0303687102869248\n",
      "96     \t [1.71512374 0.84382188]. \t  -2.706752792866186 \t 1.0303687102869248\n",
      "97     \t [0.06952256 0.69500834]. \t  0.9312456932089062 \t 1.0303687102869248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.77087388  0.75861102]. \t  0.14872796222869067 \t 1.0303687102869248\n",
      "99     \t [ 1.5261282 -0.8115012]. \t  0.0018335549230428239 \t 1.0303687102869248\n",
      "100    \t [-0.12819565  0.71412009]. \t  1.025978875259258 \t 1.0303687102869248\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.50945364 -1.5629949 ]. \t  -13.89572751148896 \t -0.027690663940903623\n",
      "init   \t [-0.08327233 -0.00067529]. \t  -0.027690663940903623 \t -0.027690663940903623\n",
      "init   \t [ 0.94265999 -1.05646116]. \t  -1.6525931811945593 \t -0.027690663940903623\n",
      "init   \t [ 0.67676934 -1.52139039]. \t  -12.56543686771716 \t -0.027690663940903623\n",
      "init   \t [ 1.27321378 -0.50975783]. \t  -0.9673942177133168 \t -0.027690663940903623\n",
      "1      \t [ 0.55583856 -0.53062014]. \t  \u001b[92m0.05886828563667812\u001b[0m \t 0.05886828563667812\n",
      "2      \t [-0.93812422  0.56405229]. \t  -0.7241197257913029 \t 0.05886828563667812\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t 0.05886828563667812\n",
      "4      \t [-0.35662755  0.6623391 ]. \t  \u001b[92m0.7457243275072701\u001b[0m \t 0.7457243275072701\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t 0.7457243275072701\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t 0.7457243275072701\n",
      "7      \t [-0.62328834  0.22953003]. \t  -0.9138615689602275 \t 0.7457243275072701\n",
      "8      \t [-0.19514789  1.86977414]. \t  -34.68981134147559 \t 0.7457243275072701\n",
      "9      \t [-0.11250342  0.36633991]. \t  0.4556980288279966 \t 0.7457243275072701\n",
      "10     \t [-0.64396831  0.80970934]. \t  0.10313400787413951 \t 0.7457243275072701\n",
      "11     \t [ 2.18620258 -0.81081723]. \t  -4.866487216589105 \t 0.7457243275072701\n",
      "12     \t [-0.48410145  0.53146093]. \t  0.24159898277000913 \t 0.7457243275072701\n",
      "13     \t [-0.39917558 -0.62406496]. \t  0.11661458806764047 \t 0.7457243275072701\n",
      "14     \t [0.08178303 0.72254602]. \t  \u001b[92m0.9122991670588343\u001b[0m \t 0.9122991670588343\n",
      "15     \t [-0.02022456  0.63533429]. \t  \u001b[92m0.9740800195290358\u001b[0m \t 0.9740800195290358\n",
      "16     \t [-1.52788137 -0.16502753]. \t  -2.280354205870446 \t 0.9740800195290358\n",
      "17     \t [ 2.96628381 -1.38106257]. \t  -102.50746023195177 \t 0.9740800195290358\n",
      "18     \t [ 1.94272894 -0.51836816]. \t  -1.3107380061153258 \t 0.9740800195290358\n",
      "19     \t [ 1.6485475  -0.85852299]. \t  0.1392183035984681 \t 0.9740800195290358\n",
      "20     \t [-0.80278648 -0.655194  ]. \t  -1.3408677496767925 \t 0.9740800195290358\n",
      "21     \t [0.09520836 0.65531563]. \t  0.8816065907550134 \t 0.9740800195290358\n",
      "22     \t [-1.84430119  0.14300644]. \t  -2.0833107106429565 \t 0.9740800195290358\n",
      "23     \t [ 2.48640317 -0.30470559]. \t  -22.133533458614437 \t 0.9740800195290358\n",
      "24     \t [0.80561089 0.80642902]. \t  -1.542674591971854 \t 0.9740800195290358\n",
      "25     \t [2.28626513 1.30269706]. \t  -18.84581186380636 \t 0.9740800195290358\n",
      "26     \t [1.62790578 1.25891918]. \t  -7.813188725055089 \t 0.9740800195290358\n",
      "27     \t [1.91770827 0.81031674]. \t  -3.5400446862162775 \t 0.9740800195290358\n",
      "28     \t [2.03252232 1.94878724]. \t  -50.648580740820705 \t 0.9740800195290358\n",
      "29     \t [2.69500782 0.72422615]. \t  -46.94108517932003 \t 0.9740800195290358\n",
      "30     \t [-0.09280874  0.7023051 ]. \t  \u001b[92m1.0306985929027668\u001b[0m \t 1.0306985929027668\n",
      "31     \t [-1.20200077  0.81677325]. \t  -0.5308281839469812 \t 1.0306985929027668\n",
      "32     \t [-0.0729474   0.66301092]. \t  1.0125384217834554 \t 1.0306985929027668\n",
      "33     \t [-0.98876642  1.71764403]. \t  -23.53245957557891 \t 1.0306985929027668\n",
      "34     \t [-1.88246282  0.25176509]. \t  -1.9256364935732981 \t 1.0306985929027668\n",
      "35     \t [ 0.25480188 -0.62146833]. \t  0.8556349328375543 \t 1.0306985929027668\n",
      "36     \t [ 0.35695422 -0.78750712]. \t  0.7470816407854051 \t 1.0306985929027668\n",
      "37     \t [-0.61527902 -1.25310886]. \t  -5.584422742384657 \t 1.0306985929027668\n",
      "38     \t [-2.85030537 -0.69498865]. \t  -73.61434162247694 \t 1.0306985929027668\n",
      "39     \t [-1.52140777 -1.52416182]. \t  -16.754476452081555 \t 1.0306985929027668\n",
      "40     \t [-0.05277617  0.64527684]. \t  0.9949624838866779 \t 1.0306985929027668\n",
      "41     \t [ 1.15332523 -1.99656566]. \t  -47.703299551702244 \t 1.0306985929027668\n",
      "42     \t [1.07341296 0.21966895]. \t  -2.382886423019017 \t 1.0306985929027668\n",
      "43     \t [-0.54372494 -0.35602197]. \t  -0.7584532578463083 \t 1.0306985929027668\n",
      "44     \t [-0.03481541 -0.77704563]. \t  0.9250037876103374 \t 1.0306985929027668\n",
      "45     \t [-2.68790805 -0.88129439]. \t  -46.66635119968425 \t 1.0306985929027668\n",
      "46     \t [-1.85916826 -0.91781474]. \t  -3.677151823072371 \t 1.0306985929027668\n",
      "47     \t [-1.61111715  0.48236638]. \t  -0.5720610369045324 \t 1.0306985929027668\n",
      "48     \t [ 0.05058689 -0.63793275]. \t  0.9874215370770407 \t 1.0306985929027668\n",
      "49     \t [-2.82242365  0.16270713]. \t  -66.54411412468377 \t 1.0306985929027668\n",
      "50     \t [-2.09761025  1.6427845 ]. \t  -20.230439203197196 \t 1.0306985929027668\n",
      "51     \t [-2.26578346 -1.39609948]. \t  -20.852309596099982 \t 1.0306985929027668\n",
      "52     \t [-0.00220962 -0.68415958]. \t  0.9943917420282791 \t 1.0306985929027668\n",
      "53     \t [-1.62314669  0.99022259]. \t  -0.37414367227150785 \t 1.0306985929027668\n",
      "54     \t [2.02561443 0.82840007]. \t  -4.9006351078639785 \t 1.0306985929027668\n",
      "55     \t [-2.32292919  0.97403634]. \t  -10.353010193082945 \t 1.0306985929027668\n",
      "56     \t [ 0.04991704 -0.6346857 ]. \t  0.9839566224674201 \t 1.0306985929027668\n",
      "57     \t [-2.0371764   0.60926246]. \t  -2.082722851895475 \t 1.0306985929027668\n",
      "58     \t [1.28133908 1.92025246]. \t  -44.47958177219236 \t 1.0306985929027668\n",
      "59     \t [-1.65258663  0.18611014]. \t  -1.6097238562181924 \t 1.0306985929027668\n",
      "60     \t [-1.38051878  0.14670585]. \t  -2.016401009076795 \t 1.0306985929027668\n",
      "61     \t [-0.77790184 -1.87867914]. \t  -38.89674298101128 \t 1.0306985929027668\n",
      "62     \t [-0.5910202 -0.0648883]. \t  -1.1767762356526867 \t 1.0306985929027668\n",
      "63     \t [-2.53301849  0.01625332]. \t  -27.21681695433323 \t 1.0306985929027668\n",
      "64     \t [ 0.90975857 -0.92924577]. \t  -0.7442177043139835 \t 1.0306985929027668\n",
      "65     \t [-1.77629651 -1.886067  ]. \t  -41.92231808284836 \t 1.0306985929027668\n",
      "66     \t [2.44049402 0.38693755]. \t  -20.191583296663858 \t 1.0306985929027668\n",
      "67     \t [ 0.04731297 -0.7139353 ]. \t  1.0244582094928152 \t 1.0306985929027668\n",
      "68     \t [ 0.29288521 -1.3619114 ]. \t  -6.270961703110473 \t 1.0306985929027668\n",
      "69     \t [1.42169473 0.7488354 ]. \t  -2.33751822645579 \t 1.0306985929027668\n",
      "70     \t [2.5870525  0.28373031]. \t  -33.074948865719705 \t 1.0306985929027668\n",
      "71     \t [-0.48707467 -0.38844806]. \t  -0.5119313273843653 \t 1.0306985929027668\n",
      "72     \t [ 0.00615173 -0.74432965]. \t  0.9927520361408804 \t 1.0306985929027668\n",
      "73     \t [ 1.5797356  -1.23899433]. \t  -3.412952074281728 \t 1.0306985929027668\n",
      "74     \t [ 0.07595577 -0.60476274]. \t  0.9508227723858835 \t 1.0306985929027668\n",
      "75     \t [ 2.03724327 -1.85181545]. \t  -33.807305855072876 \t 1.0306985929027668\n",
      "76     \t [-0.10215466  0.69211414]. \t  1.0274283897674474 \t 1.0306985929027668\n",
      "77     \t [ 0.14509126 -0.76693684]. \t  0.9968860719654099 \t 1.0306985929027668\n",
      "78     \t [-0.23920191 -0.1224888 ]. \t  -0.19224352643091214 \t 1.0306985929027668\n",
      "79     \t [ 0.03293267 -0.75069151]. \t  1.004238324531852 \t 1.0306985929027668\n",
      "80     \t [ 2.33635455 -0.16672648]. \t  -12.979379570775944 \t 1.0306985929027668\n",
      "81     \t [-0.41667052 -0.11155714]. \t  -0.6302256936198231 \t 1.0306985929027668\n",
      "82     \t [1.00194036 0.73239216]. \t  -1.9755369383083268 \t 1.0306985929027668\n",
      "83     \t [-0.19467155  0.71394043]. \t  0.9900164712700178 \t 1.0306985929027668\n",
      "84     \t [-1.05265745  0.89282795]. \t  -0.7207040572239737 \t 1.0306985929027668\n",
      "85     \t [-2.59530879  1.9820506 ]. \t  -74.40555179730832 \t 1.0306985929027668\n",
      "86     \t [-0.09640934  0.64451237]. \t  0.9965079098223476 \t 1.0306985929027668\n",
      "87     \t [ 0.19637835 -0.30262325]. \t  0.24104999713041617 \t 1.0306985929027668\n",
      "88     \t [ 0.03049975 -0.76975237]. \t  0.9855192814944557 \t 1.0306985929027668\n",
      "89     \t [-0.0640559  -0.79551586]. \t  0.8620735189213845 \t 1.0306985929027668\n",
      "90     \t [-0.09398358  0.68882134]. \t  1.0269637209067533 \t 1.0306985929027668\n",
      "91     \t [ 0.14185097 -0.73013908]. \t  1.0195484312971521 \t 1.0306985929027668\n",
      "92     \t [ 0.1335903  -0.68689956]. \t  1.0178706170302674 \t 1.0306985929027668\n",
      "93     \t [ 1.04276395 -0.95564549]. \t  -0.9816636218497132 \t 1.0306985929027668\n",
      "94     \t [-1.00181589 -0.71221086]. \t  -1.9499409567324595 \t 1.0306985929027668\n",
      "95     \t [ 0.08592029 -0.70692452]. \t  \u001b[92m1.031324021353481\u001b[0m \t 1.031324021353481\n",
      "96     \t [ 2.52572856 -0.10205333]. \t  -26.294017680311253 \t 1.031324021353481\n",
      "97     \t [-0.00316329 -0.69411294]. \t  0.9964382781101411 \t 1.031324021353481\n",
      "98     \t [-2.6643835  -1.96777663]. \t  -91.54494845216861 \t 1.031324021353481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.08281988 -0.70858959]. \t  \u001b[92m1.0313298409369251\u001b[0m \t 1.0313298409369251\n",
      "100    \t [ 2.8492419  -0.03804816]. \t  -72.30079939605417 \t 1.0313298409369251\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.03616628 1.28517986]. \t  -11.164343885550666 \t -1.5094648555070385\n",
      "init   \t [0.85324299 0.66889047]. \t  -1.5094648555070385 \t -1.5094648555070385\n",
      "init   \t [-2.76516057 -1.73731045]. \t  -85.9872978949554 \t -1.5094648555070385\n",
      "init   \t [-1.34099616 -0.73455799]. \t  -2.3318567753236477 \t -1.5094648555070385\n",
      "init   \t [-1.83844553 -1.21162847]. \t  -7.376051913381579 \t -1.5094648555070385\n",
      "1      \t [-1.01437897 -1.75256636]. \t  -29.483576565451546 \t -1.5094648555070385\n",
      "2      \t [-1.91438989 -0.42850765]. \t  -3.0825209393641564 \t -1.5094648555070385\n",
      "3      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "4      \t [-0.05640391  2.        ]. \t  -47.8998965367624 \t -1.5094648555070385\n",
      "5      \t [1.00244671 0.20551426]. \t  -2.2814346947109962 \t -1.5094648555070385\n",
      "6      \t [-2.96701333  1.47701968]. \t  -105.80292056646262 \t -1.5094648555070385\n",
      "7      \t [-1.69152227 -0.76906951]. \t  -2.395299314041103 \t -1.5094648555070385\n",
      "8      \t [-0.37581582  0.09326528]. \t  \u001b[92m-0.4544568130064052\u001b[0m \t -0.4544568130064052\n",
      "9      \t [-1.12120405 -0.00457023]. \t  -2.3770078630629117 \t -0.4544568130064052\n",
      "10     \t [-0.05116708 -0.07685558]. \t  \u001b[92m0.009097192783050417\u001b[0m \t 0.009097192783050417\n",
      "11     \t [0.13620185 0.23751596]. \t  \u001b[92m0.10709191246407179\u001b[0m \t 0.10709191246407179\n",
      "12     \t [2.90089528 1.58797867]. \t  -103.54551315512943 \t 0.10709191246407179\n",
      "13     \t [1.52380481 1.05219382]. \t  -4.216311003251198 \t 0.10709191246407179\n",
      "14     \t [-0.64070024 -0.29532758]. \t  -1.1819495636764243 \t 0.10709191246407179\n",
      "15     \t [ 0.35644758 -0.03360426]. \t  -0.45851296913255374 \t 0.10709191246407179\n",
      "16     \t [1.56922006 1.92106752]. \t  -44.825015921670634 \t 0.10709191246407179\n",
      "17     \t [2.2289841  0.59820475]. \t  -9.330734784046534 \t 0.10709191246407179\n",
      "18     \t [-1.06612122  0.70212874]. \t  -0.5745874498456677 \t 0.10709191246407179\n",
      "19     \t [0.59968305 0.33681097]. \t  -0.9820857720893055 \t 0.10709191246407179\n",
      "20     \t [ 0.61162402 -0.85555746]. \t  0.08810934323476172 \t 0.10709191246407179\n",
      "21     \t [-0.06687928  0.79188882]. \t  \u001b[92m0.9705062451128493\u001b[0m \t 0.9705062451128493\n",
      "22     \t [ 1.23470571 -0.85777865]. \t  -0.5616924861630658 \t 0.9705062451128493\n",
      "23     \t [ 0.96276072 -0.60166938]. \t  -0.665758755828095 \t 0.9705062451128493\n",
      "24     \t [ 1.03774098 -1.41537451]. \t  -8.859174953180483 \t 0.9705062451128493\n",
      "25     \t [1.71515554 0.59658607]. \t  -2.185970345519278 \t 0.9705062451128493\n",
      "26     \t [-0.29715605  0.61722153]. \t  0.7896690612223893 \t 0.9705062451128493\n",
      "27     \t [ 1.76825411 -0.48852082]. \t  -0.5752010388217574 \t 0.9705062451128493\n",
      "28     \t [ 0.12113733 -0.68158152]. \t  \u001b[92m1.0192932476930665\u001b[0m \t 1.0192932476930665\n",
      "29     \t [ 0.20457736 -0.61970175]. \t  0.9092273196485889 \t 1.0192932476930665\n",
      "30     \t [-2.79086856 -0.21948544]. \t  -61.69526754144576 \t 1.0192932476930665\n",
      "31     \t [-1.68171016  0.10517426]. \t  -1.8355176611791315 \t 1.0192932476930665\n",
      "32     \t [-1.30307549  1.18672325]. \t  -3.12289688899998 \t 1.0192932476930665\n",
      "33     \t [ 0.23317622 -0.84945214]. \t  0.7903707510127012 \t 1.0192932476930665\n",
      "34     \t [-0.02325013 -0.56856418]. \t  0.8596784265581336 \t 1.0192932476930665\n",
      "35     \t [0.02150614 0.69156258]. \t  0.9813868345065434 \t 1.0192932476930665\n",
      "36     \t [-0.50383496  0.98811789]. \t  -0.2954197517958001 \t 1.0192932476930665\n",
      "37     \t [-2.33313211  0.64413321]. \t  -10.840325346586532 \t 1.0192932476930665\n",
      "38     \t [ 0.08925651 -0.67917935]. \t  \u001b[92m1.0228918623095424\u001b[0m \t 1.0228918623095424\n",
      "39     \t [ 0.17652761 -0.72445466]. \t  1.002800402176357 \t 1.0228918623095424\n",
      "40     \t [0.18489196 0.77016641]. \t  0.6885906675508231 \t 1.0228918623095424\n",
      "41     \t [-0.37402625 -0.93030868]. \t  -0.401642068037274 \t 1.0228918623095424\n",
      "42     \t [1.74432768 0.36192449]. \t  -2.2947272832239722 \t 1.0228918623095424\n",
      "43     \t [-0.00170113 -0.65256742]. \t  0.9768820080365259 \t 1.0228918623095424\n",
      "44     \t [-0.00075889 -0.4882414 ]. \t  0.7258463677604814 \t 1.0228918623095424\n",
      "45     \t [-2.01313241  1.69248607]. \t  -21.863720800237815 \t 1.0228918623095424\n",
      "46     \t [-0.34952555 -1.65696159]. \t  -20.20655931026264 \t 1.0228918623095424\n",
      "47     \t [0.19945342 0.8981783 ]. \t  0.288711967883419 \t 1.0228918623095424\n",
      "48     \t [-0.12224617  0.73296553]. \t  \u001b[92m1.0247467842373679\u001b[0m \t 1.0247467842373679\n",
      "49     \t [-1.72158534  0.80701522]. \t  0.2111083670045243 \t 1.0247467842373679\n",
      "50     \t [-2.80745569 -0.78639926]. \t  -65.54659070114327 \t 1.0247467842373679\n",
      "51     \t [2.64482093 0.05739439]. \t  -39.45595239398992 \t 1.0247467842373679\n",
      "52     \t [2.88321692 0.50180912]. \t  -80.31301853059882 \t 1.0247467842373679\n",
      "53     \t [2.75631719 0.8736854 ]. \t  -57.033858950697564 \t 1.0247467842373679\n",
      "54     \t [ 1.94759015 -1.44321963]. \t  -9.360926545341943 \t 1.0247467842373679\n",
      "55     \t [ 1.15616927 -1.00246659]. \t  -1.2515516194025211 \t 1.0247467842373679\n",
      "56     \t [-1.6016663  -0.88536188]. \t  -2.809173142980251 \t 1.0247467842373679\n",
      "57     \t [0.00372818 1.07337042]. \t  -0.7051205099094748 \t 1.0247467842373679\n",
      "58     \t [-1.66448557 -0.56728501]. \t  -2.1228155949450414 \t 1.0247467842373679\n",
      "59     \t [-2.79303402  0.44502954]. \t  -59.775048507785044 \t 1.0247467842373679\n",
      "60     \t [ 2.48071995 -0.68503503]. \t  -20.07684884118497 \t 1.0247467842373679\n",
      "61     \t [ 0.15628587 -0.67105499]. \t  0.998548834118897 \t 1.0247467842373679\n",
      "62     \t [ 1.53081709 -1.67480463]. \t  -19.81874943512973 \t 1.0247467842373679\n",
      "63     \t [-1.00928588 -0.90835668]. \t  -2.587461425158197 \t 1.0247467842373679\n",
      "64     \t [ 1.73293054 -0.90303612]. \t  0.06557521846813807 \t 1.0247467842373679\n",
      "65     \t [0.21002109 0.56319727]. \t  0.5756621194672236 \t 1.0247467842373679\n",
      "66     \t [-1.10009728 -0.35363513]. \t  -2.3073517237133614 \t 1.0247467842373679\n",
      "67     \t [ 1.12482493 -1.58099808]. \t  -15.588889124059172 \t 1.0247467842373679\n",
      "68     \t [0.16930928 0.90281358]. \t  0.3371181676665549 \t 1.0247467842373679\n",
      "69     \t [-2.91393861 -0.96208655]. \t  -89.14848637067882 \t 1.0247467842373679\n",
      "70     \t [-0.03498823  0.69033745]. \t  1.0170634816410133 \t 1.0247467842373679\n",
      "71     \t [-0.88618045  0.86335414]. \t  -0.4833480521606608 \t 1.0247467842373679\n",
      "72     \t [0.19842418 0.9788918 ]. \t  -0.1883852766947534 \t 1.0247467842373679\n",
      "73     \t [-0.42996539 -0.08880044]. \t  -0.6767031144041831 \t 1.0247467842373679\n",
      "74     \t [-1.23406737 -1.61124739]. \t  -20.961775855228854 \t 1.0247467842373679\n",
      "75     \t [ 2.35794808 -0.98124886]. \t  -12.156894774937614 \t 1.0247467842373679\n",
      "76     \t [0.4971928 0.406593 ]. \t  -0.5157145329080287 \t 1.0247467842373679\n",
      "77     \t [2.10915116 1.65383641]. \t  -28.053107444365978 \t 1.0247467842373679\n",
      "78     \t [-0.11734432  0.72929147]. \t  \u001b[92m1.0268349729106456\u001b[0m \t 1.0268349729106456\n",
      "79     \t [ 0.6369389  -1.68238681]. \t  -20.95134466822598 \t 1.0268349729106456\n",
      "80     \t [-2.40525527 -1.66380939]. \t  -40.98022631262636 \t 1.0268349729106456\n",
      "81     \t [1.12943309 0.75342911]. \t  -2.246502150436677 \t 1.0268349729106456\n",
      "82     \t [-2.75694262  1.45525316]. \t  -60.907760328896764 \t 1.0268349729106456\n",
      "83     \t [-1.95619532  0.94174824]. \t  -0.9905924109691995 \t 1.0268349729106456\n",
      "84     \t [2.16973958 1.86296958]. \t  -45.40950165469089 \t 1.0268349729106456\n",
      "85     \t [2.62045572 0.94140433]. \t  -38.43933928147885 \t 1.0268349729106456\n",
      "86     \t [-0.06380384  0.71039889]. \t  \u001b[92m1.0289901281202716\u001b[0m \t 1.0289901281202716\n",
      "87     \t [-2.92248883  1.13975918]. \t  -86.87723689891227 \t 1.0289901281202716\n",
      "88     \t [ 2.93258401 -0.95958499]. \t  -87.99895438247323 \t 1.0289901281202716\n",
      "89     \t [ 0.06072783 -0.72264918]. \t  1.0271867430459622 \t 1.0289901281202716\n",
      "90     \t [ 1.84648262 -0.49219817]. \t  -0.7944946539199708 \t 1.0289901281202716\n",
      "91     \t [2.99130591 1.36234056]. \t  -116.88989274527349 \t 1.0289901281202716\n",
      "92     \t [ 1.61357124 -0.5160565 ]. \t  -0.44783734847806367 \t 1.0289901281202716\n",
      "93     \t [1.63628062 1.15385615]. \t  -5.706261461664688 \t 1.0289901281202716\n",
      "94     \t [0.02448335 0.35365705]. \t  0.42666421846708025 \t 1.0289901281202716\n",
      "95     \t [-0.38776684 -0.49921305]. \t  -0.00026002408885739126 \t 1.0289901281202716\n",
      "96     \t [ 1.98787556 -0.08699199]. \t  -3.380044472638614 \t 1.0289901281202716\n",
      "97     \t [1.16087083 0.00538577]. \t  -2.3986441238140785 \t 1.0289901281202716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.79473501  1.46034985]. \t  -9.276890587165429 \t 1.0289901281202716\n",
      "99     \t [-0.38490882  1.11186483]. \t  -1.287859305303532 \t 1.0289901281202716\n",
      "100    \t [ 1.88937607 -0.95511048]. \t  -0.5568805346409538 \t 1.0289901281202716\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.34411965 0.16704251]. \t  -0.3937629170174044 \t 0.3730261027784846\n",
      "init   \t [ 0.3625427  -0.47180763]. \t  0.3730261027784846 \t 0.3730261027784846\n",
      "init   \t [-2.28543493  1.38399646]. \t  -14.95148366216595 \t 0.3730261027784846\n",
      "init   \t [1.43844967 1.72943026]. \t  -28.545251683089745 \t 0.3730261027784846\n",
      "init   \t [2.15596804 0.8324185 ]. \t  -7.6401968083208285 \t 0.3730261027784846\n",
      "1      \t [ 2.81160912 -0.08415915]. \t  -64.79173078590617 \t 0.3730261027784846\n",
      "2      \t [-0.23106522 -0.45348889]. \t  0.3410229294819404 \t 0.3730261027784846\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "4      \t [ 0.0789118  -0.25498825]. \t  0.23846077990721387 \t 0.3730261027784846\n",
      "5      \t [ 0.12012353 -1.48392355]. \t  -10.466640607038924 \t 0.3730261027784846\n",
      "6      \t [0.90299428 0.07482027]. \t  -2.091365268965974 \t 0.3730261027784846\n",
      "7      \t [-0.74640357 -0.30672528]. \t  -1.5223368401391302 \t 0.3730261027784846\n",
      "8      \t [3.         1.73320159]. \t  -138.1794234168509 \t 0.3730261027784846\n",
      "9      \t [1.5446228  0.73589959]. \t  -2.2601762627851594 \t 0.3730261027784846\n",
      "10     \t [-0.34933316  1.387776  ]. \t  -7.105687500746312 \t 0.3730261027784846\n",
      "11     \t [-0.56980314 -0.75746869]. \t  -0.5421105193649437 \t 0.3730261027784846\n",
      "12     \t [-2.96453478  1.94657731]. \t  -135.7248957934397 \t 0.3730261027784846\n",
      "13     \t [-1.77239096  0.83964886]. \t  0.14459279157497595 \t 0.3730261027784846\n",
      "14     \t [-1.55266112  1.54727847]. \t  -13.05620861785108 \t 0.3730261027784846\n",
      "15     \t [-2.32928803  0.69693762]. \t  -10.499719307800447 \t 0.3730261027784846\n",
      "16     \t [0.01615899 0.72974001]. \t  \u001b[92m0.9829335016898544\u001b[0m \t 0.9829335016898544\n",
      "17     \t [-1.93969194  1.05991811]. \t  -1.5745429516476164 \t 0.9829335016898544\n",
      "18     \t [-1.17228418  0.37831003]. \t  -1.46210767892256 \t 0.9829335016898544\n",
      "19     \t [-1.62460303  0.24599812]. \t  -1.4301056809276036 \t 0.9829335016898544\n",
      "20     \t [0.09482291 0.91456236]. \t  0.42475726110870216 \t 0.9829335016898544\n",
      "21     \t [ 0.97140808 -0.60636015]. \t  -0.66570467940294 \t 0.9829335016898544\n",
      "22     \t [ 1.64565052 -1.8481631 ]. \t  -32.015586964356885 \t 0.9829335016898544\n",
      "23     \t [1.83846767 0.42491363]. \t  -2.589584805300085 \t 0.9829335016898544\n",
      "24     \t [-0.02378041  0.59582521]. \t  0.9278165174691358 \t 0.9829335016898544\n",
      "25     \t [-1.95084447 -1.05593106]. \t  -5.753903332764851 \t 0.9829335016898544\n",
      "26     \t [-1.50162579 -0.92593893]. \t  -3.0649843501923475 \t 0.9829335016898544\n",
      "27     \t [-1.95876152 -0.43696331]. \t  -3.498097426729177 \t 0.9829335016898544\n",
      "28     \t [-0.01518318  0.8066197 ]. \t  0.9205610223113977 \t 0.9829335016898544\n",
      "29     \t [ 0.62926623 -0.40990959]. \t  -0.4582112676937422 \t 0.9829335016898544\n",
      "30     \t [ 0.86424664 -0.90411914]. \t  -0.5766837885142329 \t 0.9829335016898544\n",
      "31     \t [ 0.63388754 -1.32783878]. \t  -5.830374991726465 \t 0.9829335016898544\n",
      "32     \t [ 0.05770152 -0.64065586]. \t  \u001b[92m0.9915882005324195\u001b[0m \t 0.9915882005324195\n",
      "33     \t [ 0.25800663 -0.70729968]. \t  0.9254252670220189 \t 0.9915882005324195\n",
      "34     \t [0.01053681 0.73141925]. \t  0.9869563421790106 \t 0.9915882005324195\n",
      "35     \t [-2.45517733 -0.86928745]. \t  -22.21160294481286 \t 0.9915882005324195\n",
      "36     \t [-0.73732793 -1.31955173]. \t  -7.742916969456498 \t 0.9915882005324195\n",
      "37     \t [ 0.05651427 -0.71226656]. \t  \u001b[92m1.027284641349812\u001b[0m \t 1.027284641349812\n",
      "38     \t [-1.10399041 -1.86500434]. \t  -38.897831837209274 \t 1.027284641349812\n",
      "39     \t [-1.21281292 -1.62213654]. \t  -21.53850489606362 \t 1.027284641349812\n",
      "40     \t [ 1.59900716 -1.81179592]. \t  -29.14495570125768 \t 1.027284641349812\n",
      "41     \t [-2.78399023 -1.87129091]. \t  -100.30034839322298 \t 1.027284641349812\n",
      "42     \t [ 0.14469391 -0.72665586]. \t  1.0191723577766891 \t 1.027284641349812\n",
      "43     \t [-0.5066195   0.70470737]. \t  0.46302306143381566 \t 1.027284641349812\n",
      "44     \t [-2.14028291  1.7231771 ]. \t  -26.00064756068846 \t 1.027284641349812\n",
      "45     \t [-0.3756004  0.2067667]. \t  -0.28208318095697904 \t 1.027284641349812\n",
      "46     \t [ 0.2331444  -0.77053813]. \t  0.9332320338070067 \t 1.027284641349812\n",
      "47     \t [-2.28577133 -1.23976423]. \t  -17.25036356190636 \t 1.027284641349812\n",
      "48     \t [-1.50067069  0.92554643]. \t  -0.28466284252014035 \t 1.027284641349812\n",
      "49     \t [-0.02416309  0.7657793 ]. \t  0.9862966519762904 \t 1.027284641349812\n",
      "50     \t [1.65241809 1.38526686]. \t  -11.3939211718474 \t 1.027284641349812\n",
      "51     \t [-0.2291505  -0.62290786]. \t  0.602799330676699 \t 1.027284641349812\n",
      "52     \t [-0.18948295  0.70595758]. \t  0.9928328705202233 \t 1.027284641349812\n",
      "53     \t [-0.84730347 -1.46758405]. \t  -13.096412091357216 \t 1.027284641349812\n",
      "54     \t [0.73188169 0.2254517 ]. \t  -1.5633216755247445 \t 1.027284641349812\n",
      "55     \t [ 0.13160956 -0.75572143]. \t  1.0105748646378223 \t 1.027284641349812\n",
      "56     \t [2.78270860e-04 6.49628242e-01]. \t  0.975493432825064 \t 1.027284641349812\n",
      "57     \t [-2.73765081 -1.74375766]. \t  -81.94293174449048 \t 1.027284641349812\n",
      "58     \t [-2.12866504 -0.37413172]. \t  -6.334222581869954 \t 1.027284641349812\n",
      "59     \t [2.31694503 0.40924027]. \t  -12.912886046617707 \t 1.027284641349812\n",
      "60     \t [ 2.26559285 -0.83148234]. \t  -7.544644863245212 \t 1.027284641349812\n",
      "61     \t [ 1.77547482 -0.91226199]. \t  -0.004768356986522004 \t 1.027284641349812\n",
      "62     \t [-1.42640702 -1.42301748]. \t  -12.584748621560806 \t 1.027284641349812\n",
      "63     \t [-0.89526163 -1.05356796]. \t  -3.4602208165454043 \t 1.027284641349812\n",
      "64     \t [ 0.77973851 -1.87471528]. \t  -35.61904688587131 \t 1.027284641349812\n",
      "65     \t [2.04271653 0.02346745]. \t  -4.390058271514452 \t 1.027284641349812\n",
      "66     \t [1.45073363 0.33172567]. \t  -2.3136173297813474 \t 1.027284641349812\n",
      "67     \t [ 1.66456456 -0.55198868]. \t  -0.2853457169404895 \t 1.027284641349812\n",
      "68     \t [ 0.163275   -0.77495171]. \t  0.9809401316865627 \t 1.027284641349812\n",
      "69     \t [-1.87221732  0.1381844 ]. \t  -2.2411284602456933 \t 1.027284641349812\n",
      "70     \t [-2.39422297 -1.80228989]. \t  -50.23774986710231 \t 1.027284641349812\n",
      "71     \t [ 2.15851782 -0.44039846]. \t  -5.1879436534507795 \t 1.027284641349812\n",
      "72     \t [ 0.11396787 -0.78972675]. \t  0.9772268983380612 \t 1.027284641349812\n",
      "73     \t [-2.49974569  0.1867351 ]. \t  -23.726181972301156 \t 1.027284641349812\n",
      "74     \t [ 0.11163985 -0.70772203]. \t  \u001b[92m1.029478689896796\u001b[0m \t 1.029478689896796\n",
      "75     \t [-2.23134452 -0.0290725 ]. \t  -9.060634452141048 \t 1.029478689896796\n",
      "76     \t [-0.02442857  0.6942312 ]. \t  1.013270596144027 \t 1.029478689896796\n",
      "77     \t [2.86394139 1.843679  ]. \t  -113.36538306615888 \t 1.029478689896796\n",
      "78     \t [-1.17724415  0.33368658]. \t  -1.6087738020855884 \t 1.029478689896796\n",
      "79     \t [ 0.07705502 -0.7509969 ]. \t  1.0178100143550097 \t 1.029478689896796\n",
      "80     \t [ 0.29857531 -0.85700682]. \t  0.6958579526210769 \t 1.029478689896796\n",
      "81     \t [ 1.35543692 -0.59289011]. \t  -0.6122565050807526 \t 1.029478689896796\n",
      "82     \t [-2.1751507   1.01815312]. \t  -5.1571279723890875 \t 1.029478689896796\n",
      "83     \t [-0.03688358  0.76079807]. \t  0.9977768481429655 \t 1.029478689896796\n",
      "84     \t [-0.09468621  0.70924885]. \t  \u001b[92m1.0314259105576675\u001b[0m \t 1.0314259105576675\n",
      "85     \t [-1.36919712  0.41587675]. \t  -1.1729811270120105 \t 1.0314259105576675\n",
      "86     \t [0.77036719 0.86864376]. \t  -1.6322536922725321 \t 1.0314259105576675\n",
      "87     \t [-0.08958938  0.61735895]. \t  0.966820546758358 \t 1.0314259105576675\n",
      "88     \t [ 0.16384412 -0.78389894]. \t  0.9701258030438284 \t 1.0314259105576675\n",
      "89     \t [ 2.17590065 -0.84576374]. \t  -4.586281595793041 \t 1.0314259105576675\n",
      "90     \t [-0.08988776  0.71156691]. \t  \u001b[92m1.0316186898180488\u001b[0m \t 1.0316186898180488\n",
      "91     \t [ 2.74295744 -0.15740264]. \t  -52.65936821401287 \t 1.0316186898180488\n",
      "92     \t [ 2.59494145 -1.37071372]. \t  -36.53834520005145 \t 1.0316186898180488\n",
      "93     \t [-2.25927235  0.34047687]. \t  -8.853917990078358 \t 1.0316186898180488\n",
      "94     \t [-1.60483318  1.39877428]. \t  -7.308463097182967 \t 1.0316186898180488\n",
      "95     \t [-0.12380703  0.69031718]. \t  1.0224436989579837 \t 1.0316186898180488\n",
      "96     \t [2.38486964 0.39298819]. \t  -16.561846062575402 \t 1.0316186898180488\n",
      "97     \t [ 0.07577561 -0.69760554]. \t  1.0292503691137052 \t 1.0316186898180488\n",
      "98     \t [-2.65823165  1.50589863]. \t  -48.51357447859141 \t 1.0316186898180488\n",
      "99     \t [-1.09238915  0.81764816]. \t  -0.5697297134987273 \t 1.0316186898180488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-1.065235   -1.76307983]. \t  -30.41611267523959 \t 1.0316186898180488\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.35716091 -0.67208078]. \t  -11.995980169715628 \t 0.3096187466378422\n",
      "init   \t [ 1.92737474 -1.8332135 ]. \t  -31.167927894156623 \t 0.3096187466378422\n",
      "init   \t [-2.35405992  0.38020826]. \t  -13.013381745677465 \t 0.3096187466378422\n",
      "init   \t [ 0.17890417 -0.32477029]. \t  0.3096187466378422 \t 0.3096187466378422\n",
      "init   \t [-0.9875529   0.49007773]. \t  -0.9989196280666852 \t 0.3096187466378422\n",
      "1      \t [-0.12191844  0.49506843]. \t  \u001b[92m0.741453601410202\u001b[0m \t 0.741453601410202\n",
      "2      \t [-0.37098235 -0.24344843]. \t  -0.3789004257898174 \t 0.741453601410202\n",
      "3      \t [-0.669951    1.49296697]. \t  -11.359290133322727 \t 0.741453601410202\n",
      "4      \t [0.64869573 0.24332304]. \t  -1.2712393080277513 \t 0.741453601410202\n",
      "5      \t [-0.18875744  0.20424528]. \t  0.05858970564507704 \t 0.741453601410202\n",
      "6      \t [-0.20392166 -1.33670575]. \t  -6.058546494320597 \t 0.741453601410202\n",
      "7      \t [0.41775451 1.05670176]. \t  -1.5982037606762858 \t 0.741453601410202\n",
      "8      \t [3. 2.]. \t  -162.89999999999998 \t 0.741453601410202\n",
      "9      \t [-3. -2.]. \t  -162.89999999999998 \t 0.741453601410202\n",
      "10     \t [-0.04515023  0.57978425]. \t  \u001b[92m0.9106443623061353\u001b[0m \t 0.9106443623061353\n",
      "11     \t [-0.07540089  0.85796567]. \t  0.8190355668322429 \t 0.9106443623061353\n",
      "12     \t [ 0.51571504 -0.7701858 ]. \t  0.44088788826517666 \t 0.9106443623061353\n",
      "13     \t [ 0.53016302 -0.51458355]. \t  0.08574088835462013 \t 0.9106443623061353\n",
      "14     \t [-0.02073442  0.65038988]. \t  \u001b[92m0.988054489733031\u001b[0m \t 0.988054489733031\n",
      "15     \t [ 0.23033716 -0.75181801]. \t  0.9497922304217868 \t 0.988054489733031\n",
      "16     \t [-2.93993004  1.9819571 ]. \t  -133.1039740457407 \t 0.988054489733031\n",
      "17     \t [-1.62990592  0.17267372]. \t  -1.6581228167467315 \t 0.988054489733031\n",
      "18     \t [-1.28190255  0.04903936]. \t  -2.3090401732286376 \t 0.988054489733031\n",
      "19     \t [ 1.4020299  -0.06344982]. \t  -2.175263089107674 \t 0.988054489733031\n",
      "20     \t [ 2.65066632 -0.13111749]. \t  -39.63588027963832 \t 0.988054489733031\n",
      "21     \t [ 1.38167895 -0.55557483]. \t  -0.6807864018502735 \t 0.988054489733031\n",
      "22     \t [ 0.20987165 -1.93480733]. \t  -40.846711969417285 \t 0.988054489733031\n",
      "23     \t [-0.11269532 -0.70218072]. \t  0.8702118339700617 \t 0.988054489733031\n",
      "24     \t [-1.01438666 -1.69837124]. \t  -25.721097984822222 \t 0.988054489733031\n",
      "25     \t [0.00104752 0.75330168]. \t  0.9810012604209467 \t 0.988054489733031\n",
      "26     \t [ 0.09120335 -0.67357402]. \t  \u001b[92m1.0197310763253764\u001b[0m \t 1.0197310763253764\n",
      "27     \t [ 2.95885518 -1.37818516]. \t  -100.49291343525168 \t 1.0197310763253764\n",
      "28     \t [ 1.83559548 -0.56220473]. \t  -0.49072037048500294 \t 1.0197310763253764\n",
      "29     \t [-1.69983582 -0.47112305]. \t  -2.1763907126599795 \t 1.0197310763253764\n",
      "30     \t [-2.7686966  -0.13574454]. \t  -57.716559004142354 \t 1.0197310763253764\n",
      "31     \t [-1.2260299  -0.71123319]. \t  -2.271966769039133 \t 1.0197310763253764\n",
      "32     \t [-0.11935141  0.71103793]. \t  \u001b[92m1.028185177802214\u001b[0m \t 1.028185177802214\n",
      "33     \t [-1.3416263   0.91318161]. \t  -0.560822563009315 \t 1.028185177802214\n",
      "34     \t [-0.03984047  0.72294706]. \t  1.0204062549676756 \t 1.028185177802214\n",
      "35     \t [ 2.53367887 -0.35266827]. \t  -25.990881745808792 \t 1.028185177802214\n",
      "36     \t [ 1.72961388 -0.86037823]. \t  0.1605510191323053 \t 1.028185177802214\n",
      "37     \t [-2.86338355 -1.38491355]. \t  -86.3555363466685 \t 1.028185177802214\n",
      "38     \t [ 2.82050773 -0.44522179]. \t  -64.84804885355754 \t 1.028185177802214\n",
      "39     \t [-1.76056921  0.65975877]. \t  -0.004302697335522243 \t 1.028185177802214\n",
      "40     \t [ 0.46918183 -1.93111569]. \t  -40.58740832883037 \t 1.028185177802214\n",
      "41     \t [0.35628443 0.68180985]. \t  0.2775457558364143 \t 1.028185177802214\n",
      "42     \t [ 0.070658   -0.86232102]. \t  0.803652930065609 \t 1.028185177802214\n",
      "43     \t [-0.61929583  0.54890956]. \t  -0.06200497758542112 \t 1.028185177802214\n",
      "44     \t [-0.24932822  0.74696857]. \t  0.9321789350043674 \t 1.028185177802214\n",
      "45     \t [-0.10195464  0.38407504]. \t  0.500819219076793 \t 1.028185177802214\n",
      "46     \t [ 2.17226571 -1.76336922]. \t  -29.545419149518928 \t 1.028185177802214\n",
      "47     \t [-0.00278237 -0.62369932]. \t  0.9489504180662062 \t 1.028185177802214\n",
      "48     \t [ 1.59696989 -0.83888727]. \t  0.10184090170387616 \t 1.028185177802214\n",
      "49     \t [0.1057779  0.63686963]. \t  0.8524966220729688 \t 1.028185177802214\n",
      "50     \t [ 0.05868269 -0.71205515]. \t  1.027838305561759 \t 1.028185177802214\n",
      "51     \t [-1.1381645   1.52345037]. \t  -12.911047886815567 \t 1.028185177802214\n",
      "52     \t [-0.22025487  0.71312573]. \t  0.9676324422585564 \t 1.028185177802214\n",
      "53     \t [-0.21087362  0.81136128]. \t  0.8971025504292355 \t 1.028185177802214\n",
      "54     \t [ 2.40649414 -0.53453161]. \t  -15.374250651300464 \t 1.028185177802214\n",
      "55     \t [ 1.85239383 -1.13546471]. \t  -1.8552913758824587 \t 1.028185177802214\n",
      "56     \t [-0.01009399  0.64001979]. \t  0.9733824978349569 \t 1.028185177802214\n",
      "57     \t [-1.18232872 -0.730649  ]. \t  -2.2669457097328936 \t 1.028185177802214\n",
      "58     \t [0.95681507 1.31153758]. \t  -8.36744877747643 \t 1.028185177802214\n",
      "59     \t [ 1.73797702 -0.35136805]. \t  -1.0650705072639377 \t 1.028185177802214\n",
      "60     \t [-1.96668278 -1.27530915]. \t  -9.926198408177598 \t 1.028185177802214\n",
      "61     \t [2.07965784 1.54916583]. \t  -21.645796951056074 \t 1.028185177802214\n",
      "62     \t [1.79926657 0.6059405 ]. \t  -2.410937783661596 \t 1.028185177802214\n",
      "63     \t [ 1.31448403 -0.11568594]. \t  -2.156512325751887 \t 1.028185177802214\n",
      "64     \t [-2.91565639  0.60580754]. \t  -84.33043212592362 \t 1.028185177802214\n",
      "65     \t [-2.8110672  -1.00878065]. \t  -67.86265076008986 \t 1.028185177802214\n",
      "66     \t [-0.06864935  0.64692368]. \t  0.9990442407498064 \t 1.028185177802214\n",
      "67     \t [0.71127764 0.56739309]. \t  -1.059732591878914 \t 1.028185177802214\n",
      "68     \t [-0.80793547  0.26417126]. \t  -1.335852925179555 \t 1.028185177802214\n",
      "69     \t [-1.87701998 -1.16950057]. \t  -6.810399190289848 \t 1.028185177802214\n",
      "70     \t [2.62019305 1.47188718]. \t  -50.309870190637255 \t 1.028185177802214\n",
      "71     \t [-2.23506261 -1.0726989 ]. \t  -12.221866657394914 \t 1.028185177802214\n",
      "72     \t [ 0.18188601 -0.71918624]. \t  0.9995787822765818 \t 1.028185177802214\n",
      "73     \t [ 0.3954629 -0.0086061]. \t  -0.5717769588223559 \t 1.028185177802214\n",
      "74     \t [ 0.58043634 -1.47269991]. \t  -10.407376176186657 \t 1.028185177802214\n",
      "75     \t [-1.66307653  1.10260861]. \t  -1.2668489324928376 \t 1.028185177802214\n",
      "76     \t [2.33595046 0.77983323]. \t  -14.324929778720128 \t 1.028185177802214\n",
      "77     \t [1.3986583  1.96047331]. \t  -48.740762532781545 \t 1.028185177802214\n",
      "78     \t [ 0.67201117 -1.79122895]. \t  -28.549006930803053 \t 1.028185177802214\n",
      "79     \t [-0.79921115 -1.97378121]. \t  -48.48867856065587 \t 1.028185177802214\n",
      "80     \t [-0.21156133  1.27782219]. \t  -4.037729088654012 \t 1.028185177802214\n",
      "81     \t [1.71113178 0.95239292]. \t  -3.368169503521101 \t 1.028185177802214\n",
      "82     \t [-2.43590768  0.58794679]. \t  -17.098110317723936 \t 1.028185177802214\n",
      "83     \t [-2.52668816  0.23811163]. \t  -25.86433456497141 \t 1.028185177802214\n",
      "84     \t [ 0.12329002 -0.75104704]. \t  1.0158582232518898 \t 1.028185177802214\n",
      "85     \t [-0.0681401   0.76531021]. \t  1.0042434964472557 \t 1.028185177802214\n",
      "86     \t [0.01309009 0.71240814]. \t  0.9897626472955292 \t 1.028185177802214\n",
      "87     \t [ 0.12421091 -0.74203078]. \t  1.020708204125009 \t 1.028185177802214\n",
      "88     \t [-1.08728413  0.614188  ]. \t  -0.7370833259314306 \t 1.028185177802214\n",
      "89     \t [-0.01773171 -0.7071651 ]. \t  0.9862032835088613 \t 1.028185177802214\n",
      "90     \t [-1.17958267 -0.21085851]. \t  -2.4767151198401147 \t 1.028185177802214\n",
      "91     \t [-2.4632363  -1.88681742]. \t  -62.52141456818632 \t 1.028185177802214\n",
      "92     \t [ 2.28414527 -1.19344572]. \t  -10.737017723627515 \t 1.028185177802214\n",
      "93     \t [-1.08257047  0.13255206]. \t  -2.1275261860267647 \t 1.028185177802214\n",
      "94     \t [-0.0163336   1.02756611]. \t  -0.220347337255167 \t 1.028185177802214\n",
      "95     \t [ 0.01938034 -0.65838524]. \t  0.9935532728474701 \t 1.028185177802214\n",
      "96     \t [-2.64572875 -1.27727253]. \t  -46.93032312880587 \t 1.028185177802214\n",
      "97     \t [-0.00473055 -0.66039161]. \t  0.9804623599827457 \t 1.028185177802214\n",
      "98     \t [-0.17842741  0.69956862]. \t  0.999144801016171 \t 1.028185177802214\n",
      "99     \t [-0.21374517  0.70545031]. \t  0.9723682522942024 \t 1.028185177802214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.07948172  0.62429612]. \t  0.9758103921078468 \t 1.028185177802214\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.08401759 -0.79057356]. \t  -5.778591021444629 \t -0.6139922708386365\n",
      "init   \t [-2.62778151 -0.16055863]. \t  -37.56206986588009 \t -0.6139922708386365\n",
      "init   \t [2.01152031 1.70798819]. \t  -29.692842569565112 \t -0.6139922708386365\n",
      "init   \t [1.36193391 1.07398489]. \t  -4.49224583043401 \t -0.6139922708386365\n",
      "init   \t [-1.3847696   0.57611717]. \t  -0.6139922708386365 \t -0.6139922708386365\n",
      "1      \t [-1.33546514 -1.33946171]. \t  -9.833387638121035 \t -0.6139922708386365\n",
      "2      \t [0.2838874  0.69494898]. \t  \u001b[92m0.4926474344665255\u001b[0m \t 0.4926474344665255\n",
      "3      \t [1.34037633 0.15528739]. \t  -2.4550945739258507 \t 0.4926474344665255\n",
      "4      \t [-0.87185046  1.09262018]. \t  -1.946473086581968 \t 0.4926474344665255\n",
      "5      \t [-1.05617971  0.14038367]. \t  -2.086034244247926 \t 0.4926474344665255\n",
      "6      \t [0.72509872 0.89824793]. \t  -1.5989560308506285 \t 0.4926474344665255\n",
      "7      \t [-2.46851409 -1.80673456]. \t  -55.84426900955606 \t 0.4926474344665255\n",
      "8      \t [-1.58749501 -0.58897987]. \t  -2.1072275772073694 \t 0.4926474344665255\n",
      "9      \t [-0.06021863  0.42633027]. \t  \u001b[92m0.6060823344468371\u001b[0m \t 0.6060823344468371\n",
      "10     \t [-0.81834752  0.63697275]. \t  -0.35134076309665596 \t 0.6060823344468371\n",
      "11     \t [1.86938375 0.48547085]. \t  -2.7453181438303353 \t 0.6060823344468371\n",
      "12     \t [0.36375839 0.43282163]. \t  -0.041765999433472145 \t 0.6060823344468371\n",
      "13     \t [-1.9015448  2.       ]. \t  -46.962495267115244 \t 0.6060823344468371\n",
      "14     \t [-0.09508795  1.23944688]. \t  -3.2132176596792803 \t 0.6060823344468371\n",
      "15     \t [ 1.97819702 -1.71049586]. \t  -22.624102117582986 \t 0.6060823344468371\n",
      "16     \t [ 0.12032623 -1.86380164]. \t  -34.20612263486066 \t 0.6060823344468371\n",
      "17     \t [ 2.90296293 -0.88025528]. \t  -80.81165676528063 \t 0.6060823344468371\n",
      "18     \t [1.42003398 0.58129899]. \t  -2.190601261801846 \t 0.6060823344468371\n",
      "19     \t [ 0.77436659 -0.58444295]. \t  -0.36316671490529995 \t 0.6060823344468371\n",
      "20     \t [ 1.31343166 -0.54298105]. \t  -0.8173572114281388 \t 0.6060823344468371\n",
      "21     \t [ 1.01041903 -0.81376985]. \t  -0.53262560331891 \t 0.6060823344468371\n",
      "22     \t [-0.0848125   0.03703862]. \t  -0.02004286362349033 \t 0.6060823344468371\n",
      "23     \t [-1.2298219   0.84431664]. \t  -0.5421808330192731 \t 0.6060823344468371\n",
      "24     \t [2.90542942 0.9071926 ]. \t  -86.68689063945362 \t 0.6060823344468371\n",
      "25     \t [0.17074975 0.20096563]. \t  0.005864325293150258 \t 0.6060823344468371\n",
      "26     \t [-0.187739   -0.56880381]. \t  \u001b[92m0.6302681203753914\u001b[0m \t 0.6302681203753914\n",
      "27     \t [1.39416022 1.9784682 ]. \t  -50.677890879010995 \t 0.6302681203753914\n",
      "28     \t [-0.03130072 -0.72812491]. \t  \u001b[92m0.9696523141581839\u001b[0m \t 0.9696523141581839\n",
      "29     \t [-0.04562359 -0.58870442]. \t  0.8706647307812317 \t 0.9696523141581839\n",
      "30     \t [ 1.71523979 -0.09345485]. \t  -1.884849177923638 \t 0.9696523141581839\n",
      "31     \t [-0.53281432  1.71756302]. \t  -23.069246721273927 \t 0.9696523141581839\n",
      "32     \t [ 2.25589818 -1.79479828]. \t  -34.47555745554888 \t 0.9696523141581839\n",
      "33     \t [-0.36004187  0.78662898]. \t  0.7428213766085785 \t 0.9696523141581839\n",
      "34     \t [-1.87500732  1.5058791 ]. \t  -11.266393258109213 \t 0.9696523141581839\n",
      "35     \t [-2.06343242  1.05488176]. \t  -3.015380333743159 \t 0.9696523141581839\n",
      "36     \t [-2.92395011  1.78224008]. \t  -111.44648393053825 \t 0.9696523141581839\n",
      "37     \t [-1.73358009  0.94342478]. \t  -0.07519986442727106 \t 0.9696523141581839\n",
      "38     \t [-2.58195158  1.04187562]. \t  -29.776016537517318 \t 0.9696523141581839\n",
      "39     \t [-2.52711463 -0.53737245]. \t  -27.25506880824635 \t 0.9696523141581839\n",
      "40     \t [0.08151787 0.59227592]. \t  0.836177326986924 \t 0.9696523141581839\n",
      "41     \t [-2.66974222  1.32330721]. \t  -44.25179993477073 \t 0.9696523141581839\n",
      "42     \t [2.12310088 1.84371406]. \t  -42.42829347931966 \t 0.9696523141581839\n",
      "43     \t [1.20102993 1.13867345]. \t  -5.30654927068663 \t 0.9696523141581839\n",
      "44     \t [-1.26688507  0.95187126]. \t  -0.8421450545719095 \t 0.9696523141581839\n",
      "45     \t [-0.14819344 -0.78051211]. \t  0.7497995212476773 \t 0.9696523141581839\n",
      "46     \t [ 2.20702255 -0.10006412]. \t  -7.921342044964864 \t 0.9696523141581839\n",
      "47     \t [ 1.22830956 -0.33254151]. \t  -1.5976329368265905 \t 0.9696523141581839\n",
      "48     \t [ 1.8809176  -1.75878782]. \t  -25.220761712644656 \t 0.9696523141581839\n",
      "49     \t [1.21250649 1.14353358]. \t  -5.39680775060438 \t 0.9696523141581839\n",
      "50     \t [-0.50423155  1.21533856]. \t  -3.0924040456911426 \t 0.9696523141581839\n",
      "51     \t [-1.48759749 -0.07008751]. \t  -2.2648817888920205 \t 0.9696523141581839\n",
      "52     \t [ 0.85477092 -0.22929246]. \t  -1.5362746925688928 \t 0.9696523141581839\n",
      "53     \t [1.87037849 0.33092018]. \t  -2.7929414657368534 \t 0.9696523141581839\n",
      "54     \t [ 2.85999473 -1.45185161]. \t  -79.82478357806254 \t 0.9696523141581839\n",
      "55     \t [ 1.85806924 -0.84590557]. \t  -0.1101598636411667 \t 0.9696523141581839\n",
      "56     \t [ 1.41768042 -0.76215083]. \t  -0.2084062749618627 \t 0.9696523141581839\n",
      "57     \t [-0.68608189  0.05452516]. \t  -1.4030431217535346 \t 0.9696523141581839\n",
      "58     \t [ 0.10777843 -0.72889819]. \t  \u001b[92m1.028460684445148\u001b[0m \t 1.028460684445148\n",
      "59     \t [-0.00860593 -0.78279415]. \t  0.9421017861585248 \t 1.028460684445148\n",
      "60     \t [-0.71228017  1.56528679]. \t  -14.629359817589183 \t 1.028460684445148\n",
      "61     \t [-0.08972386  0.78315247]. \t  0.9868291510041477 \t 1.028460684445148\n",
      "62     \t [2.83701769 0.95688008]. \t  -72.36055116619683 \t 1.028460684445148\n",
      "63     \t [ 2.89550491 -0.68849145]. \t  -79.37229654354084 \t 1.028460684445148\n",
      "64     \t [ 1.12307156 -0.00925813]. \t  -2.362471481871678 \t 1.028460684445148\n",
      "65     \t [-2.31629345  0.35737844]. \t  -11.21801410637446 \t 1.028460684445148\n",
      "66     \t [-2.20651298 -0.45591465]. \t  -8.51287292237533 \t 1.028460684445148\n",
      "67     \t [ 1.90233713 -0.76710649]. \t  -0.34328865900458594 \t 1.028460684445148\n",
      "68     \t [ 0.13684218 -0.73710472]. \t  1.0191904180533808 \t 1.028460684445148\n",
      "69     \t [0.03967151 0.71908542]. \t  0.964015252325956 \t 1.028460684445148\n",
      "70     \t [-2.1289087   0.69118165]. \t  -3.5555779832535146 \t 1.028460684445148\n",
      "71     \t [2.27601261 0.04318991]. \t  -10.795542546218192 \t 1.028460684445148\n",
      "72     \t [ 0.10382921 -0.75539975]. \t  1.0156004830014775 \t 1.028460684445148\n",
      "73     \t [-1.36389509  1.56942999]. \t  -14.594391196357211 \t 1.028460684445148\n",
      "74     \t [-2.07828263 -0.23852778]. \t  -5.24056221098302 \t 1.028460684445148\n",
      "75     \t [-0.91419713 -0.72463519]. \t  -1.7357666953058564 \t 1.028460684445148\n",
      "76     \t [-0.03530569  0.65626515]. \t  0.9989683250865011 \t 1.028460684445148\n",
      "77     \t [-0.15999772  0.65753484]. \t  0.9858726103392562 \t 1.028460684445148\n",
      "78     \t [-0.14352453  0.97269335]. \t  0.26196003421164393 \t 1.028460684445148\n",
      "79     \t [1.68582026 0.37829297]. \t  -2.2051799001338055 \t 1.028460684445148\n",
      "80     \t [ 1.53943768 -1.39690109]. \t  -7.396929103144068 \t 1.028460684445148\n",
      "81     \t [0.29850144 0.75169909]. \t  0.41871435526569234 \t 1.028460684445148\n",
      "82     \t [-2.05853931 -1.20006989]. \t  -9.6113730410618 \t 1.028460684445148\n",
      "83     \t [2.70584702 0.07317875]. \t  -47.71789337336684 \t 1.028460684445148\n",
      "84     \t [ 1.61813427 -1.41849948]. \t  -7.910856169388076 \t 1.028460684445148\n",
      "85     \t [ 0.04472985 -0.76885355]. \t  0.9931731482258748 \t 1.028460684445148\n",
      "86     \t [-1.6924592  -0.31644914]. \t  -2.23660983264908 \t 1.028460684445148\n",
      "87     \t [-0.63113889 -1.01439375]. \t  -2.040767516238305 \t 1.028460684445148\n",
      "88     \t [-0.49715951  0.23486027]. \t  -0.5401806250858956 \t 1.028460684445148\n",
      "89     \t [1.99126132 0.79967047]. \t  -4.294183455666888 \t 1.028460684445148\n",
      "90     \t [-2.83890557  0.13291947]. \t  -69.88361423184833 \t 1.028460684445148\n",
      "91     \t [ 0.11791946 -0.64920987]. \t  0.9966739424163479 \t 1.028460684445148\n",
      "92     \t [ 0.21363143 -0.10393113]. \t  -0.1132682463885391 \t 1.028460684445148\n",
      "93     \t [0.02640109 1.95463056]. \t  -43.15941460461513 \t 1.028460684445148\n",
      "94     \t [ 0.17675447 -0.67155499]. \t  0.9861619164999932 \t 1.028460684445148\n",
      "95     \t [-0.10512433  0.68224647]. \t  1.0230002426460265 \t 1.028460684445148\n",
      "96     \t [ 0.14167691 -0.70925847]. \t  1.0210024037349084 \t 1.028460684445148\n",
      "97     \t [ 0.19176328 -0.71077231]. \t  0.9919225433847412 \t 1.028460684445148\n",
      "98     \t [ 1.68596276 -1.4508121 ]. \t  -8.914251805503532 \t 1.028460684445148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.0800936  -0.70385952]. \t  \u001b[92m1.0307170638615488\u001b[0m \t 1.0307170638615488\n",
      "100    \t [-0.13987136  0.65554394]. \t  0.9944900394745256 \t 1.0307170638615488\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.34300075 -1.83113994]. \t  -49.66879811482005 \t 0.22190529745233212\n",
      "init   \t [ 0.59815551 -0.80051718]. \t  0.22190529745233212 \t 0.22190529745233212\n",
      "init   \t [ 1.77967069 -0.5352155 ]. \t  -0.42351602320787596 \t 0.22190529745233212\n",
      "init   \t [ 2.28365006 -0.80863347]. \t  -8.27265764318726 \t 0.22190529745233212\n",
      "init   \t [-1.02538042  0.27486766]. \t  -1.710375599764969 \t 0.22190529745233212\n",
      "1      \t [1.29037445 0.20200115]. \t  -2.480997034389709 \t 0.22190529745233212\n",
      "2      \t [ 1.18473638 -0.80920456]. \t  -0.5361185956171156 \t 0.22190529745233212\n",
      "3      \t [ 1.745343   -0.14991732]. \t  -1.7709072181220349 \t 0.22190529745233212\n",
      "4      \t [-0.04553522 -0.2940538 ]. \t  \u001b[92m0.294289344340327\u001b[0m \t 0.294289344340327\n",
      "5      \t [ 0.0496607  -0.88825695]. \t  \u001b[92m0.7001745436561713\u001b[0m \t 0.7001745436561713\n",
      "6      \t [-1.53206188  1.32201802]. \t  -5.331602106816116 \t 0.7001745436561713\n",
      "7      \t [ 1.26057476 -0.42564457]. \t  -1.2610701939933218 \t 0.7001745436561713\n",
      "8      \t [0.61353053 1.33362384]. \t  -7.58291013697965 \t 0.7001745436561713\n",
      "9      \t [ 0.39195307 -1.43455974]. \t  -8.71285239128954 \t 0.7001745436561713\n",
      "10     \t [ 0.10994135 -0.64579632]. \t  \u001b[92m0.9954366410115363\u001b[0m \t 0.9954366410115363\n",
      "11     \t [-0.25071601  0.39313884]. \t  0.37802656351648806 \t 0.9954366410115363\n",
      "12     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9954366410115363\n",
      "13     \t [-0.9893803   1.38026127]. \t  -7.747796527373921 \t 0.9954366410115363\n",
      "14     \t [-1.42246108  0.76813767]. \t  -0.1970376007396586 \t 0.9954366410115363\n",
      "15     \t [ 0.16856295 -0.74523981]. \t  \u001b[92m1.0013848564518768\u001b[0m \t 1.0013848564518768\n",
      "16     \t [0.43390964 0.53273868]. \t  -0.09900533215528107 \t 1.0013848564518768\n",
      "17     \t [1.47163416 1.10462825]. \t  -4.899536069152372 \t 1.0013848564518768\n",
      "18     \t [-1.14555173  0.88120302]. \t  -0.6824264177709897 \t 1.0013848564518768\n",
      "19     \t [2.88967721 1.36537269]. \t  -91.44288653479214 \t 1.0013848564518768\n",
      "20     \t [-0.30730913 -0.64084393]. \t  0.4118441404303309 \t 1.0013848564518768\n",
      "21     \t [1.0225967  0.85490409]. \t  -2.355039942775524 \t 1.0013848564518768\n",
      "22     \t [1.10283818 1.95152523]. \t  -47.293865859347314 \t 1.0013848564518768\n",
      "23     \t [0.11878852 0.74134389]. \t  0.8460742351120865 \t 1.0013848564518768\n",
      "24     \t [-0.42301578 -1.04797595]. \t  -1.5253764391027507 \t 1.0013848564518768\n",
      "25     \t [1.5845592  0.61026497]. \t  -2.1127778838801374 \t 1.0013848564518768\n",
      "26     \t [0.00178673 0.39117344]. \t  0.517698552246121 \t 1.0013848564518768\n",
      "27     \t [ 1.88678313 -1.43312695]. \t  -8.618528568836634 \t 1.0013848564518768\n",
      "28     \t [-2.8569924  -0.31221082]. \t  -74.55055021051692 \t 1.0013848564518768\n",
      "29     \t [-2.19674658  1.74682719]. \t  -29.05991138123884 \t 1.0013848564518768\n",
      "30     \t [-0.32335014  0.89041939]. \t  0.5492350687058929 \t 1.0013848564518768\n",
      "31     \t [-0.10952098  1.11036338]. \t  -1.074679393744134 \t 1.0013848564518768\n",
      "32     \t [ 0.05183994 -0.70781622]. \t  \u001b[92m1.0259547618839664\u001b[0m \t 1.0259547618839664\n",
      "33     \t [-0.12787045  0.71512024]. \t  \u001b[92m1.0260797537026323\u001b[0m \t 1.0260797537026323\n",
      "34     \t [-0.17290339  0.71482243]. \t  1.0053993895396158 \t 1.0260797537026323\n",
      "35     \t [ 0.10720259 -0.67581912]. \t  1.0192681706072977 \t 1.0260797537026323\n",
      "36     \t [-2.64881066  0.3572587 ]. \t  -38.42520682620916 \t 1.0260797537026323\n",
      "37     \t [ 2.99367092 -1.84723403]. \t  -134.51474621210156 \t 1.0260797537026323\n",
      "38     \t [ 0.46761963 -0.45741137]. \t  0.09794985364525177 \t 1.0260797537026323\n",
      "39     \t [ 0.06044746 -0.71427139]. \t  \u001b[92m1.028173506356821\u001b[0m \t 1.028173506356821\n",
      "40     \t [-2.71533089 -1.62402195]. \t  -70.62024298757024 \t 1.028173506356821\n",
      "41     \t [ 0.16533661 -0.76573377]. \t  0.9889974498063511 \t 1.028173506356821\n",
      "42     \t [ 0.01401377 -0.74050587]. \t  1.0002413334480986 \t 1.028173506356821\n",
      "43     \t [2.74096889 0.74717986]. \t  -53.93365770139119 \t 1.028173506356821\n",
      "44     \t [-1.03532418 -1.97686049]. \t  -49.78921925317441 \t 1.028173506356821\n",
      "45     \t [1.84284901 0.18675245]. \t  -2.6298676813666293 \t 1.028173506356821\n",
      "46     \t [-1.57591629  1.85810017]. \t  -33.029232992836036 \t 1.028173506356821\n",
      "47     \t [0.00406456 0.66029699]. \t  0.9808620648452958 \t 1.028173506356821\n",
      "48     \t [-1.90559418  1.02129907]. \t  -1.0284902330103085 \t 1.028173506356821\n",
      "49     \t [-0.03110939  0.64384788]. \t  0.9869469246807769 \t 1.028173506356821\n",
      "50     \t [-0.57756502  1.07621416]. \t  -1.2245376903795397 \t 1.028173506356821\n",
      "51     \t [1.52050383 1.19365077]. \t  -6.378267640774154 \t 1.028173506356821\n",
      "52     \t [-1.26458625 -0.87970678]. \t  -2.8019793707299465 \t 1.028173506356821\n",
      "53     \t [-2.82495844  0.51092602]. \t  -65.37940866147089 \t 1.028173506356821\n",
      "54     \t [-1.84395531  0.02676023]. \t  -2.3733454070778044 \t 1.028173506356821\n",
      "55     \t [ 1.83967182 -1.66597754]. \t  -19.05207730965373 \t 1.028173506356821\n",
      "56     \t [-1.8416812  -0.31836561]. \t  -2.6369055361297424 \t 1.028173506356821\n",
      "57     \t [ 1.80811905 -0.92581519]. \t  -0.11568910277788064 \t 1.028173506356821\n",
      "58     \t [-0.15612816  0.6883878 ]. \t  1.0084861958479863 \t 1.028173506356821\n",
      "59     \t [-1.85086687  0.08896547]. \t  -2.263032534114449 \t 1.028173506356821\n",
      "60     \t [-1.57806562  0.11406319]. \t  -1.8544341899589911 \t 1.028173506356821\n",
      "61     \t [-0.39456501 -0.24192619]. \t  -0.44813147456540553 \t 1.028173506356821\n",
      "62     \t [-2.10497047  0.87975072]. \t  -2.9400379728272563 \t 1.028173506356821\n",
      "63     \t [-0.07083412  0.46505753]. \t  0.6909333218769065 \t 1.028173506356821\n",
      "64     \t [ 2.31706773 -1.89638128]. \t  -45.48156446785852 \t 1.028173506356821\n",
      "65     \t [ 0.04036884 -0.70894721]. \t  1.022079211726255 \t 1.028173506356821\n",
      "66     \t [-2.96225484 -0.50634735]. \t  -99.36079221562287 \t 1.028173506356821\n",
      "67     \t [ 0.13872231 -0.75847507]. \t  1.006346225062667 \t 1.028173506356821\n",
      "68     \t [-1.61999573 -1.36908186]. \t  -10.832711919491926 \t 1.028173506356821\n",
      "69     \t [1.29456908 0.91053371]. \t  -2.9863539416802425 \t 1.028173506356821\n",
      "70     \t [ 0.30283035 -0.15330221]. \t  -0.21119936841362813 \t 1.028173506356821\n",
      "71     \t [-0.07680614  0.78252199]. \t  0.9860970770716072 \t 1.028173506356821\n",
      "72     \t [-0.66585064  0.9565221 ]. \t  -0.4414721104708425 \t 1.028173506356821\n",
      "73     \t [ 0.83605536 -0.03075759]. \t  -1.854268583443862 \t 1.028173506356821\n",
      "74     \t [-2.65418964 -1.04802879]. \t  -43.712444501721095 \t 1.028173506356821\n",
      "75     \t [-1.95802404 -0.93238991]. \t  -4.623918931824608 \t 1.028173506356821\n",
      "76     \t [ 2.65928074 -0.8611894 ]. \t  -38.095995525025415 \t 1.028173506356821\n",
      "77     \t [-1.77438788e+00 -4.20943140e-04]. \t  -2.181051972440066 \t 1.028173506356821\n",
      "78     \t [ 2.96206493 -1.69132174]. \t  -114.8531583137974 \t 1.028173506356821\n",
      "79     \t [-0.18429283 -1.93121574]. \t  -41.2104708174101 \t 1.028173506356821\n",
      "80     \t [0.50354726 1.40187561]. \t  -9.178460961291158 \t 1.028173506356821\n",
      "81     \t [ 1.8494766  -0.49234186]. \t  -0.8070627204068261 \t 1.028173506356821\n",
      "82     \t [-1.58589212  1.4667759 ]. \t  -9.66241801894752 \t 1.028173506356821\n",
      "83     \t [ 2.12917079 -0.60851793]. \t  -3.802860591425924 \t 1.028173506356821\n",
      "84     \t [-1.62576351  1.82523629]. \t  -30.15860884407847 \t 1.028173506356821\n",
      "85     \t [-0.46050736 -1.01357847]. \t  -1.3361208843171213 \t 1.028173506356821\n",
      "86     \t [ 0.27969873 -0.04764548]. \t  -0.2778466622892492 \t 1.028173506356821\n",
      "87     \t [0.38289648 0.48997109]. \t  -0.00021019484094653151 \t 1.028173506356821\n",
      "88     \t [-1.36318854  0.16967846]. \t  -1.9772451551515737 \t 1.028173506356821\n",
      "89     \t [ 0.45605372 -0.35690517]. \t  -0.1367085318367286 \t 1.028173506356821\n",
      "90     \t [ 0.19950363 -0.63976149]. \t  0.9388239357260448 \t 1.028173506356821\n",
      "91     \t [-0.18401169  0.70978786]. \t  0.9975050826924644 \t 1.028173506356821\n",
      "92     \t [-0.1187756   1.80241837]. \t  -29.06360017085305 \t 1.028173506356821\n",
      "93     \t [ 2.27106201 -1.31394768]. \t  -12.534642518238567 \t 1.028173506356821\n",
      "94     \t [0.96610622 1.40140075]. \t  -11.101235624535835 \t 1.028173506356821\n",
      "95     \t [-1.25641567  1.01054434]. \t  -1.2094612531411373 \t 1.028173506356821\n",
      "96     \t [-2.9526468  -0.65418685]. \t  -97.08852405515171 \t 1.028173506356821\n",
      "97     \t [-0.08834708  0.71535162]. \t  \u001b[92m1.031555998209891\u001b[0m \t 1.031555998209891\n",
      "98     \t [-0.05068548  1.93404227]. \t  -40.916101305328624 \t 1.031555998209891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.08204208 -1.10118477]. \t  -1.1484120974862886 \t 1.031555998209891\n",
      "100    \t [-0.15130269  0.71868847]. \t  1.0171752939520076 \t 1.031555998209891\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03367135 -0.0476864 ]. \t  -2.224696248130898 \t -2.224696248130898\n",
      "init   \t [ 1.95297104 -1.87421445]. \t  -34.846635035334884 \t -2.224696248130898\n",
      "init   \t [1.84829978 0.26246968]. \t  -2.674972725698239 \t -2.224696248130898\n",
      "init   \t [-1.21426501 -1.81321712]. \t  -34.68894768245752 \t -2.224696248130898\n",
      "init   \t [ 2.9437644  -1.97269707]. \t  -133.08463964627106 \t -2.224696248130898\n",
      "1      \t [ 0.99919526 -1.87802298]. \t  -36.00572890352069 \t -2.224696248130898\n",
      "2      \t [0.33113194 0.99935031]. \t  \u001b[92m-0.7395127602691062\u001b[0m \t -0.7395127602691062\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.7395127602691062\n",
      "4      \t [3. 2.]. \t  -162.89999999999998 \t -0.7395127602691062\n",
      "5      \t [ 1.67002537 -0.17243671]. \t  -1.649161781835133 \t -0.7395127602691062\n",
      "6      \t [1.12969368 0.43622179]. \t  -2.253883281745309 \t -0.7395127602691062\n",
      "7      \t [-0.16570513  0.54535842]. \t  \u001b[92m0.8179509003220468\u001b[0m \t 0.8179509003220468\n",
      "8      \t [-0.05751053  0.77012399]. \t  \u001b[92m0.9964196538279392\u001b[0m \t 0.9964196538279392\n",
      "9      \t [-0.03611517  2.        ]. \t  -47.932983303064326 \t 0.9964196538279392\n",
      "10     \t [0.30658819 0.5243986 ]. \t  0.279006630511489 \t 0.9964196538279392\n",
      "11     \t [-2.87236259 -1.07188338]. \t  -81.0213916588206 \t 0.9964196538279392\n",
      "12     \t [-0.87807407  0.26395771]. \t  -1.497412890073796 \t 0.9964196538279392\n",
      "13     \t [0.10935452 0.71330834]. \t  0.8741522283761534 \t 0.9964196538279392\n",
      "14     \t [-0.67861359  0.6859553 ]. \t  0.032763675448674534 \t 0.9964196538279392\n",
      "15     \t [-0.41191561 -0.0296586 ]. \t  -0.6285697718919135 \t 0.9964196538279392\n",
      "16     \t [-0.27314109  0.6927226 ]. \t  0.9007153596706121 \t 0.9964196538279392\n",
      "17     \t [ 0.26701021 -0.07354288]. \t  -0.23347061416041662 \t 0.9964196538279392\n",
      "18     \t [ 2.99894711 -0.09395282]. \t  -108.28539198623508 \t 0.9964196538279392\n",
      "19     \t [1.37934276 0.19139133]. \t  -2.4272139130134565 \t 0.9964196538279392\n",
      "20     \t [ 1.44139883 -0.79972248]. \t  -0.16035224860897612 \t 0.9964196538279392\n",
      "21     \t [1.42915131 0.91986021]. \t  -3.0434012089311997 \t 0.9964196538279392\n",
      "22     \t [1.81745983 1.88303539]. \t  -41.843742047563325 \t 0.9964196538279392\n",
      "23     \t [1.14520647 1.75818012]. \t  -30.25656645742504 \t 0.9964196538279392\n",
      "24     \t [ 1.32328683 -0.56641903]. \t  -0.7337670912495751 \t 0.9964196538279392\n",
      "25     \t [-0.42259568  0.49992645]. \t  0.31184867961930723 \t 0.9964196538279392\n",
      "26     \t [-2.76250716  0.89203855]. \t  -53.25869439108361 \t 0.9964196538279392\n",
      "27     \t [-1.69225979  0.3880026 ]. \t  -0.8932344876693016 \t 0.9964196538279392\n",
      "28     \t [-0.1165043  -1.20253177]. \t  -2.7742965638888464 \t 0.9964196538279392\n",
      "29     \t [-0.38870818 -0.88453742]. \t  -0.2204247578534827 \t 0.9964196538279392\n",
      "30     \t [ 1.61637563 -0.71805284]. \t  0.09894930589342121 \t 0.9964196538279392\n",
      "31     \t [-0.015529    0.69284194]. \t  \u001b[92m1.008199453457621\u001b[0m \t 1.008199453457621\n",
      "32     \t [-1.4688038   0.60984982]. \t  -0.3724067869300681 \t 1.008199453457621\n",
      "33     \t [-1.76825634  1.49344461]. \t  -10.501847986739971 \t 1.008199453457621\n",
      "34     \t [-1.27059303  1.26578692]. \t  -4.6381505811165695 \t 1.008199453457621\n",
      "35     \t [-0.41621929 -1.69107162]. \t  -22.60869382067305 \t 1.008199453457621\n",
      "36     \t [-0.08896973 -0.72659801]. \t  0.9007001180700751 \t 1.008199453457621\n",
      "37     \t [-0.49508589  0.40682219]. \t  -0.10532062250841523 \t 1.008199453457621\n",
      "38     \t [-0.53216428  1.2966519 ]. \t  -4.863848069779513 \t 1.008199453457621\n",
      "39     \t [-1.99202267  0.24380253]. \t  -2.924056629448654 \t 1.008199453457621\n",
      "40     \t [-2.29863479 -0.12409438]. \t  -11.902097916644355 \t 1.008199453457621\n",
      "41     \t [-0.87650631 -1.01097242]. \t  -2.9610558415542174 \t 1.008199453457621\n",
      "42     \t [ 0.47621191 -0.75926556]. \t  0.5351724058246855 \t 1.008199453457621\n",
      "43     \t [-1.18293077 -0.76795044]. \t  -2.339263296591527 \t 1.008199453457621\n",
      "44     \t [1.45206898 0.98275837]. \t  -3.5174771415413497 \t 1.008199453457621\n",
      "45     \t [-0.01607511  0.75685761]. \t  0.9899142336146882 \t 1.008199453457621\n",
      "46     \t [-2.75310137 -0.51940669]. \t  -55.46388912746703 \t 1.008199453457621\n",
      "47     \t [ 0.09026541 -0.90406529]. \t  0.6463500900963194 \t 1.008199453457621\n",
      "48     \t [0.93836213 1.902392  ]. \t  -41.821636818569516 \t 1.008199453457621\n",
      "49     \t [-2.52410618 -0.37430821]. \t  -26.909505225810012 \t 1.008199453457621\n",
      "50     \t [-1.8968713  -0.93606366]. \t  -4.074291143892804 \t 1.008199453457621\n",
      "51     \t [-1.70019451  0.97764946]. \t  -0.23540301265572364 \t 1.008199453457621\n",
      "52     \t [-1.84743823  0.7883419 ]. \t  -0.04485003447364899 \t 1.008199453457621\n",
      "53     \t [ 0.170916   -0.54824899]. \t  0.8195609564870565 \t 1.008199453457621\n",
      "54     \t [-1.51966336  0.75220658]. \t  -0.017440683202022744 \t 1.008199453457621\n",
      "55     \t [-2.56793601 -1.98618002]. \t  -82.21299883561676 \t 1.008199453457621\n",
      "56     \t [ 0.13846224 -0.72520902]. \t  \u001b[92m1.0218073476478498\u001b[0m \t 1.0218073476478498\n",
      "57     \t [-0.79440385  0.0028139 ]. \t  -1.7694768026248533 \t 1.0218073476478498\n",
      "58     \t [-2.19133823 -0.89059662]. \t  -8.988892584154872 \t 1.0218073476478498\n",
      "59     \t [0.95929284 0.93192789]. \t  -2.5994911534475658 \t 1.0218073476478498\n",
      "60     \t [ 0.19120558 -0.74859182]. \t  0.985099542116302 \t 1.0218073476478498\n",
      "61     \t [ 0.03264569 -0.7185973 ]. \t  1.0181250346717692 \t 1.0218073476478498\n",
      "62     \t [ 2.10329515 -1.05429921]. \t  -3.7348255282934106 \t 1.0218073476478498\n",
      "63     \t [0.68662848 1.92131871]. \t  -42.51503427646867 \t 1.0218073476478498\n",
      "64     \t [-0.13220191  0.75868485]. \t  1.0081668120498384 \t 1.0218073476478498\n",
      "65     \t [2.34770383 1.69343593]. \t  -39.46458088155146 \t 1.0218073476478498\n",
      "66     \t [0.00205563 1.33166075]. \t  -5.488133297389344 \t 1.0218073476478498\n",
      "67     \t [ 0.06466461 -0.74848175]. \t  1.0172027564334127 \t 1.0218073476478498\n",
      "68     \t [-0.22480738  0.75437177]. \t  0.9536691424243549 \t 1.0218073476478498\n",
      "69     \t [ 0.0409425  -0.65114443]. \t  0.9968495810748208 \t 1.0218073476478498\n",
      "70     \t [ 1.90146709 -1.22487579]. \t  -3.4385320379368345 \t 1.0218073476478498\n",
      "71     \t [-0.11351374  0.67742707]. \t  1.0189493657478614 \t 1.0218073476478498\n",
      "72     \t [ 0.0946507  -0.72133732]. \t  \u001b[92m1.0309555339238001\u001b[0m \t 1.0309555339238001\n",
      "73     \t [ 0.34797029 -0.59597515]. \t  0.6693611429450617 \t 1.0309555339238001\n",
      "74     \t [ 0.0865381  -0.72285594]. \t  1.0306881131547756 \t 1.0309555339238001\n",
      "75     \t [ 0.20284511 -0.17902922]. \t  -0.0006405939934639587 \t 1.0309555339238001\n",
      "76     \t [ 0.06661796 -0.74995015]. \t  1.0166620315084538 \t 1.0309555339238001\n",
      "77     \t [ 0.11980624 -0.66905245]. \t  1.012204213731107 \t 1.0309555339238001\n",
      "78     \t [-1.02033212 -1.19297079]. \t  -5.890629022588897 \t 1.0309555339238001\n",
      "79     \t [-0.68421345 -1.28284427]. \t  -6.574707115676258 \t 1.0309555339238001\n",
      "80     \t [ 0.0803055  -1.25773149]. \t  -3.606636904367955 \t 1.0309555339238001\n",
      "81     \t [1.17077375 0.20753904]. \t  -2.4738270937380227 \t 1.0309555339238001\n",
      "82     \t [-2.71459008  0.25166402]. \t  -47.90519265973831 \t 1.0309555339238001\n",
      "83     \t [-0.16283618  0.67009116]. \t  0.9941278881942943 \t 1.0309555339238001\n",
      "84     \t [-0.95532538 -0.85972632]. \t  -2.2048844752583614 \t 1.0309555339238001\n",
      "85     \t [-0.18280673  0.78603776]. \t  0.9567930499642578 \t 1.0309555339238001\n",
      "86     \t [2.61956597 0.75564002]. \t  -37.27138351629192 \t 1.0309555339238001\n",
      "87     \t [-0.34134508 -0.78996824]. \t  0.23071146764321804 \t 1.0309555339238001\n",
      "88     \t [-2.09907457  0.74081977]. \t  -2.8230845382388163 \t 1.0309555339238001\n",
      "89     \t [-1.36466746 -0.70704187]. \t  -2.28385117953788 \t 1.0309555339238001\n",
      "90     \t [1.67255505 1.45330367]. \t  -13.8792202407375 \t 1.0309555339238001\n",
      "91     \t [-1.61862826 -0.97299392]. \t  -3.4328156026929944 \t 1.0309555339238001\n",
      "92     \t [-1.60387543 -1.47769845]. \t  -14.775435162999196 \t 1.0309555339238001\n",
      "93     \t [ 1.30313978 -0.44213513]. \t  -1.1638794116202833 \t 1.0309555339238001\n",
      "94     \t [-1.83231722 -0.88869639]. \t  -3.337389277584356 \t 1.0309555339238001\n",
      "95     \t [-1.74961224 -1.02428229]. \t  -4.1262762804269535 \t 1.0309555339238001\n",
      "96     \t [ 0.0337198  -0.62460306]. \t  0.9682295408809853 \t 1.0309555339238001\n",
      "97     \t [-1.52109416 -0.93110702]. \t  -3.0965668760517855 \t 1.0309555339238001\n",
      "98     \t [-0.96007511  0.7899094 ]. \t  -0.4669214584322007 \t 1.0309555339238001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.66300677  0.88183712]. \t  -0.10450903626885111 \t 1.0309555339238001\n",
      "100    \t [-0.17807022  0.64705619]. \t  0.9640356102682575 \t 1.0309555339238001\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.92153751 -1.53997223]. \t  -13.677695110590259 \t -6.372423095293032\n",
      "init   \t [ 2.70169719 -0.07123439]. \t  -46.72852427361676 \t -6.372423095293032\n",
      "init   \t [ 2.23484721 -1.15066928]. \t  -8.26768569212749 \t -6.372423095293032\n",
      "init   \t [-2.75574225 -0.41122215]. \t  -55.82511776655432 \t -6.372423095293032\n",
      "init   \t [-1.60120682  1.3669629 ]. \t  -6.372423095293032 \t -6.372423095293032\n",
      "1      \t [ 2.58113859 -2.        ]. \t  -74.84682024424548 \t -6.372423095293032\n",
      "2      \t [ 1.57470162 -0.81229953]. \t  \u001b[92m0.08837188195077916\u001b[0m \t 0.08837188195077916\n",
      "3      \t [-0.16836986  2.        ]. \t  -47.774973884193805 \t 0.08837188195077916\n",
      "4      \t [-2.49962425  2.        ]. \t  -67.31814071799164 \t 0.08837188195077916\n",
      "5      \t [-1.21362495  0.74715066]. \t  -0.5077121230126085 \t 0.08837188195077916\n",
      "6      \t [-1.00460368 -1.88464791]. \t  -40.39026631870184 \t 0.08837188195077916\n",
      "7      \t [ 1.73224944 -0.97707968]. \t  -0.23468266608155064 \t 0.08837188195077916\n",
      "8      \t [ 0.52111376 -0.22732585]. \t  -0.623561182377702 \t 0.08837188195077916\n",
      "9      \t [-1.32471796  0.98994364]. \t  -0.9639570804829939 \t 0.08837188195077916\n",
      "10     \t [-0.17999855 -0.14544558]. \t  -0.07075718400671668 \t 0.08837188195077916\n",
      "11     \t [ 0.12099119 -0.37646287]. \t  \u001b[92m0.4739962657887828\u001b[0m \t 0.4739962657887828\n",
      "12     \t [ 0.7462506  -0.76713968]. \t  -0.09271599182304457 \t 0.4739962657887828\n",
      "13     \t [-0.54967934 -0.13782872]. \t  -1.0272868633296435 \t 0.4739962657887828\n",
      "14     \t [2.69481592 1.85940993]. \t  -84.95549658392102 \t 0.4739962657887828\n",
      "15     \t [ 0.59916777 -0.52908489]. \t  -0.05748911540703616 \t 0.4739962657887828\n",
      "16     \t [-1.58818898  0.5876488 ]. \t  -0.24034320668621223 \t 0.4739962657887828\n",
      "17     \t [-1.25160292  0.37715646]. \t  -1.4340171483967747 \t 0.4739962657887828\n",
      "18     \t [-2.78787591 -1.96479765]. \t  -110.38179581993715 \t 0.4739962657887828\n",
      "19     \t [ 1.24875125 -0.86771665]. \t  -0.5673217357331832 \t 0.4739962657887828\n",
      "20     \t [-1.39574473  0.78893703]. \t  -0.2459029307261329 \t 0.4739962657887828\n",
      "21     \t [ 0.11324526 -0.78232445]. \t  \u001b[92m0.9874369949259401\u001b[0m \t 0.9874369949259401\n",
      "22     \t [0.29550887 0.31781972]. \t  -0.06420251557533652 \t 0.9874369949259401\n",
      "23     \t [ 0.34080812 -0.75607904]. \t  0.8003476218015324 \t 0.9874369949259401\n",
      "24     \t [ 0.06131269 -0.7757057 ]. \t  \u001b[92m0.991165990753101\u001b[0m \t 0.991165990753101\n",
      "25     \t [0.08255045 0.15424011]. \t  0.05300270726969513 \t 0.991165990753101\n",
      "26     \t [2.13476842 0.6143083 ]. \t  -6.535692351817299 \t 0.991165990753101\n",
      "27     \t [1.52478423 0.73892288]. \t  -2.2727232877362256 \t 0.991165990753101\n",
      "28     \t [1.55488417 0.18797203]. \t  -2.2623773378815177 \t 0.991165990753101\n",
      "29     \t [2.67669027 0.30631829]. \t  -43.933735469628125 \t 0.991165990753101\n",
      "30     \t [ 0.06527374 -0.67772351]. \t  \u001b[92m1.0206100231097044\u001b[0m \t 1.0206100231097044\n",
      "31     \t [ 1.63428609 -0.88268599]. \t  0.07694139238262276 \t 1.0206100231097044\n",
      "32     \t [-2.86195139  0.61201958]. \t  -72.35739599088193 \t 1.0206100231097044\n",
      "33     \t [-0.2509088   0.54609945]. \t  0.730586897383692 \t 1.0206100231097044\n",
      "34     \t [-0.27040837 -0.96833813]. \t  -0.3094825676788334 \t 1.0206100231097044\n",
      "35     \t [0.01585826 0.63383449]. \t  0.9503274134701899 \t 1.0206100231097044\n",
      "36     \t [ 0.08296447 -0.70147903]. \t  \u001b[92m1.030513445938779\u001b[0m \t 1.030513445938779\n",
      "37     \t [-0.0364743 -0.6080386]. \t  0.9046034771237113 \t 1.030513445938779\n",
      "38     \t [-2.89034399  1.27204281]. \t  -81.52572547728954 \t 1.030513445938779\n",
      "39     \t [ 0.81638987 -1.42533591]. \t  -9.051161659801737 \t 1.030513445938779\n",
      "40     \t [ 0.12451982 -0.6586628 ]. \t  1.002989111153076 \t 1.030513445938779\n",
      "41     \t [-0.08702852  1.43540279]. \t  -8.644411983134919 \t 1.030513445938779\n",
      "42     \t [-0.19633567  0.82243514]. \t  0.8859167143682395 \t 1.030513445938779\n",
      "43     \t [ 1.60287997 -1.00834182]. \t  -0.5199392992897433 \t 1.030513445938779\n",
      "44     \t [-2.74504984  0.48123736]. \t  -51.488734250744066 \t 1.030513445938779\n",
      "45     \t [2.67638905 0.66092075]. \t  -44.19808676366892 \t 1.030513445938779\n",
      "46     \t [-0.11444584  0.84084159]. \t  0.8727787895521331 \t 1.030513445938779\n",
      "47     \t [ 1.66682285 -0.63849379]. \t  -0.02176355265571417 \t 1.030513445938779\n",
      "48     \t [0.1903842  0.98731711]. \t  -0.23193249536012223 \t 1.030513445938779\n",
      "49     \t [-0.67818938 -1.69170675]. \t  -23.88900013268641 \t 1.030513445938779\n",
      "50     \t [ 0.12723363 -0.70932283]. \t  1.0260056436343195 \t 1.030513445938779\n",
      "51     \t [ 1.10803087 -1.3704361 ]. \t  -7.4405067847072495 \t 1.030513445938779\n",
      "52     \t [-0.15632649  0.7504683 ]. \t  1.004837166059053 \t 1.030513445938779\n",
      "53     \t [ 0.00148233 -0.75727758]. \t  0.9795227801906831 \t 1.030513445938779\n",
      "54     \t [2.79581991 1.08658394]. \t  -66.04549951077904 \t 1.030513445938779\n",
      "55     \t [ 0.08032289 -0.62943855]. \t  0.9817349169977259 \t 1.030513445938779\n",
      "56     \t [ 0.06745902 -0.71344767]. \t  1.0296445155167424 \t 1.030513445938779\n",
      "57     \t [ 2.0281526  -0.41041358]. \t  -2.7284919541159884 \t 1.030513445938779\n",
      "58     \t [-1.27305634  0.16225623]. \t  -2.0767178133917406 \t 1.030513445938779\n",
      "59     \t [1.92506769 1.9352494 ]. \t  -47.79856746651636 \t 1.030513445938779\n",
      "60     \t [-2.07614895  0.12052891]. \t  -4.612118253698222 \t 1.030513445938779\n",
      "61     \t [ 0.02957982 -0.71012929]. \t  1.0174338440773691 \t 1.030513445938779\n",
      "62     \t [ 2.68823408 -0.07644032]. \t  -44.80791360734754 \t 1.030513445938779\n",
      "63     \t [-2.6634366  1.2686209]. \t  -42.23689288604318 \t 1.030513445938779\n",
      "64     \t [-1.96101917  1.01096365]. \t  -1.390926440574144 \t 1.030513445938779\n",
      "65     \t [ 1.86619423 -1.93773023]. \t  -40.298911362434026 \t 1.030513445938779\n",
      "66     \t [-2.06022129 -0.21581547]. \t  -4.901236965991056 \t 1.030513445938779\n",
      "67     \t [-1.42047153 -0.78074965]. \t  -2.4165963153759584 \t 1.030513445938779\n",
      "68     \t [-0.51218763 -1.91364132]. \t  -40.88454432653726 \t 1.030513445938779\n",
      "69     \t [ 0.03119401 -0.68746469]. \t  1.0145531523293214 \t 1.030513445938779\n",
      "70     \t [-2.05895995 -1.86768905]. \t  -43.17687580612669 \t 1.030513445938779\n",
      "71     \t [ 2.0583589  -0.83649929]. \t  -2.039986723574948 \t 1.030513445938779\n",
      "72     \t [ 0.45243668 -0.26156537]. \t  -0.3603770730775245 \t 1.030513445938779\n",
      "73     \t [-0.14602888  0.7081339 ]. \t  1.0190535192624401 \t 1.030513445938779\n",
      "74     \t [2.98269366 0.65422908]. \t  -105.05867184138486 \t 1.030513445938779\n",
      "75     \t [-2.54004172  1.68853225]. \t  -44.73619958935012 \t 1.030513445938779\n",
      "76     \t [0.78801118 1.19049578]. \t  -5.057649231580321 \t 1.030513445938779\n",
      "77     \t [-2.88210765  1.73017947]. \t  -98.25984341909299 \t 1.030513445938779\n",
      "78     \t [ 0.05798271 -0.77096229]. \t  0.9956457401294769 \t 1.030513445938779\n",
      "79     \t [ 0.16231511 -0.76012353]. \t  0.9952425598083661 \t 1.030513445938779\n",
      "80     \t [-2.80964243  0.40516224]. \t  -63.001967894053784 \t 1.030513445938779\n",
      "81     \t [ 2.96091139 -1.62288339]. \t  -110.67896439468223 \t 1.030513445938779\n",
      "82     \t [ 0.10763693 -0.65051043]. \t  1.0003422793997896 \t 1.030513445938779\n",
      "83     \t [-1.71323206  1.37418432]. \t  -6.433885877833734 \t 1.030513445938779\n",
      "84     \t [-2.95256081 -0.16017044]. \t  -96.48707819542581 \t 1.030513445938779\n",
      "85     \t [-2.18775637 -1.12846761]. \t  -11.447744437308861 \t 1.030513445938779\n",
      "86     \t [ 0.32999429 -1.66886543]. \t  -19.747340796148112 \t 1.030513445938779\n",
      "87     \t [-1.51538819  0.06635407]. \t  -2.029914658998604 \t 1.030513445938779\n",
      "88     \t [ 2.13563403 -0.24182188]. \t  -5.448300544973153 \t 1.030513445938779\n",
      "89     \t [-2.45383138 -0.18643227]. \t  -21.04011729421002 \t 1.030513445938779\n",
      "90     \t [-0.01958063  0.59731677]. \t  0.9281230613370918 \t 1.030513445938779\n",
      "91     \t [ 0.6765134  -0.71638581]. \t  0.06118090557916078 \t 1.030513445938779\n",
      "92     \t [1.15290845 0.05935896]. \t  -2.4437662926618007 \t 1.030513445938779\n",
      "93     \t [-0.01181787  0.6729364 ]. \t  0.9984990878226371 \t 1.030513445938779\n",
      "94     \t [ 0.11154192 -0.70472584]. \t  1.0291192993207856 \t 1.030513445938779\n",
      "95     \t [-0.11155897  0.63997966]. \t  0.989231007265727 \t 1.030513445938779\n",
      "96     \t [-1.18736519  1.04948797]. \t  -1.6001087753452512 \t 1.030513445938779\n",
      "97     \t [-0.08400921  0.71516168]. \t  \u001b[92m1.0314294790796208\u001b[0m \t 1.0314294790796208\n",
      "98     \t [-0.95675261 -0.3437366 ]. \t  -2.069648106712903 \t 1.0314294790796208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.88993136 -1.21569296]. \t  -80.421519592609 \t 1.0314294790796208\n",
      "100    \t [2.95282291 0.70006488]. \t  -97.24887489921754 \t 1.0314294790796208\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.88621398 1.2849914 ]. \t  -88.29186392936245 \t -1.3768038348803564\n",
      "init   \t [ 0.87536168 -0.30524795]. \t  -1.3768038348803564 \t -1.3768038348803564\n",
      "init   \t [-1.78613413 -0.03237292]. \t  -2.2646252293989613 \t -1.3768038348803564\n",
      "init   \t [-2.16865001 -0.18991586]. \t  -7.310805476408371 \t -1.3768038348803564\n",
      "init   \t [-2.31759223 -1.98830205]. \t  -63.86369356519016 \t -1.3768038348803564\n",
      "1      \t [-1.49201268  0.70463596]. \t  \u001b[92m-0.12368954045433\u001b[0m \t -0.12368954045433\n",
      "2      \t [-0.45308407 -0.03216398]. \t  -0.7459653765230012 \t -0.12368954045433\n",
      "3      \t [ 1.05436088 -1.42426222]. \t  -9.153262547928108 \t -0.12368954045433\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t -0.12368954045433\n",
      "5      \t [-0.76594485  0.81298218]. \t  -0.17211557732865268 \t -0.12368954045433\n",
      "6      \t [-1.25679659  0.44503189]. \t  -1.1977675868724482 \t -0.12368954045433\n",
      "7      \t [-0.36294215  1.68137575]. \t  -20.54115923248786 \t -0.12368954045433\n",
      "8      \t [0.21833137 0.1829002 ]. \t  \u001b[92m-0.09653783089941748\u001b[0m \t -0.09653783089941748\n",
      "9      \t [-0.01303573 -0.64033091]. \t  \u001b[92m0.9585902006942616\u001b[0m \t 0.9585902006942616\n",
      "10     \t [ 0.0693997  -0.27663533]. \t  0.2826646400132813 \t 0.9585902006942616\n",
      "11     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9585902006942616\n",
      "12     \t [-0.20856665 -1.03699856]. \t  -0.7105213907569612 \t 0.9585902006942616\n",
      "13     \t [ 0.39604382 -0.88814247]. \t  0.441103776268105 \t 0.9585902006942616\n",
      "14     \t [-0.18327676 -0.6282388 ]. \t  0.7084879170701088 \t 0.9585902006942616\n",
      "15     \t [ 0.50430093 -1.92791392]. \t  -40.307236540458206 \t 0.9585902006942616\n",
      "16     \t [ 1.15195345 -0.81468984]. \t  -0.5576976618042925 \t 0.9585902006942616\n",
      "17     \t [ 0.77654391 -0.7787171 ]. \t  -0.1621200383700261 \t 0.9585902006942616\n",
      "18     \t [-0.20576693  0.34482121]. \t  0.324388263207919 \t 0.9585902006942616\n",
      "19     \t [ 1.69940457 -0.25802557]. \t  -1.37894768894172 \t 0.9585902006942616\n",
      "20     \t [ 1.48500689 -0.07060689]. \t  -2.058544326881689 \t 0.9585902006942616\n",
      "21     \t [-1.1869001   1.36586043]. \t  -7.237349357192135 \t 0.9585902006942616\n",
      "22     \t [ 0.03554955 -0.75298165]. \t  \u001b[92m1.003770414333272\u001b[0m \t 1.003770414333272\n",
      "23     \t [-1.76950043  0.25577592]. \t  -1.4715192151055598 \t 1.003770414333272\n",
      "24     \t [-0.95521572 -1.06706791]. \t  -3.8053183751625403 \t 1.003770414333272\n",
      "25     \t [-1.78686797  0.34020014]. \t  -1.1957818858157583 \t 1.003770414333272\n",
      "26     \t [ 1.89310866 -0.95189241]. \t  -0.5643228107184426 \t 1.003770414333272\n",
      "27     \t [ 1.84666589 -0.70195988]. \t  -0.14241716686047412 \t 1.003770414333272\n",
      "28     \t [-0.70171499 -0.46429048]. \t  -1.1496538318568794 \t 1.003770414333272\n",
      "29     \t [ 0.29235642 -0.69172901]. \t  0.8736248586690547 \t 1.003770414333272\n",
      "30     \t [ 2.47715485 -0.32312617]. \t  -21.31600615236496 \t 1.003770414333272\n",
      "31     \t [-1.60814438 -0.39630624]. \t  -2.172734221647407 \t 1.003770414333272\n",
      "32     \t [-2.62528448  0.49761074]. \t  -34.89236242277745 \t 1.003770414333272\n",
      "33     \t [-2.84471413 -1.38775983]. \t  -82.57609248232174 \t 1.003770414333272\n",
      "34     \t [-2.81598185  1.04472462]. \t  -63.33651911687615 \t 1.003770414333272\n",
      "35     \t [0.11174621 0.5023075 ]. \t  0.6488512378946321 \t 1.003770414333272\n",
      "36     \t [-0.03319448  0.79596349]. \t  0.9506658273058585 \t 1.003770414333272\n",
      "37     \t [-0.15751271  0.85234849]. \t  0.8310969981086489 \t 1.003770414333272\n",
      "38     \t [-0.40375941  1.89230518]. \t  -36.79946486551535 \t 1.003770414333272\n",
      "39     \t [ 0.06166358 -0.76529036]. \t  1.0026543597050042 \t 1.003770414333272\n",
      "40     \t [-2.90117951 -1.46434979]. \t  -97.71886353155439 \t 1.003770414333272\n",
      "41     \t [ 0.07044918 -0.68206161]. \t  \u001b[92m1.0234080985991807\u001b[0m \t 1.0234080985991807\n",
      "42     \t [ 0.0936433  -0.66308675]. \t  1.0126265556046126 \t 1.0234080985991807\n",
      "43     \t [1.19588378 1.70614667]. \t  -26.691262660646913 \t 1.0234080985991807\n",
      "44     \t [1.37579412 0.30093761]. \t  -2.3925647295717716 \t 1.0234080985991807\n",
      "45     \t [0.79691694 1.092594  ]. \t  -3.5746374065231636 \t 1.0234080985991807\n",
      "46     \t [-0.01017729  0.09984365]. \t  0.04007936285370981 \t 1.0234080985991807\n",
      "47     \t [2.82544107 0.32685592]. \t  -68.22940741215106 \t 1.0234080985991807\n",
      "48     \t [-0.84808781 -1.15124713]. \t  -4.615959633772685 \t 1.0234080985991807\n",
      "49     \t [-0.83222183  0.59999437]. \t  -0.452854965556835 \t 1.0234080985991807\n",
      "50     \t [-1.01297559  1.22118176]. \t  -3.9470400971033053 \t 1.0234080985991807\n",
      "51     \t [ 1.57295258 -0.96784742]. \t  -0.33059879533611636 \t 1.0234080985991807\n",
      "52     \t [-2.75526272 -1.67922377]. \t  -80.32772808622109 \t 1.0234080985991807\n",
      "53     \t [-2.00942936 -1.00411499]. \t  -5.908005156330962 \t 1.0234080985991807\n",
      "54     \t [ 1.42606026 -1.23164889]. \t  -3.633523845217296 \t 1.0234080985991807\n",
      "55     \t [ 1.65985302 -1.65079666]. \t  -18.115849921007353 \t 1.0234080985991807\n",
      "56     \t [-0.06900149 -1.68501767]. \t  -21.024274713613476 \t 1.0234080985991807\n",
      "57     \t [-0.26597277  0.42024479]. \t  0.4208632346927586 \t 1.0234080985991807\n",
      "58     \t [ 0.70304787 -1.37012648]. \t  -7.128276134389456 \t 1.0234080985991807\n",
      "59     \t [2.51110724 1.44589507]. \t  -38.048639387264956 \t 1.0234080985991807\n",
      "60     \t [1.81954223 1.09986889]. \t  -5.337214643874206 \t 1.0234080985991807\n",
      "61     \t [-2.36156302 -0.91920798]. \t  -16.45867051626573 \t 1.0234080985991807\n",
      "62     \t [ 0.07810499 -0.66550892]. \t  1.0146154162804344 \t 1.0234080985991807\n",
      "63     \t [ 1.92745182 -1.09265228]. \t  -1.7879926467221452 \t 1.0234080985991807\n",
      "64     \t [1.28382217 1.38446992]. \t  -11.186711424391039 \t 1.0234080985991807\n",
      "65     \t [-0.05328788  1.35059511]. \t  -5.952410054366719 \t 1.0234080985991807\n",
      "66     \t [0.59229679 0.50726533]. \t  -0.695233212282974 \t 1.0234080985991807\n",
      "67     \t [2.28820001 1.96924646]. \t  -60.36695879294162 \t 1.0234080985991807\n",
      "68     \t [-1.7936637  -1.32138328]. \t  -9.813495848696078 \t 1.0234080985991807\n",
      "69     \t [-0.07057971  0.18675095]. \t  0.127945270687914 \t 1.0234080985991807\n",
      "70     \t [ 0.0127297  -0.70754001]. \t  1.008357144187566 \t 1.0234080985991807\n",
      "71     \t [-0.13984841  0.71297915]. \t  1.0220012692033893 \t 1.0234080985991807\n",
      "72     \t [-1.03620367  1.01245747]. \t  -1.3401459475904591 \t 1.0234080985991807\n",
      "73     \t [ 0.69834889 -1.19663496]. \t  -3.12830403023527 \t 1.0234080985991807\n",
      "74     \t [1.39055452 0.0702551 ]. \t  -2.3707200523887493 \t 1.0234080985991807\n",
      "75     \t [-1.1834965  -1.41171051]. \t  -11.984792929440168 \t 1.0234080985991807\n",
      "76     \t [-1.91163791  0.1638563 ]. \t  -2.4227614503194763 \t 1.0234080985991807\n",
      "77     \t [ 2.90355641 -0.5537879 ]. \t  -81.74299183075733 \t 1.0234080985991807\n",
      "78     \t [ 0.09251057 -0.76871682]. \t  1.0039653887246158 \t 1.0234080985991807\n",
      "79     \t [-0.2094308  -1.47319762]. \t  -10.639724372690395 \t 1.0234080985991807\n",
      "80     \t [2.29297314 0.912743  ]. \t  -12.963521977474125 \t 1.0234080985991807\n",
      "81     \t [ 0.41832801 -0.85291199]. \t  0.5123784196557906 \t 1.0234080985991807\n",
      "82     \t [0.82349402 1.38105833]. \t  -9.910297015095287 \t 1.0234080985991807\n",
      "83     \t [ 0.18464951 -0.70243342]. \t  0.995576690142621 \t 1.0234080985991807\n",
      "84     \t [ 2.49697937 -1.06633592]. \t  -22.056959876333448 \t 1.0234080985991807\n",
      "85     \t [-1.9039389   1.29214545]. \t  -4.794974384183428 \t 1.0234080985991807\n",
      "86     \t [-2.394412    0.04328208]. \t  -16.611638546871088 \t 1.0234080985991807\n",
      "87     \t [-2.9442584   0.83301228]. \t  -90.7038455136966 \t 1.0234080985991807\n",
      "88     \t [-1.95744191  0.87594868]. \t  -0.8178366777045933 \t 1.0234080985991807\n",
      "89     \t [-2.61651762  1.39054953]. \t  -39.500320653278514 \t 1.0234080985991807\n",
      "90     \t [-0.62303647  1.72432567]. \t  -23.650281643898392 \t 1.0234080985991807\n",
      "91     \t [-0.34059905 -0.8592493 ]. \t  0.043885211548270964 \t 1.0234080985991807\n",
      "92     \t [-0.15046339  0.70093556]. \t  1.0156786499109973 \t 1.0234080985991807\n",
      "93     \t [ 0.92488281 -1.6168044 ]. \t  -17.47523928938321 \t 1.0234080985991807\n",
      "94     \t [ 0.13896094 -0.772967  ]. \t  0.9929444737170331 \t 1.0234080985991807\n",
      "95     \t [1.73158128 0.4847771 ]. \t  -2.219664094025473 \t 1.0234080985991807\n",
      "96     \t [0.16438231 0.17275979]. \t  -0.019137404813361625 \t 1.0234080985991807\n",
      "97     \t [-0.02086051  0.65419053]. \t  0.9911504769009783 \t 1.0234080985991807\n",
      "98     \t [-0.064117    1.04860763]. \t  -0.38714987860657246 \t 1.0234080985991807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-1.57141442  1.27918624]. \t  -4.246101476539559 \t 1.0234080985991807\n",
      "100    \t [ 2.92572179 -0.67351473]. \t  -86.47125216976039 \t 1.0234080985991807\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.8508833   0.48843508]. \t  -0.8285352707628978 \t 0.04378866326980524\n",
      "init   \t [-0.37363357  1.14143433]. \t  -1.670326523786612 \t 0.04378866326980524\n",
      "init   \t [ 1.67985485 -0.90962958]. \t  0.04378866326980524 \t 0.04378866326980524\n",
      "init   \t [-1.34121447  1.20748871]. \t  -3.3921424222614553 \t 0.04378866326980524\n",
      "init   \t [2.74883612 1.50373054]. \t  -69.67081810821485 \t 0.04378866326980524\n",
      "1      \t [ 1.36865588 -1.53139099]. \t  -12.837582610424613 \t 0.04378866326980524\n",
      "2      \t [ 2.01907328 -0.67017216]. \t  -1.6471745213009104 \t 0.04378866326980524\n",
      "3      \t [ 0.95632687 -0.32053923]. \t  -1.4814491220278343 \t 0.04378866326980524\n",
      "4      \t [-2.52185414 -0.42846579]. \t  -26.72553497150102 \t 0.04378866326980524\n",
      "5      \t [-1.03910342  0.3660143 ]. \t  -1.4458929377440608 \t 0.04378866326980524\n",
      "6      \t [-2.434201    1.15945148]. \t  -18.34577343196723 \t 0.04378866326980524\n",
      "7      \t [-1.47562192  0.5276854 ]. \t  -0.6120664191308257 \t 0.04378866326980524\n",
      "8      \t [-0.65079799  2.        ]. \t  -48.0411741284007 \t 0.04378866326980524\n",
      "9      \t [-0.03651927  0.5134332 ]. \t  \u001b[92m0.7899052791479299\u001b[0m \t 0.7899052791479299\n",
      "10     \t [-0.55874873  0.75255645]. \t  0.3486262521261403 \t 0.7899052791479299\n",
      "11     \t [-0.68182621 -0.81871942]. \t  -1.1134213200050573 \t 0.7899052791479299\n",
      "12     \t [0.22307286 0.93451028]. \t  0.04020922368880081 \t 0.7899052791479299\n",
      "13     \t [ 0.03221771 -0.38128678]. \t  0.5051120347511271 \t 0.7899052791479299\n",
      "14     \t [-0.51766729 -0.35534593]. \t  -0.6701704794358964 \t 0.7899052791479299\n",
      "15     \t [ 0.47904722 -0.55358755]. \t  0.30398313795682164 \t 0.7899052791479299\n",
      "16     \t [-0.83132317 -2.        ]. \t  -51.53407072745749 \t 0.7899052791479299\n",
      "17     \t [ 2.31542293 -1.25072605]. \t  -13.085265399650483 \t 0.7899052791479299\n",
      "18     \t [-1.44252754 -0.13020417]. \t  -2.354994764354096 \t 0.7899052791479299\n",
      "19     \t [-0.06720929  0.79749381]. \t  \u001b[92m0.9615934031884311\u001b[0m \t 0.9615934031884311\n",
      "20     \t [0.14427793 0.58986659]. \t  0.74005190942764 \t 0.9615934031884311\n",
      "21     \t [0.29842325 0.08959069]. \t  -0.3346936586703786 \t 0.9615934031884311\n",
      "22     \t [-0.00189491 -0.74426156]. \t  \u001b[92m0.9869435940775936\u001b[0m \t 0.9869435940775936\n",
      "23     \t [ 1.51258176 -0.69507829]. \t  -0.10094614286422676 \t 0.9869435940775936\n",
      "24     \t [ 2.99550529 -0.45718293]. \t  -105.60224688104036 \t 0.9869435940775936\n",
      "25     \t [ 1.69287965 -0.46364454]. \t  -0.6018128861330503 \t 0.9869435940775936\n",
      "26     \t [ 1.80909271 -0.76912914]. \t  0.07501228331487741 \t 0.9869435940775936\n",
      "27     \t [1.29985675 0.52093886]. \t  -2.257435621107326 \t 0.9869435940775936\n",
      "28     \t [ 0.58458935 -0.83163814]. \t  0.2042657853247679 \t 0.9869435940775936\n",
      "29     \t [-0.18447187 -0.69256024]. \t  0.7368831911707445 \t 0.9869435940775936\n",
      "30     \t [-1.65284816 -0.97238512]. \t  -3.4522292218358306 \t 0.9869435940775936\n",
      "31     \t [-2.70880324 -1.70215847]. \t  -74.57223000826514 \t 0.9869435940775936\n",
      "32     \t [1.87635432 1.24449673]. \t  -8.334172120069411 \t 0.9869435940775936\n",
      "33     \t [1.58938744 1.84458695]. \t  -37.70688640722808 \t 0.9869435940775936\n",
      "34     \t [1.97257784 0.44109598]. \t  -3.650109450636586 \t 0.9869435940775936\n",
      "35     \t [-1.837763   -0.88272457]. \t  -3.3310873710124618 \t 0.9869435940775936\n",
      "36     \t [-2.24322914 -1.36402883]. \t  -18.890759911801425 \t 0.9869435940775936\n",
      "37     \t [0.02073162 0.66950814]. \t  0.9736852872082312 \t 0.9869435940775936\n",
      "38     \t [-0.0651702  -1.20238265]. \t  -2.672886050392152 \t 0.9869435940775936\n",
      "39     \t [1.72610936 0.72147309]. \t  -2.3391847567846917 \t 0.9869435940775936\n",
      "40     \t [-2.45673118 -0.14933551]. \t  -21.21041555327551 \t 0.9869435940775936\n",
      "41     \t [-0.09990478  0.66854638]. \t  \u001b[92m1.015820583675183\u001b[0m \t 1.015820583675183\n",
      "42     \t [-0.00416547  0.67687597]. \t  0.9957481200262653 \t 1.015820583675183\n",
      "43     \t [ 0.25235405 -1.06776635]. \t  -0.6158846625426558 \t 1.015820583675183\n",
      "44     \t [ 2.88701349 -1.34849216]. \t  -82.51936413894451 \t 1.015820583675183\n",
      "45     \t [-0.107537    0.65360158]. \t  1.0031075900127144 \t 1.015820583675183\n",
      "46     \t [ 0.04831812 -0.06504942]. \t  0.010670036435382228 \t 1.015820583675183\n",
      "47     \t [ 1.09435985 -1.59371871]. \t  -16.252352467078627 \t 1.015820583675183\n",
      "48     \t [-2.32926869 -1.14458586]. \t  -17.4122586560799 \t 1.015820583675183\n",
      "49     \t [2.97237129 0.86403187]. \t  -103.10932787910203 \t 1.015820583675183\n",
      "50     \t [2.11670865 1.93998777]. \t  -51.45584114792257 \t 1.015820583675183\n",
      "51     \t [ 0.30291667 -1.86842167]. \t  -34.5679467648692 \t 1.015820583675183\n",
      "52     \t [-0.01638743  0.7556086 ]. \t  0.9911760358422166 \t 1.015820583675183\n",
      "53     \t [ 0.31161461 -0.70407949]. \t  0.850409749511773 \t 1.015820583675183\n",
      "54     \t [-2.88617111 -0.57472514]. \t  -81.04646365654582 \t 1.015820583675183\n",
      "55     \t [-2.7191748   1.89455628]. \t  -81.53487914351996 \t 1.015820583675183\n",
      "56     \t [-1.63261223  0.79611001]. \t  0.17370621835981903 \t 1.015820583675183\n",
      "57     \t [-0.12549952  0.73024472]. \t  \u001b[92m1.0247402765264615\u001b[0m \t 1.0247402765264615\n",
      "58     \t [ 0.02521145 -0.65355836]. \t  0.9927003561827339 \t 1.0247402765264615\n",
      "59     \t [ 0.11700194 -0.66453601]. \t  1.0097484210932042 \t 1.0247402765264615\n",
      "60     \t [-0.10939444  0.78034791]. \t  0.9903231300311188 \t 1.0247402765264615\n",
      "61     \t [-0.2230927   0.77553679]. \t  0.9379216260841357 \t 1.0247402765264615\n",
      "62     \t [1.00250607 1.67031535]. \t  -23.887367103614753 \t 1.0247402765264615\n",
      "63     \t [ 2.03276638 -0.94796054]. \t  -1.898761720426021 \t 1.0247402765264615\n",
      "64     \t [ 2.05737341 -1.31175784]. \t  -6.847153012497693 \t 1.0247402765264615\n",
      "65     \t [ 2.05294824 -0.04145615]. \t  -4.418864376096343 \t 1.0247402765264615\n",
      "66     \t [ 0.91101854 -1.01279506]. \t  -1.2468445427077464 \t 1.0247402765264615\n",
      "67     \t [-1.45950325  1.38951564]. \t  -7.373869993156253 \t 1.0247402765264615\n",
      "68     \t [-1.14379772 -0.9990079 ]. \t  -3.5199274814126724 \t 1.0247402765264615\n",
      "69     \t [-0.13867673  0.79162449]. \t  0.9694487707594929 \t 1.0247402765264615\n",
      "70     \t [-0.11312156  0.66204084]. \t  1.0088198059878934 \t 1.0247402765264615\n",
      "71     \t [ 0.08695291 -0.74229564]. \t  1.0240162851663395 \t 1.0247402765264615\n",
      "72     \t [1.86328927 0.21380227]. \t  -2.7480195517584076 \t 1.0247402765264615\n",
      "73     \t [-1.08174226  1.74116814]. \t  -25.093102583029605 \t 1.0247402765264615\n",
      "74     \t [ 0.28125908 -1.77199494]. \t  -26.682743625884864 \t 1.0247402765264615\n",
      "75     \t [1.02921709 1.38264057]. \t  -10.671534166266749 \t 1.0247402765264615\n",
      "76     \t [ 0.11694851 -0.7630651 ]. \t  1.0078511553572105 \t 1.0247402765264615\n",
      "77     \t [ 0.60381498 -1.20212563]. \t  -3.0424168445531055 \t 1.0247402765264615\n",
      "78     \t [ 1.38690398 -1.03172382]. \t  -1.1400628159400346 \t 1.0247402765264615\n",
      "79     \t [0.47823896 1.91941125]. \t  -41.28190383168363 \t 1.0247402765264615\n",
      "80     \t [1.87919569 0.75327489]. \t  -3.050423098347907 \t 1.0247402765264615\n",
      "81     \t [-0.08535765 -1.48175638]. \t  -10.655775753871147 \t 1.0247402765264615\n",
      "82     \t [-0.03599048  0.4888465 ]. \t  0.739871329880445 \t 1.0247402765264615\n",
      "83     \t [ 0.16167637 -0.63669909]. \t  0.9640036600467126 \t 1.0247402765264615\n",
      "84     \t [-0.21630035  0.73482275]. \t  0.9699730006286056 \t 1.0247402765264615\n",
      "85     \t [-0.55261944  1.11049629]. \t  -1.571855403543893 \t 1.0247402765264615\n",
      "86     \t [-0.00835113 -0.68797991]. \t  0.9911275677308704 \t 1.0247402765264615\n",
      "87     \t [-0.11505374  0.77714354]. \t  0.9936068771787001 \t 1.0247402765264615\n",
      "88     \t [-2.86285431 -0.71178107]. \t  -76.27373555122846 \t 1.0247402765264615\n",
      "89     \t [ 1.80031842 -1.36858942]. \t  -6.330494902776361 \t 1.0247402765264615\n",
      "90     \t [-0.15464849  0.66019155]. \t  0.9911701974689976 \t 1.0247402765264615\n",
      "91     \t [-0.95302087 -0.50270019]. \t  -1.8741129246672243 \t 1.0247402765264615\n",
      "92     \t [ 1.80445316 -0.66892002]. \t  -0.07103172905641963 \t 1.0247402765264615\n",
      "93     \t [-0.85269625 -1.5213297 ]. \t  -15.392364583001486 \t 1.0247402765264615\n",
      "94     \t [-2.84071651  0.48809332]. \t  -68.57962407536536 \t 1.0247402765264615\n",
      "95     \t [-2.05457297  1.97172799]. \t  -45.39332497851873 \t 1.0247402765264615\n",
      "96     \t [ 0.3143381  -1.58899008]. \t  -15.27628234675918 \t 1.0247402765264615\n",
      "97     \t [1.47101432 1.85895123]. \t  -38.87908100913851 \t 1.0247402765264615\n",
      "98     \t [-1.84385328  1.19822411]. \t  -2.7181663616412473 \t 1.0247402765264615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-1.47616923 -1.88652854]. \t  -41.40825666775498 \t 1.0247402765264615\n",
      "100    \t [-0.6322706  -0.54470962]. \t  -0.7944667087506341 \t 1.0247402765264615\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.96386586 -0.34559739]. \t  -1.4171064314549415 \t -1.4171064314549415\n",
      "init   \t [ 1.00861535 -1.08022631]. \t  -1.9362798876488938 \t -1.4171064314549415\n",
      "init   \t [1.84607002 0.54727386]. \t  -2.606716488173805 \t -1.4171064314549415\n",
      "init   \t [-1.96683689 -0.95845653]. \t  -4.9306493500188235 \t -1.4171064314549415\n",
      "init   \t [ 2.49191994 -0.14873796]. \t  -23.22037861975103 \t -1.4171064314549415\n",
      "1      \t [1.31005933 1.26173611]. \t  -7.787111565332043 \t -1.4171064314549415\n",
      "2      \t [ 0.43242082 -2.        ]. \t  -47.81186343553399 \t -1.4171064314549415\n",
      "3      \t [ 1.33270836 -0.88429756]. \t  \u001b[92m-0.4870135931577916\u001b[0m \t -0.4870135931577916\n",
      "4      \t [2.38805277 1.31001373]. \t  -24.381446893352688 \t -0.4870135931577916\n",
      "5      \t [-3.  2.]. \t  -150.89999999999998 \t -0.4870135931577916\n",
      "6      \t [-3. -2.]. \t  -162.89999999999998 \t -0.4870135931577916\n",
      "7      \t [-1.46667321 -0.44283794]. \t  -2.223987287878843 \t -0.4870135931577916\n",
      "8      \t [-1.73961311 -0.6865412 ]. \t  -2.3087298048291416 \t -0.4870135931577916\n",
      "9      \t [-0.58506742  0.21552186]. \t  -0.833260699196913 \t -0.4870135931577916\n",
      "10     \t [-1.37186716 -1.15223027]. \t  -5.632542500554207 \t -0.4870135931577916\n",
      "11     \t [-0.92432765  0.04450565]. \t  -2.0434390386925823 \t -0.4870135931577916\n",
      "12     \t [-0.46780613 -0.19935973]. \t  -0.7188934949334121 \t -0.4870135931577916\n",
      "13     \t [0.29466439 0.80799102]. \t  \u001b[92m0.3367671138883971\u001b[0m \t 0.3367671138883971\n",
      "14     \t [1.31492263 0.50902516]. \t  -2.2625297271412563 \t 0.3367671138883971\n",
      "15     \t [0.0349059  1.21093654]. \t  -2.782602526326895 \t 0.3367671138883971\n",
      "16     \t [ 2.39459191 -1.89610523]. \t  -49.51480204750642 \t 0.3367671138883971\n",
      "17     \t [0.0285412  0.52359908]. \t  \u001b[92m0.7777767827507556\u001b[0m \t 0.7777767827507556\n",
      "18     \t [0.11811478 0.76598634]. \t  \u001b[92m0.824037210221053\u001b[0m \t 0.824037210221053\n",
      "19     \t [-0.95839959 -0.34582277]. \t  -2.070948914728223 \t 0.824037210221053\n",
      "20     \t [ 1.08321173 -0.74600562]. \t  -0.5453914232582714 \t 0.824037210221053\n",
      "21     \t [ 1.81053674 -0.06463817]. \t  -2.15428724522402 \t 0.824037210221053\n",
      "22     \t [-0.24421332  0.47108851]. \t  0.5745802046747878 \t 0.824037210221053\n",
      "23     \t [0.7930704 1.7306475]. \t  -27.043472590755005 \t 0.824037210221053\n",
      "24     \t [ 0.08299261 -0.11120322]. \t  0.030630408920149933 \t 0.824037210221053\n",
      "25     \t [-0.48325168  1.11502071]. \t  -1.494818425982562 \t 0.824037210221053\n",
      "26     \t [ 2.55651886 -1.9862726 ]. \t  -70.90235447738172 \t 0.824037210221053\n",
      "27     \t [ 1.55639598 -1.80871066]. \t  -29.013317327871505 \t 0.824037210221053\n",
      "28     \t [ 1.90344864 -1.08303389]. \t  -1.529354674870678 \t 0.824037210221053\n",
      "29     \t [ 0.35259878 -0.81303233]. \t  0.7174778129130254 \t 0.824037210221053\n",
      "30     \t [-0.06124098  0.83519581]. \t  \u001b[92m0.8800686721216469\u001b[0m \t 0.8800686721216469\n",
      "31     \t [ 0.86720838 -1.58874497]. \t  -15.972587040590096 \t 0.8800686721216469\n",
      "32     \t [-2.39903864  0.67523273]. \t  -14.396055314849475 \t 0.8800686721216469\n",
      "33     \t [-2.79255387 -0.2255001 ]. \t  -62.00397383228671 \t 0.8800686721216469\n",
      "34     \t [-1.46057876  0.91622901]. \t  -0.33511334669296833 \t 0.8800686721216469\n",
      "35     \t [-1.6440341  -1.07331517]. \t  -4.516873609960143 \t 0.8800686721216469\n",
      "36     \t [-1.75530258  0.6386886 ]. \t  -0.051351041266828656 \t 0.8800686721216469\n",
      "37     \t [ 0.08557911 -0.98719519]. \t  0.15449333551347597 \t 0.8800686721216469\n",
      "38     \t [-1.71838241  0.86045747]. \t  0.16434913394980144 \t 0.8800686721216469\n",
      "39     \t [-1.93179542 -0.90794685]. \t  -4.180232284485839 \t 0.8800686721216469\n",
      "40     \t [ 1.747245   -1.80644961]. \t  -28.509827715705118 \t 0.8800686721216469\n",
      "41     \t [-0.01215502 -0.72553865]. \t  \u001b[92m0.9878009488939742\u001b[0m \t 0.9878009488939742\n",
      "42     \t [ 0.04715145 -0.73247423]. \t  \u001b[92m1.0203201601930978\u001b[0m \t 1.0203201601930978\n",
      "43     \t [-2.41854561  1.57273742]. \t  -29.033047115733403 \t 1.0203201601930978\n",
      "44     \t [-1.60992792  2.        ]. \t  -46.844144858075786 \t 1.0203201601930978\n",
      "45     \t [-0.17399644  0.77564032]. \t  0.9744703404468986 \t 1.0203201601930978\n",
      "46     \t [-1.25297873  0.62479867]. \t  -0.6588926050167921 \t 1.0203201601930978\n",
      "47     \t [-0.19427374 -0.76524269]. \t  0.6740308143097935 \t 1.0203201601930978\n",
      "48     \t [ 1.30105651 -1.29726398]. \t  -5.279596075054974 \t 1.0203201601930978\n",
      "49     \t [-0.96661677 -0.13451372]. \t  -2.2349332119192056 \t 1.0203201601930978\n",
      "50     \t [2.83626181 1.26246322]. \t  -77.17141313318591 \t 1.0203201601930978\n",
      "51     \t [-0.90902633 -1.72516509]. \t  -27.153851646091333 \t 1.0203201601930978\n",
      "52     \t [-0.08557321  0.7377791 ]. \t  \u001b[92m1.0260991674638449\u001b[0m \t 1.0260991674638449\n",
      "53     \t [-2.49303784  1.43513443]. \t  -28.921217089979546 \t 1.0260991674638449\n",
      "54     \t [-1.69252462 -0.78432174]. \t  -2.442051036097802 \t 1.0260991674638449\n",
      "55     \t [-0.94419946  0.19469842]. \t  -1.8034552125236238 \t 1.0260991674638449\n",
      "56     \t [ 0.10044682 -0.67460576]. \t  1.019550605930943 \t 1.0260991674638449\n",
      "57     \t [2.01794246 1.90540455]. \t  -46.020810099682826 \t 1.0260991674638449\n",
      "58     \t [1.79663667 0.88683149]. \t  -3.163427451513118 \t 1.0260991674638449\n",
      "59     \t [0.6869659  0.96378109]. \t  -1.8528468462154646 \t 1.0260991674638449\n",
      "60     \t [2.2222916  1.10391883]. \t  -12.205153863231672 \t 1.0260991674638449\n",
      "61     \t [-2.99710815 -0.0030341 ]. \t  -108.09255324458013 \t 1.0260991674638449\n",
      "62     \t [ 1.43774044 -1.23302165]. \t  -3.631107448170682 \t 1.0260991674638449\n",
      "63     \t [0.39880528 1.71915394]. \t  -24.387740824734028 \t 1.0260991674638449\n",
      "64     \t [-1.2297937   0.39450762]. \t  -1.3884714176753867 \t 1.0260991674638449\n",
      "65     \t [-0.65559679 -1.92096486]. \t  -42.3242368576616 \t 1.0260991674638449\n",
      "66     \t [ 1.78891087 -1.60164163]. \t  -15.41476226025681 \t 1.0260991674638449\n",
      "67     \t [1.22694393 0.14558724]. \t  -2.4953548654668545 \t 1.0260991674638449\n",
      "68     \t [-0.0557093  0.7131464]. \t  \u001b[92m1.027040684817492\u001b[0m \t 1.027040684817492\n",
      "69     \t [ 1.07294836 -1.73211895]. \t  -24.476554163319086 \t 1.027040684817492\n",
      "70     \t [-2.32270043  1.14837687]. \t  -11.813225294815636 \t 1.027040684817492\n",
      "71     \t [1.15969125 1.68067308]. \t  -24.95735053577011 \t 1.027040684817492\n",
      "72     \t [-1.19657914  0.13310674]. \t  -2.1716268447924287 \t 1.027040684817492\n",
      "73     \t [2.18610107 1.50484932]. \t  -22.28156656774463 \t 1.027040684817492\n",
      "74     \t [-0.11627878  0.68244715]. \t  1.02095757853666 \t 1.027040684817492\n",
      "75     \t [-0.08551962 -0.45798396]. \t  0.5947092947821175 \t 1.027040684817492\n",
      "76     \t [ 1.62086379 -0.75164169]. \t  0.14273549843694178 \t 1.027040684817492\n",
      "77     \t [0.2102203  0.87408539]. \t  0.36471432165255346 \t 1.027040684817492\n",
      "78     \t [ 1.20210704 -0.84362883]. \t  -0.5660323794188019 \t 1.027040684817492\n",
      "79     \t [ 0.43868533 -1.32577587]. \t  -5.439835621608516 \t 1.027040684817492\n",
      "80     \t [0.60578523 1.84009159]. \t  -34.630798189964956 \t 1.027040684817492\n",
      "81     \t [ 1.70130709 -0.76429569]. \t  0.20456437072344746 \t 1.027040684817492\n",
      "82     \t [0.6419439  0.94451402]. \t  -1.5363892153679015 \t 1.027040684817492\n",
      "83     \t [ 0.08449206 -0.71513343]. \t  \u001b[92m1.0314530828837465\u001b[0m \t 1.0314530828837465\n",
      "84     \t [-0.97621935 -1.78289534]. \t  -31.635843118061782 \t 1.0314530828837465\n",
      "85     \t [-1.90434584  0.91978009]. \t  -0.5131550202266988 \t 1.0314530828837465\n",
      "86     \t [-0.16305448  0.73387875]. \t  1.0088402439342428 \t 1.0314530828837465\n",
      "87     \t [-1.71792663 -0.20698257]. \t  -2.274162579555493 \t 1.0314530828837465\n",
      "88     \t [-0.55281132  1.98414813]. \t  -46.18651465663658 \t 1.0314530828837465\n",
      "89     \t [ 1.60525888 -0.94601304]. \t  -0.17195007905176535 \t 1.0314530828837465\n",
      "90     \t [1.04047799 0.37476817]. \t  -2.199132117765278 \t 1.0314530828837465\n",
      "91     \t [1.84590816 1.96331092]. \t  -50.07202860187505 \t 1.0314530828837465\n",
      "92     \t [ 2.63088989 -1.92145613]. \t  -72.31278555254752 \t 1.0314530828837465\n",
      "93     \t [-0.89164821  1.41926786]. \t  -8.927469161027126 \t 1.0314530828837465\n",
      "94     \t [-0.5559054  -1.92776883]. \t  -42.495221664314876 \t 1.0314530828837465\n",
      "95     \t [-0.05174774  0.200276  ]. \t  0.15367406674279468 \t 1.0314530828837465\n",
      "96     \t [-2.11805478  0.63698247]. \t  -3.4627648976880536 \t 1.0314530828837465\n",
      "97     \t [-1.25272698  1.42225095]. \t  -8.88772114223142 \t 1.0314530828837465\n",
      "98     \t [-1.61716448  1.57313368]. \t  -14.114876185932932 \t 1.0314530828837465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 1.79900006 -0.13919475]. \t  -1.9228507851187322 \t 1.0314530828837465\n",
      "100    \t [-0.02473527  0.77148006]. \t  0.9803981624053298 \t 1.0314530828837465\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8853063  0.02859875]. \t  -2.0275945291834825 \t -2.0275945291834825\n",
      "init   \t [0.17004828 1.58514082]. \t  -15.586853032333524 \t -2.0275945291834825\n",
      "init   \t [1.19994714 0.85718841]. \t  -2.6498170273522446 \t -2.0275945291834825\n",
      "init   \t [ 1.30403029 -1.10872215]. \t  -2.0500441913631415 \t -2.0275945291834825\n",
      "init   \t [-1.94907286 -0.17263405]. \t  -3.38472265204694 \t -2.0275945291834825\n",
      "1      \t [2.22212993 0.71391762]. \t  -9.267467852913526 \t -2.0275945291834825\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "3      \t [-1.76631702 -0.9542249 ]. \t  -3.5211107459231856 \t -2.0275945291834825\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "5      \t [ 0.59863203 -1.33262018]. \t  -5.892789939167 \t -2.0275945291834825\n",
      "6      \t [ 1.22521833 -0.55778933]. \t  \u001b[92m-0.8592166103441335\u001b[0m \t -0.8592166103441335\n",
      "7      \t [2.86493458 1.95006643]. \t  -123.89427694145698 \t -0.8592166103441335\n",
      "8      \t [1.84408725 0.19782139]. \t  -2.640568209557654 \t -0.8592166103441335\n",
      "9      \t [-2.43372984 -1.17158059]. \t  -24.18099320584752 \t -0.8592166103441335\n",
      "10     \t [-1.19668044 -0.34246918]. \t  -2.39623121039255 \t -0.8592166103441335\n",
      "11     \t [-1.2698406  -1.97667729]. \t  -50.33481686808442 \t -0.8592166103441335\n",
      "12     \t [-1.65539649 -0.53709486]. \t  -2.119064475951476 \t -0.8592166103441335\n",
      "13     \t [-0.27828293  0.11499358]. \t  \u001b[92m-0.21313090597920756\u001b[0m \t -0.21313090597920756\n",
      "14     \t [ 0.97294219 -0.92051979]. \t  -0.7744551750891745 \t -0.21313090597920756\n",
      "15     \t [-0.75450226  0.45429017]. \t  -0.6601265802077101 \t -0.21313090597920756\n",
      "16     \t [-0.05270931  0.49262473]. \t  \u001b[92m0.7500128897410501\u001b[0m \t 0.7500128897410501\n",
      "17     \t [0.12705145 0.45934689]. \t  0.5435318103932159 \t 0.7500128897410501\n",
      "18     \t [-0.92110091 -0.80118432]. \t  -1.9041520701096513 \t 0.7500128897410501\n",
      "19     \t [ 0.99111508 -1.86840252]. \t  -35.14963296511663 \t 0.7500128897410501\n",
      "20     \t [-0.23629567 -0.86913022]. \t  0.3168839462767482 \t 0.7500128897410501\n",
      "21     \t [-0.04143346 -0.83864002]. \t  \u001b[92m0.7930401690402418\u001b[0m \t 0.7930401690402418\n",
      "22     \t [2.90520404 0.32450415]. \t  -85.14757073466616 \t 0.7930401690402418\n",
      "23     \t [1.70649561 0.67367013]. \t  -2.2297154552443628 \t 0.7930401690402418\n",
      "24     \t [0.54746647 0.71098197]. \t  -0.4085657348914663 \t 0.7930401690402418\n",
      "25     \t [0.90161569 1.91494816]. \t  -42.889750016103875 \t 0.7930401690402418\n",
      "26     \t [-0.43938629  0.80575097]. \t  0.5685839737469045 \t 0.7930401690402418\n",
      "27     \t [ 2.70659158 -1.61929638]. \t  -60.28030912160772 \t 0.7930401690402418\n",
      "28     \t [ 2.14080046 -0.91975417]. \t  -3.820636547509968 \t 0.7930401690402418\n",
      "29     \t [ 1.85660516 -1.09728028]. \t  -1.4337583833973955 \t 0.7930401690402418\n",
      "30     \t [-0.54522033  1.84954709]. \t  -33.128697758108025 \t 0.7930401690402418\n",
      "31     \t [-1.66765019  0.1739341 ]. \t  -1.64465429536343 \t 0.7930401690402418\n",
      "32     \t [ 0.15039673 -0.56293855]. \t  \u001b[92m0.8611552539810666\u001b[0m \t 0.8611552539810666\n",
      "33     \t [-0.12304489 -0.6593215 ]. \t  0.8417400486838296 \t 0.8611552539810666\n",
      "34     \t [-0.21679237  0.76764576]. \t  \u001b[92m0.9511444158953275\u001b[0m \t 0.9511444158953275\n",
      "35     \t [1.09516905 1.14525105]. \t  -5.240779269036343 \t 0.9511444158953275\n",
      "36     \t [ 1.13094065 -1.21259074]. \t  -3.7733070913793583 \t 0.9511444158953275\n",
      "37     \t [-1.08179979 -1.60495721]. \t  -20.312793644925716 \t 0.9511444158953275\n",
      "38     \t [ 2.16573892 -1.28897526]. \t  -8.562451589721018 \t 0.9511444158953275\n",
      "39     \t [0.35808799 0.65486896]. \t  0.26616963652630576 \t 0.9511444158953275\n",
      "40     \t [ 0.15327534 -0.10396972]. \t  -0.03411115926070339 \t 0.9511444158953275\n",
      "41     \t [0.6593961 1.8653659]. \t  -37.111440235724615 \t 0.9511444158953275\n",
      "42     \t [2.10011274 0.31309679]. \t  -5.693889565865355 \t 0.9511444158953275\n",
      "43     \t [-0.10791116  0.68471664]. \t  \u001b[92m1.0237089685534349\u001b[0m \t 1.0237089685534349\n",
      "44     \t [-2.22394161  0.79089015]. \t  -6.04654147429078 \t 1.0237089685534349\n",
      "45     \t [-2.21840895  0.4034016 ]. \t  -7.115241544566829 \t 1.0237089685534349\n",
      "46     \t [-2.88621869  1.55338924]. \t  -89.43820997480013 \t 1.0237089685534349\n",
      "47     \t [-1.64989912  0.87866362]. \t  0.10247195430325118 \t 1.0237089685534349\n",
      "48     \t [-0.09212575  0.72308019]. \t  \u001b[92m1.0307291834318548\u001b[0m \t 1.0307291834318548\n",
      "49     \t [ 0.10220664 -0.80951208]. \t  0.944696954014915 \t 1.0307291834318548\n",
      "50     \t [-0.07532625  1.99383881]. \t  -47.18587221993711 \t 1.0307291834318548\n",
      "51     \t [ 0.10648995 -0.76516185]. \t  1.007168876615758 \t 1.0307291834318548\n",
      "52     \t [ 0.43781402 -1.43377894]. \t  -8.74527030585627 \t 1.0307291834318548\n",
      "53     \t [-2.42007473 -0.09673544]. \t  -18.556136911437832 \t 1.0307291834318548\n",
      "54     \t [0.07551448 0.3643988 ]. \t  0.41035802448501757 \t 1.0307291834318548\n",
      "55     \t [0.03270282 0.74571641]. \t  0.9587517948146356 \t 1.0307291834318548\n",
      "56     \t [ 0.48430012 -1.70372237]. \t  -22.09313973590703 \t 1.0307291834318548\n",
      "57     \t [-2.3157445  -0.21862567]. \t  -12.789620113637667 \t 1.0307291834318548\n",
      "58     \t [-1.28151099  1.06928366]. \t  -1.6670903211021577 \t 1.0307291834318548\n",
      "59     \t [1.64034857 1.96576279]. \t  -49.549044592541584 \t 1.0307291834318548\n",
      "60     \t [-1.93488522  0.17919941]. \t  -2.5615017531809587 \t 1.0307291834318548\n",
      "61     \t [-1.44121622 -1.88793319]. \t  -41.515838564113196 \t 1.0307291834318548\n",
      "62     \t [ 0.12585199 -0.65868511]. \t  1.0025726435130191 \t 1.0307291834318548\n",
      "63     \t [-2.44222575  0.55814753]. \t  -17.657978830799458 \t 1.0307291834318548\n",
      "64     \t [ 0.03643459 -0.67015704]. \t  1.0087516990154541 \t 1.0307291834318548\n",
      "65     \t [2.55096516 0.46749753]. \t  -29.466997174844437 \t 1.0307291834318548\n",
      "66     \t [-0.07051334  0.72067078]. \t  1.0294800290370425 \t 1.0307291834318548\n",
      "67     \t [0.01954082 0.74888036]. \t  0.969042035983855 \t 1.0307291834318548\n",
      "68     \t [ 2.98527148 -1.09888872]. \t  -102.51457751696026 \t 1.0307291834318548\n",
      "69     \t [ 0.04879414 -0.67179856]. \t  1.0137866845821022 \t 1.0307291834318548\n",
      "70     \t [ 0.01219218 -0.73988296]. \t  0.9994290287373999 \t 1.0307291834318548\n",
      "71     \t [-0.81959939  0.91554542]. \t  -0.5476097190111732 \t 1.0307291834318548\n",
      "72     \t [ 0.04350959 -0.63626204]. \t  0.9838888963714706 \t 1.0307291834318548\n",
      "73     \t [-2.96057673  1.65924245]. \t  -112.57863068680221 \t 1.0307291834318548\n",
      "74     \t [ 1.75642857 -0.7788879 ]. \t  0.18181974765279496 \t 1.0307291834318548\n",
      "75     \t [-1.95708465  1.74740684]. \t  -24.903281358737694 \t 1.0307291834318548\n",
      "76     \t [ 2.4734786  -1.65451984]. \t  -37.13482608911188 \t 1.0307291834318548\n",
      "77     \t [ 0.95603388 -1.84167032]. \t  -32.84434949799984 \t 1.0307291834318548\n",
      "78     \t [-2.48615387 -1.88059049]. \t  -63.76760783501793 \t 1.0307291834318548\n",
      "79     \t [ 2.02351394 -1.69285078]. \t  -22.014836770849858 \t 1.0307291834318548\n",
      "80     \t [ 0.14347648 -0.70125623]. \t  1.0188871964587585 \t 1.0307291834318548\n",
      "81     \t [-2.18901025  0.51048487]. \t  -5.735334761615723 \t 1.0307291834318548\n",
      "82     \t [-1.55490134 -1.18913026]. \t  -6.297215392267328 \t 1.0307291834318548\n",
      "83     \t [-0.7067576  -1.05849114]. \t  -2.803303437001512 \t 1.0307291834318548\n",
      "84     \t [ 2.03972401 -1.32236183]. \t  -6.83631869632155 \t 1.0307291834318548\n",
      "85     \t [ 1.00833903 -1.50728794]. \t  -12.28532548398758 \t 1.0307291834318548\n",
      "86     \t [1.48891204 0.59208065]. \t  -2.149505595439773 \t 1.0307291834318548\n",
      "87     \t [1.99594863 0.34924764]. \t  -3.950720095656112 \t 1.0307291834318548\n",
      "88     \t [2.45008353 0.59344997]. \t  -20.98449250856693 \t 1.0307291834318548\n",
      "89     \t [-0.15638063  1.41651631]. \t  -7.95344433075686 \t 1.0307291834318548\n",
      "90     \t [ 2.44570897 -0.88399802]. \t  -17.282280926879103 \t 1.0307291834318548\n",
      "91     \t [-0.09347063  0.66943619]. \t  1.01702981816735 \t 1.0307291834318548\n",
      "92     \t [-0.48323742  0.50783211]. \t  0.18713766016017597 \t 1.0307291834318548\n",
      "93     \t [ 1.46615004 -0.7306553 ]. \t  -0.13904126292789754 \t 1.0307291834318548\n",
      "94     \t [-0.46281242  0.29894351]. \t  -0.2998321858296047 \t 1.0307291834318548\n",
      "95     \t [0.7504753  0.84422357]. \t  -1.4608226973697307 \t 1.0307291834318548\n",
      "96     \t [ 1.75428451 -0.73863175]. \t  0.15089273627477506 \t 1.0307291834318548\n",
      "97     \t [ 1.9030235  -1.07106191]. \t  -1.4133324654497175 \t 1.0307291834318548\n",
      "98     \t [ 2.91042024 -0.23785788]. \t  -84.88874768631914 \t 1.0307291834318548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.69790981 -0.2587271 ]. \t  -45.45107101086421 \t 1.0307291834318548\n",
      "100    \t [-0.98991277 -1.91084742]. \t  -42.832123111171335 \t 1.0307291834318548\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.46729115 -0.97799468]. \t  -0.15324775214966774 \t -0.10477079024230418\n",
      "init   \t [-1.48483878  0.83961742]. \t  -0.10477079024230418 \t -0.10477079024230418\n",
      "init   \t [-0.31468582 -1.09222817]. \t  -1.6403350770531386 \t -0.10477079024230418\n",
      "init   \t [-0.58974261  1.52903094]. \t  -12.761482581115207 \t -0.10477079024230418\n",
      "init   \t [-0.3767192   1.51370072]. \t  -11.790964688915137 \t -0.10477079024230418\n",
      "1      \t [-2.12826797  0.32652419]. \t  -4.934123393463919 \t -0.10477079024230418\n",
      "2      \t [-0.88114718  0.18471356]. \t  -1.7011777695465604 \t -0.10477079024230418\n",
      "3      \t [1.30747098 1.31648618]. \t  -9.17002452824579 \t -0.10477079024230418\n",
      "4      \t [ 0.99645987 -1.97738227]. \t  -45.770750922776465 \t -0.10477079024230418\n",
      "5      \t [ 0.46125796 -0.28130057]. \t  -0.3379606132854113 \t -0.10477079024230418\n",
      "6      \t [ 0.25749526 -0.69779905]. \t  \u001b[92m0.9229155845722549\u001b[0m \t 0.9229155845722549\n",
      "7      \t [ 0.98466881 -0.4899728 ]. \t  -0.9957501949142898 \t 0.9229155845722549\n",
      "8      \t [ 0.47743892 -0.68312734]. \t  0.5150829177410041 \t 0.9229155845722549\n",
      "9      \t [ 2.18035285 -0.06687561]. \t  -7.205171443921705 \t 0.9229155845722549\n",
      "10     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9229155845722549\n",
      "11     \t [-1.38067625  0.49998483]. \t  -0.8627199769170979 \t 0.9229155845722549\n",
      "12     \t [-0.22558852  0.50400929]. \t  0.6735185773894292 \t 0.9229155845722549\n",
      "13     \t [-3.         -0.69952834]. \t  -109.99903957465264 \t 0.9229155845722549\n",
      "14     \t [-1.8325366   0.64519493]. \t  -0.2197602862835628 \t 0.9229155845722549\n",
      "15     \t [-0.25808623  0.2194048 ]. \t  -0.017305489446127836 \t 0.9229155845722549\n",
      "16     \t [0.55001786 0.59367052]. \t  -0.4407388487642898 \t 0.9229155845722549\n",
      "17     \t [0.92754134 0.01192162]. \t  -2.109723330951134 \t 0.9229155845722549\n",
      "18     \t [0.08728408 0.64163789]. \t  0.8824547845376621 \t 0.9229155845722549\n",
      "19     \t [ 2.857054  -0.5307976]. \t  -71.69727766213151 \t 0.9229155845722549\n",
      "20     \t [2.24225599 0.6516924 ]. \t  -9.874536126355009 \t 0.9229155845722549\n",
      "21     \t [ 1.84514496 -0.12996827]. \t  -2.124961143833328 \t 0.9229155845722549\n",
      "22     \t [ 0.15302981 -0.92372258]. \t  0.5496518367599035 \t 0.9229155845722549\n",
      "23     \t [-0.90985098  0.80071061]. \t  -0.41243977398451237 \t 0.9229155845722549\n",
      "24     \t [-0.65283978 -0.61839555]. \t  -0.8081678184059434 \t 0.9229155845722549\n",
      "25     \t [0.69644078 1.87769703]. \t  -38.412384059828305 \t 0.9229155845722549\n",
      "26     \t [-1.21411749 -1.82230836]. \t  -35.44120904135842 \t 0.9229155845722549\n",
      "27     \t [-0.09552351 -0.61588959]. \t  0.846589031422058 \t 0.9229155845722549\n",
      "28     \t [-0.4179639   0.86765849]. \t  0.4704906179734629 \t 0.9229155845722549\n",
      "29     \t [2.59661688 1.79081733]. \t  -66.63587545198251 \t 0.9229155845722549\n",
      "30     \t [-0.11242265  0.72179608]. \t  \u001b[92m1.0291633506171936\u001b[0m \t 1.0291633506171936\n",
      "31     \t [-0.04905967  0.61613668]. \t  0.962651089289744 \t 1.0291633506171936\n",
      "32     \t [ 2.72757434 -0.40090776]. \t  -49.15178086195256 \t 1.0291633506171936\n",
      "33     \t [-0.1158351   0.69995743]. \t  1.027381046851688 \t 1.0291633506171936\n",
      "34     \t [ 0.1546346  -0.71857804]. \t  1.0155958974209214 \t 1.0291633506171936\n",
      "35     \t [-0.84449985  0.69228797]. \t  -0.3226029563542343 \t 1.0291633506171936\n",
      "36     \t [1.00737241 0.08566848]. \t  -2.3020919903908 \t 1.0291633506171936\n",
      "37     \t [-0.06770591  0.6339393 ]. \t  0.9861182745260134 \t 1.0291633506171936\n",
      "38     \t [-0.07124171  0.70816797]. \t  \u001b[92m1.0301946013322725\u001b[0m \t 1.0301946013322725\n",
      "39     \t [-1.45413003  0.50560714]. \t  -0.7236899118730549 \t 1.0301946013322725\n",
      "40     \t [-0.18822731  0.71327892]. \t  0.9948542678033747 \t 1.0301946013322725\n",
      "41     \t [-0.20066151  0.66974516]. \t  0.9661299434538988 \t 1.0301946013322725\n",
      "42     \t [-0.67226021  0.90796726]. \t  -0.22015371246861315 \t 1.0301946013322725\n",
      "43     \t [2.62829925 1.14763938]. \t  -41.98916315898547 \t 1.0301946013322725\n",
      "44     \t [0.25510179 0.69925623]. \t  0.5696248375504335 \t 1.0301946013322725\n",
      "45     \t [ 2.47180275 -1.37287676]. \t  -25.349737741921395 \t 1.0301946013322725\n",
      "46     \t [ 1.78994273 -1.15104644]. \t  -1.8833844184388606 \t 1.0301946013322725\n",
      "47     \t [ 1.79549503 -0.76434081]. \t  0.10566390202407683 \t 1.0301946013322725\n",
      "48     \t [1.9879498  1.83868299]. \t  -39.43411598304237 \t 1.0301946013322725\n",
      "49     \t [-1.6482847  -1.23789187]. \t  -7.354931116601106 \t 1.0301946013322725\n",
      "50     \t [ 2.18057362 -1.97053779]. \t  -47.85755457335124 \t 1.0301946013322725\n",
      "51     \t [-1.38827904 -0.28345532]. \t  -2.3930328622164616 \t 1.0301946013322725\n",
      "52     \t [0.01707227 0.70188657]. \t  0.9866351317247339 \t 1.0301946013322725\n",
      "53     \t [-2.9113778 -0.6819999]. \t  -87.00909785522047 \t 1.0301946013322725\n",
      "54     \t [-2.09719261 -0.78627487]. \t  -6.034853641759609 \t 1.0301946013322725\n",
      "55     \t [ 0.19928146 -1.91554461]. \t  -38.951961089367835 \t 1.0301946013322725\n",
      "56     \t [0.7293508  1.03744074]. \t  -2.6688100575816995 \t 1.0301946013322725\n",
      "57     \t [-2.52322304 -1.74120743]. \t  -55.40078308820749 \t 1.0301946013322725\n",
      "58     \t [-0.62219426  1.0020765 ]. \t  -0.646334716846837 \t 1.0301946013322725\n",
      "59     \t [-1.31535873 -0.02814128]. \t  -2.394620274990766 \t 1.0301946013322725\n",
      "60     \t [ 0.19607251 -0.67715481]. \t  0.975202355365616 \t 1.0301946013322725\n",
      "61     \t [-0.10614547  0.75700587]. \t  1.0142015858372546 \t 1.0301946013322725\n",
      "62     \t [-0.08734134  0.64435134]. \t  0.9971146726685534 \t 1.0301946013322725\n",
      "63     \t [-2.72348769  0.70805455]. \t  -47.23312268740964 \t 1.0301946013322725\n",
      "64     \t [ 0.86875936 -0.65100078]. \t  -0.42370541355190106 \t 1.0301946013322725\n",
      "65     \t [-1.57538804  0.42502143]. \t  -0.826382623869653 \t 1.0301946013322725\n",
      "66     \t [-0.02301843  0.72670201]. \t  1.0114512361700414 \t 1.0301946013322725\n",
      "67     \t [0.08753176 0.75359478]. \t  0.8850680157934472 \t 1.0301946013322725\n",
      "68     \t [2.82208785 0.56733025]. \t  -67.76999401806682 \t 1.0301946013322725\n",
      "69     \t [1.77682387 0.91680534]. \t  -3.27919681246194 \t 1.0301946013322725\n",
      "70     \t [ 0.66000406 -1.27355475]. \t  -4.565955876600416 \t 1.0301946013322725\n",
      "71     \t [-0.02913911 -0.90641129]. \t  0.5565346979403937 \t 1.0301946013322725\n",
      "72     \t [1.47981044 0.97489083]. \t  -3.4435457383386563 \t 1.0301946013322725\n",
      "73     \t [-1.29970164  1.57028808]. \t  -14.7879592976306 \t 1.0301946013322725\n",
      "74     \t [-2.79788892  0.7094115 ]. \t  -59.54381372619115 \t 1.0301946013322725\n",
      "75     \t [-0.14107122  0.67301945]. \t  1.0073155385870756 \t 1.0301946013322725\n",
      "76     \t [ 0.0872596  -0.70906487]. \t  \u001b[92m1.031506611614352\u001b[0m \t 1.031506611614352\n",
      "77     \t [-0.07424501  0.77499964]. \t  0.9950532079782564 \t 1.031506611614352\n",
      "78     \t [ 2.95015327 -0.13826422]. \t  -95.01606706240212 \t 1.031506611614352\n",
      "79     \t [-1.8449315   1.08919984]. \t  -1.3050210984911756 \t 1.031506611614352\n",
      "80     \t [ 1.09064255 -1.04456335]. \t  -1.6061120261755024 \t 1.031506611614352\n",
      "81     \t [2.3243922  1.96256259]. \t  -61.37740809352031 \t 1.031506611614352\n",
      "82     \t [0.1558512  1.05658319]. \t  -0.7802321876099787 \t 1.031506611614352\n",
      "83     \t [-0.09826194 -1.5896447 ]. \t  -15.629061696141676 \t 1.031506611614352\n",
      "84     \t [1.00367686 0.17366662]. \t  -2.2964708748809715 \t 1.031506611614352\n",
      "85     \t [0.5785536  1.84994887]. \t  -35.34601009790853 \t 1.031506611614352\n",
      "86     \t [ 0.11849792 -0.7452084 ]. \t  1.0203036554856164 \t 1.031506611614352\n",
      "87     \t [-2.47854072 -0.00312966]. \t  -22.60745912620147 \t 1.031506611614352\n",
      "88     \t [-0.0217407  -0.61538547]. \t  0.9258754636758458 \t 1.031506611614352\n",
      "89     \t [-0.59130309 -0.81206048]. \t  -0.7379459896098622 \t 1.031506611614352\n",
      "90     \t [-0.86941445 -0.20246397]. \t  -1.9864156929290815 \t 1.031506611614352\n",
      "91     \t [1.94169735 0.05078113]. \t  -3.182549720479885 \t 1.031506611614352\n",
      "92     \t [-1.21929528  0.07409524]. \t  -2.288374901218474 \t 1.031506611614352\n",
      "93     \t [-2.71023071  1.26752489]. \t  -48.64488377839112 \t 1.031506611614352\n",
      "94     \t [2.24354666 0.86524169]. \t  -10.626450767349077 \t 1.031506611614352\n",
      "95     \t [0.01989147 0.67569128]. \t  0.9774285647622631 \t 1.031506611614352\n",
      "96     \t [ 2.23148288 -1.70035642]. \t  -27.081269546199533 \t 1.031506611614352\n",
      "97     \t [1.62122784 0.21600302]. \t  -2.2308047436755936 \t 1.031506611614352\n",
      "98     \t [ 1.2162527 -0.2986543]. \t  -1.712581562735385 \t 1.031506611614352\n",
      "99     \t [-1.27928006 -0.50410063]. \t  -2.269551004138511 \t 1.031506611614352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.15803657 -0.74048775]. \t  1.0090865496155785 \t 1.031506611614352\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.18368999 -0.73453599]. \t  -4.868217712547965 \t 0.15481417016151777\n",
      "init   \t [1.03290467 0.03164667]. \t  -2.310710708702905 \t 0.15481417016151777\n",
      "init   \t [ 1.68996256 -0.86527329]. \t  0.15481417016151777 \t 0.15481417016151777\n",
      "init   \t [-1.5977473   0.25519875]. \t  -1.420015353077075 \t 0.15481417016151777\n",
      "init   \t [2.25014619 0.87577168]. \t  -10.939300534748709 \t 0.15481417016151777\n",
      "1      \t [ 1.00985701 -1.38885985]. \t  -8.0135863880244 \t 0.15481417016151777\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "3      \t [-1.49519072  1.00020557]. \t  -0.6773842439076193 \t 0.15481417016151777\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "5      \t [-0.78766292  0.9890635 ]. \t  -0.8887681231591341 \t 0.15481417016151777\n",
      "6      \t [-0.97361154  1.11769552]. \t  -2.3458988599770754 \t 0.15481417016151777\n",
      "7      \t [-1.25461811  0.63257035]. \t  -0.6393902750501259 \t 0.15481417016151777\n",
      "8      \t [0.27070396 1.05977269]. \t  -1.121966185510173 \t 0.15481417016151777\n",
      "9      \t [-1.90768754  0.50421999]. \t  -1.090295683914801 \t 0.15481417016151777\n",
      "10     \t [-0.03357332  0.51433132]. \t  \u001b[92m0.790989949130232\u001b[0m \t 0.790989949130232\n",
      "11     \t [-0.14448104  0.8499634 ]. \t  \u001b[92m0.8423022893978533\u001b[0m \t 0.8423022893978533\n",
      "12     \t [1.22343918 2.        ]. \t  -52.847032050428126 \t 0.8423022893978533\n",
      "13     \t [-0.75371193 -0.73114257]. \t  -1.2115823410133308 \t 0.8423022893978533\n",
      "14     \t [ 1.83488093 -1.32971119]. \t  -5.37701565932792 \t 0.8423022893978533\n",
      "15     \t [ 0.68939697 -0.51758881]. \t  -0.3211704413224429 \t 0.8423022893978533\n",
      "16     \t [ 1.42211637 -0.71903314]. \t  -0.2362625555448956 \t 0.8423022893978533\n",
      "17     \t [ 0.00152732 -0.55664215]. \t  \u001b[92m0.8562134885929198\u001b[0m \t 0.8562134885929198\n",
      "18     \t [0.27446137 0.62924383]. \t  0.49444715013862267 \t 0.8562134885929198\n",
      "19     \t [ 0.14666144 -0.24473092]. \t  0.17604666117368392 \t 0.8562134885929198\n",
      "20     \t [ 0.04959141 -0.81967294]. \t  \u001b[92m0.9126756388727633\u001b[0m \t 0.9126756388727633\n",
      "21     \t [2.95642866 1.51867499]. \t  -113.65092779445001 \t 0.9126756388727633\n",
      "22     \t [1.73411388 0.57456856]. \t  -2.2146984186505403 \t 0.9126756388727633\n",
      "23     \t [2.67912602 0.26364761]. \t  -44.23169528294859 \t 0.9126756388727633\n",
      "24     \t [ 0.1807103  -1.96951697]. \t  -44.44296998401471 \t 0.9126756388727633\n",
      "25     \t [-0.18610861 -0.85796922]. \t  0.48129038675012836 \t 0.9126756388727633\n",
      "26     \t [-1.74476969 -0.40249317]. \t  -2.278709154576716 \t 0.9126756388727633\n",
      "27     \t [1.75432254 0.88293776]. \t  -2.998256802540766 \t 0.9126756388727633\n",
      "28     \t [-0.0263908   0.65667486]. \t  \u001b[92m0.9956235774067448\u001b[0m \t 0.9956235774067448\n",
      "29     \t [ 2.7787833  -1.93238217]. \t  -94.60890787685028 \t 0.9956235774067448\n",
      "30     \t [0.0254775  0.79436433]. \t  0.9085066659785855 \t 0.9956235774067448\n",
      "31     \t [ 1.16006307 -1.42403575]. \t  -9.07788399308901 \t 0.9956235774067448\n",
      "32     \t [-0.590951   -1.87775729]. \t  -37.890679461121174 \t 0.9956235774067448\n",
      "33     \t [1.58476494 1.21281658]. \t  -6.773319884253189 \t 0.9956235774067448\n",
      "34     \t [0.0011967  0.69418676]. \t  \u001b[92m0.997852411692912\u001b[0m \t 0.997852411692912\n",
      "35     \t [-0.92499965 -1.60176692]. \t  -19.64329435847691 \t 0.997852411692912\n",
      "36     \t [ 1.59121066 -0.50113126]. \t  -0.526099360979377 \t 0.997852411692912\n",
      "37     \t [-0.43552111  0.14346979]. \t  -0.5423118285680255 \t 0.997852411692912\n",
      "38     \t [-1.65152292  1.87485109]. \t  -34.317232286957825 \t 0.997852411692912\n",
      "39     \t [-2.08942913  0.06695537]. \t  -5.016291437368888 \t 0.997852411692912\n",
      "40     \t [ 0.11093152 -1.98101709]. \t  -45.73599653867532 \t 0.997852411692912\n",
      "41     \t [-0.49690119  1.97896082]. \t  -45.56532430579879 \t 0.997852411692912\n",
      "42     \t [2.21827117 1.96065619]. \t  -56.63377226491582 \t 0.997852411692912\n",
      "43     \t [ 0.04940441 -0.75562736]. \t  \u001b[92m1.0074321494287037\u001b[0m \t 1.0074321494287037\n",
      "44     \t [-0.10645872  0.71478019]. \t  \u001b[92m1.0305538272659756\u001b[0m \t 1.0305538272659756\n",
      "45     \t [-0.89775871 -1.6124698 ]. \t  -20.122860266160462 \t 1.0305538272659756\n",
      "46     \t [0.99149593 1.09304245]. \t  -4.233879355183807 \t 1.0305538272659756\n",
      "47     \t [-0.32324254 -0.37129543]. \t  -0.039996073069283766 \t 1.0305538272659756\n",
      "48     \t [ 0.162227   -0.67779611]. \t  0.9995438639326215 \t 1.0305538272659756\n",
      "49     \t [1.40229604 0.22165236]. \t  -2.4039258096964105 \t 1.0305538272659756\n",
      "50     \t [-0.0451006  -0.09975659]. \t  0.026782738471090778 \t 1.0305538272659756\n",
      "51     \t [-2.40975476  0.82515777]. \t  -14.827831552487309 \t 1.0305538272659756\n",
      "52     \t [-0.98902035 -0.98631503]. \t  -3.0850448128396297 \t 1.0305538272659756\n",
      "53     \t [-1.86370466 -1.82081755]. \t  -36.625260451302594 \t 1.0305538272659756\n",
      "54     \t [ 1.20875711 -1.30544156]. \t  -5.623231024010353 \t 1.0305538272659756\n",
      "55     \t [-0.93842378 -1.43326778]. \t  -12.129471971749108 \t 1.0305538272659756\n",
      "56     \t [-0.10384646  0.714943  ]. \t  \u001b[92m1.0308550455040408\u001b[0m \t 1.0308550455040408\n",
      "57     \t [-1.76279819  0.64155545]. \t  -0.054119100443345114 \t 1.0308550455040408\n",
      "58     \t [ 2.55713393 -1.85211105]. \t  -58.17204951785956 \t 1.0308550455040408\n",
      "59     \t [ 0.13169418 -0.72033082]. \t  1.0246945864787034 \t 1.0308550455040408\n",
      "60     \t [-0.20861753  0.74525062]. \t  0.9730614138466095 \t 1.0308550455040408\n",
      "61     \t [-1.14461476  0.52225754]. \t  -0.9943654894472209 \t 1.0308550455040408\n",
      "62     \t [ 1.98461331 -0.39381778]. \t  -2.2384753512446833 \t 1.0308550455040408\n",
      "63     \t [-0.00913986 -0.71092033]. \t  0.9930511794555171 \t 1.0308550455040408\n",
      "64     \t [ 0.22835015 -0.7083746 ]. \t  0.9588319866793009 \t 1.0308550455040408\n",
      "65     \t [-0.5267027   1.94130659]. \t  -41.669520784412626 \t 1.0308550455040408\n",
      "66     \t [-2.28575626 -1.68877761]. \t  -36.10148036780675 \t 1.0308550455040408\n",
      "67     \t [-2.08193529 -1.02056755]. \t  -7.326488782809572 \t 1.0308550455040408\n",
      "68     \t [ 2.15379535 -0.1148423 ]. \t  -6.340509925469137 \t 1.0308550455040408\n",
      "69     \t [1.90139351 1.03652246]. \t  -5.0549803120941 \t 1.0308550455040408\n",
      "70     \t [ 1.87066426 -0.05430607]. \t  -2.4523633639431703 \t 1.0308550455040408\n",
      "71     \t [-0.14323188  0.76567151]. \t  0.9987298338020258 \t 1.0308550455040408\n",
      "72     \t [-0.30060069 -0.20153082]. \t  -0.24926219649462772 \t 1.0308550455040408\n",
      "73     \t [0.97526732 0.73008194]. \t  -1.907972443799164 \t 1.0308550455040408\n",
      "74     \t [ 0.13192895 -0.63195665]. \t  0.9738805783926698 \t 1.0308550455040408\n",
      "75     \t [-0.0897056   0.78608958]. \t  0.982827695221619 \t 1.0308550455040408\n",
      "76     \t [-1.66110534  1.01313952]. \t  -0.47688773519325334 \t 1.0308550455040408\n",
      "77     \t [-1.86750568  0.70426339]. \t  -0.2324756996214492 \t 1.0308550455040408\n",
      "78     \t [-0.0307248   0.02334333]. \t  -0.0008785049358762055 \t 1.0308550455040408\n",
      "79     \t [ 0.13185308 -0.7798099 ]. \t  0.9871669093224957 \t 1.0308550455040408\n",
      "80     \t [2.80156731 0.96082609]. \t  -65.60661081112652 \t 1.0308550455040408\n",
      "81     \t [-2.89226964 -1.95592372]. \t  -130.53011185358824 \t 1.0308550455040408\n",
      "82     \t [-0.08641601  0.72006289]. \t  \u001b[92m1.0311034228951594\u001b[0m \t 1.0311034228951594\n",
      "83     \t [-0.13307561  0.65766346]. \t  0.9991256924045604 \t 1.0311034228951594\n",
      "84     \t [ 0.21706747 -0.73010678]. \t  0.9702659255941001 \t 1.0311034228951594\n",
      "85     \t [-1.05880544 -1.26619925]. \t  -7.524050429022926 \t 1.0311034228951594\n",
      "86     \t [0.3473684  1.15989825]. \t  -2.714141654336306 \t 1.0311034228951594\n",
      "87     \t [ 0.76379738 -1.69355155]. \t  -21.823395040306888 \t 1.0311034228951594\n",
      "88     \t [-2.43203835 -0.79476976]. \t  -20.169540227053655 \t 1.0311034228951594\n",
      "89     \t [0.18337496 0.88162559]. \t  0.398687728010978 \t 1.0311034228951594\n",
      "90     \t [ 0.40001542 -1.3566967 ]. \t  -6.234058703140568 \t 1.0311034228951594\n",
      "91     \t [1.53165474 1.54256082]. \t  -17.62277393064972 \t 1.0311034228951594\n",
      "92     \t [-0.10296326 -1.54889969]. \t  -13.6278251233956 \t 1.0311034228951594\n",
      "93     \t [-2.30940352 -1.23800489]. \t  -18.292581845782465 \t 1.0311034228951594\n",
      "94     \t [ 0.00372845 -0.74159598]. \t  0.992723552577655 \t 1.0311034228951594\n",
      "95     \t [-0.99061714  1.71493951]. \t  -23.35342319187397 \t 1.0311034228951594\n",
      "96     \t [ 0.15352823 -0.75439378]. \t  1.0035946854663889 \t 1.0311034228951594\n",
      "97     \t [-1.70697496  1.38578229]. \t  -6.776549152514686 \t 1.0311034228951594\n",
      "98     \t [ 0.20155206 -0.72713384]. \t  0.9842053783441223 \t 1.0311034228951594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-2.12458244  1.61086327]. \t  -19.056246462062564 \t 1.0311034228951594\n",
      "100    \t [-0.01343013  0.6652738 ]. \t  0.9950293221425383 \t 1.0311034228951594\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.91094417 0.9220785 ]. \t  -2.3942909876906837 \t -0.5838248233399961\n",
      "init   \t [-1.83979583  0.07870895]. \t  -2.2368016331814964 \t -0.5838248233399961\n",
      "init   \t [1.0582996  1.21777949]. \t  -6.46789670014418 \t -0.5838248233399961\n",
      "init   \t [0.20107719 1.63213672]. \t  -18.215840822341526 \t -0.5838248233399961\n",
      "init   \t [-0.33911323 -0.97611116]. \t  -0.5838248233399961 \t -0.5838248233399961\n",
      "1      \t [1.13213455 0.01299289]. \t  -2.3928963638963574 \t -0.5838248233399961\n",
      "2      \t [0.47727594 0.30619857]. \t  -0.6124145357534219 \t -0.5838248233399961\n",
      "3      \t [-0.85766014 -1.81420745]. \t  -33.66120969958933 \t -0.5838248233399961\n",
      "4      \t [-0.24894107 -0.46100906]. \t  \u001b[92m0.31477749397480403\u001b[0m \t 0.31477749397480403\n",
      "5      \t [ 0.3283862  -1.04779217]. \t  -0.49305340924925994 \t 0.31477749397480403\n",
      "6      \t [-3.          0.51148913]. \t  -106.59283053857072 \t 0.31477749397480403\n",
      "7      \t [-1.31704565 -0.20891068]. \t  -2.467738349647818 \t 0.31477749397480403\n",
      "8      \t [-1.44088248  0.48424505]. \t  -0.8200122021217195 \t 0.31477749397480403\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.31477749397480403\n",
      "10     \t [ 0.04634518 -0.73675737]. \t  \u001b[92m1.018232062789341\u001b[0m \t 1.018232062789341\n",
      "11     \t [1.45076767 0.53945396]. \t  -2.1813907185536836 \t 1.018232062789341\n",
      "12     \t [2.92691825 1.45802502]. \t  -103.56372379292706 \t 1.018232062789341\n",
      "13     \t [ 0.0093861  -1.06405503]. \t  -0.5891386810201593 \t 1.018232062789341\n",
      "14     \t [1.09416666 0.41391158]. \t  -2.23587627829629 \t 1.018232062789341\n",
      "15     \t [ 1.83197558 -0.24686772]. \t  -1.6904850228241102 \t 1.018232062789341\n",
      "16     \t [0.07892163 0.8027183 ]. \t  0.8284596740740193 \t 1.018232062789341\n",
      "17     \t [0.43008179 0.5388358 ]. \t  -0.0777074541894186 \t 1.018232062789341\n",
      "18     \t [-0.13963323  0.59876753]. \t  0.9263504619925327 \t 1.018232062789341\n",
      "19     \t [-1.00981011  1.08670255]. \t  -2.0059396777206446 \t 1.018232062789341\n",
      "20     \t [0.18777925 0.49748522]. \t  0.5130927212989009 \t 1.018232062789341\n",
      "21     \t [-0.31840793  0.81692042]. \t  0.7637811614328618 \t 1.018232062789341\n",
      "22     \t [-2.59421844 -1.38180644]. \t  -43.94155432342252 \t 1.018232062789341\n",
      "23     \t [2.01348983 0.04922234]. \t  -4.001648681577594 \t 1.018232062789341\n",
      "24     \t [ 0.93794917 -1.57471347]. \t  -15.320850092271204 \t 1.018232062789341\n",
      "25     \t [-0.07489019  0.71567881]. \t  \u001b[92m1.030634172873397\u001b[0m \t 1.030634172873397\n",
      "26     \t [ 0.72465278 -0.70557673]. \t  -0.05839428172986938 \t 1.030634172873397\n",
      "27     \t [ 0.1203504  -0.57429792]. \t  0.8957721457123444 \t 1.030634172873397\n",
      "28     \t [-0.65148094  0.68896455]. \t  0.101377493494474 \t 1.030634172873397\n",
      "29     \t [-1.35471937  1.9619322 ]. \t  -43.538461102405925 \t 1.030634172873397\n",
      "30     \t [-1.93228055  1.18346564]. \t  -2.967104989893754 \t 1.030634172873397\n",
      "31     \t [-0.21800843  0.71297827]. \t  0.9697543451263169 \t 1.030634172873397\n",
      "32     \t [-1.43111299 -0.59190564]. \t  -2.1839003640556665 \t 1.030634172873397\n",
      "33     \t [-0.1175397   0.76424929]. \t  1.0066915011705022 \t 1.030634172873397\n",
      "34     \t [-1.52790622  0.91660941]. \t  -0.19653127920935298 \t 1.030634172873397\n",
      "35     \t [-0.00412997 -0.69620151]. \t  0.9961197002105225 \t 1.030634172873397\n",
      "36     \t [-2.42197371  1.84728455]. \t  -46.941052440101735 \t 1.030634172873397\n",
      "37     \t [-2.2038044  -1.05044206]. \t  -10.85069669643094 \t 1.030634172873397\n",
      "38     \t [-1.93926691 -0.45688236]. \t  -3.2972828228992768 \t 1.030634172873397\n",
      "39     \t [-1.71581059 -1.08432951]. \t  -4.7675326588951314 \t 1.030634172873397\n",
      "40     \t [ 0.16407685 -0.68002028]. \t  0.9997594657028358 \t 1.030634172873397\n",
      "41     \t [-0.78308704 -0.71217801]. \t  -1.2979775588026223 \t 1.030634172873397\n",
      "42     \t [-2.12005203  0.72207186]. \t  -3.2922894799006936 \t 1.030634172873397\n",
      "43     \t [ 0.00396788 -0.73537917]. \t  0.9962020647101746 \t 1.030634172873397\n",
      "44     \t [-1.9318373   0.86079393]. \t  -0.5751171370886103 \t 1.030634172873397\n",
      "45     \t [-0.07270525  0.71842764]. \t  1.0301061017031308 \t 1.030634172873397\n",
      "46     \t [2.57017079 1.79643139]. \t  -64.23765088309628 \t 1.030634172873397\n",
      "47     \t [ 0.22681119 -1.29316655]. \t  -4.403915479673336 \t 1.030634172873397\n",
      "48     \t [ 2.15982273 -1.25616497]. \t  -7.733379726496054 \t 1.030634172873397\n",
      "49     \t [ 1.86660992 -0.90445782]. \t  -0.25891182550108094 \t 1.030634172873397\n",
      "50     \t [ 2.24485755 -0.84218651]. \t  -6.770869564566761 \t 1.030634172873397\n",
      "51     \t [2.54033949 1.42592283]. \t  -39.96754511275637 \t 1.030634172873397\n",
      "52     \t [1.69541705 0.81596996]. \t  -2.5566819402525507 \t 1.030634172873397\n",
      "53     \t [-0.248762    1.33304453]. \t  -5.430951350760462 \t 1.030634172873397\n",
      "54     \t [-0.85430215 -0.38910705]. \t  -1.7488269490371118 \t 1.030634172873397\n",
      "55     \t [-2.88248936  0.38871025]. \t  -77.82619363645762 \t 1.030634172873397\n",
      "56     \t [-0.06082658  0.72249963]. \t  1.0272394153584885 \t 1.030634172873397\n",
      "57     \t [-2.79334878  0.95228897]. \t  -58.711832529534135 \t 1.030634172873397\n",
      "58     \t [-2.05051606  1.21270545]. \t  -4.752518000759231 \t 1.030634172873397\n",
      "59     \t [ 0.01148549 -0.95595997]. \t  0.32533342447844954 \t 1.030634172873397\n",
      "60     \t [-0.02611478  0.68642678]. \t  1.0118769606015303 \t 1.030634172873397\n",
      "61     \t [ 0.17929713 -0.7595871 ]. \t  0.9860620313161906 \t 1.030634172873397\n",
      "62     \t [-2.6966726  0.1213066]. \t  -45.837779417492754 \t 1.030634172873397\n",
      "63     \t [ 0.05568978 -0.74705507]. \t  1.0157197247992749 \t 1.030634172873397\n",
      "64     \t [ 0.05678926 -0.68515891]. \t  1.0222964284674123 \t 1.030634172873397\n",
      "65     \t [-0.15795444  0.74720813]. \t  1.0059235750052078 \t 1.030634172873397\n",
      "66     \t [1.65581427 1.1997202 ]. \t  -6.56684895211724 \t 1.030634172873397\n",
      "67     \t [-2.79092106  1.87072585]. \t  -91.04520775490309 \t 1.030634172873397\n",
      "68     \t [-2.72328735 -0.03660815]. \t  -50.22543373541524 \t 1.030634172873397\n",
      "69     \t [2.20967779 0.24191683]. \t  -8.58161737385623 \t 1.030634172873397\n",
      "70     \t [-0.0548514   1.10793357]. \t  -1.0683666874262419 \t 1.030634172873397\n",
      "71     \t [-0.06066418  0.6552652 ]. \t  1.0051059964855433 \t 1.030634172873397\n",
      "72     \t [2.41857944 1.72118125]. \t  -45.67790883537593 \t 1.030634172873397\n",
      "73     \t [-1.21721353  0.37638637]. \t  -1.4561829761753438 \t 1.030634172873397\n",
      "74     \t [ 0.08829875 -0.76730917]. \t  1.005177522522379 \t 1.030634172873397\n",
      "75     \t [2.31581916 0.26280664]. \t  -12.820353321665147 \t 1.030634172873397\n",
      "76     \t [ 0.26182785 -0.74040568]. \t  0.9201121748068555 \t 1.030634172873397\n",
      "77     \t [ 0.15703372 -0.67447123]. \t  1.0004164544659797 \t 1.030634172873397\n",
      "78     \t [-1.4930781  1.6574841]. \t  -18.899555094610104 \t 1.030634172873397\n",
      "79     \t [2.81309369 0.18958664]. \t  -65.7295870887448 \t 1.030634172873397\n",
      "80     \t [ 0.22770293 -0.76375639]. \t  0.9443425561417805 \t 1.030634172873397\n",
      "81     \t [0.46681699 1.07306979]. \t  -1.9740220329833469 \t 1.030634172873397\n",
      "82     \t [2.85242299 1.9098808 ]. \t  -117.14490381232577 \t 1.030634172873397\n",
      "83     \t [ 1.10182886 -1.37356323]. \t  -7.535483616365992 \t 1.030634172873397\n",
      "84     \t [ 0.07965184 -0.74731186]. \t  1.0205542341796388 \t 1.030634172873397\n",
      "85     \t [2.84489502 1.65781661]. \t  -95.46908078915511 \t 1.030634172873397\n",
      "86     \t [-1.08520893 -1.61539391]. \t  -20.895577754342526 \t 1.030634172873397\n",
      "87     \t [-0.66717693  1.60934127]. \t  -16.792151593581714 \t 1.030634172873397\n",
      "88     \t [-0.83344645 -0.7140815 ]. \t  -1.4725157938370281 \t 1.030634172873397\n",
      "89     \t [-0.14401812  0.65986007]. \t  0.9962826453892079 \t 1.030634172873397\n",
      "90     \t [-0.00116813  0.71396141]. \t  1.000449002627258 \t 1.030634172873397\n",
      "91     \t [ 0.97808965 -1.21057817]. \t  -3.7412707018661573 \t 1.030634172873397\n",
      "92     \t [-1.47511109 -1.23407059]. \t  -7.2009415769992575 \t 1.030634172873397\n",
      "93     \t [0.23459966 0.23693481]. \t  -0.05748087029356072 \t 1.030634172873397\n",
      "94     \t [-0.43418483  0.62553449]. \t  0.5426607798626597 \t 1.030634172873397\n",
      "95     \t [-0.08591447  0.71205314]. \t  \u001b[92m1.03156766146739\u001b[0m \t 1.03156766146739\n",
      "96     \t [-2.80059451 -0.98251403]. \t  -65.63850827229243 \t 1.03156766146739\n",
      "97     \t [-0.47859325 -0.19908632]. \t  -0.7530598121334977 \t 1.03156766146739\n",
      "98     \t [2.04633673 0.04001267]. \t  -4.477789381508556 \t 1.03156766146739\n",
      "99     \t [2.45543409 1.89781373]. \t  -62.976893431328726 \t 1.03156766146739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [0.78846872 1.53984025]. \t  -15.973491298829508 \t 1.03156766146739\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.24057642 1.87416265]. \t  -48.82879577505094 \t -0.03190064766213015\n",
      "init   \t [2.21516724 0.12342277]. \t  -8.660623551418242 \t -0.03190064766213015\n",
      "init   \t [-1.60363003 -1.95440478]. \t  -48.28332373939171 \t -0.03190064766213015\n",
      "init   \t [-0.41718709 -0.39059456]. \t  -0.2801229037700297 \t -0.03190064766213015\n",
      "init   \t [ 0.13604803 -0.08643282]. \t  -0.03190064766213015 \t -0.03190064766213015\n",
      "1      \t [-0.7915585   0.36468929]. \t  -1.0139151812710225 \t -0.03190064766213015\n",
      "2      \t [ 2.62087196 -1.3222448 ]. \t  -38.19228889832653 \t -0.03190064766213015\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "4      \t [ 0.36487142 -0.61183585]. \t  \u001b[92m0.6639918373230551\u001b[0m \t 0.6639918373230551\n",
      "5      \t [ 0.22539049 -0.69499916]. \t  \u001b[92m0.9576657826671826\u001b[0m \t 0.9576657826671826\n",
      "6      \t [ 0.36708218 -1.22260349]. \t  -3.011070611645343 \t 0.9576657826671826\n",
      "7      \t [-0.16577965  1.85141002]. \t  -33.08758207573004 \t 0.9576657826671826\n",
      "8      \t [ 0.04893066 -0.42729865]. \t  0.6083318766004618 \t 0.9576657826671826\n",
      "9      \t [ 0.12582291 -0.69206111]. \t  \u001b[92m1.0225038826294923\u001b[0m \t 1.0225038826294923\n",
      "10     \t [1.11345104 0.15166399]. \t  -2.4454854662367813 \t 1.0225038826294923\n",
      "11     \t [-1.49587996 -0.04322049]. \t  -2.227624776261556 \t 1.0225038826294923\n",
      "12     \t [-2.85174389 -0.66002883]. \t  -73.82578365948193 \t 1.0225038826294923\n",
      "13     \t [-1.09397978  0.04312097]. \t  -2.2961101124198096 \t 1.0225038826294923\n",
      "14     \t [-0.2289851   0.51182253]. \t  0.686540824828949 \t 1.0225038826294923\n",
      "15     \t [-0.00347317 -0.77295862]. \t  0.9592694103159312 \t 1.0225038826294923\n",
      "16     \t [-1.41251368  0.59112099]. \t  -0.5243123706545838 \t 1.0225038826294923\n",
      "17     \t [-0.80479565  0.72152936]. \t  -0.2213990230456927 \t 1.0225038826294923\n",
      "18     \t [ 1.07950036 -0.83532284]. \t  -0.5917436309775882 \t 1.0225038826294923\n",
      "19     \t [ 1.29444128 -0.45728857]. \t  -1.1210568439486899 \t 1.0225038826294923\n",
      "20     \t [-1.56876767  0.29600157]. \t  -1.3095444879031297 \t 1.0225038826294923\n",
      "21     \t [2.87690738 0.24906094]. \t  -78.72379852933588 \t 1.0225038826294923\n",
      "22     \t [1.74014107 0.23391415]. \t  -2.3121095532098135 \t 1.0225038826294923\n",
      "23     \t [0.29099247 0.5463768 ]. \t  0.35479239051485123 \t 1.0225038826294923\n",
      "24     \t [ 1.91336779 -0.25043255]. \t  -2.139547170037043 \t 1.0225038826294923\n",
      "25     \t [ 1.66947248 -1.93659216]. \t  -40.07945159497484 \t 1.0225038826294923\n",
      "26     \t [-0.01737654 -0.66394796]. \t  0.9732494693763833 \t 1.0225038826294923\n",
      "27     \t [ 0.67539255 -0.8171781 ]. \t  0.02001919246102657 \t 1.0225038826294923\n",
      "28     \t [-2.60021909  0.9563041 ]. \t  -31.2719439793236 \t 1.0225038826294923\n",
      "29     \t [ 1.90741339 -1.02250575]. \t  -1.0485644032316137 \t 1.0225038826294923\n",
      "30     \t [-1.52283517  1.92386952]. \t  -39.202584247376464 \t 1.0225038826294923\n",
      "31     \t [-2.93196646 -1.69547913]. \t  -117.48019821846701 \t 1.0225038826294923\n",
      "32     \t [-0.75263632 -1.5985435 ]. \t  -18.75342307967317 \t 1.0225038826294923\n",
      "33     \t [-0.45206572 -1.48669755]. \t  -12.104767100266098 \t 1.0225038826294923\n",
      "34     \t [ 1.03141014 -1.9831555 ]. \t  -46.373907674674726 \t 1.0225038826294923\n",
      "35     \t [0.79277552 1.43699572]. \t  -11.702790040325198 \t 1.0225038826294923\n",
      "36     \t [-1.09710754  1.5353724 ]. \t  -13.468246995670192 \t 1.0225038826294923\n",
      "37     \t [-2.69086437 -1.22676121]. \t  -51.74440884466302 \t 1.0225038826294923\n",
      "38     \t [-1.82488921 -0.98855805]. \t  -4.057300153008309 \t 1.0225038826294923\n",
      "39     \t [-2.76177044  1.9548925 ]. \t  -93.98362702954604 \t 1.0225038826294923\n",
      "40     \t [-0.42711216  0.88580295]. \t  0.3924045404793665 \t 1.0225038826294923\n",
      "41     \t [-2.00389929  1.19793775]. \t  -3.880520963644551 \t 1.0225038826294923\n",
      "42     \t [-0.43762129  0.67418825]. \t  0.5953997198965988 \t 1.0225038826294923\n",
      "43     \t [ 0.1059613  -0.73584125]. \t  \u001b[92m1.0264472648565535\u001b[0m \t 1.0264472648565535\n",
      "44     \t [-0.75156827  0.75964351]. \t  -0.10229366183722222 \t 1.0264472648565535\n",
      "45     \t [ 0.00341448 -0.72174587]. \t  1.0006676538931332 \t 1.0264472648565535\n",
      "46     \t [ 0.09425095 -0.65123438]. \t  1.0029725113892194 \t 1.0264472648565535\n",
      "47     \t [1.38817035 1.1798556 ]. \t  -6.116116966453971 \t 1.0264472648565535\n",
      "48     \t [ 0.15510418 -0.66959604]. \t  0.9981714277153473 \t 1.0264472648565535\n",
      "49     \t [ 0.09510098 -0.72671615]. \t  \u001b[92m1.0299440415408652\u001b[0m \t 1.0299440415408652\n",
      "50     \t [ 1.39689306 -1.92142211]. \t  -39.3538013909084 \t 1.0299440415408652\n",
      "51     \t [ 0.32859218 -0.0127359 ]. \t  -0.4029951711077447 \t 1.0299440415408652\n",
      "52     \t [ 0.14630184 -0.69764612]. \t  1.0167023381576823 \t 1.0299440415408652\n",
      "53     \t [-2.84223556 -0.84811832]. \t  -72.59955350318138 \t 1.0299440415408652\n",
      "54     \t [0.10933846 1.79458702]. \t  -28.849145298752564 \t 1.0299440415408652\n",
      "55     \t [ 1.68425434 -0.8699521 ]. \t  0.14417110895038465 \t 1.0299440415408652\n",
      "56     \t [-1.4244854  -0.52674357]. \t  -2.20336333382401 \t 1.0299440415408652\n",
      "57     \t [-2.55463205  1.28137014]. \t  -30.2575255844188 \t 1.0299440415408652\n",
      "58     \t [0.07337205 0.8489675 ]. \t  0.7213216136573339 \t 1.0299440415408652\n",
      "59     \t [-0.05213195  0.75526361]. \t  1.0086802474667467 \t 1.0299440415408652\n",
      "60     \t [ 0.97012635 -0.69282134]. \t  -0.5118467337645538 \t 1.0299440415408652\n",
      "61     \t [-0.09629965  0.69006861]. \t  1.0272724384237262 \t 1.0299440415408652\n",
      "62     \t [ 0.90131944 -0.04121331]. \t  -1.9983814024464437 \t 1.0299440415408652\n",
      "63     \t [-1.84978151  1.68859447]. \t  -20.44559432253321 \t 1.0299440415408652\n",
      "64     \t [ 0.10069779 -0.75084103]. \t  1.0190009304245184 \t 1.0299440415408652\n",
      "65     \t [-1.94219693 -1.65945027]. \t  -25.63984931620373 \t 1.0299440415408652\n",
      "66     \t [ 1.74209392 -0.79830464]. \t  0.20026084575978864 \t 1.0299440415408652\n",
      "67     \t [ 2.84732368 -1.41880092]. \t  -76.14126246054262 \t 1.0299440415408652\n",
      "68     \t [ 0.11170653 -0.72651241]. \t  1.028473253764476 \t 1.0299440415408652\n",
      "69     \t [-1.85453607  1.25378907]. \t  -3.749098678664283 \t 1.0299440415408652\n",
      "70     \t [-0.18158659  0.67245193]. \t  0.9833422299209952 \t 1.0299440415408652\n",
      "71     \t [-2.43737172 -0.01060075]. \t  -19.5625535312107 \t 1.0299440415408652\n",
      "72     \t [-0.13591375  0.65926674]. \t  0.9993361469746563 \t 1.0299440415408652\n",
      "73     \t [-0.08337479  0.76605364]. \t  1.0060020217980716 \t 1.0299440415408652\n",
      "74     \t [-1.15234777 -1.08191252]. \t  -4.434342465676329 \t 1.0299440415408652\n",
      "75     \t [0.4855699  0.88039393]. \t  -0.5609382439949269 \t 1.0299440415408652\n",
      "76     \t [-0.04056383  0.68071721]. \t  1.0156711908380995 \t 1.0299440415408652\n",
      "77     \t [ 0.17092523 -1.92118805]. \t  -39.51569133924025 \t 1.0299440415408652\n",
      "78     \t [-1.29268562  1.99943502]. \t  -47.72767416217624 \t 1.0299440415408652\n",
      "79     \t [-0.6379283  -1.97115382]. \t  -47.404963989223546 \t 1.0299440415408652\n",
      "80     \t [-0.015983    0.36855873]. \t  0.474405968954032 \t 1.0299440415408652\n",
      "81     \t [-0.17697495  1.45243478]. \t  -9.229007287194559 \t 1.0299440415408652\n",
      "82     \t [ 1.86029389 -1.63189365]. \t  -17.187688793382062 \t 1.0299440415408652\n",
      "83     \t [0.05649968 1.93521163]. \t  -41.24332959688418 \t 1.0299440415408652\n",
      "84     \t [1.76901617 0.77944067]. \t  -2.5926560214294785 \t 1.0299440415408652\n",
      "85     \t [ 0.56058972 -1.21033858]. \t  -3.1057681552973615 \t 1.0299440415408652\n",
      "86     \t [-0.01309983  0.71297119]. \t  1.008376023774279 \t 1.0299440415408652\n",
      "87     \t [-0.06073714  0.77421198]. \t  0.9927712045725299 \t 1.0299440415408652\n",
      "88     \t [-1.35760287 -1.11474798]. \t  -5.0452611426740095 \t 1.0299440415408652\n",
      "89     \t [-0.76963046 -1.55434442]. \t  -16.582089597861742 \t 1.0299440415408652\n",
      "90     \t [-2.76971627 -1.3347041 ]. \t  -66.85092440362372 \t 1.0299440415408652\n",
      "91     \t [-0.42433228 -0.76250859]. \t  -0.004167086705356149 \t 1.0299440415408652\n",
      "92     \t [-1.67424272 -0.46906555]. \t  -2.1524663184686608 \t 1.0299440415408652\n",
      "93     \t [-1.27523867 -0.69605985]. \t  -2.2734013795154975 \t 1.0299440415408652\n",
      "94     \t [2.31831579 1.73660466]. \t  -40.93072020895758 \t 1.0299440415408652\n",
      "95     \t [-0.05916504  0.72790852]. \t  1.0255261626276844 \t 1.0299440415408652\n",
      "96     \t [-0.11888172  0.64680051]. \t  0.9941135071804182 \t 1.0299440415408652\n",
      "97     \t [-0.85603745 -1.96652865]. \t  -47.971185074003685 \t 1.0299440415408652\n",
      "98     \t [-1.42667977  1.75155667]. \t  -25.13096446186454 \t 1.0299440415408652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.93228413 -1.23583275]. \t  -90.62857274863212 \t 1.0299440415408652\n",
      "100    \t [-0.29463268  1.09391463]. \t  -0.9506042993461121 \t 1.0299440415408652\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.35667346 1.04596796]. \t  -1.260209694806778 \t -1.260209694806778\n",
      "init   \t [-1.53281067  1.08675788]. \t  -1.318361654110369 \t -1.260209694806778\n",
      "init   \t [-0.87316751  1.1867896 ]. \t  -3.241714295197782 \t -1.260209694806778\n",
      "init   \t [-0.92399987 -0.22450226]. \t  -2.107788234046822 \t -1.260209694806778\n",
      "init   \t [-1.18017846 -0.23154432]. \t  -2.468366013151972 \t -1.260209694806778\n",
      "1      \t [-2.6937212   1.50277262]. \t  -53.123940891052406 \t -1.260209694806778\n",
      "2      \t [1.1791765  0.66235955]. \t  -2.1938988924775304 \t -1.260209694806778\n",
      "3      \t [-1.35556192  0.5882584 ]. \t  \u001b[92m-0.624963540344051\u001b[0m \t -0.624963540344051\n",
      "4      \t [-0.14296728 -1.09287656]. \t  -1.1657794508744628 \t -0.624963540344051\n",
      "5      \t [0.99535331 1.90215572]. \t  -42.01173315105823 \t -0.624963540344051\n",
      "6      \t [0.45523383 0.25106708]. \t  \u001b[92m-0.6197773365429832\u001b[0m \t -0.6197773365429832\n",
      "7      \t [0.44021484 0.65955151]. \t  \u001b[92m-0.005958538267673119\u001b[0m \t -0.005958538267673119\n",
      "8      \t [ 1.92528835 -0.2206769 ]. \t  -2.339713028365267 \t -0.005958538267673119\n",
      "9      \t [1.0932032  0.13744039]. \t  -2.4261337203904847 \t -0.005958538267673119\n",
      "10     \t [2.69616501 0.38574781]. \t  -46.68423672984286 \t -0.005958538267673119\n",
      "11     \t [ 2.10781668 -1.27926985]. \t  -7.02258077227836 \t -0.005958538267673119\n",
      "12     \t [-0.09520733 -0.45466171]. \t  \u001b[92m0.5765683938166284\u001b[0m \t 0.5765683938166284\n",
      "13     \t [-0.75292199 -2.        ]. \t  -51.159268013505766 \t 0.5765683938166284\n",
      "14     \t [ 0.47062336 -1.31517233]. \t  -5.21603275106072 \t 0.5765683938166284\n",
      "15     \t [-1.03172653  1.80781976]. \t  -30.067241323023197 \t 0.5765683938166284\n",
      "16     \t [-0.13393583  1.01235891]. \t  -0.03744666239657306 \t 0.5765683938166284\n",
      "17     \t [-0.77757188 -0.69795626]. \t  -1.2678375877035273 \t 0.5765683938166284\n",
      "18     \t [-2.62496851 -0.94779358]. \t  -39.02907970043853 \t 0.5765683938166284\n",
      "19     \t [-0.05640245 -0.73074754]. \t  \u001b[92m0.9414585313118096\u001b[0m \t 0.9414585313118096\n",
      "20     \t [-1.1697871   0.87749386]. \t  -0.6605386243031159 \t 0.9414585313118096\n",
      "21     \t [-1.73923605  0.76351821]. \t  0.18986702760846264 \t 0.9414585313118096\n",
      "22     \t [-1.50233709  0.78255906]. \t  -0.03777987735096633 \t 0.9414585313118096\n",
      "23     \t [ 1.92674847 -0.55628429]. \t  -1.0355656278777778 \t 0.9414585313118096\n",
      "24     \t [-1.96647175  0.31124575]. \t  -2.378575003427972 \t 0.9414585313118096\n",
      "25     \t [-0.30161947 -0.73949771]. \t  0.4214028973928532 \t 0.9414585313118096\n",
      "26     \t [ 2.97758847 -1.55812178]. \t  -111.92513088694884 \t 0.9414585313118096\n",
      "27     \t [ 1.6095449  -1.75203444]. \t  -24.656114571914493 \t 0.9414585313118096\n",
      "28     \t [ 1.64920897 -1.23686716]. \t  -3.253677787357346 \t 0.9414585313118096\n",
      "29     \t [2.00897948 1.5775873 ]. \t  -21.84135769623316 \t 0.9414585313118096\n",
      "30     \t [-0.30707288  0.03577979]. \t  -0.34268149842498496 \t 0.9414585313118096\n",
      "31     \t [ 1.82196605 -0.8294947 ]. \t  0.03923401947033911 \t 0.9414585313118096\n",
      "32     \t [2.61633004 1.91005909]. \t  -79.54102215180171 \t 0.9414585313118096\n",
      "33     \t [-0.16469302  0.51573154]. \t  0.7589172725673777 \t 0.9414585313118096\n",
      "34     \t [-0.85970772  1.10068474]. \t  -2.022515243888862 \t 0.9414585313118096\n",
      "35     \t [2.98561421 0.70523437]. \t  -105.99224564957036 \t 0.9414585313118096\n",
      "36     \t [-0.03892457  0.75567245]. \t  \u001b[92m1.0031713457562699\u001b[0m \t 1.0031713457562699\n",
      "37     \t [ 1.95780413 -1.37790233]. \t  -7.377192974630941 \t 1.0031713457562699\n",
      "38     \t [ 1.92312212 -0.95400459]. \t  -0.770036529519903 \t 1.0031713457562699\n",
      "39     \t [ 0.92176742 -0.69056286]. \t  -0.45266112060336383 \t 1.0031713457562699\n",
      "40     \t [ 1.40510009 -1.35368486]. \t  -6.476637162373654 \t 1.0031713457562699\n",
      "41     \t [ 1.41198193 -1.63313684]. \t  -17.74913813628892 \t 1.0031713457562699\n",
      "42     \t [-0.3615254  -1.73951775]. \t  -25.637680752411736 \t 1.0031713457562699\n",
      "43     \t [-0.85257763 -0.51197168]. \t  -1.5888576668774759 \t 1.0031713457562699\n",
      "44     \t [-1.37633581 -0.83965887]. \t  -2.631232565221661 \t 1.0031713457562699\n",
      "45     \t [ 0.62029188 -0.52614174]. \t  -0.12001388081785569 \t 1.0031713457562699\n",
      "46     \t [-0.13963214  0.73526178]. \t  \u001b[92m1.0188768065018763\u001b[0m \t 1.0188768065018763\n",
      "47     \t [1.16135382 0.68659946]. \t  -2.1933345180377493 \t 1.0188768065018763\n",
      "48     \t [-0.19770199  0.7174539 ]. \t  0.9878169811127523 \t 1.0188768065018763\n",
      "49     \t [-0.35546651 -0.74355793]. \t  0.25193582438135464 \t 1.0188768065018763\n",
      "50     \t [ 1.50646609 -1.33221999]. \t  -5.65177010784323 \t 1.0188768065018763\n",
      "51     \t [2.08783872 0.44943854]. \t  -5.436344038665195 \t 1.0188768065018763\n",
      "52     \t [-1.45305386 -0.34856998]. \t  -2.3008774639519785 \t 1.0188768065018763\n",
      "53     \t [1.65376027 0.95022269]. \t  -3.271828619864523 \t 1.0188768065018763\n",
      "54     \t [2.20044236 0.12277229]. \t  -8.184137643768748 \t 1.0188768065018763\n",
      "55     \t [-0.27248587 -1.68949914]. \t  -21.918853808438246 \t 1.0188768065018763\n",
      "56     \t [-2.9649158   0.71469175]. \t  -96.20285419413186 \t 1.0188768065018763\n",
      "57     \t [-0.03908832  1.52429823]. \t  -12.246916075350724 \t 1.0188768065018763\n",
      "58     \t [ 0.06118469 -0.60839842]. \t  0.9548343161575438 \t 1.0188768065018763\n",
      "59     \t [-2.81943489 -1.60024499]. \t  -87.03388190982164 \t 1.0188768065018763\n",
      "60     \t [-0.07933293  0.76480393]. \t  1.0067333196639645 \t 1.0188768065018763\n",
      "61     \t [ 0.14353488 -0.71219178]. \t  \u001b[92m1.0204954013647223\u001b[0m \t 1.0204954013647223\n",
      "62     \t [-1.58257969  0.18530123]. \t  -1.6563179025136354 \t 1.0204954013647223\n",
      "63     \t [-1.58032713  0.8188882 ]. \t  0.09374647447396878 \t 1.0204954013647223\n",
      "64     \t [-2.60818333  0.03771494]. \t  -34.85945773019259 \t 1.0204954013647223\n",
      "65     \t [-2.55474319 -1.27128561]. \t  -36.557144218172 \t 1.0204954013647223\n",
      "66     \t [-0.03789537 -1.65611557]. \t  -19.187648864389928 \t 1.0204954013647223\n",
      "67     \t [ 2.69738938 -0.55125018]. \t  -43.99173716040437 \t 1.0204954013647223\n",
      "68     \t [0.4258326  0.81592786]. \t  -0.11559474197424568 \t 1.0204954013647223\n",
      "69     \t [ 1.1347592  -0.26344266]. \t  -1.823091754041529 \t 1.0204954013647223\n",
      "70     \t [-2.5967251  -1.10896766]. \t  -37.69588921345289 \t 1.0204954013647223\n",
      "71     \t [-0.10400095  0.6400702 ]. \t  0.99092468711703 \t 1.0204954013647223\n",
      "72     \t [ 0.11557289 -0.70501129]. \t  \u001b[92m1.0283906665434865\u001b[0m \t 1.0283906665434865\n",
      "73     \t [-0.05610261  0.7138529 ]. \t  1.027112230628762 \t 1.0283906665434865\n",
      "74     \t [0.82403448 0.31942618]. \t  -1.7489445201560758 \t 1.0283906665434865\n",
      "75     \t [ 0.15774598 -0.77761722]. \t  0.9805872426076191 \t 1.0283906665434865\n",
      "76     \t [ 0.15308587 -0.67024391]. \t  0.9997011589866988 \t 1.0283906665434865\n",
      "77     \t [-1.86597671  0.33298528]. \t  -1.5233391741694335 \t 1.0283906665434865\n",
      "78     \t [-0.37470388 -1.86731947]. \t  -35.90668535930074 \t 1.0283906665434865\n",
      "79     \t [-1.48232832  1.24797864]. \t  -3.809308878301353 \t 1.0283906665434865\n",
      "80     \t [-0.07080778  0.42071336]. \t  0.5924708603077229 \t 1.0283906665434865\n",
      "81     \t [0.21493049 0.85901793]. \t  0.4086308083959589 \t 1.0283906665434865\n",
      "82     \t [ 2.33954439 -0.84215168]. \t  -10.84472818874074 \t 1.0283906665434865\n",
      "83     \t [-0.14517488  0.6584975 ]. \t  0.9945982160404532 \t 1.0283906665434865\n",
      "84     \t [-2.66400556  1.14826982]. \t  -40.38804091939894 \t 1.0283906665434865\n",
      "85     \t [-0.35692482  1.8012942 ]. \t  -28.96591328040761 \t 1.0283906665434865\n",
      "86     \t [-1.79036479  1.36361529]. \t  -6.174035781723444 \t 1.0283906665434865\n",
      "87     \t [-1.34978112 -1.60704806]. \t  -20.850960601027268 \t 1.0283906665434865\n",
      "88     \t [-0.2051117  -1.31876297]. \t  -5.5769146212012695 \t 1.0283906665434865\n",
      "89     \t [-0.83881349  0.88334614]. \t  -0.46421823057038614 \t 1.0283906665434865\n",
      "90     \t [ 0.01520152 -0.6470992 ]. \t  0.9824981953250173 \t 1.0283906665434865\n",
      "91     \t [-0.09334287  0.64079587]. \t  0.9931657537994472 \t 1.0283906665434865\n",
      "92     \t [2.66476303 1.48070808]. \t  -56.27023939005207 \t 1.0283906665434865\n",
      "93     \t [ 1.29508841 -0.30486184]. \t  -1.6421082069525126 \t 1.0283906665434865\n",
      "94     \t [1.30075989 0.18903078]. \t  -2.4787070683515418 \t 1.0283906665434865\n",
      "95     \t [-1.36987247 -0.74622182]. \t  -2.3490503986980915 \t 1.0283906665434865\n",
      "96     \t [-1.28869068 -1.9300101 ]. \t  -45.465960802926766 \t 1.0283906665434865\n",
      "97     \t [1.79005905 0.64195341]. \t  -2.4021734601384312 \t 1.0283906665434865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-2.21651463  0.47165672]. \t  -6.754605027893274 \t 1.0283906665434865\n",
      "99     \t [-2.96590443  1.52060883]. \t  -107.20873029801652 \t 1.0283906665434865\n",
      "100    \t [ 0.15531981 -0.63170859]. \t  0.9620783809565833 \t 1.0283906665434865\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.42310371 0.25811502]. \t  -0.5111512856704337 \t 0.9146183273252478\n",
      "init   \t [-0.069349   -0.65408899]. \t  0.9146183273252478 \t 0.9146183273252478\n",
      "init   \t [-0.74479093  0.12814348]. \t  -1.4695211123708676 \t 0.9146183273252478\n",
      "init   \t [-2.59136227  0.33811624]. \t  -31.819735064764004 \t 0.9146183273252478\n",
      "init   \t [-1.57261342 -1.35697367]. \t  -10.421507729581124 \t 0.9146183273252478\n",
      "1      \t [-0.1410994  -0.15343052]. \t  -0.008508372031263561 \t 0.9146183273252478\n",
      "2      \t [ 0.5005314  -1.42718009]. \t  -8.608756717094366 \t 0.9146183273252478\n",
      "3      \t [1.38215761 0.87577001]. \t  -2.7970299264195067 \t 0.9146183273252478\n",
      "4      \t [-0.39610392  1.66004094]. \t  -19.273014065521274 \t 0.9146183273252478\n",
      "5      \t [1.02776732 0.0387556 ]. \t  -2.3087827753134285 \t 0.9146183273252478\n",
      "6      \t [-0.4732578 -0.7758918]. \t  -0.20311220017100673 \t 0.9146183273252478\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "8      \t [0.91712204 0.80482995]. \t  -1.902565927851525 \t 0.9146183273252478\n",
      "9      \t [1.09063056 0.59591867]. \t  -2.08157898064945 \t 0.9146183273252478\n",
      "10     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "11     \t [ 0.34255669 -0.41440248]. \t  0.26990781880783854 \t 0.9146183273252478\n",
      "12     \t [-0.28548568 -1.26341423]. \t  -4.479679859597242 \t 0.9146183273252478\n",
      "13     \t [1.03096635 1.82076077]. \t  -34.85724569420337 \t 0.9146183273252478\n",
      "14     \t [-0.06424198  0.66310979]. \t  \u001b[92m1.0115897831405252\u001b[0m \t 1.0115897831405252\n",
      "15     \t [1.73875072 0.07913889]. \t  -2.2225003429877965 \t 1.0115897831405252\n",
      "16     \t [0.05067324 0.56896764]. \t  0.8366186269448552 \t 1.0115897831405252\n",
      "17     \t [-0.54685574  0.71234399]. \t  0.3720150607136654 \t 1.0115897831405252\n",
      "18     \t [-2.45443226 -1.76743731]. \t  -51.63710217674941 \t 1.0115897831405252\n",
      "19     \t [-0.3539167  0.6164445]. \t  0.6918380695327568 \t 1.0115897831405252\n",
      "20     \t [-2.93308739  1.92957439]. \t  -126.12625451200103 \t 1.0115897831405252\n",
      "21     \t [-1.46468038 -0.45383003]. \t  -2.2180126657296784 \t 1.0115897831405252\n",
      "22     \t [-1.49665459  0.0429097 ]. \t  -2.0979520264098555 \t 1.0115897831405252\n",
      "23     \t [ 0.10172959 -0.50067633]. \t  0.7611142455075717 \t 1.0115897831405252\n",
      "24     \t [2.40576203 0.22100924]. \t  -17.77628783602815 \t 1.0115897831405252\n",
      "25     \t [ 1.15768582 -0.75681001]. \t  -0.5363429178601061 \t 1.0115897831405252\n",
      "26     \t [ 1.24532974 -0.34713449]. \t  -1.5397272611429655 \t 1.0115897831405252\n",
      "27     \t [ 0.21042367 -0.87409273]. \t  0.7320410593980813 \t 1.0115897831405252\n",
      "28     \t [-0.1249432   0.69344902]. \t  \u001b[92m1.0232454309393322\u001b[0m \t 1.0232454309393322\n",
      "29     \t [ 0.73655129 -0.67690304]. \t  -0.11360890499210385 \t 1.0232454309393322\n",
      "30     \t [-1.66145743 -0.36139022]. \t  -2.1974947754210867 \t 1.0232454309393322\n",
      "31     \t [0.63498595 0.24208286]. \t  -1.226310123768032 \t 1.0232454309393322\n",
      "32     \t [1.76454506 1.75355096]. \t  -30.773058715602147 \t 1.0232454309393322\n",
      "33     \t [-0.70063536 -1.49000954]. \t  -13.376267448872674 \t 1.0232454309393322\n",
      "34     \t [-1.76588361 -0.05751031]. \t  -2.248865967433256 \t 1.0232454309393322\n",
      "35     \t [-0.042137   -0.70719929]. \t  0.9631051838337794 \t 1.0232454309393322\n",
      "36     \t [-1.20861921  1.21160752]. \t  -3.684685961525207 \t 1.0232454309393322\n",
      "37     \t [1.93528781 1.15383591]. \t  -7.0336234600326355 \t 1.0232454309393322\n",
      "38     \t [ 0.01025272 -0.79312933]. \t  0.9410913960677343 \t 1.0232454309393322\n",
      "39     \t [-0.21181931  1.31563517]. \t  -4.957014144297905 \t 1.0232454309393322\n",
      "40     \t [ 0.09041254 -0.71871263]. \t  \u001b[92m1.0313277428674157\u001b[0m \t 1.0313277428674157\n",
      "41     \t [0.33567907 0.84587171]. \t  0.10576419435501183 \t 1.0313277428674157\n",
      "42     \t [-0.12862961  0.64521919]. \t  0.9893678250213382 \t 1.0313277428674157\n",
      "43     \t [ 0.08158329 -0.62395067]. \t  0.9753683746920428 \t 1.0313277428674157\n",
      "44     \t [-0.10882415  0.73852438]. \t  1.025041203582134 \t 1.0313277428674157\n",
      "45     \t [ 0.220423   -0.67723323]. \t  0.9530106735638181 \t 1.0313277428674157\n",
      "46     \t [-1.26778253  0.79166584]. \t  -0.4487301246208374 \t 1.0313277428674157\n",
      "47     \t [ 1.86169714 -1.66893029]. \t  -19.299309473598427 \t 1.0313277428674157\n",
      "48     \t [2.22582705 0.85300953]. \t  -9.913042978039233 \t 1.0313277428674157\n",
      "49     \t [-2.73300823  0.36249973]. \t  -50.17615223964773 \t 1.0313277428674157\n",
      "50     \t [-1.93294282  0.71893414]. \t  -0.6269092246740491 \t 1.0313277428674157\n",
      "51     \t [-1.7739137   0.54186476]. \t  -0.38830279172706494 \t 1.0313277428674157\n",
      "52     \t [2.86596799 1.35194377]. \t  -85.81964848600569 \t 1.0313277428674157\n",
      "53     \t [ 2.50809148 -1.9411586 ]. \t  -61.890306381826484 \t 1.0313277428674157\n",
      "54     \t [0.68684727 1.576156  ]. \t  -17.28651740726995 \t 1.0313277428674157\n",
      "55     \t [ 1.6876843  -1.06361766]. \t  -0.8578843170988343 \t 1.0313277428674157\n",
      "56     \t [1.61908395 0.84616162]. \t  -2.6160829030643655 \t 1.0313277428674157\n",
      "57     \t [ 1.73465985 -0.76689056]. \t  0.19554113814734309 \t 1.0313277428674157\n",
      "58     \t [ 0.08015378 -0.71414736]. \t  1.0312291710560308 \t 1.0313277428674157\n",
      "59     \t [ 0.18133602 -0.76305834]. \t  0.9820322506520267 \t 1.0313277428674157\n",
      "60     \t [-0.95114137 -0.52735464]. \t  -1.8453259498234824 \t 1.0313277428674157\n",
      "61     \t [-0.16063737  0.72765648]. \t  1.011586740702901 \t 1.0313277428674157\n",
      "62     \t [-1.37697643  1.99735281]. \t  -47.26063798593281 \t 1.0313277428674157\n",
      "63     \t [ 0.83770616 -1.62121248]. \t  -17.64909715689385 \t 1.0313277428674157\n",
      "64     \t [0.65001593 1.09752317]. \t  -3.039336773075536 \t 1.0313277428674157\n",
      "65     \t [1.73840271 1.06707025]. \t  -4.595694977471223 \t 1.0313277428674157\n",
      "66     \t [ 0.19764621 -0.70416827]. \t  0.9860360228473105 \t 1.0313277428674157\n",
      "67     \t [0.12038614 0.56886193]. \t  0.7495233589753024 \t 1.0313277428674157\n",
      "68     \t [ 0.11798213 -0.78541251]. \t  0.9827544844772327 \t 1.0313277428674157\n",
      "69     \t [1.3353567  1.04700935]. \t  -4.165381765206539 \t 1.0313277428674157\n",
      "70     \t [0.00615685 0.23329082]. \t  0.20426231579659207 \t 1.0313277428674157\n",
      "71     \t [0.87655999 0.96016329]. \t  -2.5385368155372108 \t 1.0313277428674157\n",
      "72     \t [2.96244727 0.78664517]. \t  -100.06067106051304 \t 1.0313277428674157\n",
      "73     \t [-0.92118385 -1.14758204]. \t  -4.8125465261924765 \t 1.0313277428674157\n",
      "74     \t [0.01525143 0.69519879]. \t  0.9873515367409686 \t 1.0313277428674157\n",
      "75     \t [0.56516436 1.69716506]. \t  -23.698064532884224 \t 1.0313277428674157\n",
      "76     \t [-2.88492799 -0.52827373]. \t  -80.71660737594533 \t 1.0313277428674157\n",
      "77     \t [ 1.69156201 -0.93166013]. \t  -0.0266658478363333 \t 1.0313277428674157\n",
      "78     \t [-0.74782999  1.64496886]. \t  -18.872726333510375 \t 1.0313277428674157\n",
      "79     \t [-1.55413022 -0.38824081]. \t  -2.198501298752952 \t 1.0313277428674157\n",
      "80     \t [ 0.52780801 -0.00876636]. \t  -0.9536217085611318 \t 1.0313277428674157\n",
      "81     \t [-2.56522298 -1.49674733]. \t  -45.32165847174537 \t 1.0313277428674157\n",
      "82     \t [-0.1538417   0.74760567]. \t  1.0076321782521729 \t 1.0313277428674157\n",
      "83     \t [ 0.15159605 -1.21007528]. \t  -2.626738233352941 \t 1.0313277428674157\n",
      "84     \t [ 0.07384001 -0.71883278]. \t  1.0302132968791833 \t 1.0313277428674157\n",
      "85     \t [-1.54479312  1.77040155]. \t  -26.140109675074886 \t 1.0313277428674157\n",
      "86     \t [-1.25157199 -1.40329153]. \t  -11.784961769668035 \t 1.0313277428674157\n",
      "87     \t [-1.81141394 -0.5683477 ]. \t  -2.4458799782390996 \t 1.0313277428674157\n",
      "88     \t [-2.09444163  1.19667487]. \t  -5.242459497536333 \t 1.0313277428674157\n",
      "89     \t [-2.75848662  1.78661372]. \t  -78.76463662692397 \t 1.0313277428674157\n",
      "90     \t [-0.00709024 -0.69535786]. \t  0.9937826376602398 \t 1.0313277428674157\n",
      "91     \t [-0.06683585  0.6538729 ]. \t  1.0048797576315551 \t 1.0313277428674157\n",
      "92     \t [2.34358413 0.81141137]. \t  -14.850598955553561 \t 1.0313277428674157\n",
      "93     \t [-0.06449487  0.71157144]. \t  1.0291301921296678 \t 1.0313277428674157\n",
      "94     \t [1.74749468 0.86841508]. \t  -2.9000546789739157 \t 1.0313277428674157\n",
      "95     \t [ 1.83828023 -0.60114099]. \t  -0.37116675153433265 \t 1.0313277428674157\n",
      "96     \t [2.30985615 1.83555002]. \t  -48.35912842854276 \t 1.0313277428674157\n",
      "97     \t [0.88287554 1.78244895]. \t  -31.241494035851236 \t 1.0313277428674157\n",
      "98     \t [-2.91923265 -1.74870989]. \t  -118.15330446754632 \t 1.0313277428674157\n",
      "99     \t [ 0.16676387 -0.67378003]. \t  0.994266858801935 \t 1.0313277428674157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-1.05347936  1.81735804]. \t  -30.816346862556998 \t 1.0313277428674157\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.67302105 -1.32372098]. \t  -5.793449752432556 \t -0.8736935954900025\n",
      "init   \t [-0.38364588  1.07704989]. \t  -0.8736935954900025 \t -0.8736935954900025\n",
      "init   \t [-1.22804817 -1.40334817]. \t  -11.759316761133794 \t -0.8736935954900025\n",
      "init   \t [-2.86513005 -0.31910203]. \t  -76.26436708944966 \t -0.8736935954900025\n",
      "init   \t [-1.56790715 -0.64937523]. \t  -2.1371114994016214 \t -0.8736935954900025\n",
      "1      \t [-0.72039453 -0.1371268 ]. \t  -1.5818597673824353 \t -0.8736935954900025\n",
      "2      \t [1.59677188 2.        ]. \t  -53.26548510953384 \t -0.8736935954900025\n",
      "3      \t [ 2.90216003 -2.        ]. \t  -126.07597340186048 \t -0.8736935954900025\n",
      "4      \t [-1.73176739  2.        ]. \t  -46.63607340504773 \t -0.8736935954900025\n",
      "5      \t [3.         0.55568107]. \t  -109.71330134504963 \t -0.8736935954900025\n",
      "6      \t [-3. -2.]. \t  -162.89999999999998 \t -0.8736935954900025\n",
      "7      \t [0.77475837 0.22202288]. \t  -1.7010179825560732 \t -0.8736935954900025\n",
      "8      \t [0.0678822 2.       ]. \t  -48.154151803120946 \t -0.8736935954900025\n",
      "9      \t [-0.04021906 -2.        ]. \t  -48.08690290963544 \t -0.8736935954900025\n",
      "10     \t [-3.  2.]. \t  -150.89999999999998 \t -0.8736935954900025\n",
      "11     \t [-1.38655449  0.69675334]. \t  \u001b[92m-0.331656816653472\u001b[0m \t -0.331656816653472\n",
      "12     \t [ 1.36303276 -0.65251878]. \t  -0.453175150372235 \t -0.331656816653472\n",
      "13     \t [3. 2.]. \t  -162.89999999999998 \t -0.331656816653472\n",
      "14     \t [ 1.3058486 -2.       ]. \t  -47.75564837862806 \t -0.331656816653472\n",
      "15     \t [ 0.6039478  -0.53928768]. \t  \u001b[92m-0.045098787130278706\u001b[0m \t -0.045098787130278706\n",
      "16     \t [1.5409261  0.54402761]. \t  -2.1251909654764676 \t -0.045098787130278706\n",
      "17     \t [-0.97176448  1.26086032]. \t  -4.710451573041335 \t -0.045098787130278706\n",
      "18     \t [0.9279524  1.08171494]. \t  -3.9000472681475014 \t -0.045098787130278706\n",
      "19     \t [-0.8569109   0.57114958]. \t  -0.5682443101822141 \t -0.045098787130278706\n",
      "20     \t [-0.83508133 -0.87595533]. \t  -1.8985191208622616 \t -0.045098787130278706\n",
      "21     \t [-1.42609375 -0.04169305]. \t  -2.3055853043270864 \t -0.045098787130278706\n",
      "22     \t [ 2.35043352 -0.45912351]. \t  -12.464401585462562 \t -0.045098787130278706\n",
      "23     \t [ 1.71072843 -0.10970865]. \t  -1.8401085394692955 \t -0.045098787130278706\n",
      "24     \t [0.03732301 0.51538923]. \t  \u001b[92m0.75547158744904\u001b[0m \t 0.75547158744904\n",
      "25     \t [-3.          0.69274223]. \t  -105.82339068391235 \t 0.75547158744904\n",
      "26     \t [ 3.         -0.67414201]. \t  -105.88586681392468 \t 0.75547158744904\n",
      "27     \t [ 1.85719491 -0.84121354]. \t  -0.10161804378537964 \t 0.75547158744904\n",
      "28     \t [-1.18477312 -0.63557615]. \t  -2.188862689393435 \t 0.75547158744904\n",
      "29     \t [-0.03401906 -0.68745543]. \t  \u001b[92m0.9689828936974032\u001b[0m \t 0.9689828936974032\n",
      "30     \t [-0.84624935  2.        ]. \t  -48.21748526716927 \t 0.9689828936974032\n",
      "31     \t [ 1.23873182 -1.08265175]. \t  -1.8635241402126657 \t 0.9689828936974032\n",
      "32     \t [-1.9991562   0.18760929]. \t  -3.211662676632231 \t 0.9689828936974032\n",
      "33     \t [ 1.87177493 -0.54550368]. \t  -0.7150179916878657 \t 0.9689828936974032\n",
      "34     \t [-1.74288073 -2.        ]. \t  -53.602107162061415 \t 0.9689828936974032\n",
      "35     \t [ 0.51449233 -0.91633686]. \t  0.09208631126837946 \t 0.9689828936974032\n",
      "36     \t [-0.35271352  0.67256018]. \t  0.7623667873128989 \t 0.9689828936974032\n",
      "37     \t [0.29893803 0.92451914]. \t  -0.12064667928716205 \t 0.9689828936974032\n",
      "38     \t [-0.17818989 -0.26637047]. \t  0.09131088583748423 \t 0.9689828936974032\n",
      "39     \t [2.07518189 0.31469432]. \t  -5.197833499201545 \t 0.9689828936974032\n",
      "40     \t [-1.84653411 -1.25479132]. \t  -8.373083392850646 \t 0.9689828936974032\n",
      "41     \t [0.92063797 2.        ]. \t  -51.925931270766476 \t 0.9689828936974032\n",
      "42     \t [1.84358141 1.21946326]. \t  -7.5693610015965795 \t 0.9689828936974032\n",
      "43     \t [ 2.08856327 -2.        ]. \t  -48.97985739712391 \t 0.9689828936974032\n",
      "44     \t [-2.11962383  1.20887434]. \t  -5.946257302544996 \t 0.9689828936974032\n",
      "45     \t [-1.85040443  0.85291915]. \t  -0.08552911975612898 \t 0.9689828936974032\n",
      "46     \t [-1.72030325  1.17128512]. \t  -2.1111698537026644 \t 0.9689828936974032\n",
      "47     \t [-3.         -1.17296163]. \t  -114.48726053870347 \t 0.9689828936974032\n",
      "48     \t [-0.35128629 -0.59568303]. \t  0.24420132109723536 \t 0.9689828936974032\n",
      "49     \t [ 0.00394895 -1.07516462]. \t  -0.7170502306959832 \t 0.9689828936974032\n",
      "50     \t [-1.47035192 -1.02373768]. \t  -3.90732686403523 \t 0.9689828936974032\n",
      "51     \t [-2.13427186 -0.65631364]. \t  -6.572154565391067 \t 0.9689828936974032\n",
      "52     \t [1.51681241 1.07053676]. \t  -4.439720828675692 \t 0.9689828936974032\n",
      "53     \t [-0.87481755 -2.        ]. \t  -51.73031441536085 \t 0.9689828936974032\n",
      "54     \t [0.53264515 0.70993045]. \t  -0.3516277382077544 \t 0.9689828936974032\n",
      "55     \t [ 2.46287374 -1.27071729]. \t  -22.23081790968592 \t 0.9689828936974032\n",
      "56     \t [3.         1.29776165]. \t  -117.40246413663338 \t 0.9689828936974032\n",
      "57     \t [1.99069788 0.8236433 ]. \t  -4.384058841103383 \t 0.9689828936974032\n",
      "58     \t [ 1.89105425 -1.38259854]. \t  -7.0485008208071385 \t 0.9689828936974032\n",
      "59     \t [-0.86687451  0.89976964]. \t  -0.5648355793925927 \t 0.9689828936974032\n",
      "60     \t [-2.34580016  2.        ]. \t  -57.272738498308044 \t 0.9689828936974032\n",
      "61     \t [2.34079659 2.        ]. \t  -66.38589137274425 \t 0.9689828936974032\n",
      "62     \t [-0.12357633  0.68870105]. \t  \u001b[92m1.021871177088366\u001b[0m \t 1.021871177088366\n",
      "63     \t [-0.0631177   0.72537512]. \t  \u001b[92m1.027142660930498\u001b[0m \t 1.027142660930498\n",
      "64     \t [-2.394013 -2.      ]. \t  -69.48635537529105 \t 1.027142660930498\n",
      "65     \t [-0.09959853  0.71084796]. \t  \u001b[92m1.0312137012431786\u001b[0m \t 1.0312137012431786\n",
      "66     \t [ 0.0311237  -0.68787414]. \t  1.0146572026815428 \t 1.0312137012431786\n",
      "67     \t [-0.13658883  0.62171955]. \t  0.9595256423132337 \t 1.0312137012431786\n",
      "68     \t [ 0.05526671 -0.71793523]. \t  1.0265274011257572 \t 1.0312137012431786\n",
      "69     \t [-0.08627193  0.75742197]. \t  1.0139693537690022 \t 1.0312137012431786\n",
      "70     \t [-0.01113098  0.67541766]. \t  0.9993448887927233 \t 1.0312137012431786\n",
      "71     \t [-0.16437794  0.61344604]. \t  0.9330922951279681 \t 1.0312137012431786\n",
      "72     \t [-0.12988261  0.68633578]. \t  1.0189102949372086 \t 1.0312137012431786\n",
      "73     \t [ 0.0381787  -0.73015278]. \t  1.0176617365562917 \t 1.0312137012431786\n",
      "74     \t [-0.06607737  0.72973884]. \t  1.0265644720881038 \t 1.0312137012431786\n",
      "75     \t [-3.          1.38595674]. \t  -111.81768505617234 \t 1.0312137012431786\n",
      "76     \t [-0.05513234  0.70401147]. \t  1.0265985796844748 \t 1.0312137012431786\n",
      "77     \t [-0.01856034 -0.63588117]. \t  0.9502205153629615 \t 1.0312137012431786\n",
      "78     \t [-0.08203848 -0.67599403]. \t  0.9103092078279882 \t 1.0312137012431786\n",
      "79     \t [-0.0541573   0.69713328]. \t  1.0252562778896799 \t 1.0312137012431786\n",
      "80     \t [ 0.09994141 -0.74690472]. \t  1.021508541923002 \t 1.0312137012431786\n",
      "81     \t [-1.72109994  0.48517722]. \t  -0.5311635779456577 \t 1.0312137012431786\n",
      "82     \t [ 0.05763878 -0.70425293]. \t  1.0272616380887012 \t 1.0312137012431786\n",
      "83     \t [ 3.         -1.41725063]. \t  -112.7517360004694 \t 1.0312137012431786\n",
      "84     \t [-0.04728684  0.69423859]. \t  1.0225939397640997 \t 1.0312137012431786\n",
      "85     \t [ 0.67038718 -2.        ]. \t  -48.063006457176414 \t 1.0312137012431786\n",
      "86     \t [-0.15249239  0.71418392]. \t  1.016618563624927 \t 1.0312137012431786\n",
      "87     \t [-0.02600046 -0.73076718]. \t  0.9736669586444895 \t 1.0312137012431786\n",
      "88     \t [-0.04858827  0.7374395 ]. \t  1.0187196056831938 \t 1.0312137012431786\n",
      "89     \t [-0.0958376   0.67898215]. \t  1.0224307654321658 \t 1.0312137012431786\n",
      "90     \t [-0.05184565  0.73554289]. \t  1.0206663219045626 \t 1.0312137012431786\n",
      "91     \t [ 0.05704518 -0.68486528]. \t  1.0222399067962002 \t 1.0312137012431786\n",
      "92     \t [-0.04036316 -0.70603115]. \t  0.9649819427841357 \t 1.0312137012431786\n",
      "93     \t [-0.1360731   0.67988406]. \t  1.0134656544870404 \t 1.0312137012431786\n",
      "94     \t [ 0.07459151 -0.69242536]. \t  1.0277696949284703 \t 1.0312137012431786\n",
      "95     \t [ 0.16742581 -0.77859141]. \t  0.974755794210209 \t 1.0312137012431786\n",
      "96     \t [ 0.0180324  -0.71899305]. \t  1.0105153734569523 \t 1.0312137012431786\n",
      "97     \t [-0.10992234  0.65427943]. \t  1.0032052051799574 \t 1.0312137012431786\n",
      "98     \t [-0.05585918  0.680449  ]. \t  1.020075974446887 \t 1.0312137012431786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [2.43537279 1.23607838]. \t  -25.634379278665175 \t 1.0312137012431786\n",
      "100    \t [-0.01501596  0.66646933]. \t  0.9966429847841853 \t 1.0312137012431786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.11191296 -1.7043856 ]. \t  -24.175973303775706 \t -2.2733752808550527\n",
      "init   \t [ 2.36959058 -0.25402581]. \t  -14.417173913696448 \t -2.2733752808550527\n",
      "init   \t [-2.23393359  0.3034315 ]. \t  -8.078384314238003 \t -2.2733752808550527\n",
      "init   \t [ 2.04282552 -0.2595178 ]. \t  -3.564649730949853 \t -2.2733752808550527\n",
      "init   \t [1.17546336 0.73855239]. \t  -2.2733752808550527 \t -2.2733752808550527\n",
      "1      \t [ 0.91048109 -0.47911169]. \t  \u001b[92m-0.9190273225763074\u001b[0m \t -0.9190273225763074\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -0.9190273225763074\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.9190273225763074\n",
      "4      \t [3. 2.]. \t  -162.89999999999998 \t -0.9190273225763074\n",
      "5      \t [-0.21670941  2.        ]. \t  -47.74983598557856 \t -0.9190273225763074\n",
      "6      \t [-0.27502883 -2.        ]. \t  -48.840750107054184 \t -0.9190273225763074\n",
      "7      \t [-0.88986235  0.04014283]. \t  -1.973997644811896 \t -0.9190273225763074\n",
      "8      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.9190273225763074\n",
      "9      \t [ 1.17781774 -2.        ]. \t  -48.04189716024143 \t -0.9190273225763074\n",
      "10     \t [-1.44712634 -0.8321092 ]. \t  -2.580635309313118 \t -0.9190273225763074\n",
      "11     \t [1.09578766 2.        ]. \t  -52.54387504427414 \t -0.9190273225763074\n",
      "12     \t [0.26117451 0.50749794]. \t  \u001b[92m0.36915130084291864\u001b[0m \t 0.36915130084291864\n",
      "13     \t [-3.         -0.16502285]. \t  -109.28910482636121 \t 0.36915130084291864\n",
      "14     \t [-1.47466202  0.90650846]. \t  -0.2728592047475359 \t 0.36915130084291864\n",
      "15     \t [-1.61788931  0.12891456]. \t  -1.786071632589837 \t 0.36915130084291864\n",
      "16     \t [-0.57018569 -0.83657866]. \t  -0.7267238105628634 \t 0.36915130084291864\n",
      "17     \t [ 1.74406887 -1.00591622]. \t  -0.41195149543726867 \t 0.36915130084291864\n",
      "18     \t [-1.32527461 -2.        ]. \t  -53.00391708494387 \t 0.36915130084291864\n",
      "19     \t [0.93617881 0.14014503]. \t  -2.1712340140874917 \t 0.36915130084291864\n",
      "20     \t [-1.43452762  2.        ]. \t  -47.374193349351174 \t 0.36915130084291864\n",
      "21     \t [ 0.0085097  -0.24603584]. \t  0.22928130093973498 \t 0.36915130084291864\n",
      "22     \t [2.16559867 0.60242189]. \t  -7.3340539499438835 \t 0.36915130084291864\n",
      "23     \t [-0.56185983  0.91985511]. \t  -0.026354847517948254 \t 0.36915130084291864\n",
      "24     \t [3.         0.42652201]. \t  -109.58426296452653 \t 0.36915130084291864\n",
      "25     \t [1.7166505  0.56981813]. \t  -2.1823535157221654 \t 0.36915130084291864\n",
      "26     \t [1.80212005 1.2810908 ]. \t  -8.777279355393524 \t 0.36915130084291864\n",
      "27     \t [-1.04936496 -0.63988956]. \t  -2.0076223605062635 \t 0.36915130084291864\n",
      "28     \t [-0.995142    0.63447183]. \t  -0.6320687108776211 \t 0.36915130084291864\n",
      "29     \t [-1.9893827   0.73855654]. \t  -1.1401729907040474 \t 0.36915130084291864\n",
      "30     \t [ 0.25617265 -1.02653994]. \t  -0.2172846243260032 \t 0.36915130084291864\n",
      "31     \t [0.34315899 1.14301171]. \t  -2.4363034784916433 \t 0.36915130084291864\n",
      "32     \t [ 2.15587358 -0.91449457]. \t  -4.175035488181463 \t 0.36915130084291864\n",
      "33     \t [ 0.07742014 -0.71978106]. \t  \u001b[92m1.0305171767170516\u001b[0m \t 1.0305171767170516\n",
      "34     \t [-3.          0.86103966]. \t  -105.54995618846522 \t 1.0305171767170516\n",
      "35     \t [-1.80084833  0.53739386]. \t  -0.46584628852403565 \t 1.0305171767170516\n",
      "36     \t [-1.8847164  -0.41372985]. \t  -2.863637789094117 \t 1.0305171767170516\n",
      "37     \t [-0.40796519  0.46371744]. \t  0.2552502925612493 \t 1.0305171767170516\n",
      "38     \t [ 0.48270613 -0.23296996]. \t  -0.5044526271100888 \t 1.0305171767170516\n",
      "39     \t [2.00441249 2.        ]. \t  -55.799295573931744 \t 1.0305171767170516\n",
      "40     \t [-2.08903525 -2.        ]. \t  -57.34434430562407 \t 1.0305171767170516\n",
      "41     \t [ 3.         -0.94676655]. \t  -105.68812723276771 \t 1.0305171767170516\n",
      "42     \t [-2.12811003  2.        ]. \t  -49.75014562518429 \t 1.0305171767170516\n",
      "43     \t [ 1.84125894 -0.72731577]. \t  -0.07714173107063793 \t 1.0305171767170516\n",
      "44     \t [-0.32874974 -0.54465381]. \t  0.2473394110695798 \t 1.0305171767170516\n",
      "45     \t [ 0.65568289 -0.84273119]. \t  0.017813108736117855 \t 1.0305171767170516\n",
      "46     \t [-2.40558598 -1.23159899]. \t  -23.517800051865954 \t 1.0305171767170516\n",
      "47     \t [-1.0275964   1.11561711]. \t  -2.34603292765143 \t 1.0305171767170516\n",
      "48     \t [-1.84983899  1.29962389]. \t  -4.704998716491423 \t 1.0305171767170516\n",
      "49     \t [0.50982221 0.78233696]. \t  -0.35273124020810875 \t 1.0305171767170516\n",
      "50     \t [-1.86664272 -1.28615359]. \t  -9.272175740128976 \t 1.0305171767170516\n",
      "51     \t [ 0.45137515 -2.        ]. \t  -47.827856297076266 \t 1.0305171767170516\n",
      "52     \t [2.43703064 1.30039668]. \t  -27.356662763557132 \t 1.0305171767170516\n",
      "53     \t [-0.0578969   0.80252184]. \t  0.9500873111287941 \t 1.0305171767170516\n",
      "54     \t [-3.         -1.08354388]. \t  -112.96809874407825 \t 1.0305171767170516\n",
      "55     \t [0.40690087 2.        ]. \t  -49.42002087616906 \t 1.0305171767170516\n",
      "56     \t [ 1.12330867 -1.28547382]. \t  -5.241883053899707 \t 1.0305171767170516\n",
      "57     \t [-0.10293269 -0.81281135]. \t  0.7709404532321007 \t 1.0305171767170516\n",
      "58     \t [1.92282206 0.922897  ]. \t  -4.198869177605431 \t 1.0305171767170516\n",
      "59     \t [-0.09051464  0.68304961]. \t  1.0247213686294272 \t 1.0305171767170516\n",
      "60     \t [3.         1.22897058]. \t  -115.6702912846554 \t 1.0305171767170516\n",
      "61     \t [-1.95164912 -0.88803904]. \t  -4.255290947413052 \t 1.0305171767170516\n",
      "62     \t [ 1.97272955 -2.        ]. \t  -47.46306898540265 \t 1.0305171767170516\n",
      "63     \t [-2.50845925  1.42999939]. \t  -30.02848924923466 \t 1.0305171767170516\n",
      "64     \t [ 0.10730448 -0.71907622]. \t  1.0302153762803261 \t 1.0305171767170516\n",
      "65     \t [-0.04232208  0.71929355]. \t  1.0220753960045983 \t 1.0305171767170516\n",
      "66     \t [-0.72252036 -1.46475596]. \t  -12.452399107210713 \t 1.0305171767170516\n",
      "67     \t [ 0.08101666 -0.71741642]. \t  \u001b[92m1.031095511836255\u001b[0m \t 1.031095511836255\n",
      "68     \t [-2.40314824 -0.54167525]. \t  -17.737671421117625 \t 1.031095511836255\n",
      "69     \t [-0.03734241  0.65069626]. \t  0.9952580333532739 \t 1.031095511836255\n",
      "70     \t [-0.1360702   0.71621944]. \t  1.0234405959233843 \t 1.031095511836255\n",
      "71     \t [-0.03967656  0.71903748]. \t  1.0210791899659988 \t 1.031095511836255\n",
      "72     \t [ 0.08233446 -0.71661052]. \t  \u001b[92m1.0312499748105015\u001b[0m \t 1.0312499748105015\n",
      "73     \t [-0.10124701  0.69125034]. \t  1.027236953008115 \t 1.0312499748105015\n",
      "74     \t [-0.13754764  0.71608923]. \t  1.0229146897855326 \t 1.0312499748105015\n",
      "75     \t [-0.07952651  0.69729855]. \t  1.029480796978159 \t 1.0312499748105015\n",
      "76     \t [-0.07566621  0.65618747]. \t  1.00754305779178 \t 1.0312499748105015\n",
      "77     \t [ 0.06356639 -0.72978435]. \t  1.026014097347365 \t 1.0312499748105015\n",
      "78     \t [-0.03015144  0.7647814 ]. \t  0.99059890714206 \t 1.0312499748105015\n",
      "79     \t [ 0.07160336 -0.74342708]. \t  1.0216765283938964 \t 1.0312499748105015\n",
      "80     \t [ 0.13567873 -0.6955568 ]. \t  1.020397107042901 \t 1.0312499748105015\n",
      "81     \t [-0.12704606  0.68475836]. \t  1.019108410162316 \t 1.0312499748105015\n",
      "82     \t [-0.12670194  0.7238983 ]. \t  1.0257360823688184 \t 1.0312499748105015\n",
      "83     \t [ 0.01580952 -0.74634041]. \t  0.9977927008092208 \t 1.0312499748105015\n",
      "84     \t [ 0.20543868 -0.73471069]. \t  0.9794972914441412 \t 1.0312499748105015\n",
      "85     \t [-0.09331571  0.70084431]. \t  1.0304165367789024 \t 1.0312499748105015\n",
      "86     \t [ 0.08970809 -0.66062703]. \t  1.0110436899531574 \t 1.0312499748105015\n",
      "87     \t [-0.07160424  0.71389524]. \t  1.0302922036394941 \t 1.0312499748105015\n",
      "88     \t [0.98712179 1.29745401]. \t  -8.094498304118272 \t 1.0312499748105015\n",
      "89     \t [ 0.12614846 -0.73142321]. \t  1.0242502539729317 \t 1.0312499748105015\n",
      "90     \t [ 0.14288704 -0.71117548]. \t  1.0206902661311068 \t 1.0312499748105015\n",
      "91     \t [-0.05405876  0.67274418]. \t  1.015703415540623 \t 1.0312499748105015\n",
      "92     \t [ 0.00935872 -0.72711581]. \t  1.0031603835109077 \t 1.0312499748105015\n",
      "93     \t [ 0.10507941 -0.71350319]. \t  1.0307330656019598 \t 1.0312499748105015\n",
      "94     \t [ 0.00258144 -0.64384172]. \t  0.9724163716175056 \t 1.0312499748105015\n",
      "95     \t [ 0.04586843 -0.6969448 ]. \t  1.0227471090350118 \t 1.0312499748105015\n",
      "96     \t [ 0.04988116 -0.77756723]. \t  0.9850727583845034 \t 1.0312499748105015\n",
      "97     \t [-0.08358273  0.70470227]. \t  1.0310129292046066 \t 1.0312499748105015\n",
      "98     \t [-0.46249524  1.39949332]. \t  -7.625367791604585 \t 1.0312499748105015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.09061134 -0.7247068 ]. \t  1.0304262235653805 \t 1.0312499748105015\n",
      "100    \t [-0.12819303  0.71412082]. \t  1.0259796566477326 \t 1.0312499748105015\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.50945364 -1.5629949 ]. \t  -13.89572751148896 \t -0.027690663940903623\n",
      "init   \t [-0.08327233 -0.00067529]. \t  -0.027690663940903623 \t -0.027690663940903623\n",
      "init   \t [ 0.94265999 -1.05646116]. \t  -1.6525931811945593 \t -0.027690663940903623\n",
      "init   \t [ 0.67676934 -1.52139039]. \t  -12.56543686771716 \t -0.027690663940903623\n",
      "init   \t [ 1.27321378 -0.50975783]. \t  -0.9673942177133168 \t -0.027690663940903623\n",
      "1      \t [-3.  2.]. \t  -150.89999999999998 \t -0.027690663940903623\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -0.027690663940903623\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -0.027690663940903623\n",
      "4      \t [-0.0249632  2.       ]. \t  -47.95256543425552 \t -0.027690663940903623\n",
      "5      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.027690663940903623\n",
      "6      \t [-1.55326789 -0.0196059 ]. \t  -2.1369196826613903 \t -0.027690663940903623\n",
      "7      \t [-0.93925784 -1.20619265]. \t  -5.903557026656928 \t -0.027690663940903623\n",
      "8      \t [ 3.         -0.03032725]. \t  -108.80534267208157 \t -0.027690663940903623\n",
      "9      \t [-3.          0.01389629]. \t  -108.85753884601172 \t -0.027690663940903623\n",
      "10     \t [-1.01199085  0.89420572]. \t  -0.7061200693032513 \t -0.027690663940903623\n",
      "11     \t [1.02437    0.94473045]. \t  -2.8541908208736837 \t -0.027690663940903623\n",
      "12     \t [-0.90567573 -0.06367394]. \t  -2.093570746418255 \t -0.027690663940903623\n",
      "13     \t [-0.86684768 -2.        ]. \t  -51.69508031264207 \t -0.027690663940903623\n",
      "14     \t [0.7291054  0.26290878]. \t  -1.517323580724712 \t -0.027690663940903623\n",
      "15     \t [-1.35410381  2.        ]. \t  -47.6207153216069 \t -0.027690663940903623\n",
      "16     \t [1.26037611 2.        ]. \t  -52.91185181348595 \t -0.027690663940903623\n",
      "17     \t [0.0351918  0.86488898]. \t  \u001b[92m0.7185310052738977\u001b[0m \t 0.7185310052738977\n",
      "18     \t [-1.63841884 -0.85418677]. \t  -2.663368312210846 \t 0.7185310052738977\n",
      "19     \t [1.74561827 0.57376179]. \t  -2.239167974540605 \t 0.7185310052738977\n",
      "20     \t [-0.10352908 -0.79808976]. \t  \u001b[92m0.7997240491374038\u001b[0m \t 0.7997240491374038\n",
      "21     \t [-1.23739195 -0.6430834 ]. \t  -2.223507650966964 \t 0.7997240491374038\n",
      "22     \t [-0.51970051  0.61159365]. \t  0.32065727282064305 \t 0.7997240491374038\n",
      "23     \t [-1.70995706  0.79125532]. \t  0.21478345703261548 \t 0.7997240491374038\n",
      "24     \t [-1.38417212  0.57763425]. \t  -0.6105179025348009 \t 0.7997240491374038\n",
      "25     \t [1.39435108 0.32670815]. \t  -2.362776814122743 \t 0.7997240491374038\n",
      "26     \t [ 1.24008042 -2.        ]. \t  -47.91710991793826 \t 0.7997240491374038\n",
      "27     \t [ 1.60639534 -0.94799096]. \t  -0.17892985182765742 \t 0.7997240491374038\n",
      "28     \t [-0.22130769 -1.31321014]. \t  -5.479339172334114 \t 0.7997240491374038\n",
      "29     \t [1.72619306 1.12161152]. \t  -5.326712732546197 \t 0.7997240491374038\n",
      "30     \t [-1.84779911 -2.        ]. \t  -54.13958348356854 \t 0.7997240491374038\n",
      "31     \t [-0.49799526 -0.6849746 ]. \t  -0.2128348364830619 \t 0.7997240491374038\n",
      "32     \t [3.         0.93632195]. \t  -111.27657376435734 \t 0.7997240491374038\n",
      "33     \t [-0.47916585  1.1933748 ]. \t  -2.656070057945163 \t 0.7997240491374038\n",
      "34     \t [-3.          0.97457476]. \t  -105.78554133763674 \t 0.7997240491374038\n",
      "35     \t [ 1.37412902 -0.99506366]. \t  -0.9033086038773782 \t 0.7997240491374038\n",
      "36     \t [ 0.58680778 -0.48679764]. \t  -0.13305978313931677 \t 0.7997240491374038\n",
      "37     \t [-1.94846852  0.40954317]. \t  -1.8017235369359461 \t 0.7997240491374038\n",
      "38     \t [ 3.         -1.07145304]. \t  -106.36531710923431 \t 0.7997240491374038\n",
      "39     \t [-0.00947809  0.59361356]. \t  \u001b[92m0.9180968706464822\u001b[0m \t 0.9180968706464822\n",
      "40     \t [ 1.87670514 -0.26080143]. \t  -1.858472086592563 \t 0.9180968706464822\n",
      "41     \t [-3.         -1.03555725]. \t  -112.31714185602334 \t 0.9180968706464822\n",
      "42     \t [2.05328365 2.        ]. \t  -56.62299944959068 \t 0.9180968706464822\n",
      "43     \t [ 0.28039858 -0.92995842]. \t  0.4267027055408657 \t 0.9180968706464822\n",
      "44     \t [-2.13856791  2.        ]. \t  -49.97903103839894 \t 0.9180968706464822\n",
      "45     \t [-2.09854303 -0.45780503]. \t  -5.65573574821766 \t 0.9180968706464822\n",
      "46     \t [-2.17154758  1.18277231]. \t  -6.782482735133081 \t 0.9180968706464822\n",
      "47     \t [0.29448482 0.66615101]. \t  0.459866214207968 \t 0.9180968706464822\n",
      "48     \t [ 2.16257244 -2.        ]. \t  -50.54709594629476 \t 0.9180968706464822\n",
      "49     \t [-1.73612498  1.33074283]. \t  -5.256004877135464 \t 0.9180968706464822\n",
      "50     \t [ 1.66162184 -0.58913737]. \t  -0.16586903152219346 \t 0.9180968706464822\n",
      "51     \t [0.58656004 1.37898321]. \t  -8.807931508983685 \t 0.9180968706464822\n",
      "52     \t [-2.25624182 -1.3234354 ]. \t  -18.166562099630095 \t 0.9180968706464822\n",
      "53     \t [0.26706881 0.0965474 ]. \t  -0.26358724305635456 \t 0.9180968706464822\n",
      "54     \t [ 0.05267797 -2.        ]. \t  -47.905727772680116 \t 0.9180968706464822\n",
      "55     \t [2.28286362 1.1769302 ]. \t  -15.81206859507429 \t 0.9180968706464822\n",
      "56     \t [ 2.22063432 -0.66638217]. \t  -6.162736265812839 \t 0.9180968706464822\n",
      "57     \t [-1.66543135 -1.41103856]. \t  -12.294373418489654 \t 0.9180968706464822\n",
      "58     \t [ 0.08944287 -0.63539794]. \t  \u001b[92m0.9878947021847565\u001b[0m \t 0.9878947021847565\n",
      "59     \t [-0.12103661  0.73082314]. \t  \u001b[92m1.025654649639004\u001b[0m \t 1.025654649639004\n",
      "60     \t [ 2.29996799 -1.39962233]. \t  -16.032216647720254 \t 1.025654649639004\n",
      "61     \t [2.31476192 0.53466664]. \t  -12.839924472292934 \t 1.025654649639004\n",
      "62     \t [ 0.10820522 -0.71669499]. \t  \u001b[92m1.0302585297479578\u001b[0m \t 1.0302585297479578\n",
      "63     \t [-0.13770806  0.7546445 ]. \t  1.009504999118874 \t 1.0302585297479578\n",
      "64     \t [-0.65783826  2.        ]. \t  -48.04906795181575 \t 1.0302585297479578\n",
      "65     \t [-0.20107755  0.79227453]. \t  0.9357627993222033 \t 1.0302585297479578\n",
      "66     \t [ 0.13414668 -0.63114353]. \t  0.9720253229928921 \t 1.0302585297479578\n",
      "67     \t [-0.11601108  0.72736215]. \t  1.027550419158946 \t 1.0302585297479578\n",
      "68     \t [ 0.09108396 -0.74737147]. \t  1.0213137267869268 \t 1.0302585297479578\n",
      "69     \t [ 0.17744225 -0.77623885]. \t  0.971802653556931 \t 1.0302585297479578\n",
      "70     \t [ 0.12366361 -0.74391273]. \t  1.019905240291263 \t 1.0302585297479578\n",
      "71     \t [-0.1788369   0.74704536]. \t  0.9943142487587373 \t 1.0302585297479578\n",
      "72     \t [ 0.12884462 -0.69416156]. \t  1.0222962569205323 \t 1.0302585297479578\n",
      "73     \t [ 0.15158916 -0.72122711]. \t  1.016890893138605 \t 1.0302585297479578\n",
      "74     \t [ 0.1407049  -0.70689187]. \t  1.021091824636409 \t 1.0302585297479578\n",
      "75     \t [ 0.09728092 -0.69869411]. \t  1.0297436210038409 \t 1.0302585297479578\n",
      "76     \t [-0.14126132  0.67580029]. \t  1.008981481045494 \t 1.0302585297479578\n",
      "77     \t [ 0.14509126 -0.76693684]. \t  0.9968860719654099 \t 1.0302585297479578\n",
      "78     \t [ 0.16590808 -0.70953995]. \t  1.009153039914107 \t 1.0302585297479578\n",
      "79     \t [ 0.03293222 -0.75069136]. \t  1.0042382091545892 \t 1.0302585297479578\n",
      "80     \t [-2.31954801  0.68751027]. \t  -10.05512750843044 \t 1.0302585297479578\n",
      "81     \t [-0.00995469 -0.76236601]. \t  0.9656395032889691 \t 1.0302585297479578\n",
      "82     \t [ 0.12940464 -0.70269599]. \t  1.024382509368444 \t 1.0302585297479578\n",
      "83     \t [ 0.08485665 -0.7294347 ]. \t  1.0290883211568072 \t 1.0302585297479578\n",
      "84     \t [ 0.08951711 -0.72327915]. \t  \u001b[92m1.0306867903878212\u001b[0m \t 1.0306867903878212\n",
      "85     \t [ 0.11625496 -0.77203116]. \t  0.9991856842976242 \t 1.0306867903878212\n",
      "86     \t [ 0.08238227 -0.71686521]. \t  \u001b[92m1.0312338890510895\u001b[0m \t 1.0312338890510895\n",
      "87     \t [-0.09380879  0.69957393]. \t  1.0301391005469678 \t 1.0312338890510895\n",
      "88     \t [ 0.11710579 -0.72761067]. \t  1.02728493303495 \t 1.0312338890510895\n",
      "89     \t [-0.03653796  0.68896458]. \t  1.017271019407866 \t 1.0312338890510895\n",
      "90     \t [ 0.11284331 -0.71693478]. \t  1.0295231659837798 \t 1.0312338890510895\n",
      "91     \t [-0.05077098  0.7091875 ]. \t  1.0256745838271815 \t 1.0312338890510895\n",
      "92     \t [-0.10966975  0.71796531]. \t  1.0299746627120994 \t 1.0312338890510895\n",
      "93     \t [ 1.88530985 -0.92994274]. \t  -0.43423884047983136 \t 1.0312338890510895\n",
      "94     \t [-0.17107288  0.77625558]. \t  0.9754381892508337 \t 1.0312338890510895\n",
      "95     \t [ 0.09741183 -0.6867863 ]. \t  1.0259245281930827 \t 1.0312338890510895\n",
      "96     \t [-0.07157399  0.75731228]. \t  1.012145708682968 \t 1.0312338890510895\n",
      "97     \t [ 0.16510767 -0.71585547]. \t  1.010084977989739 \t 1.0312338890510895\n",
      "98     \t [-0.05965388  0.69827631]. \t  1.0268310768392128 \t 1.0312338890510895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.09032666 -0.66916024]. \t  1.017037375151285 \t 1.0312338890510895\n",
      "100    \t [-0.15428449  0.73478311]. \t  1.012966195220642 \t 1.0312338890510895\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.03616628 1.28517986]. \t  -11.164343885550666 \t -1.5094648555070385\n",
      "init   \t [0.85324299 0.66889047]. \t  -1.5094648555070385 \t -1.5094648555070385\n",
      "init   \t [-2.76516057 -1.73731045]. \t  -85.9872978949554 \t -1.5094648555070385\n",
      "init   \t [-1.34099616 -0.73455799]. \t  -2.3318567753236477 \t -1.5094648555070385\n",
      "init   \t [-1.83844553 -1.21162847]. \t  -7.376051913381579 \t -1.5094648555070385\n",
      "1      \t [-0.68325498 -2.        ]. \t  -50.810105208838905 \t -1.5094648555070385\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "3      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "4      \t [-0.08025519  2.        ]. \t  -47.86516616714019 \t -1.5094648555070385\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t -1.5094648555070385\n",
      "6      \t [2.00724342 0.1157433 ]. \t  -4.007282736606023 \t -1.5094648555070385\n",
      "7      \t [-2.45109398 -0.07998463]. \t  -20.687217684023878 \t -1.5094648555070385\n",
      "8      \t [ 0.84833026 -1.05630004]. \t  -1.5358724042576282 \t -1.5094648555070385\n",
      "9      \t [1.21739892 2.        ]. \t  -52.835499809318286 \t -1.5094648555070385\n",
      "10     \t [-0.9729455   0.76873549]. \t  \u001b[92m-0.47259994840049446\u001b[0m \t -0.47259994840049446\n",
      "11     \t [ 0.04369611 -0.15454202]. \t  \u001b[92m0.09237444146363645\u001b[0m \t 0.09237444146363645\n",
      "12     \t [ 0.96631356 -2.        ]. \t  -48.24279262193738 \t 0.09237444146363645\n",
      "13     \t [1.65360909 0.72145893]. \t  -2.245695942318109 \t 0.09237444146363645\n",
      "14     \t [3.         0.22392382]. \t  -109.38126078324693 \t 0.09237444146363645\n",
      "15     \t [ 1.32429496 -0.43502132]. \t  -1.1643049875551448 \t 0.09237444146363645\n",
      "16     \t [-0.10520566  0.768694  ]. \t  \u001b[92m1.0038105429843382\u001b[0m \t 1.0038105429843382\n",
      "17     \t [-1.2982912  2.       ]. \t  -47.77560985643656 \t 1.0038105429843382\n",
      "18     \t [-1.62064548  0.10185369]. \t  -1.8526501543327603 \t 1.0038105429843382\n",
      "19     \t [ 1.96496544 -0.83963524]. \t  -0.8428130555332577 \t 1.0038105429843382\n",
      "20     \t [-1.88709415  0.86713205]. \t  -0.28420706026729714 \t 1.0038105429843382\n",
      "21     \t [-0.03160482 -1.0106261 ]. \t  -0.12322032128875357 \t 1.0038105429843382\n",
      "22     \t [-1.70262784 -2.        ]. \t  -53.47367979024944 \t 1.0038105429843382\n",
      "23     \t [-1.53397453  0.76082702]. \t  0.014573928708416894 \t 1.0038105429843382\n",
      "24     \t [-3.          0.64442907]. \t  -105.99541672304402 \t 1.0038105429843382\n",
      "25     \t [-1.9139453  -0.60102434]. \t  -3.0856921973774845 \t 1.0038105429843382\n",
      "26     \t [ 1.89959783 -0.46263238]. \t  -1.1999990271951284 \t 1.0038105429843382\n",
      "27     \t [ 1.54612009 -1.13717773]. \t  -1.873352437986959 \t 1.0038105429843382\n",
      "28     \t [0.57919075 0.17609821]. \t  -1.0999072726293209 \t 1.0038105429843382\n",
      "29     \t [-0.65354284  0.20658626]. \t  -1.052904165394733 \t 1.0038105429843382\n",
      "30     \t [ 0.39235989 -0.67119326]. \t  0.6863160526526029 \t 1.0038105429843382\n",
      "31     \t [-1.94542676  0.46255226]. \t  -1.556510019894896 \t 1.0038105429843382\n",
      "32     \t [-0.45093556 -0.5735348 ]. \t  -0.10501304261689459 \t 1.0038105429843382\n",
      "33     \t [2.13799703 0.7776463 ]. \t  -6.948728163706214 \t 1.0038105429843382\n",
      "34     \t [-3.         -0.84457095]. \t  -110.61570056315287 \t 1.0038105429843382\n",
      "35     \t [-1.47873929  1.20617266]. \t  -3.05404050545859 \t 1.0038105429843382\n",
      "36     \t [ 0.15587307 -2.        ]. \t  -47.784204634007814 \t 1.0038105429843382\n",
      "37     \t [ 3.         -0.92379484]. \t  -105.62817297691234 \t 1.0038105429843382\n",
      "38     \t [-0.92123192 -1.20090255]. \t  -5.742945221318502 \t 1.0038105429843382\n",
      "39     \t [-2.05711626  2.        ]. \t  -48.46680132474505 \t 1.0038105429843382\n",
      "40     \t [0.38872887 1.21198276]. \t  -3.783857472852029 \t 1.0038105429843382\n",
      "41     \t [ 1.65331782 -0.80438855]. \t  0.19241454506685496 \t 1.0038105429843382\n",
      "42     \t [ 2.01620725 -2.        ]. \t  -47.917354306682874 \t 1.0038105429843382\n",
      "43     \t [-0.35994857  1.16047226]. \t  -1.9335902785706427 \t 1.0038105429843382\n",
      "44     \t [1.5520828  0.12665539]. \t  -2.242599437764325 \t 1.0038105429843382\n",
      "45     \t [1.26362326 1.25640302]. \t  -7.630537049963532 \t 1.0038105429843382\n",
      "46     \t [-0.43881167  0.77593676]. \t  0.6040711355510346 \t 1.0038105429843382\n",
      "47     \t [0.29818691 0.81288387]. \t  0.31491402623837805 \t 1.0038105429843382\n",
      "48     \t [-0.030175  -0.7066898]. \t  0.975033863728806 \t 1.0038105429843382\n",
      "49     \t [2.01646113 2.        ]. \t  -55.98622686720378 \t 1.0038105429843382\n",
      "50     \t [3.         1.16674327]. \t  -114.36751070991463 \t 1.0038105429843382\n",
      "51     \t [-3. -2.]. \t  -162.89999999999998 \t 1.0038105429843382\n",
      "52     \t [-0.45324545 -0.91696028]. \t  -0.6162219316293246 \t 1.0038105429843382\n",
      "53     \t [ 2.1215018  -1.39985779]. \t  -10.40606950256308 \t 1.0038105429843382\n",
      "54     \t [ 0.26787862 -0.84476146]. \t  0.7674108643599757 \t 1.0038105429843382\n",
      "55     \t [-2.32234988 -2.        ]. \t  -65.42672558404932 \t 1.0038105429843382\n",
      "56     \t [-0.04509035  0.84760081]. \t  0.8392530916332687 \t 1.0038105429843382\n",
      "57     \t [-2.26897292  1.45054336]. \t  -16.418379885150188 \t 1.0038105429843382\n",
      "58     \t [-2.30072198 -1.00526718]. \t  -14.126695725756779 \t 1.0038105429843382\n",
      "59     \t [-1.85120288  1.25209573]. \t  -3.7032062696994505 \t 1.0038105429843382\n",
      "60     \t [-0.11376735  0.51936167]. \t  0.795580405537982 \t 1.0038105429843382\n",
      "61     \t [-3.          1.34445677]. \t  -110.70552420084714 \t 1.0038105429843382\n",
      "62     \t [ 0.58567804 -1.46289946]. \t  -10.041032389959794 \t 1.0038105429843382\n",
      "63     \t [0.55417614 2.        ]. \t  -50.148386039404926 \t 1.0038105429843382\n",
      "64     \t [ 0.13705184 -0.71401608]. \t  \u001b[92m1.0230774310647839\u001b[0m \t 1.0230774310647839\n",
      "65     \t [ 0.09411733 -0.69419294]. \t  \u001b[92m1.0287579590548726\u001b[0m \t 1.0287579590548726\n",
      "66     \t [ 0.05064105 -0.71244364]. \t  1.0256050563891654 \t 1.0287579590548726\n",
      "67     \t [-0.13585651  0.72077395]. \t  1.0232838052816338 \t 1.0287579590548726\n",
      "68     \t [-0.03540086 -0.78600613]. \t  0.9116524387099405 \t 1.0287579590548726\n",
      "69     \t [-0.14411636  0.73302474]. \t  1.017892950548968 \t 1.0287579590548726\n",
      "70     \t [-0.14342472  0.70897852]. \t  1.0202600610312336 \t 1.0287579590548726\n",
      "71     \t [ 0.16007699 -0.73786131]. \t  1.0090899362808021 \t 1.0287579590548726\n",
      "72     \t [-0.09546288  0.70517644]. \t  \u001b[92m1.0310099518756812\u001b[0m \t 1.0310099518756812\n",
      "73     \t [-0.15884266  0.67585513]. \t  1.000290353374323 \t 1.0310099518756812\n",
      "74     \t [ 0.04275905 -0.69309528]. \t  1.0207901565996123 \t 1.0310099518756812\n",
      "75     \t [ 0.07643754 -0.73726147]. \t  1.025467314775072 \t 1.0310099518756812\n",
      "76     \t [ 0.14241702 -0.76326799]. \t  1.0011565273421565 \t 1.0310099518756812\n",
      "77     \t [-0.00350221 -0.6405036 ]. \t  0.9654838184911808 \t 1.0310099518756812\n",
      "78     \t [-0.11734232  0.72929046]. \t  1.0268356207865852 \t 1.0310099518756812\n",
      "79     \t [ 0.02964557 -0.72609193]. \t  1.015050165041277 \t 1.0310099518756812\n",
      "80     \t [ 0.15316894 -0.69278121]. \t  1.0118125341115043 \t 1.0310099518756812\n",
      "81     \t [ 0.07330177 -0.73535015]. \t  1.0258315044807584 \t 1.0310099518756812\n",
      "82     \t [-0.22621581  0.60322652]. \t  0.8631074974557607 \t 1.0310099518756812\n",
      "83     \t [ 0.12121015 -0.72181133]. \t  1.0274095398724654 \t 1.0310099518756812\n",
      "84     \t [ 0.11296745 -0.68915633]. \t  1.024634240703328 \t 1.0310099518756812\n",
      "85     \t [ 3.         -1.47572072]. \t  -114.73224012995763 \t 1.0310099518756812\n",
      "86     \t [ 0.05961907 -0.63985719]. \t  0.9911355147538058 \t 1.0310099518756812\n",
      "87     \t [ 0.10447148 -0.69401693]. \t  1.027752040575998 \t 1.0310099518756812\n",
      "88     \t [-0.19608384  0.68731787]. \t  0.9810162055760542 \t 1.0310099518756812\n",
      "89     \t [ 0.11056856 -0.75168473]. \t  1.017608763167722 \t 1.0310099518756812\n",
      "90     \t [-0.16536017  0.79149606]. \t  0.9590947108253138 \t 1.0310099518756812\n",
      "91     \t [-0.02987896 -0.66123675]. \t  0.9609153349818147 \t 1.0310099518756812\n",
      "92     \t [-0.09043661  0.76117762]. \t  1.0110515385262815 \t 1.0310099518756812\n",
      "93     \t [-0.08999904  0.81256324]. \t  0.9381363944764777 \t 1.0310099518756812\n",
      "94     \t [ 0.03253968 -0.76081319]. \t  0.9956627445390124 \t 1.0310099518756812\n",
      "95     \t [-0.12621034  0.6977909 ]. \t  1.0241985557867181 \t 1.0310099518756812\n",
      "96     \t [-0.10721014  0.65290642]. \t  1.0025645109316161 \t 1.0310099518756812\n",
      "97     \t [-0.2208905   0.77360804]. \t  0.9418881175918792 \t 1.0310099518756812\n",
      "98     \t [-0.14307794  0.76867258]. \t  0.9959517263093983 \t 1.0310099518756812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.08611814  0.73715326]. \t  1.0263998224967552 \t 1.0310099518756812\n",
      "100    \t [ 0.21893797 -0.7083782 ]. \t  0.9681309481090786 \t 1.0310099518756812\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.34411965 0.16704251]. \t  -0.3937629170174044 \t 0.3730261027784846\n",
      "init   \t [ 0.3625427  -0.47180763]. \t  0.3730261027784846 \t 0.3730261027784846\n",
      "init   \t [-2.28543493  1.38399646]. \t  -14.95148366216595 \t 0.3730261027784846\n",
      "init   \t [1.43844967 1.72943026]. \t  -28.545251683089745 \t 0.3730261027784846\n",
      "init   \t [2.15596804 0.8324185 ]. \t  -7.6401968083208285 \t 0.3730261027784846\n",
      "1      \t [ 3.        -0.5503718]. \t  -106.4042638451189 \t 0.3730261027784846\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "4      \t [-0.08702235 -2.        ]. \t  -48.20421596514376 \t 0.3730261027784846\n",
      "5      \t [-0.61993404  2.        ]. \t  -48.00615507341331 \t 0.3730261027784846\n",
      "6      \t [1.37725607 0.26844065]. \t  -2.4087434117698567 \t 0.3730261027784846\n",
      "7      \t [-1.19623361 -0.13211485]. \t  -2.489924456621763 \t 0.3730261027784846\n",
      "8      \t [-3.          0.32992555]. \t  -107.52221390903775 \t 0.3730261027784846\n",
      "9      \t [-3.  2.]. \t  -150.89999999999998 \t 0.3730261027784846\n",
      "10     \t [-1.49853573  0.97927612]. \t  -0.5425058867975823 \t 0.3730261027784846\n",
      "11     \t [ 1.71457338 -2.        ]. \t  -46.6499509011945 \t 0.3730261027784846\n",
      "12     \t [-0.62850773 -0.78737956]. \t  -0.825387686847043 \t 0.3730261027784846\n",
      "13     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.3730261027784846\n",
      "14     \t [-1.66928745  2.        ]. \t  -46.7138064576386 \t 0.3730261027784846\n",
      "15     \t [-1.35649901 -2.        ]. \t  -53.03970868865224 \t 0.3730261027784846\n",
      "16     \t [0.58084881 1.30667385]. \t  -6.7135078229987695 \t 0.3730261027784846\n",
      "17     \t [-0.45344078 -0.08111593]. \t  -0.7471895496405072 \t 0.3730261027784846\n",
      "18     \t [-1.93395469  0.8575694 ]. \t  -0.5875266338093388 \t 0.3730261027784846\n",
      "19     \t [1.32896311 0.88975538]. \t  -2.873202521774714 \t 0.3730261027784846\n",
      "20     \t [ 1.12579738 -1.09136509]. \t  -2.0566771324396096 \t 0.3730261027784846\n",
      "21     \t [0.57365817 2.        ]. \t  -50.24810884882753 \t 0.3730261027784846\n",
      "22     \t [-0.16944182  0.90311832]. \t  \u001b[92m0.6414362645230316\u001b[0m \t 0.6414362645230316\n",
      "23     \t [ 0.90155958 -2.        ]. \t  -48.239731279912675 \t 0.6414362645230316\n",
      "24     \t [ 1.14865947 -0.54488299]. \t  -0.9266179168875852 \t 0.6414362645230316\n",
      "25     \t [3.         0.52604001]. \t  -109.67753926927755 \t 0.6414362645230316\n",
      "26     \t [ 1.94526637 -0.06520691]. \t  -2.9837888687386713 \t 0.6414362645230316\n",
      "27     \t [-1.70601842 -0.86529662]. \t  -2.7948647943006906 \t 0.6414362645230316\n",
      "28     \t [-1.26592597 -0.72126088]. \t  -2.303616893247856 \t 0.6414362645230316\n",
      "29     \t [0.24261525 0.80045755]. \t  0.4983338466733146 \t 0.6414362645230316\n",
      "30     \t [ 1.70945816 -0.95986953]. \t  -0.14346800107115876 \t 0.6414362645230316\n",
      "31     \t [-1.86039969 -0.39650033]. \t  -2.7161139195049038 \t 0.6414362645230316\n",
      "32     \t [1.80567565 0.61873105]. \t  -2.4432781373224888 \t 0.6414362645230316\n",
      "33     \t [ 0.01924905 -1.04081527]. \t  -0.3423859456383154 \t 0.6414362645230316\n",
      "34     \t [-3.         -0.83948896]. \t  -110.58614358236782 \t 0.6414362645230316\n",
      "35     \t [-0.13468636 -0.61894978]. \t  \u001b[92m0.7900997790242528\u001b[0m \t 0.7900997790242528\n",
      "36     \t [-0.76012952  0.57249402]. \t  -0.3579146154966699 \t 0.7900997790242528\n",
      "37     \t [-1.86409947  1.14617057]. \t  -2.040489548768662 \t 0.7900997790242528\n",
      "38     \t [ 1.72875461 -0.55220926]. \t  -0.2931206477900449 \t 0.7900997790242528\n",
      "39     \t [-0.77204597  1.12119153]. \t  -2.135729086347049 \t 0.7900997790242528\n",
      "40     \t [-0.81444539 -1.3753514 ]. \t  -9.692837431734759 \t 0.7900997790242528\n",
      "41     \t [1.88705181 1.25111405]. \t  -8.56669791562402 \t 0.7900997790242528\n",
      "42     \t [-1.58667596  0.51058564]. \t  -0.4980014750719367 \t 0.7900997790242528\n",
      "43     \t [-3.          1.17409538]. \t  -107.46476255040261 \t 0.7900997790242528\n",
      "44     \t [2.0894641 2.       ]. \t  -57.35366010090469 \t 0.7900997790242528\n",
      "45     \t [0.7757764  0.41041962]. \t  -1.4774710742288342 \t 0.7900997790242528\n",
      "46     \t [ 2.29517079 -1.13578005]. \t  -10.413101283969677 \t 0.7900997790242528\n",
      "47     \t [-2.15396667 -2.        ]. \t  -58.95230521233282 \t 0.7900997790242528\n",
      "48     \t [ 2.03058476 -0.76883136]. \t  -1.629330456745694 \t 0.7900997790242528\n",
      "49     \t [ 0.55533053 -0.87158978]. \t  0.17068620564697257 \t 0.7900997790242528\n",
      "50     \t [-1.62446296 -1.41540315]. \t  -12.396929656269272 \t 0.7900997790242528\n",
      "51     \t [-0.49125791  0.81013558]. \t  0.4525282834090236 \t 0.7900997790242528\n",
      "52     \t [ 0.50696938 -1.3631986 ]. \t  -6.583937901112337 \t 0.7900997790242528\n",
      "53     \t [-2.37899717 -1.35447167]. \t  -25.148015049487988 \t 0.7900997790242528\n",
      "54     \t [-2.24054412  2.        ]. \t  -52.84704898505488 \t 0.7900997790242528\n",
      "55     \t [3.         1.30262932]. \t  -117.53762213013044 \t 0.7900997790242528\n",
      "56     \t [-2.06457187  0.26836537]. \t  -3.8887091194166534 \t 0.7900997790242528\n",
      "57     \t [-2.33620928 -0.54785866]. \t  -13.909309040143224 \t 0.7900997790242528\n",
      "58     \t [ 2.36220058 -2.        ]. \t  -58.12276437989074 \t 0.7900997790242528\n",
      "59     \t [ 1.88352529 -1.3729374 ]. \t  -6.73023100820465 \t 0.7900997790242528\n",
      "60     \t [-1.05783702  0.78071838]. \t  -0.5356352942041156 \t 0.7900997790242528\n",
      "61     \t [-0.16240784  0.38290035]. \t  0.45860527156581365 \t 0.7900997790242528\n",
      "62     \t [ 0.06200903 -0.74198058]. \t  \u001b[92m1.0204448281704752\u001b[0m \t 1.0204448281704752\n",
      "63     \t [ 0.09888224 -0.71403348]. \t  \u001b[92m1.0313072931071396\u001b[0m \t 1.0313072931071396\n",
      "64     \t [-0.04738855  1.60938044]. \t  -16.40686805897371 \t 1.0313072931071396\n",
      "65     \t [ 0.05884149 -0.59450261]. \t  0.9352301747299591 \t 1.0313072931071396\n",
      "66     \t [-0.16532452  0.72989936]. \t  1.008612414001249 \t 1.0313072931071396\n",
      "67     \t [ 0.09889574 -0.70374021]. \t  1.0305857645962966 \t 1.0313072931071396\n",
      "68     \t [ 0.04909249 -0.73904702]. \t  1.0181193111820852 \t 1.0313072931071396\n",
      "69     \t [ 0.11925159 -0.72499143]. \t  1.0273723347956696 \t 1.0313072931071396\n",
      "70     \t [ 0.0502685 -0.7317276]. \t  1.0216687648158354 \t 1.0313072931071396\n",
      "71     \t [ 0.13563873 -0.74260962]. \t  1.0172476469108747 \t 1.0313072931071396\n",
      "72     \t [ 0.15553637 -0.72612671]. \t  1.014424703129735 \t 1.0313072931071396\n",
      "73     \t [-0.20134323  0.7357444 ]. \t  0.9825804271459423 \t 1.0313072931071396\n",
      "74     \t [ 0.11165031 -0.70772671]. \t  1.0294773480079276 \t 1.0313072931071396\n",
      "75     \t [-0.1145208   0.74073762]. \t  1.0232465353263989 \t 1.0313072931071396\n",
      "76     \t [ 0.06547501 -0.68506943]. \t  1.0239805668092474 \t 1.0313072931071396\n",
      "77     \t [-0.02633309 -0.76923539]. \t  0.9433184393207583 \t 1.0313072931071396\n",
      "78     \t [-0.14639553  0.75230308]. \t  1.007965637134045 \t 1.0313072931071396\n",
      "79     \t [ 0.09073543 -0.75328887]. \t  1.0173656354239968 \t 1.0313072931071396\n",
      "80     \t [-0.12329082  0.68153529]. \t  1.0186648494626214 \t 1.0313072931071396\n",
      "81     \t [ 0.00765735 -0.6696754 ]. \t  0.994270039994818 \t 1.0313072931071396\n",
      "82     \t [ 0.05901007 -0.69797741]. \t  1.0266262095541416 \t 1.0313072931071396\n",
      "83     \t [ 0.13807706 -0.77745628]. \t  0.9882193822883841 \t 1.0313072931071396\n",
      "84     \t [-0.10467206  0.67714684]. \t  1.0204252787548203 \t 1.0313072931071396\n",
      "85     \t [ 0.19363506 -0.70805981]. \t  0.9900544757680152 \t 1.0313072931071396\n",
      "86     \t [-0.11539768  0.74832291]. \t  1.0190659925544112 \t 1.0313072931071396\n",
      "87     \t [ 0.08591917 -0.73654734]. \t  1.0266437538921152 \t 1.0313072931071396\n",
      "88     \t [ 0.16384412 -0.78389894]. \t  0.9701258030438284 \t 1.0313072931071396\n",
      "89     \t [ 0.09017946 -0.77025187]. \t  1.0022597042600423 \t 1.0313072931071396\n",
      "90     \t [ 0.09984834 -0.68590516]. \t  1.0253270692124523 \t 1.0313072931071396\n",
      "91     \t [-0.18510278  0.74391164]. \t  0.9916916831831498 \t 1.0313072931071396\n",
      "92     \t [ 0.12244903 -0.72277092]. \t  1.0269918743251778 \t 1.0313072931071396\n",
      "93     \t [ 3.         -1.29449311]. \t  -109.545719373684 \t 1.0313072931071396\n",
      "94     \t [ 0.1597423  -0.73209308]. \t  1.0110651278595026 \t 1.0313072931071396\n",
      "95     \t [ 0.09147728 -0.7510911 ]. \t  1.018927606547194 \t 1.0313072931071396\n",
      "96     \t [ 0.11160909 -0.61407309]. \t  0.9586033422504149 \t 1.0313072931071396\n",
      "97     \t [ 0.07577647 -0.69760475]. \t  1.02925027293626 \t 1.0313072931071396\n",
      "98     \t [-0.02216681 -0.64431325]. \t  0.9549474955183377 \t 1.0313072931071396\n",
      "99     \t [ 0.05625927 -0.67152986]. \t  1.0155176987678702 \t 1.0313072931071396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.04596611 -0.70970594]. \t  1.024126015197628 \t 1.0313072931071396\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.35716091 -0.67208078]. \t  -11.995980169715628 \t 0.3096187466378422\n",
      "init   \t [ 1.92737474 -1.8332135 ]. \t  -31.167927894156623 \t 0.3096187466378422\n",
      "init   \t [-2.35405992  0.38020826]. \t  -13.013381745677465 \t 0.3096187466378422\n",
      "init   \t [ 0.17890417 -0.32477029]. \t  0.3096187466378422 \t 0.3096187466378422\n",
      "init   \t [-0.9875529   0.49007773]. \t  -0.9989196280666852 \t 0.3096187466378422\n",
      "1      \t [0.47232795 1.55555584]. \t  -15.268111041797926 \t 0.3096187466378422\n",
      "2      \t [-1.18826513 -2.        ]. \t  -52.77605505456779 \t 0.3096187466378422\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.3096187466378422\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t 0.3096187466378422\n",
      "5      \t [-3.         -1.02429781]. \t  -112.17931455038413 \t 0.3096187466378422\n",
      "6      \t [1.29446478 0.23244537]. \t  -2.470960975748472 \t 0.3096187466378422\n",
      "7      \t [-0.79137868  2.        ]. \t  -48.18056831357767 \t 0.3096187466378422\n",
      "8      \t [ 0.41348088 -2.        ]. \t  -47.797187749739614 \t 0.3096187466378422\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.3096187466378422\n",
      "10     \t [ 1.33174228 -0.85131949]. \t  -0.4165546199646455 \t 0.3096187466378422\n",
      "11     \t [-1.19882428 -0.40910321]. \t  -2.333716414569881 \t 0.3096187466378422\n",
      "12     \t [3.         0.15690323]. \t  -109.27465950417032 \t 0.3096187466378422\n",
      "13     \t [0.24364223 0.51293984]. \t  \u001b[92m0.42043763077478835\u001b[0m \t 0.42043763077478835\n",
      "14     \t [1.29885487 2.        ]. \t  -52.9695523334744 \t 0.42043763077478835\n",
      "15     \t [-0.536181   -0.11491218]. \t  -0.9938064972536046 \t 0.42043763077478835\n",
      "16     \t [-1.61515413  0.21700765]. \t  -1.5313099236729006 \t 0.42043763077478835\n",
      "17     \t [ 1.7277968  -0.50866267]. \t  -0.44827083491317465 \t 0.42043763077478835\n",
      "18     \t [-0.45958188 -1.01303277]. \t  -1.3275840362321307 \t 0.42043763077478835\n",
      "19     \t [-1.70699185  1.02285147]. \t  -0.519447672677587 \t 0.42043763077478835\n",
      "20     \t [0.84485354 0.81330751]. \t  -1.6978383938000388 \t 0.42043763077478835\n",
      "21     \t [0.78723812 0.12273761]. \t  -1.7890199256037962 \t 0.42043763077478835\n",
      "22     \t [-3. -2.]. \t  -162.89999999999998 \t 0.42043763077478835\n",
      "23     \t [ 1.81433463 -1.05831853]. \t  -0.9192898081940655 \t 0.42043763077478835\n",
      "24     \t [-1.55994393  0.72364874]. \t  0.024959255520105472 \t 0.42043763077478835\n",
      "25     \t [-0.35837273  1.02543503]. \t  -0.3289877487674448 \t 0.42043763077478835\n",
      "26     \t [ 0.22532919 -0.90409096]. \t  \u001b[92m0.6030736945384928\u001b[0m \t 0.6030736945384928\n",
      "27     \t [-1.99824912 -0.44927501]. \t  -3.964366696019383 \t 0.6030736945384928\n",
      "28     \t [-1.27853177  1.17555284]. \t  -2.9913856698800982 \t 0.6030736945384928\n",
      "29     \t [-0.33542913  0.64174075]. \t  \u001b[92m0.7602221323189167\u001b[0m \t 0.7602221323189167\n",
      "30     \t [-0.17357628 -0.67894483]. \t  0.7574389252419281 \t 0.7602221323189167\n",
      "31     \t [ 1.23668647 -1.51635331]. \t  -12.47308275792842 \t 0.7602221323189167\n",
      "32     \t [1.87661697 1.07946342]. \t  -5.396873382087264 \t 0.7602221323189167\n",
      "33     \t [1.53899696 0.85139913]. \t  -2.634948916403373 \t 0.7602221323189167\n",
      "34     \t [0.17617479 0.96622278]. \t  -0.04435019541956298 \t 0.7602221323189167\n",
      "35     \t [-1.58822032 -1.18101685]. \t  -6.156316482489945 \t 0.7602221323189167\n",
      "36     \t [-3.         0.8510196]. \t  -105.54806539213428 \t 0.7602221323189167\n",
      "37     \t [-1.89610828  2.        ]. \t  -46.93503453962426 \t 0.7602221323189167\n",
      "38     \t [2.02180527 0.54267366]. \t  -4.29490961177407 \t 0.7602221323189167\n",
      "39     \t [-1.16261521 -1.00679312]. \t  -3.6189140688044357 \t 0.7602221323189167\n",
      "40     \t [-3.         -0.09502922]. \t  -109.14929164531976 \t 0.7602221323189167\n",
      "41     \t [-1.92068166  0.65645219]. \t  -0.6701619369326736 \t 0.7602221323189167\n",
      "42     \t [ 3.         -1.04536528]. \t  -106.16949732170283 \t 0.7602221323189167\n",
      "43     \t [0.15240186 2.        ]. \t  -48.396580336357545 \t 0.7602221323189167\n",
      "44     \t [3.         1.12727979]. \t  -113.65812159779225 \t 0.7602221323189167\n",
      "45     \t [-2.02568339 -2.        ]. \t  -56.1362381277288 \t 0.7602221323189167\n",
      "46     \t [ 0.63205334 -0.78300939]. \t  0.15966227644266584 \t 0.7602221323189167\n",
      "47     \t [2.10179106 2.        ]. \t  -57.62848214495432 \t 0.7602221323189167\n",
      "48     \t [ 1.84700032 -0.76699188]. \t  -0.05464486316005879 \t 0.7602221323189167\n",
      "49     \t [ 1.96740304 -0.12382441]. \t  -3.046546821019935 \t 0.7602221323189167\n",
      "50     \t [-0.34579225 -2.        ]. \t  -49.140418650000186 \t 0.7602221323189167\n",
      "51     \t [-1.61994593 -0.67282123]. \t  -2.1579764181742447 \t 0.7602221323189167\n",
      "52     \t [ 0.64934524 -1.18740686]. \t  -2.8791202952728185 \t 0.7602221323189167\n",
      "53     \t [-0.78870256  0.83760207]. \t  -0.25776394523451396 \t 0.7602221323189167\n",
      "54     \t [-2.21750987 -1.18051691]. \t  -13.33712451519261 \t 0.7602221323189167\n",
      "55     \t [1.33530351 1.34621377]. \t  -10.031380831820472 \t 0.7602221323189167\n",
      "56     \t [-0.01280651  0.27617413]. \t  0.28469968558836795 \t 0.7602221323189167\n",
      "57     \t [-2.37763671  1.47648826]. \t  -22.50126076088536 \t 0.7602221323189167\n",
      "58     \t [-0.68418724 -0.67399093]. \t  -0.9159732975940554 \t 0.7602221323189167\n",
      "59     \t [ 2.37878391 -1.34473397]. \t  -18.436597388955196 \t 0.7602221323189167\n",
      "60     \t [-0.09172025  0.70061224]. \t  \u001b[92m1.0304239951835068\u001b[0m \t 1.0304239951835068\n",
      "61     \t [ 0.0618464  -0.71602554]. \t  1.0283699925269834 \t 1.0304239951835068\n",
      "62     \t [-0.12194868  0.73563471]. \t  1.023911037227914 \t 1.0304239951835068\n",
      "63     \t [ 1.06895072 -0.47925795]. \t  -1.1060106478576113 \t 1.0304239951835068\n",
      "64     \t [ 0.0878534  -0.71304062]. \t  \u001b[92m1.0316110565948287\u001b[0m \t 1.0316110565948287\n",
      "65     \t [-0.08375441  0.7360934 ]. \t  1.0266949180113878 \t 1.0316110565948287\n",
      "66     \t [-0.18789325  0.74649151]. \t  0.9885378636550465 \t 1.0316110565948287\n",
      "67     \t [-0.03642679  0.67560449]. \t  1.0117167600078987 \t 1.0316110565948287\n",
      "68     \t [-0.11947729  0.72782432]. \t  1.0267510605292631 \t 1.0316110565948287\n",
      "69     \t [ 0.07997913 -0.71167592]. \t  1.031250317633252 \t 1.0316110565948287\n",
      "70     \t [ 0.05425901 -0.74166465]. \t  1.0184574194884097 \t 1.0316110565948287\n",
      "71     \t [-2.41597203  2.        ]. \t  -61.25675837994664 \t 1.0316110565948287\n",
      "72     \t [ 0.07404324 -0.74540641]. \t  1.0209466581847841 \t 1.0316110565948287\n",
      "73     \t [ 1.31435747 -2.        ]. \t  -47.732771502120045 \t 1.0316110565948287\n",
      "74     \t [ 0.11614061 -0.75913187]. \t  1.011317450836947 \t 1.0316110565948287\n",
      "75     \t [0.00571697 0.6369514 ]. \t  0.9606632167912481 \t 1.0316110565948287\n",
      "76     \t [-0.12037773  0.71536323]. \t  1.0280388239143181 \t 1.0316110565948287\n",
      "77     \t [-0.07693718  0.71476193]. \t  1.0309140602444786 \t 1.0316110565948287\n",
      "78     \t [ 0.00715022 -0.62671624]. \t  0.9582863565274191 \t 1.0316110565948287\n",
      "79     \t [-0.12722253  0.73781209]. \t  1.0217991505223545 \t 1.0316110565948287\n",
      "80     \t [-0.04689028  0.64803536]. \t  0.9959697951524054 \t 1.0316110565948287\n",
      "81     \t [2.49090951 1.52718935]. \t  -39.82818037276169 \t 1.0316110565948287\n",
      "82     \t [ 0.08380107 -0.72928864]. \t  1.0290674144568188 \t 1.0316110565948287\n",
      "83     \t [ 0.04404512 -0.7081411 ]. \t  1.0234296005500156 \t 1.0316110565948287\n",
      "84     \t [ 0.07329712 -0.74284071]. \t  1.0222807077715523 \t 1.0316110565948287\n",
      "85     \t [-0.06813897  0.76531025]. \t  1.0042432022817152 \t 1.0316110565948287\n",
      "86     \t [ 0.08996294 -0.70341164]. \t  1.0309363716782116 \t 1.0316110565948287\n",
      "87     \t [ 0.12421091 -0.74203078]. \t  1.020708204125009 \t 1.0316110565948287\n",
      "88     \t [ 0.11220713 -0.69922501]. \t  1.0279370406439008 \t 1.0316110565948287\n",
      "89     \t [-0.12227975  0.74495792]. \t  1.019668886360801 \t 1.0316110565948287\n",
      "90     \t [-0.07963327 -0.6980481 ]. \t  0.9184825844972682 \t 1.0316110565948287\n",
      "91     \t [ 2.43519052 -2.        ]. \t  -62.51479420842308 \t 1.0316110565948287\n",
      "92     \t [ 0.13280493 -0.75841971]. \t  1.008204264886178 \t 1.0316110565948287\n",
      "93     \t [ 0.12377229 -0.80071114]. \t  0.958639064341511 \t 1.0316110565948287\n",
      "94     \t [-2.25064398  0.96409518]. \t  -7.270325088167809 \t 1.0316110565948287\n",
      "95     \t [ 0.01937695 -0.65838673]. \t  0.993552639222706 \t 1.0316110565948287\n",
      "96     \t [ 0.10563612 -0.81095378]. \t  0.9418819964692047 \t 1.0316110565948287\n",
      "97     \t [ 0.10530887 -0.66789219]. \t  1.0146034796851218 \t 1.0316110565948287\n",
      "98     \t [-0.0598845  0.6993526]. \t  1.0270870014242184 \t 1.0316110565948287\n",
      "99     \t [ 0.10534575 -0.71153859]. \t  1.0306667242555654 \t 1.0316110565948287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.18561225  0.76066626]. \t  0.981140165812911 \t 1.0316110565948287\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.08401759 -0.79057356]. \t  -5.778591021444629 \t -0.6139922708386365\n",
      "init   \t [-2.62778151 -0.16055863]. \t  -37.56206986588009 \t -0.6139922708386365\n",
      "init   \t [2.01152031 1.70798819]. \t  -29.692842569565112 \t -0.6139922708386365\n",
      "init   \t [1.36193391 1.07398489]. \t  -4.49224583043401 \t -0.6139922708386365\n",
      "init   \t [-1.3847696   0.57611717]. \t  -0.6139922708386365 \t -0.6139922708386365\n",
      "1      \t [-0.9736695  -1.34854278]. \t  -9.45625907282596 \t -0.6139922708386365\n",
      "2      \t [-0.02554478  0.8483251 ]. \t  \u001b[92m0.826066925197582\u001b[0m \t 0.826066925197582\n",
      "3      \t [ 1.61290002 -0.41879724]. \t  -0.8084418752138337 \t 0.826066925197582\n",
      "4      \t [-2.53360083 -2.        ]. \t  -80.38003956701601 \t 0.826066925197582\n",
      "5      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.826066925197582\n",
      "6      \t [-1.22546383  2.        ]. \t  -47.948982407544506 \t 0.826066925197582\n",
      "7      \t [-1.00470459 -0.28067667]. \t  -2.2324851549499027 \t 0.826066925197582\n",
      "8      \t [ 0.51358718 -2.        ]. \t  -47.88792149576979 \t 0.826066925197582\n",
      "9      \t [3.        0.4253253]. \t  -109.5832710309234 \t 0.826066925197582\n",
      "10     \t [ 0.83028388 -0.0050339 ]. \t  -1.8644194077153768 \t 0.826066925197582\n",
      "11     \t [0.67834482 2.        ]. \t  -50.78512074850822 \t 0.826066925197582\n",
      "12     \t [-3.  2.]. \t  -150.89999999999998 \t 0.826066925197582\n",
      "13     \t [3. 2.]. \t  -162.89999999999998 \t 0.826066925197582\n",
      "14     \t [-0.71112679  0.71525949]. \t  -0.02077022291544428 \t 0.826066925197582\n",
      "15     \t [ 1.12534924 -0.98688933]. \t  -1.162613948809991 \t 0.826066925197582\n",
      "16     \t [-1.62253942 -0.35855597]. \t  -2.1916240880225333 \t 0.826066925197582\n",
      "17     \t [ 0.05374302 -0.7764374 ]. \t  \u001b[92m0.9878757155421836\u001b[0m \t 0.9878757155421836\n",
      "18     \t [-0.04838893  0.14227131]. \t  0.07685560427083604 \t 0.9878757155421836\n",
      "19     \t [-3.         -1.04958195]. \t  -112.49654332676255 \t 0.9878757155421836\n",
      "20     \t [-1.59415726 -1.08998591]. \t  -4.705072423347367 \t 0.9878757155421836\n",
      "21     \t [-1.29792225 -2.        ]. \t  -52.96826457525743 \t 0.9878757155421836\n",
      "22     \t [ 1.61430568 -2.        ]. \t  -46.83311603006019 \t 0.9878757155421836\n",
      "23     \t [-0.52725581 -0.78051046]. \t  -0.4160856934337449 \t 0.9878757155421836\n",
      "24     \t [1.75769697 0.48551251]. \t  -2.276001885689956 \t 0.9878757155421836\n",
      "25     \t [ 1.07563524 -0.53146769]. \t  -0.9507380406459253 \t 0.9878757155421836\n",
      "26     \t [-0.35311205  2.        ]. \t  -47.76052559151948 \t 0.9878757155421836\n",
      "27     \t [1.33354354 0.35957594]. \t  -2.3759869497740205 \t 0.9878757155421836\n",
      "28     \t [ 3.         -0.72018851]. \t  -105.74082897214457 \t 0.9878757155421836\n",
      "29     \t [0.36393207 0.5873929 ]. \t  0.19644432110044907 \t 0.9878757155421836\n",
      "30     \t [-3.          0.81519715]. \t  -105.56271060961672 \t 0.9878757155421836\n",
      "31     \t [ 1.74880367 -1.04944755]. \t  -0.7376145416098352 \t 0.9878757155421836\n",
      "32     \t [-1.08905757  0.31863542]. \t  -1.6343481378545444 \t 0.9878757155421836\n",
      "33     \t [1.57324464 2.        ]. \t  -53.23629082779634 \t 0.9878757155421836\n",
      "34     \t [1.81380273 1.01390836]. \t  -4.253908016745134 \t 0.9878757155421836\n",
      "35     \t [ 0.19638486 -0.40272242]. \t  0.47145002013848974 \t 0.9878757155421836\n",
      "36     \t [-1.70981842  1.24989218]. \t  -3.4507576932055266 \t 0.9878757155421836\n",
      "37     \t [-1.35022674  1.03927971]. \t  -1.2752594946439162 \t 0.9878757155421836\n",
      "38     \t [-0.35926971 -2.        ]. \t  -49.20056852079475 \t 0.9878757155421836\n",
      "39     \t [ 0.49481464 -0.76316648]. \t  0.49208252987533224 \t 0.9878757155421836\n",
      "40     \t [-1.72032619  0.86849718]. \t  0.15018882957367807 \t 0.9878757155421836\n",
      "41     \t [ 1.54632962 -0.87051544]. \t  -0.03459368173780519 \t 0.9878757155421836\n",
      "42     \t [0.47259298 1.07383308]. \t  -2.006072494718823 \t 0.9878757155421836\n",
      "43     \t [-0.28127393  0.5358357 ]. \t  0.6659639865084543 \t 0.9878757155421836\n",
      "44     \t [-1.96981918  0.16713198]. \t  -2.9388037993976375 \t 0.9878757155421836\n",
      "45     \t [-0.37354937 -0.28134936]. \t  -0.3317043705367577 \t 0.9878757155421836\n",
      "46     \t [-2.04353055  2.        ]. \t  -48.27021066237554 \t 0.9878757155421836\n",
      "47     \t [ 0.02810584 -1.21837154]. \t  -2.845318486194065 \t 0.9878757155421836\n",
      "48     \t [-3. -2.]. \t  -162.89999999999998 \t 0.9878757155421836\n",
      "49     \t [ 2.22822185 -0.35319335]. \t  -7.666261879967318 \t 0.9878757155421836\n",
      "50     \t [-1.98348835 -1.43413649]. \t  -15.069425332849152 \t 0.9878757155421836\n",
      "51     \t [ 1.97868775 -0.78438101]. \t  -0.9765364871769 \t 0.9878757155421836\n",
      "52     \t [2.62793212 1.19488939]. \t  -42.84161775843474 \t 0.9878757155421836\n",
      "53     \t [-0.47256489  1.03814623]. \t  -0.6368317602301479 \t 0.9878757155421836\n",
      "54     \t [ 2.28268348 -2.        ]. \t  -54.41824160827922 \t 0.9878757155421836\n",
      "55     \t [-2.40313667  1.34334922]. \t  -19.844041787078588 \t 0.9878757155421836\n",
      "56     \t [-0.02140738  0.66946334]. \t  \u001b[92m1.0017580239155275\u001b[0m \t 1.0017580239155275\n",
      "57     \t [2.26094429 0.59336293]. \t  -10.527461093133462 \t 1.0017580239155275\n",
      "58     \t [-0.14794081  0.71567363]. \t  \u001b[92m1.0187395851619419\u001b[0m \t 1.0187395851619419\n",
      "59     \t [ 2.53694942 -1.39730802]. \t  -31.517641533149497 \t 1.0187395851619419\n",
      "60     \t [2.32176889 2.        ]. \t  -65.39742951796137 \t 1.0187395851619419\n",
      "61     \t [-2.05328547 -0.33138777]. \t  -4.805864736761783 \t 1.0187395851619419\n",
      "62     \t [-0.23669068  0.75029042]. \t  0.9441855037970283 \t 1.0187395851619419\n",
      "63     \t [ 0.05761247 -0.73055036]. \t  \u001b[92m1.0242913560461777\u001b[0m \t 1.0242913560461777\n",
      "64     \t [-2.15890718  0.8097857 ]. \t  -4.122998685394748 \t 1.0242913560461777\n",
      "65     \t [0.82692609 0.79670001]. \t  -1.5912869612715743 \t 1.0242913560461777\n",
      "66     \t [-0.07729903  0.72160957]. \t  \u001b[92m1.0302367283457867\u001b[0m \t 1.0302367283457867\n",
      "67     \t [ 0.07006864 -0.71671438]. \t  1.0298828096432255 \t 1.0302367283457867\n",
      "68     \t [-3.29695283e-04 -7.54013970e-01]. \t  0.9809616524505111 \t 1.0302367283457867\n",
      "69     \t [ 0.08055011 -0.69265167]. \t  1.0282905984778472 \t 1.0302367283457867\n",
      "70     \t [-0.15676334  0.68129339]. \t  1.00462830189284 \t 1.0302367283457867\n",
      "71     \t [-1.92477809 -2.        ]. \t  -54.795182968975965 \t 1.0302367283457867\n",
      "72     \t [-0.11693037  0.72643954]. \t  1.0275713750869635 \t 1.0302367283457867\n",
      "73     \t [-0.02608984  0.71579985]. \t  1.0153413448837452 \t 1.0302367283457867\n",
      "74     \t [-0.11814267  0.75883525]. \t  1.0112269484695864 \t 1.0302367283457867\n",
      "75     \t [-0.0497384   0.68501654]. \t  1.0204060144016815 \t 1.0302367283457867\n",
      "76     \t [-0.04834234  0.70235763]. \t  1.0244379232611938 \t 1.0302367283457867\n",
      "77     \t [-0.15999664  0.65753535]. \t  0.9858736896306116 \t 1.0302367283457867\n",
      "78     \t [-0.04510003  0.63100342]. \t  0.9788496907670569 \t 1.0302367283457867\n",
      "79     \t [-0.09745684  0.69925233]. \t  1.029856660672655 \t 1.0302367283457867\n",
      "80     \t [-0.00520145  0.61747379]. \t  0.9467199745340155 \t 1.0302367283457867\n",
      "81     \t [-0.09705587  0.66553443]. \t  1.0140755556514642 \t 1.0302367283457867\n",
      "82     \t [ 0.1324624  -0.69763976]. \t  1.0221631901294173 \t 1.0302367283457867\n",
      "83     \t [ 0.03696376 -0.65952787]. \t  1.0020053185896036 \t 1.0302367283457867\n",
      "84     \t [ 0.17300584 -0.74973761]. \t  0.996428419976726 \t 1.0302367283457867\n",
      "85     \t [ 0.06955453 -0.71221695]. \t  1.0300252728213573 \t 1.0302367283457867\n",
      "86     \t [-0.10334209  0.73829072]. \t  1.0256908949987062 \t 1.0302367283457867\n",
      "87     \t [-0.09543707  0.74631532]. \t  1.0219773007854274 \t 1.0302367283457867\n",
      "88     \t [ 0.16178342 -0.78221011]. \t  0.9732417337047197 \t 1.0302367283457867\n",
      "89     \t [0.12086793 1.4325507 ]. \t  -8.868456562109218 \t 1.0302367283457867\n",
      "90     \t [-0.08918418  0.7359168 ]. \t  1.0270361127891734 \t 1.0302367283457867\n",
      "91     \t [-0.06034489  0.72658606]. \t  1.0261878267105433 \t 1.0302367283457867\n",
      "92     \t [-0.1179443   0.67598349]. \t  1.017078502284617 \t 1.0302367283457867\n",
      "93     \t [-0.0358956   0.57560997]. \t  0.9017088294854648 \t 1.0302367283457867\n",
      "94     \t [ 0.05707301 -0.70562724]. \t  1.027247751384552 \t 1.0302367283457867\n",
      "95     \t [-0.10512899  0.68224814]. \t  1.0230003554311728 \t 1.0302367283457867\n",
      "96     \t [ 0.0632181  -0.70952265]. \t  1.0288552301438973 \t 1.0302367283457867\n",
      "97     \t [ 0.06111833 -0.72349209]. \t  1.02710822590805 \t 1.0302367283457867\n",
      "98     \t [-0.18882156  0.76378884]. \t  0.97645534967824 \t 1.0302367283457867\n",
      "99     \t [ 0.05644897 -0.69023827]. \t  1.0240162129981711 \t 1.0302367283457867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.09281098 -0.75487039]. \t  1.0162558291442374 \t 1.0302367283457867\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.34300075 -1.83113994]. \t  -49.66879811482005 \t 0.22190529745233212\n",
      "init   \t [ 0.59815551 -0.80051718]. \t  0.22190529745233212 \t 0.22190529745233212\n",
      "init   \t [ 1.77967069 -0.5352155 ]. \t  -0.42351602320787596 \t 0.22190529745233212\n",
      "init   \t [ 2.28365006 -0.80863347]. \t  -8.27265764318726 \t 0.22190529745233212\n",
      "init   \t [-1.02538042  0.27486766]. \t  -1.710375599764969 \t 0.22190529745233212\n",
      "1      \t [1.23488019 1.00139261]. \t  -3.6461654249610436 \t 0.22190529745233212\n",
      "2      \t [-2.66503756  2.        ]. \t  -84.57225600623991 \t 0.22190529745233212\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.22190529745233212\n",
      "4      \t [-0.02537424  2.        ]. \t  -47.95182606642032 \t 0.22190529745233212\n",
      "5      \t [ 1.30122342 -2.        ]. \t  -47.767905520046746 \t 0.22190529745233212\n",
      "6      \t [0.49095106 0.20941717]. \t  -0.7818806892519532 \t 0.22190529745233212\n",
      "7      \t [-0.67572108 -1.08951613]. \t  -3.0446478971235504 \t 0.22190529745233212\n",
      "8      \t [-3.         -0.09729236]. \t  -109.15437226903498 \t 0.22190529745233212\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.22190529745233212\n",
      "10     \t [ 3.         -0.06370204]. \t  -108.69272795073326 \t 0.22190529745233212\n",
      "11     \t [-0.26401246 -2.        ]. \t  -48.796745384935406 \t 0.22190529745233212\n",
      "12     \t [-0.38219631 -0.32204213]. \t  -0.2917888075478619 \t 0.22190529745233212\n",
      "13     \t [1.17270366 2.        ]. \t  -52.74165827262928 \t 0.22190529745233212\n",
      "14     \t [-1.21551307  1.43952763]. \t  -9.538805305360937 \t 0.22190529745233212\n",
      "15     \t [-0.35132118  0.88751239]. \t  \u001b[92m0.5184254035717119\u001b[0m \t 0.5184254035717119\n",
      "16     \t [ 1.61385151 -1.03375676]. \t  -0.6870894099733322 \t 0.5184254035717119\n",
      "17     \t [-1.46538714 -0.73376672]. \t  -2.2877727675204844 \t 0.5184254035717119\n",
      "18     \t [0.61839421 0.95317902]. \t  -1.4982794853392467 \t 0.5184254035717119\n",
      "19     \t [-1.39763779 -2.        ]. \t  -53.080334707570344 \t 0.5184254035717119\n",
      "20     \t [-0.98115819 -0.59006346]. \t  -1.8730693955225002 \t 0.5184254035717119\n",
      "21     \t [ 0.00205338 -0.82699875]. \t  \u001b[92m0.8663648185151227\u001b[0m \t 0.8663648185151227\n",
      "22     \t [-0.88917149  0.83596637]. \t  -0.4293849235193785 \t 0.8663648185151227\n",
      "23     \t [-1.16622561  2.        ]. \t  -48.06188542164645 \t 0.8663648185151227\n",
      "24     \t [-1.8195126   0.83840571]. \t  0.039696161746725855 \t 0.8663648185151227\n",
      "25     \t [-3. -2.]. \t  -162.89999999999998 \t 0.8663648185151227\n",
      "26     \t [ 0.53815504 -1.40773399]. \t  -8.014742057680557 \t 0.8663648185151227\n",
      "27     \t [-1.74460522 -1.26576264]. \t  -8.18644044515922 \t 0.8663648185151227\n",
      "28     \t [-0.05844995  0.47517806]. \t  0.7133777464101491 \t 0.8663648185151227\n",
      "29     \t [-1.6862435   0.29136938]. \t  -1.2560787363859458 \t 0.8663648185151227\n",
      "30     \t [-3.          1.09433142]. \t  -106.56337252507618 \t 0.8663648185151227\n",
      "31     \t [1.19089814 0.40470706]. \t  -2.3340143554269868 \t 0.8663648185151227\n",
      "32     \t [-1.50916742  0.86654869]. \t  -0.09910595272044564 \t 0.8663648185151227\n",
      "33     \t [ 1.38225601 -0.68390477]. \t  -0.3602124354573647 \t 0.8663648185151227\n",
      "34     \t [-2.18208698 -0.8105018 ]. \t  -8.286199313946597 \t 0.8663648185151227\n",
      "35     \t [ 1.82249375 -0.80010028]. \t  0.04691282646687012 \t 0.8663648185151227\n",
      "36     \t [2.09783477 0.76468137]. \t  -5.976009826903779 \t 0.8663648185151227\n",
      "37     \t [1.77582096 0.55557553]. \t  -2.3168799452747235 \t 0.8663648185151227\n",
      "38     \t [1.94778372 1.42657651]. \t  -14.356549705967291 \t 0.8663648185151227\n",
      "39     \t [ 1.06721708 -1.05924167]. \t  -1.7412108765681877 \t 0.8663648185151227\n",
      "40     \t [-1.87906993 -0.35000245]. \t  -2.843613351887462 \t 0.8663648185151227\n",
      "41     \t [ 2.10218176 -2.        ]. \t  -49.228691084259545 \t 0.8663648185151227\n",
      "42     \t [-0.06934758 -1.10271063]. \t  -1.1461144489857227 \t 0.8663648185151227\n",
      "43     \t [-1.88563821  1.63893213]. \t  -17.682959842675416 \t 0.8663648185151227\n",
      "44     \t [-0.45021868  0.58117181]. \t  0.429085203243811 \t 0.8663648185151227\n",
      "45     \t [3.         1.10910423]. \t  -113.35956798928795 \t 0.8663648185151227\n",
      "46     \t [1.81515752 0.97698543]. \t  -3.9043385360888987 \t 0.8663648185151227\n",
      "47     \t [ 2.07944842 -1.24000336]. \t  -5.709405911793959 \t 0.8663648185151227\n",
      "48     \t [-3.         -1.06603967]. \t  -112.71834540371124 \t 0.8663648185151227\n",
      "49     \t [0.53340607 0.64023374]. \t  -0.3497420428936032 \t 0.8663648185151227\n",
      "50     \t [ 3.         -1.08478381]. \t  -106.47764283104522 \t 0.8663648185151227\n",
      "51     \t [-0.33303173 -0.77870667]. \t  0.2771326009321893 \t 0.8663648185151227\n",
      "52     \t [2.00075727 2.        ]. \t  -55.744560160063955 \t 0.8663648185151227\n",
      "53     \t [ 0.17534097 -0.32432922]. \t  0.3123641623207819 \t 0.8663648185151227\n",
      "54     \t [-1.86155889  1.13511367]. \t  -1.8884421153636197 \t 0.8663648185151227\n",
      "55     \t [-0.4778837   1.40364821]. \t  -7.783442094081267 \t 0.8663648185151227\n",
      "56     \t [ 1.40595288 -0.14583806]. \t  -1.9876198104174718 \t 0.8663648185151227\n",
      "57     \t [-2.22912977  0.23948423]. \t  -8.171501611664302 \t 0.8663648185151227\n",
      "58     \t [ 2.08688477 -0.0035656 ]. \t  -5.116613549108921 \t 0.8663648185151227\n",
      "59     \t [ 0.44585616 -2.        ]. \t  -47.82307224009968 \t 0.8663648185151227\n",
      "60     \t [-3.          1.80435031]. \t  -132.86203934527583 \t 0.8663648185151227\n",
      "61     \t [-2.26873552 -1.30668141]. \t  -18.20375040188926 \t 0.8663648185151227\n",
      "62     \t [0.02245729 0.86571029]. \t  0.7296318904796508 \t 0.8663648185151227\n",
      "63     \t [2.48366337 1.49539684]. \t  -37.779263153301784 \t 0.8663648185151227\n",
      "64     \t [ 0.1515832  -0.81554238]. \t  \u001b[92m0.9237729255252642\u001b[0m \t 0.9237729255252642\n",
      "65     \t [ 0.04036737 -0.70894585]. \t  \u001b[92m1.0220786285025283\u001b[0m \t 1.0220786285025283\n",
      "66     \t [-0.15625775  0.72752928]. \t  1.0138295562009039 \t 1.0220786285025283\n",
      "67     \t [ 0.13871515 -0.75846955]. \t  1.0063528541546523 \t 1.0220786285025283\n",
      "68     \t [ 0.07506688 -0.71974086]. \t  \u001b[92m1.0302553005181803\u001b[0m \t 1.0302553005181803\n",
      "69     \t [-0.15501527  0.71584727]. \t  1.0154375494316061 \t 1.0302553005181803\n",
      "70     \t [ 0.11110791 -0.76229967]. \t  1.0093277120126185 \t 1.0302553005181803\n",
      "71     \t [-0.10848829  0.7165112 ]. \t  1.0302276128023604 \t 1.0302553005181803\n",
      "72     \t [-0.10276296  0.74824361]. \t  1.0205476679194916 \t 1.0302553005181803\n",
      "73     \t [-0.09112782  0.71344435]. \t  \u001b[92m1.0316179327958976\u001b[0m \t 1.0316179327958976\n",
      "74     \t [-0.11287374  0.6867411 ]. \t  1.0236700829339827 \t 1.0316179327958976\n",
      "75     \t [ 0.09306715 -0.72498454]. \t  1.0303616275035736 \t 1.0316179327958976\n",
      "76     \t [-0.1725067   0.76443473]. \t  0.9862200388837125 \t 1.0316179327958976\n",
      "77     \t [-0.08549218  0.68621352]. \t  1.0261524893882232 \t 1.0316179327958976\n",
      "78     \t [ 0.18946784 -0.74424319]. \t  0.9884887479348389 \t 1.0316179327958976\n",
      "79     \t [ 0.10479794 -0.78916493]. \t  0.9787243036860215 \t 1.0316179327958976\n",
      "80     \t [-0.09654863  0.72034579]. \t  1.0310155068785836 \t 1.0316179327958976\n",
      "81     \t [-2.43263113  1.38567441]. \t  -22.903936588707456 \t 1.0316179327958976\n",
      "82     \t [-0.13794773  0.70862453]. \t  1.0223745383042557 \t 1.0316179327958976\n",
      "83     \t [-0.08545017  0.78979877]. \t  0.9771053216743827 \t 1.0316179327958976\n",
      "84     \t [0.48610983 1.62236844]. \t  -18.804045997757097 \t 1.0316179327958976\n",
      "85     \t [ 0.08264403 -0.71601464]. \t  1.0313092286224328 \t 1.0316179327958976\n",
      "86     \t [-0.11740185  0.74055516]. \t  1.0228291060096129 \t 1.0316179327958976\n",
      "87     \t [-0.03545048 -0.69618697]. \t  0.9693569465194084 \t 1.0316179327958976\n",
      "88     \t [-0.08680745  0.80883234]. \t  0.9450670467977874 \t 1.0316179327958976\n",
      "89     \t [ 0.05431154 -0.7029676 ]. \t  1.0262622804901984 \t 1.0316179327958976\n",
      "90     \t [ 0.19949497 -0.6397555 ]. \t  0.9388248871054001 \t 1.0316179327958976\n",
      "91     \t [-0.12472922  0.72994937]. \t  1.0250133091788696 \t 1.0316179327958976\n",
      "92     \t [ 0.04779987 -0.70309419]. \t  1.0243513816748833 \t 1.0316179327958976\n",
      "93     \t [ 0.04966285 -0.71929144]. \t  1.0246609568914378 \t 1.0316179327958976\n",
      "94     \t [ 0.09433614 -0.83030431]. \t  0.8993991523185512 \t 1.0316179327958976\n",
      "95     \t [-0.05452159  0.80949929]. \t  0.9358045076823982 \t 1.0316179327958976\n",
      "96     \t [ 0.01925576 -0.71042752]. \t  1.0121083430013211 \t 1.0316179327958976\n",
      "97     \t [-0.09471123  0.71957669]. \t  1.0311738367367222 \t 1.0316179327958976\n",
      "98     \t [-0.15519592  0.69864352]. \t  1.0127309577241637 \t 1.0316179327958976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.10769652 -0.69183863]. \t  1.0265715560801818 \t 1.0316179327958976\n",
      "100    \t [-0.15130273  0.71868843]. \t  1.0171752764539281 \t 1.0316179327958976\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03367135 -0.0476864 ]. \t  -2.224696248130898 \t -2.224696248130898\n",
      "init   \t [ 1.95297104 -1.87421445]. \t  -34.846635035334884 \t -2.224696248130898\n",
      "init   \t [1.84829978 0.26246968]. \t  -2.674972725698239 \t -2.224696248130898\n",
      "init   \t [-1.21426501 -1.81321712]. \t  -34.68894768245752 \t -2.224696248130898\n",
      "init   \t [ 2.9437644  -1.97269707]. \t  -133.08463964627106 \t -2.224696248130898\n",
      "1      \t [ 0.73251566 -2.        ]. \t  -48.128156061922354 \t -2.224696248130898\n",
      "2      \t [-1.02771758  2.        ]. \t  -48.21944538560117 \t -2.224696248130898\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -2.224696248130898\n",
      "4      \t [-3.        -0.0103161]. \t  -108.93052266704689 \t -2.224696248130898\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -2.224696248130898\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t -2.224696248130898\n",
      "7      \t [0.6543289 2.       ]. \t  -50.66245466830957 \t -2.224696248130898\n",
      "8      \t [-0.74528823 -0.02026764]. \t  \u001b[92m-1.644493249346357\u001b[0m \t -1.644493249346357\n",
      "9      \t [ 1.81682612 -0.68613511]. \t  \u001b[92m-0.06771046228550681\u001b[0m \t -0.06771046228550681\n",
      "10     \t [ 3.         -0.02782184]. \t  -108.81344065254933 \t -0.06771046228550681\n",
      "11     \t [-0.23164112  0.8539178 ]. \t  \u001b[92m0.7790809013256618\u001b[0m \t 0.7790809013256618\n",
      "12     \t [-1.26672285  0.68985206]. \t  -0.5170616247509552 \t 0.7790809013256618\n",
      "13     \t [1.12443109 0.85656902]. \t  -2.5557358254873987 \t 0.7790809013256618\n",
      "14     \t [-1.54824493 -0.50512117]. \t  -2.1348051073325154 \t 0.7790809013256618\n",
      "15     \t [-0.63940111 -1.05508724]. \t  -2.485834126527512 \t 0.7790809013256618\n",
      "16     \t [ 1.27436289 -1.03323378]. \t  -1.3570471193343379 \t 0.7790809013256618\n",
      "17     \t [-1.10045962 -0.75722314]. \t  -2.211131886524492 \t 0.7790809013256618\n",
      "18     \t [-1.38375939  0.06695618]. \t  -2.189314974142447 \t 0.7790809013256618\n",
      "19     \t [-0.7703002   0.76203888]. \t  -0.1427739901456171 \t 0.7790809013256618\n",
      "20     \t [ 0.07557454 -0.54977111]. \t  \u001b[92m0.8623481134231602\u001b[0m \t 0.8623481134231602\n",
      "21     \t [0.09528264 0.34223332]. \t  0.34487159787636934 \t 0.8623481134231602\n",
      "22     \t [ 1.56314132 -0.2893603 ]. \t  -1.3395150194140064 \t 0.8623481134231602\n",
      "23     \t [1.40224249 0.43363252]. \t  -2.2773678008305955 \t 0.8623481134231602\n",
      "24     \t [-0.40302209 -2.        ]. \t  -49.40177663307693 \t 0.8623481134231602\n",
      "25     \t [0.46473691 0.86916015]. \t  -0.43424794190293714 \t 0.8623481134231602\n",
      "26     \t [1.93619456 0.99362536]. \t  -4.917879833039582 \t 0.8623481134231602\n",
      "27     \t [1.69126254 2.        ]. \t  -53.4433123666051 \t 0.8623481134231602\n",
      "28     \t [-1.871639  2.      ]. \t  -46.82810200054794 \t 0.8623481134231602\n",
      "29     \t [ 1.61123429 -0.93301156]. \t  -0.10911431154136897 \t 0.8623481134231602\n",
      "30     \t [ 0.59041569 -0.7347197 ]. \t  0.2741497350701212 \t 0.8623481134231602\n",
      "31     \t [-1.39722014  1.26363888]. \t  -4.3316643730392235 \t 0.8623481134231602\n",
      "32     \t [-2.231051    0.92450349]. \t  -6.4294808256439815 \t 0.8623481134231602\n",
      "33     \t [-1.80867095  0.92521702]. \t  -0.11501692333627622 \t 0.8623481134231602\n",
      "34     \t [ 0.05931659 -1.0604683 ]. \t  -0.5116102164225121 \t 0.8623481134231602\n",
      "35     \t [-1.95906176 -1.27450489]. \t  -9.81677864979551 \t 0.8623481134231602\n",
      "36     \t [-1.57213083 -1.05624597]. \t  -4.267433425994542 \t 0.8623481134231602\n",
      "37     \t [-0.25175074 -0.80118441]. \t  0.47259955661491143 \t 0.8623481134231602\n",
      "38     \t [-3.        -1.0432101]. \t  -112.4139581152426 \t 0.8623481134231602\n",
      "39     \t [-2.0144178  1.2471057]. \t  -4.867185354564327 \t 0.8623481134231602\n",
      "40     \t [ 0.451261   -0.11063739]. \t  -0.6319887948933353 \t 0.8623481134231602\n",
      "41     \t [ 3.         -1.03909744]. \t  -106.12702497054819 \t 0.8623481134231602\n",
      "42     \t [3.         1.00747205]. \t  -111.98331581198542 \t 0.8623481134231602\n",
      "43     \t [-0.08102373  0.67089265]. \t  \u001b[92m1.0182279945638468\u001b[0m \t 1.0182279945638468\n",
      "44     \t [-3.          1.01005341]. \t  -105.95230478317741 \t 1.0182279945638468\n",
      "45     \t [-2.11728563 -0.1716403 ]. \t  -6.008248586344435 \t 1.0182279945638468\n",
      "46     \t [-2.01671913 -2.        ]. \t  -55.99034085469292 \t 1.0182279945638468\n",
      "47     \t [1.42069499 1.32491856]. \t  -9.445799441257229 \t 1.0182279945638468\n",
      "48     \t [-0.1345617  2.       ]. \t  -47.80261747728246 \t 1.0182279945638468\n",
      "49     \t [-2.05857791 -0.77788142]. \t  -5.251494670664376 \t 1.0182279945638468\n",
      "50     \t [ 2.04493627 -1.22766009]. \t  -4.9266659773972865 \t 1.0182279945638468\n",
      "51     \t [ 1.54609695 -1.38969906]. \t  -7.160545377591359 \t 1.0182279945638468\n",
      "52     \t [ 0.5985886  -1.11993987]. \t  -1.784240274890571 \t 1.0182279945638468\n",
      "53     \t [1.70395683 0.7713887 ]. \t  -2.420018009203515 \t 1.0182279945638468\n",
      "54     \t [0.29363294 0.61264029]. \t  0.4284527453482747 \t 1.0182279945638468\n",
      "55     \t [-1.89539807  0.44452284]. \t  -1.2455797171719656 \t 1.0182279945638468\n",
      "56     \t [ 0.15120607 -0.77066276]. \t  0.9908848548913304 \t 1.0182279945638468\n",
      "57     \t [0.01577088 0.71099009]. \t  0.9876710003189899 \t 1.0182279945638468\n",
      "58     \t [2.32580099 2.        ]. \t  -65.60188132811831 \t 1.0182279945638468\n",
      "59     \t [-0.02018293  0.693137  ]. \t  1.010829934990809 \t 1.0182279945638468\n",
      "60     \t [ 1.39515495 -2.        ]. \t  -47.497435986743255 \t 1.0182279945638468\n",
      "61     \t [-0.04335852  0.75543622]. \t  1.0052573277205488 \t 1.0182279945638468\n",
      "62     \t [-0.83059454  1.13335053]. \t  -2.3898172961612394 \t 1.0182279945638468\n",
      "63     \t [ 2.10309805 -0.26008986]. \t  -4.652957166151482 \t 1.0182279945638468\n",
      "64     \t [ 0.10635618 -0.74173182]. \t  \u001b[92m1.0238428885592374\u001b[0m \t 1.0238428885592374\n",
      "65     \t [-0.09709049  0.8184852 ]. \t  0.9264586697730657 \t 1.0238428885592374\n",
      "66     \t [ 0.09762832 -0.73132777]. \t  \u001b[92m1.028608245592542\u001b[0m \t 1.028608245592542\n",
      "67     \t [-0.04730349  0.7689621 ]. \t  0.9940898775475384 \t 1.028608245592542\n",
      "68     \t [ 0.09207226 -0.73727037]. \t  1.0265311158977297 \t 1.028608245592542\n",
      "69     \t [ 0.01645821 -0.72878123]. \t  1.0070367597917897 \t 1.028608245592542\n",
      "70     \t [-0.09872569  0.75154924]. \t  1.0185995821555265 \t 1.028608245592542\n",
      "71     \t [-0.01565983  0.79749825]. \t  0.9375201363664172 \t 1.028608245592542\n",
      "72     \t [ 0.08138834 -0.73575965]. \t  1.0266413255628573 \t 1.028608245592542\n",
      "73     \t [-0.0662497   0.72145032]. \t  1.028600749778595 \t 1.028608245592542\n",
      "74     \t [ 0.09441738 -0.70062476]. \t  \u001b[92m1.0303261821126672\u001b[0m \t 1.0303261821126672\n",
      "75     \t [-0.00120878 -0.62363682]. \t  0.9498878140035095 \t 1.0303261821126672\n",
      "76     \t [ 0.08420371 -0.71987153]. \t  \u001b[92m1.0310330984543812\u001b[0m \t 1.0310330984543812\n",
      "77     \t [ 0.12048096 -0.71713441]. \t  1.0279639796627986 \t 1.0310330984543812\n",
      "78     \t [ 0.1114957  -0.64562374]. \t  0.9949140103347492 \t 1.0310330984543812\n",
      "79     \t [ 0.05546208 -0.72718696]. \t  1.0247290328665848 \t 1.0310330984543812\n",
      "80     \t [-0.01114469  0.74466626]. \t  0.9959091552662364 \t 1.0310330984543812\n",
      "81     \t [0.21371093 1.45810441]. \t  -10.066348289145886 \t 1.0310330984543812\n",
      "82     \t [ 0.14036661 -0.73065265]. \t  1.019976580227846 \t 1.0310330984543812\n",
      "83     \t [ 0.19422233 -0.72832828]. \t  0.9898269572337636 \t 1.0310330984543812\n",
      "84     \t [-0.15670397  0.73837261]. \t  1.010572720541442 \t 1.0310330984543812\n",
      "85     \t [ 0.04999928 -0.68575793]. \t  1.0207638753508588 \t 1.0310330984543812\n",
      "86     \t [ 0.01155591 -0.71125299]. \t  1.0075467196742045 \t 1.0310330984543812\n",
      "87     \t [ 0.09199128 -0.71756408]. \t  \u001b[92m1.0314224128009926\u001b[0m \t 1.0314224128009926\n",
      "88     \t [ 0.04143687 -0.72743563]. \t  1.0198789598189066 \t 1.0314224128009926\n",
      "89     \t [ 0.19480094 -0.78458916]. \t  0.9506197990071039 \t 1.0314224128009926\n",
      "90     \t [2.44448937 0.53913982]. \t  -20.533344012551055 \t 1.0314224128009926\n",
      "91     \t [ 0.14488948 -0.69788638]. \t  1.017395655684803 \t 1.0314224128009926\n",
      "92     \t [-0.01166615 -0.63481594]. \t  0.9544070218782755 \t 1.0314224128009926\n",
      "93     \t [-0.10327188  0.73481319]. \t  1.0270795267495219 \t 1.0314224128009926\n",
      "94     \t [ 0.16414448 -0.71002067]. \t  1.0102220961416097 \t 1.0314224128009926\n",
      "95     \t [-0.00471349  0.73777588]. \t  0.9955339755674039 \t 1.0314224128009926\n",
      "96     \t [-0.06427298  0.70134659]. \t  1.0283261048922832 \t 1.0314224128009926\n",
      "97     \t [ 0.16903745 -0.72741535]. \t  1.006977626129794 \t 1.0314224128009926\n",
      "98     \t [ 0.21040804 -0.72761788]. \t  0.9766335140728151 \t 1.0314224128009926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.00451792  0.67292641]. \t  0.994058533467118 \t 1.0314224128009926\n",
      "100    \t [-0.07577489  0.70926496]. \t  1.0308089344968259 \t 1.0314224128009926\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.92153751 -1.53997223]. \t  -13.677695110590259 \t -6.372423095293032\n",
      "init   \t [ 2.70169719 -0.07123439]. \t  -46.72852427361676 \t -6.372423095293032\n",
      "init   \t [ 2.23484721 -1.15066928]. \t  -8.26768569212749 \t -6.372423095293032\n",
      "init   \t [-2.75574225 -0.41122215]. \t  -55.82511776655432 \t -6.372423095293032\n",
      "init   \t [-1.60120682  1.3669629 ]. \t  -6.372423095293032 \t -6.372423095293032\n",
      "1      \t [0.39317266 2.        ]. \t  -49.355733082732876 \t -6.372423095293032\n",
      "2      \t [ 3. -2.]. \t  -150.90000000000023 \t -6.372423095293032\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -6.372423095293032\n",
      "4      \t [-0.70445603  0.21865018]. \t  \u001b[92m-1.1724809119341668\u001b[0m \t -1.1724809119341668\n",
      "5      \t [-1.3514984 -2.       ]. \t  -53.03429626097272 \t -1.1724809119341668\n",
      "6      \t [ 1.36558106 -0.45226916]. \t  \u001b[92m-1.0496496376034106\u001b[0m \t -1.0496496376034106\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t -1.0496496376034106\n",
      "8      \t [-3. -2.]. \t  -162.89999999999998 \t -1.0496496376034106\n",
      "9      \t [-0.90961682  2.        ]. \t  -48.24153923333587 \t -1.0496496376034106\n",
      "10     \t [-1.65323797  0.39637004]. \t  \u001b[92m-0.8660159775661\u001b[0m \t -0.8660159775661\n",
      "11     \t [-0.39285815 -0.97438835]. \t  \u001b[92m-0.7593086514555384\u001b[0m \t -0.7593086514555384\n",
      "12     \t [-0.1638185 -2.       ]. \t  -48.43347702394037 \t -0.7593086514555384\n",
      "13     \t [0.57787473 0.57656434]. \t  \u001b[92m-0.5594931766101716\u001b[0m \t -0.5594931766101716\n",
      "14     \t [ 0.2244464  -0.30188738]. \t  \u001b[92m0.20286035362390797\u001b[0m \t 0.20286035362390797\n",
      "15     \t [ 1.64642193 -1.07532621]. \t  -1.0041509328393363 \t 0.20286035362390797\n",
      "16     \t [-1.2862132  -0.65996502]. \t  -2.244684027410751 \t 0.20286035362390797\n",
      "17     \t [1.40763    0.97983279]. \t  -3.4999692373925684 \t 0.20286035362390797\n",
      "18     \t [-1.17860447  0.92599686]. \t  -0.8174675787712551 \t 0.20286035362390797\n",
      "19     \t [1.19042324 0.38926822]. \t  -2.348952070144628 \t 0.20286035362390797\n",
      "20     \t [1.39626083 2.        ]. \t  -53.07907321300223 \t 0.20286035362390797\n",
      "21     \t [-3.          0.54948217]. \t  -106.40847934159662 \t 0.20286035362390797\n",
      "22     \t [ 2.03542806 -0.66678078]. \t  -1.8857321946676069 \t 0.20286035362390797\n",
      "23     \t [0.939144   1.05160313]. \t  -3.5789765507912903 \t 0.20286035362390797\n",
      "24     \t [ 1.49569343 -2.        ]. \t  -47.179261597767244 \t 0.20286035362390797\n",
      "25     \t [ 0.55384825 -0.88579545]. \t  0.12751652686134318 \t 0.20286035362390797\n",
      "26     \t [ 3.         -0.78788169]. \t  -105.59468447956816 \t 0.20286035362390797\n",
      "27     \t [2.00846979 0.4797348 ]. \t  -4.0989979583873515 \t 0.20286035362390797\n",
      "28     \t [-0.77467798 -0.47068379]. \t  -1.3910128354224875 \t 0.20286035362390797\n",
      "29     \t [-0.13965626  0.86076224]. \t  \u001b[92m0.81083804727001\u001b[0m \t 0.81083804727001\n",
      "30     \t [-0.08338585  0.46920722]. \t  0.6981618984510521 \t 0.81083804727001\n",
      "31     \t [3.         0.83811115]. \t  -110.57824556107445 \t 0.81083804727001\n",
      "32     \t [-1.78813351  2.        ]. \t  -46.64034411266181 \t 0.81083804727001\n",
      "33     \t [-1.63049587 -0.19959049]. \t  -2.2275166382851466 \t 0.81083804727001\n",
      "34     \t [-2.00351652 -1.07787319]. \t  -6.690271701814723 \t 0.81083804727001\n",
      "35     \t [ 0.00930641 -0.79906984]. \t  \u001b[92m0.9303470712374433\u001b[0m \t 0.9303470712374433\n",
      "36     \t [-1.52490343  0.82913935]. \t  -0.013653785395077112 \t 0.9303470712374433\n",
      "37     \t [-1.51072977 -1.15898021]. \t  -5.748362626464249 \t 0.9303470712374433\n",
      "38     \t [-1.31987832  0.26675234]. \t  -1.7410088587042736 \t 0.9303470712374433\n",
      "39     \t [ 0.74418565 -0.28907699]. \t  -1.1063256007793925 \t 0.9303470712374433\n",
      "40     \t [1.81190206 0.0152449 ]. \t  -2.3195172267476387 \t 0.9303470712374433\n",
      "41     \t [ 1.84404314 -0.89382302]. \t  -0.13513084485439875 \t 0.9303470712374433\n",
      "42     \t [-0.22207743 -0.5755181 ]. \t  0.5660394293284721 \t 0.9303470712374433\n",
      "43     \t [0.23947662 1.01182516]. \t  -0.5622853658780841 \t 0.9303470712374433\n",
      "44     \t [-2.09840527 -2.        ]. \t  -57.55162921627238 \t 0.9303470712374433\n",
      "45     \t [-0.44271389  1.29161693]. \t  -4.593464661896028 \t 0.9303470712374433\n",
      "46     \t [ 0.1360096  -1.19095955]. \t  -2.285017519595316 \t 0.9303470712374433\n",
      "47     \t [2.14934538 1.45649226]. \t  -19.171132301828056 \t 0.9303470712374433\n",
      "48     \t [-2.25339864  1.16991823]. \t  -9.189149831898451 \t 0.9303470712374433\n",
      "49     \t [-1.88338117  0.91504747]. \t  -0.37458779589140345 \t 0.9303470712374433\n",
      "50     \t [-3.        -1.1115323]. \t  -113.39846179871486 \t 0.9303470712374433\n",
      "51     \t [2.13705826 2.        ]. \t  -58.49354255314842 \t 0.9303470712374433\n",
      "52     \t [ 2.23876472 -2.        ]. \t  -52.786099948266816 \t 0.9303470712374433\n",
      "53     \t [-0.50803755  0.76747593]. \t  0.4599640501157759 \t 0.9303470712374433\n",
      "54     \t [-2.00016873 -0.59855315]. \t  -4.0130555863419115 \t 0.9303470712374433\n",
      "55     \t [ 0.54786051 -2.        ]. \t  -47.92470666289088 \t 0.9303470712374433\n",
      "56     \t [ 1.89235092 -1.38240699]. \t  -7.049803845899275 \t 0.9303470712374433\n",
      "57     \t [-3.          1.32098372]. \t  -110.13712883496981 \t 0.9303470712374433\n",
      "58     \t [-2.18964485  0.08188956]. \t  -7.436594128951913 \t 0.9303470712374433\n",
      "59     \t [1.61139312 1.43011824]. \t  -12.918745275121015 \t 0.9303470712374433\n",
      "60     \t [0.16158905 0.11818338]. \t  -0.06702650887951411 \t 0.9303470712374433\n",
      "61     \t [-0.0082924   0.72701808]. \t  \u001b[92m1.0024920620052\u001b[0m \t 1.0024920620052\n",
      "62     \t [-0.25552049  2.        ]. \t  -47.741262634155454 \t 1.0024920620052\n",
      "63     \t [1.92679007 0.9788111 ]. \t  -4.687782944958158 \t 1.0024920620052\n",
      "64     \t [-0.04863334  0.71395792]. \t  \u001b[92m1.0248939423759604\u001b[0m \t 1.0248939423759604\n",
      "65     \t [-0.0493785  0.7630792]. \t  1.0008527559619826 \t 1.0248939423759604\n",
      "66     \t [ 0.053568   -0.73819668]. \t  1.0200065031077499 \t 1.0248939423759604\n",
      "67     \t [ 0.05048815 -0.71444973]. \t  \u001b[92m1.0254528279780106\u001b[0m \t 1.0254528279780106\n",
      "68     \t [-0.07700908 -0.7425916 ]. \t  0.9085806611045798 \t 1.0254528279780106\n",
      "69     \t [ 0.03119112 -0.68746218]. \t  1.0145510482836928 \t 1.0254528279780106\n",
      "70     \t [-0.06228752  0.71060129]. \t  \u001b[92m1.028676065087568\u001b[0m \t 1.028676065087568\n",
      "71     \t [ 1.04912116 -0.76274738]. \t  -0.5295925987914485 \t 1.028676065087568\n",
      "72     \t [-0.04403594 -0.68713106]. \t  0.958890081557573 \t 1.028676065087568\n",
      "73     \t [-0.08689431  0.7324163 ]. \t  1.0282502670677571 \t 1.028676065087568\n",
      "74     \t [-0.10956246  0.7357637 ]. \t  1.0260596679288891 \t 1.028676065087568\n",
      "75     \t [ 0.00580744 -0.77508527]. \t  0.9637583235773447 \t 1.028676065087568\n",
      "76     \t [-0.01570105  0.7221693 ]. \t  1.008498947622765 \t 1.028676065087568\n",
      "77     \t [-0.04443905  0.73316667]. \t  1.0190550867285146 \t 1.028676065087568\n",
      "78     \t [ 0.05798123 -0.77096095]. \t  0.9956467659052857 \t 1.028676065087568\n",
      "79     \t [ 0.16232632 -0.76012843]. \t  0.9952330841264985 \t 1.028676065087568\n",
      "80     \t [ 0.17435989 -0.72918979]. \t  1.00344344594462 \t 1.028676065087568\n",
      "81     \t [-0.18801505  0.74946444]. \t  0.9868953343640425 \t 1.028676065087568\n",
      "82     \t [ 0.07304899 -0.68077418]. \t  1.0231023997864632 \t 1.028676065087568\n",
      "83     \t [ 0.11365727 -0.80100401]. \t  0.9595070875539907 \t 1.028676065087568\n",
      "84     \t [ 0.10588767 -0.7339593 ]. \t  1.0271423745051471 \t 1.028676065087568\n",
      "85     \t [-0.05975119  0.65926624]. \t  1.0080459604213303 \t 1.028676065087568\n",
      "86     \t [ 0.07411202 -0.7393916 ]. \t  1.0241671985256422 \t 1.028676065087568\n",
      "87     \t [ 0.20320501 -0.70056775]. \t  0.9804079971363368 \t 1.028676065087568\n",
      "88     \t [-0.12310449  0.69330577]. \t  1.023717179462792 \t 1.028676065087568\n",
      "89     \t [-0.83351791 -1.55418758]. \t  -16.849149875274122 \t 1.028676065087568\n",
      "90     \t [0.034588   0.68284202]. \t  0.9670495709213215 \t 1.028676065087568\n",
      "91     \t [ 0.10917724 -0.6559244 ]. \t  1.0047635356813542 \t 1.028676065087568\n",
      "92     \t [-2.21798908  1.6443959 ]. \t  -23.324980836724894 \t 1.028676065087568\n",
      "93     \t [-0.14639322  0.71581801]. \t  1.019413637279392 \t 1.028676065087568\n",
      "94     \t [ 0.0964307  -0.68815052]. \t  1.0265464650058211 \t 1.028676065087568\n",
      "95     \t [-0.11155897  0.63997966]. \t  0.989231007265727 \t 1.028676065087568\n",
      "96     \t [-0.03273187 -0.60604453]. \t  0.9054321293291419 \t 1.028676065087568\n",
      "97     \t [-0.07076744  0.72250359]. \t  \u001b[92m1.029212221179088\u001b[0m \t 1.029212221179088\n",
      "98     \t [ 0.00698387 -0.57661782]. \t  0.8915909952301285 \t 1.029212221179088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.17268579 -0.62480854]. \t  0.9424114583102275 \t 1.029212221179088\n",
      "100    \t [-0.17685913  0.74269685]. \t  0.9976308552644034 \t 1.029212221179088\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.88621398 1.2849914 ]. \t  -88.29186392936245 \t -1.3768038348803564\n",
      "init   \t [ 0.87536168 -0.30524795]. \t  -1.3768038348803564 \t -1.3768038348803564\n",
      "init   \t [-1.78613413 -0.03237292]. \t  -2.2646252293989613 \t -1.3768038348803564\n",
      "init   \t [-2.16865001 -0.18991586]. \t  -7.310805476408371 \t -1.3768038348803564\n",
      "init   \t [-2.31759223 -1.98830205]. \t  -63.86369356519016 \t -1.3768038348803564\n",
      "1      \t [-2.45083335  1.96417842]. \t  -59.78831185140723 \t -1.3768038348803564\n",
      "2      \t [ 2.61846243 -2.        ]. \t  -78.9062929875011 \t -1.3768038348803564\n",
      "3      \t [0.05567862 1.3625647 ]. \t  -6.449511031629841 \t -1.3768038348803564\n",
      "4      \t [ 0.07997966 -2.        ]. \t  -47.865541823636015 \t -1.3768038348803564\n",
      "5      \t [-0.30820891  0.03007353]. \t  \u001b[92m-0.34842372060568827\u001b[0m \t -0.34842372060568827\n",
      "6      \t [1.01630722 2.        ]. \t  -52.291074672843436 \t -0.34842372060568827\n",
      "7      \t [-0.91490464  1.00170776]. \t  -1.1695770777948553 \t -0.34842372060568827\n",
      "8      \t [-0.63408717  2.        ]. \t  -48.02227675246879 \t -0.34842372060568827\n",
      "9      \t [0.41758942 0.55706207]. \t  \u001b[92m-0.011973015713889201\u001b[0m \t -0.011973015713889201\n",
      "10     \t [-3.          0.47399357]. \t  -106.7812459275774 \t -0.011973015713889201\n",
      "11     \t [-1.66997918 -0.73972001]. \t  -2.296749601454798 \t -0.011973015713889201\n",
      "12     \t [ 2.09712722 -0.3459838 ]. \t  -4.18156183811535 \t -0.011973015713889201\n",
      "13     \t [-0.36447943  0.7035983 ]. \t  \u001b[92m0.7612471251478683\u001b[0m \t 0.7612471251478683\n",
      "14     \t [ 3.         -0.42461944]. \t  -107.03496976627588 \t 0.7612471251478683\n",
      "15     \t [1.55054536 0.11820127]. \t  -2.2388352817037576 \t 0.7612471251478683\n",
      "16     \t [ 1.60216058 -0.76231491]. \t  0.12653165164063107 \t 0.7612471251478683\n",
      "17     \t [-3.         -1.04420979]. \t  -112.42679556133515 \t 0.7612471251478683\n",
      "18     \t [-1.11009425 -2.        ]. \t  -52.5841839345876 \t 0.7612471251478683\n",
      "19     \t [-0.11563492 -0.82113291]. \t  0.73047213635317 \t 0.7612471251478683\n",
      "20     \t [ 1.66442308 -0.35781792]. \t  -1.0094416439805212 \t 0.7612471251478683\n",
      "21     \t [ 1.32059274 -2.        ]. \t  -47.71574504883989 \t 0.7612471251478683\n",
      "22     \t [ 0.25958611 -0.51490966]. \t  0.6529051551228678 \t 0.7612471251478683\n",
      "23     \t [-0.84516771 -0.57776828]. \t  -1.5060051366255593 \t 0.7612471251478683\n",
      "24     \t [1.111128   0.97528123]. \t  -3.262670500005047 \t 0.7612471251478683\n",
      "25     \t [2.28794513 2.        ]. \t  -63.7841435631434 \t 0.7612471251478683\n",
      "26     \t [ 2.03437327 -0.94525147]. \t  -1.910895028548587 \t 0.7612471251478683\n",
      "27     \t [-0.90443808  0.41211641]. \t  -1.1125864783115043 \t 0.7612471251478683\n",
      "28     \t [-1.78099855 -0.39597564]. \t  -2.3734751567357333 \t 0.7612471251478683\n",
      "29     \t [-1.72268351  0.91935643]. \t  0.019076519569992678 \t 0.7612471251478683\n",
      "30     \t [-1.47423833  0.7322316 ]. \t  -0.12179365708257828 \t 0.7612471251478683\n",
      "31     \t [ 0.62951259 -1.13412364]. \t  -2.0348252129920024 \t 0.7612471251478683\n",
      "32     \t [-1.64671327  2.        ]. \t  -46.75807239623979 \t 0.7612471251478683\n",
      "33     \t [0.96668869 0.53704832]. \t  -1.8743294885734385 \t 0.7612471251478683\n",
      "34     \t [1.85971146 0.86005089]. \t  -3.334014269564381 \t 0.7612471251478683\n",
      "35     \t [-0.87090084 -1.12681364]. \t  -4.322390410474755 \t 0.7612471251478683\n",
      "36     \t [ 0.51416767 -0.78077427]. \t  0.43653066233984994 \t 0.7612471251478683\n",
      "37     \t [3. 2.]. \t  -162.89999999999998 \t 0.7612471251478683\n",
      "38     \t [2.19332674 0.44180705]. \t  -8.094457113172599 \t 0.7612471251478683\n",
      "39     \t [-3. -2.]. \t  -162.89999999999998 \t 0.7612471251478683\n",
      "40     \t [-3.         1.6060095]. \t  -120.37536832925235 \t 0.7612471251478683\n",
      "41     \t [ 3.         -1.49196448]. \t  -115.33983073455552 \t 0.7612471251478683\n",
      "42     \t [ 1.75000139 -1.20882264]. \t  -2.7091812118879446 \t 0.7612471251478683\n",
      "43     \t [1.63420365 1.48949715]. \t  -15.302488020349863 \t 0.7612471251478683\n",
      "44     \t [ 0.1124848  -1.09259046]. \t  -0.852553998648475 \t 0.7612471251478683\n",
      "45     \t [-1.64280647 -1.43755427]. \t  -13.230153152597651 \t 0.7612471251478683\n",
      "46     \t [0.30249007 0.93831981]. \t  -0.21145767904700846 \t 0.7612471251478683\n",
      "47     \t [-1.56172061  1.1997983 ]. \t  -2.757015739913302 \t 0.7612471251478683\n",
      "48     \t [ 1.81611386 -0.8155921 ]. \t  0.06383531409479415 \t 0.7612471251478683\n",
      "49     \t [-0.27895768  1.01400715]. \t  -0.13187020460287766 \t 0.7612471251478683\n",
      "50     \t [-2.18394191 -1.15271166]. \t  -11.738071990274438 \t 0.7612471251478683\n",
      "51     \t [3.         0.51126905]. \t  -109.6615345508554 \t 0.7612471251478683\n",
      "52     \t [ 1.9860905 -2.       ]. \t  -47.58951380193522 \t 0.7612471251478683\n",
      "53     \t [0.13916137 2.        ]. \t  -48.35500113901607 \t 0.7612471251478683\n",
      "54     \t [-3.  2.]. \t  -150.89999999999998 \t 0.7612471251478683\n",
      "55     \t [-2.02296639  1.33228649]. \t  -6.852672605561985 \t 0.7612471251478683\n",
      "56     \t [1.69570639 2.        ]. \t  -53.45490447908024 \t 0.7612471251478683\n",
      "57     \t [-3.         -0.31038476]. \t  -109.4829240619875 \t 0.7612471251478683\n",
      "58     \t [2.18479331 1.18742387]. \t  -12.404866195474968 \t 0.7612471251478683\n",
      "59     \t [-0.41913261 -1.5556055 ]. \t  -15.035887424592003 \t 0.7612471251478683\n",
      "60     \t [-2.32787679  0.86664222]. \t  -10.287128852935554 \t 0.7612471251478683\n",
      "61     \t [-0.05132795  0.68458042]. \t  \u001b[92m1.0206832455896313\u001b[0m \t 1.0206832455896313\n",
      "62     \t [-0.1050853   0.72433255]. \t  \u001b[92m1.029768662795715\u001b[0m \t 1.029768662795715\n",
      "63     \t [-1.86430627  0.59428146]. \t  -0.5080444990788275 \t 1.029768662795715\n",
      "64     \t [0.21142422 0.05865935]. \t  -0.17332024317601513 \t 1.029768662795715\n",
      "65     \t [ 0.08300098 -0.73154565]. \t  1.02831728198273 \t 1.029768662795715\n",
      "66     \t [-0.14411668  0.75780906]. \t  1.0049705173521175 \t 1.029768662795715\n",
      "67     \t [-0.08767501  0.70886013]. \t  \u001b[92m1.031500969033141\u001b[0m \t 1.031500969033141\n",
      "68     \t [-0.11000976  0.67790233]. \t  1.0199299619204714 \t 1.031500969033141\n",
      "69     \t [ 0.08418316 -0.70670887]. \t  1.0312498510589272 \t 1.031500969033141\n",
      "70     \t [ 0.09583394 -0.69720703]. \t  1.029483290568792 \t 1.031500969033141\n",
      "71     \t [-0.13984841  0.71297915]. \t  1.0220012692033893 \t 1.031500969033141\n",
      "72     \t [ 0.05289235 -0.71942751]. \t  1.0256425806033551 \t 1.031500969033141\n",
      "73     \t [-0.05725659  0.70472598]. \t  1.0272143021643967 \t 1.031500969033141\n",
      "74     \t [ 0.03943533 -0.67524377]. \t  1.0126528061267817 \t 1.031500969033141\n",
      "75     \t [-0.10708418  0.70660876]. \t  1.0300721820096266 \t 1.031500969033141\n",
      "76     \t [-2.05099856 -0.71960057]. \t  -4.955642540565799 \t 1.031500969033141\n",
      "77     \t [-0.04476547  0.72194437]. \t  1.0225124373068042 \t 1.031500969033141\n",
      "78     \t [-0.03710115  0.72169718]. \t  1.0195354341807383 \t 1.031500969033141\n",
      "79     \t [ 0.07784746 -0.69198791]. \t  1.0279158922637364 \t 1.031500969033141\n",
      "80     \t [-0.18576467 -0.55089814]. \t  0.6076489210081015 \t 1.031500969033141\n",
      "81     \t [-0.17151473  0.73406747]. \t  1.0040041221739995 \t 1.031500969033141\n",
      "82     \t [ 0.0490465  -0.71131347]. \t  1.0251349321001455 \t 1.031500969033141\n",
      "83     \t [ 0.06714356 -0.67214826]. \t  1.0178406423674584 \t 1.031500969033141\n",
      "84     \t [-0.09987515  0.77845176]. \t  0.9931233315568329 \t 1.031500969033141\n",
      "85     \t [ 0.05145696 -0.7298539 ]. \t  1.0227058188093574 \t 1.031500969033141\n",
      "86     \t [-0.00163646  0.69601055]. \t  1.0001586703161316 \t 1.031500969033141\n",
      "87     \t [-0.20233814  0.63354251]. \t  0.9290172632429705 \t 1.031500969033141\n",
      "88     \t [ 0.07204237 -0.71528483]. \t  1.030285673414239 \t 1.031500969033141\n",
      "89     \t [ 0.06967545 -0.70770968]. \t  1.0299378614622967 \t 1.031500969033141\n",
      "90     \t [ 0.02858897 -0.58283246]. \t  0.9106021908389954 \t 1.031500969033141\n",
      "91     \t [-0.21895976  0.71616418]. \t  0.9691631596266423 \t 1.031500969033141\n",
      "92     \t [ 0.07919736 -0.72643525]. \t  1.029454429609031 \t 1.031500969033141\n",
      "93     \t [ 1.28442091 -1.07737515]. \t  -1.742671010832419 \t 1.031500969033141\n",
      "94     \t [ 0.0489548  -0.70967798]. \t  1.0251148293389218 \t 1.031500969033141\n",
      "95     \t [-0.15830213  0.73087757]. \t  1.0121010890968691 \t 1.031500969033141\n",
      "96     \t [ 3. -2.]. \t  -150.89999999999998 \t 1.031500969033141\n",
      "97     \t [-0.02085848  0.65419143]. \t  0.9911501881720979 \t 1.031500969033141\n",
      "98     \t [ 0.15964559 -0.76311743]. \t  0.9941151435202529 \t 1.031500969033141\n",
      "99     \t [-0.17591138  0.68039488]. \t  0.9924163824114094 \t 1.031500969033141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.12923911 -0.68838691]. \t  1.0200100905668963 \t 1.031500969033141\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.8508833   0.48843508]. \t  -0.8285352707628978 \t 0.04378866326980524\n",
      "init   \t [-0.37363357  1.14143433]. \t  -1.670326523786612 \t 0.04378866326980524\n",
      "init   \t [ 1.67985485 -0.90962958]. \t  0.04378866326980524 \t 0.04378866326980524\n",
      "init   \t [-1.34121447  1.20748871]. \t  -3.3921424222614553 \t 0.04378866326980524\n",
      "init   \t [2.74883612 1.50373054]. \t  -69.67081810821485 \t 0.04378866326980524\n",
      "1      \t [-0.5327144  -1.69780704]. \t  -23.584236348688943 \t 0.04378866326980524\n",
      "2      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.04378866326980524\n",
      "3      \t [ 0.82044571 -0.16502666]. \t  -1.6013055754874577 \t 0.04378866326980524\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t 0.04378866326980524\n",
      "5      \t [-3.  2.]. \t  -150.89999999999998 \t 0.04378866326980524\n",
      "6      \t [ 0.82353531 -2.        ]. \t  -48.2038206923064 \t 0.04378866326980524\n",
      "7      \t [-0.89848645 -0.16423642]. \t  -2.078493732319299 \t 0.04378866326980524\n",
      "8      \t [0.79263062 2.        ]. \t  -51.35207501895623 \t 0.04378866326980524\n",
      "9      \t [ 2.09470433 -0.0412197 ]. \t  -5.18627822391284 \t 0.04378866326980524\n",
      "10     \t [-0.70682519  2.        ]. \t  -48.102160013201626 \t 0.04378866326980524\n",
      "11     \t [-0.99478842  0.61106537]. \t  -0.6811258744239231 \t 0.04378866326980524\n",
      "12     \t [-3.         -0.03106451]. \t  -108.98933723808926 \t 0.04378866326980524\n",
      "13     \t [ 3.         -0.17576571]. \t  -108.25294616356854 \t 0.04378866326980524\n",
      "14     \t [1.23523175 0.63883578]. \t  -2.2212010199370655 \t 0.04378866326980524\n",
      "15     \t [ 1.5264143  -0.24052198]. \t  -1.550624276601522 \t 0.04378866326980524\n",
      "16     \t [0.16366285 0.45957314]. \t  \u001b[92m0.4855386232390997\u001b[0m \t 0.4855386232390997\n",
      "17     \t [ 0.03122174 -0.78896672]. \t  \u001b[92m0.9607416006751954\u001b[0m \t 0.9607416006751954\n",
      "18     \t [-1.44722876 -0.96530076]. \t  -3.3711019486286538 \t 0.9607416006751954\n",
      "19     \t [-1.39580892 -2.        ]. \t  -53.07865791043657 \t 0.9607416006751954\n",
      "20     \t [-1.56296232 -0.21448646]. \t  -2.258546829113674 \t 0.9607416006751954\n",
      "21     \t [-0.90020695 -0.88732911]. \t  -2.1688707131721565 \t 0.9607416006751954\n",
      "22     \t [1.91835916 0.69327992]. \t  -3.2246428344595897 \t 0.9607416006751954\n",
      "23     \t [ 0.88045093 -0.96411114]. \t  -0.8831735720229903 \t 0.9607416006751954\n",
      "24     \t [1.91531534 2.        ]. \t  -54.69967848809507 \t 0.9607416006751954\n",
      "25     \t [-1.5347446   0.73066287]. \t  -0.010032372426471925 \t 0.9607416006751954\n",
      "26     \t [ 0.08186413 -0.26766278]. \t  0.26124163129823497 \t 0.9607416006751954\n",
      "27     \t [ 1.79808309 -2.        ]. \t  -46.65020019483196 \t 0.9607416006751954\n",
      "28     \t [-1.79103821  2.        ]. \t  -46.64291193199708 \t 0.9607416006751954\n",
      "29     \t [1.72951545 0.32006653]. \t  -2.2823261813660105 \t 0.9607416006751954\n",
      "30     \t [ 0.4948451  -0.63621724]. \t  0.42009522101259855 \t 0.9607416006751954\n",
      "31     \t [0.56560989 0.97477482]. \t  -1.4376587989838856 \t 0.9607416006751954\n",
      "32     \t [ 1.39255164 -0.81850934]. \t  -0.26626779260270084 \t 0.9607416006751954\n",
      "33     \t [-2.23743731  1.02972552]. \t  -7.167734570399113 \t 0.9607416006751954\n",
      "34     \t [-0.42986743  0.72944398]. \t  0.6399045823864746 \t 0.9607416006751954\n",
      "35     \t [-3.          0.86739444]. \t  -105.55258543706881 \t 0.9607416006751954\n",
      "36     \t [-1.86711833  0.91617732]. \t  -0.29554396631880764 \t 0.9607416006751954\n",
      "37     \t [-2.31907211 -0.9038974 ]. \t  -14.12225674707802 \t 0.9607416006751954\n",
      "38     \t [1.37444484 1.27637796]. \t  -8.163488497793587 \t 0.9607416006751954\n",
      "39     \t [-0.79918103  0.99400126]. \t  -0.9433034502381894 \t 0.9607416006751954\n",
      "40     \t [3.         0.76596389]. \t  -110.22795970478451 \t 0.9607416006751954\n",
      "41     \t [ 1.89128753 -0.57382114]. \t  -0.7257468995033886 \t 0.9607416006751954\n",
      "42     \t [3. 2.]. \t  -162.89999999999998 \t 0.9607416006751954\n",
      "43     \t [-3.         -1.05655627]. \t  -112.58902649262203 \t 0.9607416006751954\n",
      "44     \t [-1.96678567 -0.58805367]. \t  -3.5956174664463334 \t 0.9607416006751954\n",
      "45     \t [ 1.37373675 -1.29922492]. \t  -5.170497392354787 \t 0.9607416006751954\n",
      "46     \t [-1.92407624 -1.27432656]. \t  -9.444369155198824 \t 0.9607416006751954\n",
      "47     \t [1.95892972 1.25228714]. \t  -9.279373373392206 \t 0.9607416006751954\n",
      "48     \t [ 3.         -1.16761692]. \t  -107.37849891556483 \t 0.9607416006751954\n",
      "49     \t [ 0.1304965  -1.25589528]. \t  -3.5456882317645144 \t 0.9607416006751954\n",
      "50     \t [-0.05852372 -2.        ]. \t  -48.13072292215869 \t 0.9607416006751954\n",
      "51     \t [-1.49002647  0.30734641]. \t  -1.3771958414471022 \t 0.9607416006751954\n",
      "52     \t [-0.03270921  0.83199963]. \t  0.8751375560825253 \t 0.9607416006751954\n",
      "53     \t [-2.21791308 -2.        ]. \t  -60.97438602152841 \t 0.9607416006751954\n",
      "54     \t [-0.20934765  0.13284584]. \t  -0.074143005700556 \t 0.9607416006751954\n",
      "55     \t [0.03565241 2.        ]. \t  -48.07638579453604 \t 0.9607416006751954\n",
      "56     \t [ 2.24182246 -1.4175538 ]. \t  -14.310700731230497 \t 0.9607416006751954\n",
      "57     \t [-0.24036015 -0.96485423]. \t  -0.19890942055613553 \t 0.9607416006751954\n",
      "58     \t [-2.14399865  1.53477444]. \t  -15.871714934510482 \t 0.9607416006751954\n",
      "59     \t [-2.22923689 -0.13729859]. \t  -9.157421159431001 \t 0.9607416006751954\n",
      "60     \t [ 2.14572696 -0.88479156]. \t  -3.8550874492640945 \t 0.9607416006751954\n",
      "61     \t [-0.09009253  0.6655212 ]. \t  \u001b[92m1.0145967947143193\u001b[0m \t 1.0145967947143193\n",
      "62     \t [-0.17865964  0.73342369]. \t  0.9997361031478291 \t 1.0145967947143193\n",
      "63     \t [0.40457948 1.40823059]. \t  -8.968178135898395 \t 1.0145967947143193\n",
      "64     \t [ 0.14470305 -0.76695152]. \t  0.9970147141429055 \t 1.0145967947143193\n",
      "65     \t [ 0.13712176 -0.76269019]. \t  1.003414951357615 \t 1.0145967947143193\n",
      "66     \t [-0.17218826 -0.59672182]. \t  0.6976382491147375 \t 1.0145967947143193\n",
      "67     \t [-1.26353637 -1.45946742]. \t  -13.862162421908499 \t 1.0145967947143193\n",
      "68     \t [-0.14953969  0.73778177]. \t  1.0140678512847943 \t 1.0145967947143193\n",
      "69     \t [ 0.04594193 -0.77116256]. \t  0.9911293343095018 \t 1.0145967947143193\n",
      "70     \t [-0.11312063  0.66203993]. \t  1.0088193289597713 \t 1.0145967947143193\n",
      "71     \t [ 0.11711848 -0.73863002]. \t  \u001b[92m1.0237264535427233\u001b[0m \t 1.0237264535427233\n",
      "72     \t [ 0.05679079 -0.68478801]. \t  1.0221504766918261 \t 1.0237264535427233\n",
      "73     \t [-0.12043465  0.7180507 ]. \t  \u001b[92m1.0279279052827455\u001b[0m \t 1.0279279052827455\n",
      "74     \t [ 0.16038291 -0.70420125]. \t  1.0113676772984412 \t 1.0279279052827455\n",
      "75     \t [-0.11603686  0.70382985]. \t  \u001b[92m1.028106389713256\u001b[0m \t 1.028106389713256\n",
      "76     \t [-0.11116163  0.70733194]. \t  \u001b[92m1.0295201613669394\u001b[0m \t 1.0295201613669394\n",
      "77     \t [-2.40113384  2.        ]. \t  -60.336563001540156 \t 1.0295201613669394\n",
      "78     \t [ 0.08077267 -0.62571454]. \t  0.9774602448625478 \t 1.0295201613669394\n",
      "79     \t [-0.08678226  0.7554219 ]. \t  1.015578931228908 \t 1.0295201613669394\n",
      "80     \t [-0.02004265 -0.73501827]. \t  0.9771809451531381 \t 1.0295201613669394\n",
      "81     \t [2.45858881 2.        ]. \t  -73.98593799458094 \t 1.0295201613669394\n",
      "82     \t [-0.04238361  0.69889398]. \t  1.0219095909980684 \t 1.0295201613669394\n",
      "83     \t [-0.1036565   0.68237641]. \t  1.023272987229707 \t 1.0295201613669394\n",
      "84     \t [-0.21629904  0.7348232 ]. \t  0.9699740783445884 \t 1.0295201613669394\n",
      "85     \t [-0.09894177  0.75279356]. \t  1.0177311982214876 \t 1.0295201613669394\n",
      "86     \t [-0.00836113 -0.68797093]. \t  0.9911174544460267 \t 1.0295201613669394\n",
      "87     \t [-0.11880311  0.7655924 ]. \t  1.0052407513093042 \t 1.0295201613669394\n",
      "88     \t [ 0.07961961 -0.76975641]. \t  1.0017714365958654 \t 1.0295201613669394\n",
      "89     \t [ 0.05943941 -0.70094641]. \t  1.0272569028527339 \t 1.0295201613669394\n",
      "90     \t [-0.15464672  0.6601923 ]. \t  0.9911717898763919 \t 1.0295201613669394\n",
      "91     \t [ 2.39270362 -2.        ]. \t  -59.83298770403316 \t 1.0295201613669394\n",
      "92     \t [ 0.0979255  -0.73056682]. \t  1.028826044166165 \t 1.0295201613669394\n",
      "93     \t [-0.12081069  0.69791232]. \t  1.0257131438045641 \t 1.0295201613669394\n",
      "94     \t [-0.10686371  0.58015701]. \t  0.9097701194334663 \t 1.0295201613669394\n",
      "95     \t [ 0.0862697  -0.74476738]. \t  1.0226382632683113 \t 1.0295201613669394\n",
      "96     \t [ 0.06072521 -0.73748584]. \t  1.0223586174490744 \t 1.0295201613669394\n",
      "97     \t [ 0.07089709 -0.67081222]. \t  1.0175016713919467 \t 1.0295201613669394\n",
      "98     \t [ 0.09452953 -0.76154457]. \t  1.0108444698230745 \t 1.0295201613669394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [2.34908946 0.85660284]. \t  -15.368459249286992 \t 1.0295201613669394\n",
      "100    \t [ 0.06506194 -0.73850993]. \t  1.0229107496281786 \t 1.0295201613669394\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.96386586 -0.34559739]. \t  -1.4171064314549415 \t -1.4171064314549415\n",
      "init   \t [ 1.00861535 -1.08022631]. \t  -1.9362798876488938 \t -1.4171064314549415\n",
      "init   \t [1.84607002 0.54727386]. \t  -2.606716488173805 \t -1.4171064314549415\n",
      "init   \t [-1.96683689 -0.95845653]. \t  -4.9306493500188235 \t -1.4171064314549415\n",
      "init   \t [ 2.49191994 -0.14873796]. \t  -23.22037861975103 \t -1.4171064314549415\n",
      "1      \t [1.16128915 1.46788954]. \t  -14.049427408245991 \t -1.4171064314549415\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -1.4171064314549415\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -1.4171064314549415\n",
      "4      \t [-0.47754629 -2.        ]. \t  -49.762033109950636 \t -1.4171064314549415\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -1.4171064314549415\n",
      "6      \t [-1.2511983   0.05425908]. \t  -2.3146144831338 \t -1.4171064314549415\n",
      "7      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.4171064314549415\n",
      "8      \t [-0.41873306  2.        ]. \t  -47.80111948376361 \t -1.4171064314549415\n",
      "9      \t [-2.68382196 -0.03836983]. \t  -44.52338802561415 \t -1.4171064314549415\n",
      "10     \t [-0.93983447 -0.75674979]. \t  -1.8567942196073899 \t -1.4171064314549415\n",
      "11     \t [0.11585354 0.6316933 ]. \t  \u001b[92m0.8327308592872732\u001b[0m \t 0.8327308592872732\n",
      "12     \t [ 1.01578771 -2.        ]. \t  -48.22611430445445 \t 0.8327308592872732\n",
      "13     \t [ 0.18379586 -0.69224711]. \t  \u001b[92m0.992762491889908\u001b[0m \t 0.992762491889908\n",
      "14     \t [-1.64870684 -0.50763008]. \t  -2.1230828308933507 \t 0.992762491889908\n",
      "15     \t [-0.43047471  0.10589294]. \t  -0.5813080836041529 \t 0.992762491889908\n",
      "16     \t [-1.01894847  1.00265253]. \t  -1.2620569215615116 \t 0.992762491889908\n",
      "17     \t [1.07938635 0.70099656]. \t  -2.0938601896776077 \t 0.992762491889908\n",
      "18     \t [-1.5205217 -2.       ]. \t  -53.18330446241745 \t 0.992762491889908\n",
      "19     \t [3.         0.43965156]. \t  -109.59523020567454 \t 0.992762491889908\n",
      "20     \t [ 1.75713132 -0.23240763]. \t  -1.5293656403211024 \t 0.992762491889908\n",
      "21     \t [-0.6072646   0.71643061]. \t  0.22814275627625158 \t 0.992762491889908\n",
      "22     \t [-1.55955392  2.        ]. \t  -46.98290781806273 \t 0.992762491889908\n",
      "23     \t [-1.79241759  0.79072612]. \t  0.1255082296293305 \t 0.992762491889908\n",
      "24     \t [-1.38673525  0.6978511 ]. \t  -0.32965014634219003 \t 0.992762491889908\n",
      "25     \t [ 0.58672446 -0.98374537]. \t  -0.43971143494508996 \t 0.992762491889908\n",
      "26     \t [0.24316606 1.23618248]. \t  -3.7581891297044097 \t 0.992762491889908\n",
      "27     \t [-3.          0.79084839]. \t  -105.59039678601599 \t 0.992762491889908\n",
      "28     \t [ 3.         -0.82824913]. \t  -105.55363158216693 \t 0.992762491889908\n",
      "29     \t [0.86782235 2.        ]. \t  -51.69940742314311 \t 0.992762491889908\n",
      "30     \t [ 0.75026533 -0.71302675]. \t  -0.11097406109163321 \t 0.992762491889908\n",
      "31     \t [-0.25459429  1.0244206 ]. \t  -0.19725574186867903 \t 0.992762491889908\n",
      "32     \t [-3.        -0.8472147]. \t  -110.63134400071722 \t 0.992762491889908\n",
      "33     \t [-1.9433885   0.28620631]. \t  -2.2529161560765445 \t 0.992762491889908\n",
      "34     \t [-0.26073813 -0.91148071]. \t  0.062299076741798576 \t 0.992762491889908\n",
      "35     \t [1.92256375 2.        ]. \t  -54.77242376182289 \t 0.992762491889908\n",
      "36     \t [ 1.92319434 -2.        ]. \t  -47.08610186545082 \t 0.992762491889908\n",
      "37     \t [ 1.8807424  -1.00544652]. \t  -0.7794500881544545 \t 0.992762491889908\n",
      "38     \t [-1.52879107 -1.13041185]. \t  -5.281443398715435 \t 0.992762491889908\n",
      "39     \t [ 1.60883069 -0.89246446]. \t  0.019591888585056116 \t 0.992762491889908\n",
      "40     \t [0.51193187 0.0569177 ]. \t  -0.9262843196079377 \t 0.992762491889908\n",
      "41     \t [ 1.6379741  -1.23359762]. \t  -3.208422998577786 \t 0.992762491889908\n",
      "42     \t [-0.50816549 -0.4874498 ]. \t  -0.4217374171747734 \t 0.992762491889908\n",
      "43     \t [1.73786482 1.09897463]. \t  -5.021921370826094 \t 0.992762491889908\n",
      "44     \t [-1.57541411  1.15355765]. \t  -2.0308670063795766 \t 0.992762491889908\n",
      "45     \t [ 1.95424489 -0.64366256]. \t  -0.9860912929127981 \t 0.992762491889908\n",
      "46     \t [1.52450552 0.70618917]. \t  -2.214460358829267 \t 0.992762491889908\n",
      "47     \t [0.24462525 0.84661692]. \t  0.373038787813575 \t 0.992762491889908\n",
      "48     \t [-2.19298328 -2.        ]. \t  -60.129322696250284 \t 0.992762491889908\n",
      "49     \t [-2.24326431  2.        ]. \t  -52.941040743111536 \t 0.992762491889908\n",
      "50     \t [-2.27124583  1.33590263]. \t  -13.07635732593847 \t 0.992762491889908\n",
      "51     \t [ 0.24600763 -2.        ]. \t  -47.74244609753786 \t 0.992762491889908\n",
      "52     \t [-0.02357289 -0.74525089]. \t  0.9679340637276459 \t 0.992762491889908\n",
      "53     \t [-2.08201189 -0.40447071]. \t  -5.324836234438056 \t 0.992762491889908\n",
      "54     \t [ 0.02840928 -0.18853978]. \t  0.13926386646212205 \t 0.992762491889908\n",
      "55     \t [2.51313398 1.34754647]. \t  -34.78676064364817 \t 0.992762491889908\n",
      "56     \t [1.33694989 0.23078977]. \t  -2.4508196372418594 \t 0.992762491889908\n",
      "57     \t [-0.19246437  0.52846561]. \t  0.7615287319340934 \t 0.992762491889908\n",
      "58     \t [3.         1.27456783]. \t  -116.78191288267708 \t 0.992762491889908\n",
      "59     \t [-0.09701423  0.72909893]. \t  \u001b[92m1.029281159629289\u001b[0m \t 1.029281159629289\n",
      "60     \t [-2.26898426 -1.48308363]. \t  -24.336638435741165 \t 1.029281159629289\n",
      "61     \t [ 0.09920985 -0.75681346]. \t  1.0147361728948419 \t 1.029281159629289\n",
      "62     \t [ 0.10992059 -0.73962345]. \t  1.0244235686960426 \t 1.029281159629289\n",
      "63     \t [-2.26649202  0.71122021]. \t  -7.705992312419106 \t 1.029281159629289\n",
      "64     \t [-0.03066928  0.78273133]. \t  0.9694686297708467 \t 1.029281159629289\n",
      "65     \t [ 0.04632504 -0.72810351]. \t  1.021522647189049 \t 1.029281159629289\n",
      "66     \t [-0.07268863  0.70118484]. \t  \u001b[92m1.0296139842875314\u001b[0m \t 1.0296139842875314\n",
      "67     \t [-0.01737376 -0.71049068]. \t  0.9863568602483435 \t 1.0296139842875314\n",
      "68     \t [-0.0557093  0.7131464]. \t  1.027040684817492 \t 1.0296139842875314\n",
      "69     \t [0.00139696 0.69602388]. \t  0.9980525717515718 \t 1.0296139842875314\n",
      "70     \t [-0.01470289  0.70758296]. \t  1.009537096676229 \t 1.0296139842875314\n",
      "71     \t [-0.1149676   0.73242162]. \t  1.0263887540886496 \t 1.0296139842875314\n",
      "72     \t [ 0.03374773 -0.73787644]. \t  1.0124413923937503 \t 1.0296139842875314\n",
      "73     \t [ 0.14037191 -0.80439526]. \t  0.9484133575187988 \t 1.0296139842875314\n",
      "74     \t [-0.0812487   0.69850623]. \t  \u001b[92m1.0298541561763186\u001b[0m \t 1.0298541561763186\n",
      "75     \t [ 0.06396633 -0.66653929]. \t  1.0138831725793642 \t 1.0298541561763186\n",
      "76     \t [ 2.39010785 -1.56213415]. \t  -26.785712705877764 \t 1.0298541561763186\n",
      "77     \t [ 0.06778177 -0.72880282]. \t  1.027184191112631 \t 1.0298541561763186\n",
      "78     \t [-0.89060514 -1.45235247]. \t  -12.671088482150436 \t 1.0298541561763186\n",
      "79     \t [-0.0032673   0.70297508]. \t  1.002118359718619 \t 1.0298541561763186\n",
      "80     \t [-0.10852917  0.70596566]. \t  1.0297839430658953 \t 1.0298541561763186\n",
      "81     \t [-0.03013806  0.7677752 ]. \t  0.9874819884421888 \t 1.0298541561763186\n",
      "82     \t [2.29772349 0.82521698]. \t  -12.663955701462523 \t 1.0298541561763186\n",
      "83     \t [ 0.10440507 -0.67974992]. \t  1.0218589625578172 \t 1.0298541561763186\n",
      "84     \t [ 0.15739491 -0.72581431]. \t  1.013556232495997 \t 1.0298541561763186\n",
      "85     \t [ 0.10323033 -0.70075934]. \t  1.0296322564753733 \t 1.0298541561763186\n",
      "86     \t [ 0.10998834 -0.76110895]. \t  1.0104844999649858 \t 1.0298541561763186\n",
      "87     \t [-0.99995728  1.57249289]. \t  -15.227555274229369 \t 1.0298541561763186\n",
      "88     \t [-0.02788452 -0.74422895]. \t  0.9645278045860315 \t 1.0298541561763186\n",
      "89     \t [ 0.10362477 -0.6874821 ]. \t  1.0255334234239548 \t 1.0298541561763186\n",
      "90     \t [-0.03973589 -1.43171186]. \t  -8.670706840079388 \t 1.0298541561763186\n",
      "91     \t [-0.07470984  0.69977846]. \t  1.0295942685874167 \t 1.0298541561763186\n",
      "92     \t [0.05589392 0.67519783]. \t  0.9420024902648025 \t 1.0298541561763186\n",
      "93     \t [-0.02304841 -0.76007781]. \t  0.9561965700534504 \t 1.0298541561763186\n",
      "94     \t [2.30862072e-04 6.35226586e-01]. \t  0.9626140302007569 \t 1.0298541561763186\n",
      "95     \t [ 3.         -1.45137128]. \t  -113.86898011784156 \t 1.0298541561763186\n",
      "96     \t [-0.22374341  0.70052885]. \t  0.9613723383057599 \t 1.0298541561763186\n",
      "97     \t [ 0.12895368 -0.6886561 ]. \t  1.020214880123935 \t 1.0298541561763186\n",
      "98     \t [ 0.09298608 -0.70855714]. \t  \u001b[92m1.031440225304449\u001b[0m \t 1.031440225304449\n",
      "99     \t [-0.05891662  0.69855238]. \t  1.0267185992218892 \t 1.031440225304449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.12666447 -0.74501594]. \t  1.0186092147053856 \t 1.031440225304449\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8853063  0.02859875]. \t  -2.0275945291834825 \t -2.0275945291834825\n",
      "init   \t [0.17004828 1.58514082]. \t  -15.586853032333524 \t -2.0275945291834825\n",
      "init   \t [1.19994714 0.85718841]. \t  -2.6498170273522446 \t -2.0275945291834825\n",
      "init   \t [ 1.30403029 -1.10872215]. \t  -2.0500441913631415 \t -2.0275945291834825\n",
      "init   \t [-1.94907286 -0.17263405]. \t  -3.38472265204694 \t -2.0275945291834825\n",
      "1      \t [2.73748894 0.77072872]. \t  -53.46849010500125 \t -2.0275945291834825\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t -2.0275945291834825\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "5      \t [-0.01747262 -2.        ]. \t  -48.036166216672946 \t -2.0275945291834825\n",
      "6      \t [-0.81681395  0.17582325]. \t  \u001b[92m-1.5695032993544962\u001b[0m \t -1.5695032993544962\n",
      "7      \t [1.69482052 2.        ]. \t  -53.452565210305266 \t -1.5695032993544962\n",
      "8      \t [-1.03067577  2.        ]. \t  -48.2176280039075 \t -1.5695032993544962\n",
      "9      \t [-1.07977818 -0.84907411]. \t  -2.449356318431845 \t -1.5695032993544962\n",
      "10     \t [ 1.83981552 -0.27301025]. \t  -1.6281216050903609 \t -1.5695032993544962\n",
      "11     \t [-3.          0.07104165]. \t  -108.66678926984122 \t -1.5695032993544962\n",
      "12     \t [-1.39283739 -0.2069131 ]. \t  -2.414512202801073 \t -1.5695032993544962\n",
      "13     \t [0.05366144 0.69678092]. \t  \u001b[92m0.9502683570182751\u001b[0m \t 0.9502683570182751\n",
      "14     \t [-1.31283961 -2.        ]. \t  -52.98824148442324 \t 0.9502683570182751\n",
      "15     \t [ 0.01507036 -0.69261   ]. \t  \u001b[92m1.0078825645190652\u001b[0m \t 1.0078825645190652\n",
      "16     \t [3. 2.]. \t  -162.89999999999998 \t 1.0078825645190652\n",
      "17     \t [ 3.         -0.25003524]. \t  -107.91545759503867 \t 1.0078825645190652\n",
      "18     \t [1.75058151 0.41885696]. \t  -2.2842492278032687 \t 1.0078825645190652\n",
      "19     \t [-1.52204209  0.8350182 ]. \t  -0.025282814373855267 \t 1.0078825645190652\n",
      "20     \t [-0.78960552  0.93378409]. \t  -0.5744493001748878 \t 1.0078825645190652\n",
      "21     \t [ 1.30066069 -0.55101811]. \t  -0.8082808168130998 \t 1.0078825645190652\n",
      "22     \t [ 1.35930623 -2.        ]. \t  -47.60547248775861 \t 1.0078825645190652\n",
      "23     \t [0.81135303 2.        ]. \t  -51.440935333478414 \t 1.0078825645190652\n",
      "24     \t [-1.64298527  0.36779243]. \t  -0.979860810515224 \t 1.0078825645190652\n",
      "25     \t [-0.10412706 -0.16157479]. \t  0.041751839809226435 \t 1.0078825645190652\n",
      "26     \t [-1.87651594 -0.94863693]. \t  -4.020190378210736 \t 1.0078825645190652\n",
      "27     \t [ 1.79294891 -1.02318643]. \t  -0.5925602105813367 \t 1.0078825645190652\n",
      "28     \t [-0.40781122 -0.7678653 ]. \t  0.04604163075704748 \t 1.0078825645190652\n",
      "29     \t [-1.22414912  0.6492255 ]. \t  -0.6299757647734364 \t 1.0078825645190652\n",
      "30     \t [1.3025098  0.33687476]. \t  -2.4058881782343784 \t 1.0078825645190652\n",
      "31     \t [ 1.64773951 -0.78254151]. \t  0.18753072050089548 \t 1.0078825645190652\n",
      "32     \t [0.47999402 1.02992951]. \t  -1.5663261636389083 \t 1.0078825645190652\n",
      "33     \t [-3.         -0.92087126]. \t  -111.1470406026443 \t 1.0078825645190652\n",
      "34     \t [-1.95044436  2.        ]. \t  -47.27630680034634 \t 1.0078825645190652\n",
      "35     \t [-0.26949392  0.86938371]. \t  0.6929429186676506 \t 1.0078825645190652\n",
      "36     \t [-0.4584568  -1.32778437]. \t  -6.740576808676989 \t 1.0078825645190652\n",
      "37     \t [ 0.36133511 -1.18603928]. \t  -2.3469794321623607 \t 1.0078825645190652\n",
      "38     \t [-1.67992925 -0.65391901]. \t  -2.174952645307313 \t 1.0078825645190652\n",
      "39     \t [ 0.5956185  -0.77125484]. \t  0.2537680363183503 \t 1.0078825645190652\n",
      "40     \t [1.93665191 1.2095362 ]. \t  -8.100066510499069 \t 1.0078825645190652\n",
      "41     \t [ 2.07654615 -2.        ]. \t  -48.77391524874889 \t 1.0078825645190652\n",
      "42     \t [-1.3512802  1.2365053]. \t  -3.895546179820534 \t 1.0078825645190652\n",
      "43     \t [-2.56043681  1.11875558]. \t  -28.283739388104202 \t 1.0078825645190652\n",
      "44     \t [-1.94529091  1.1882815 ]. \t  -3.143317824740753 \t 1.0078825645190652\n",
      "45     \t [-2.09229182 -2.        ]. \t  -57.415494932451736 \t 1.0078825645190652\n",
      "46     \t [1.39504265 1.3300717 ]. \t  -9.585738713043316 \t 1.0078825645190652\n",
      "47     \t [-1.5496254  -1.31823488]. \t  -9.28237531164495 \t 1.0078825645190652\n",
      "48     \t [-0.34015827  0.5727014 ]. \t  0.6412234614200953 \t 1.0078825645190652\n",
      "49     \t [ 3.         -1.14351318]. \t  -107.07847622056687 \t 1.0078825645190652\n",
      "50     \t [-2.07974723  0.68350418]. \t  -2.569839553587873 \t 1.0078825645190652\n",
      "51     \t [ 0.18317431 -0.78791721]. \t  0.9540828196372806 \t 1.0078825645190652\n",
      "52     \t [ 0.27192622 -0.49289417]. \t  0.5852929768742634 \t 1.0078825645190652\n",
      "53     \t [-0.14414442  2.        ]. \t  -47.793918017381124 \t 1.0078825645190652\n",
      "54     \t [1.68678461 0.89129107]. \t  -2.9085173810526364 \t 1.0078825645190652\n",
      "55     \t [3.         1.33092815]. \t  -118.35830860663049 \t 1.0078825645190652\n",
      "56     \t [ 2.2727076  -0.77575667]. \t  -7.8472956986566045 \t 1.0078825645190652\n",
      "57     \t [0.12849438 0.28545555]. \t  0.19722866333962585 \t 1.0078825645190652\n",
      "58     \t [-3.         0.9280922]. \t  -105.63803351757711 \t 1.0078825645190652\n",
      "59     \t [2.34608936 2.        ]. \t  -66.67167843641037 \t 1.0078825645190652\n",
      "60     \t [-0.10003116  0.70529526]. \t  \u001b[92m1.0307103130666861\u001b[0m \t 1.0307103130666861\n",
      "61     \t [-0.10899386  0.69802005]. \t  1.0282049722155269 \t 1.0307103130666861\n",
      "62     \t [ 2.3951719  -1.45582812]. \t  -22.772679723698957 \t 1.0307103130666861\n",
      "63     \t [ 0.64461239 -2.        ]. \t  -48.03420238524096 \t 1.0307103130666861\n",
      "64     \t [-0.11215173  0.70672223]. \t  1.0292784660018224 \t 1.0307103130666861\n",
      "65     \t [-0.00254891 -0.79407229]. \t  0.9297759389965204 \t 1.0307103130666861\n",
      "66     \t [-0.09576813  0.73678497]. \t  1.026705560527826 \t 1.0307103130666861\n",
      "67     \t [ 0.07817677 -0.70122941]. \t  1.030177728787421 \t 1.0307103130666861\n",
      "68     \t [-0.10638182  0.69078723]. \t  1.0264055808536388 \t 1.0307103130666861\n",
      "69     \t [-0.06718696  0.6988102 ]. \t  1.028393119825282 \t 1.0307103130666861\n",
      "70     \t [-0.05878998  0.67299863]. \t  1.0169021878877385 \t 1.0307103130666861\n",
      "71     \t [ 0.04879426 -0.75468528]. \t  1.0079639521402166 \t 1.0307103130666861\n",
      "72     \t [-0.05330718  0.68061538]. \t  1.019526025899916 \t 1.0307103130666861\n",
      "73     \t [ 0.0765182  -0.74747257]. \t  1.0200571226495068 \t 1.0307103130666861\n",
      "74     \t [-0.10439532  0.67649758]. \t  1.020104208625251 \t 1.0307103130666861\n",
      "75     \t [ 0.12839718 -0.68705043]. \t  1.0197138150990257 \t 1.0307103130666861\n",
      "76     \t [ 0.04170951 -0.67189709]. \t  1.0116420403656152 \t 1.0307103130666861\n",
      "77     \t [-0.15296563  0.77358419]. \t  0.9871274837280871 \t 1.0307103130666861\n",
      "78     \t [ 0.10283885 -0.76484886]. \t  1.0076915791197016 \t 1.0307103130666861\n",
      "79     \t [-2.43022057 -1.52315276]. \t  -34.99370409483406 \t 1.0307103130666861\n",
      "80     \t [-0.07804611  0.73480513]. \t  1.026681381568232 \t 1.0307103130666861\n",
      "81     \t [2.30112449 0.05021306]. \t  -11.894787085491874 \t 1.0307103130666861\n",
      "82     \t [-0.2017115   0.71079274]. \t  0.9839697385718845 \t 1.0307103130666861\n",
      "83     \t [-0.62354108  1.44200791]. \t  -9.336051896880754 \t 1.0307103130666861\n",
      "84     \t [ 0.06039897 -0.75164344]. \t  1.013950982205218 \t 1.0307103130666861\n",
      "85     \t [-0.05732541  0.73518997]. \t  1.0224605323363838 \t 1.0307103130666861\n",
      "86     \t [-0.10114421  0.7088633 ]. \t  \u001b[92m1.0309714902558629\u001b[0m \t 1.0309714902558629\n",
      "87     \t [-0.0571509   0.74574289]. \t  1.0169739585503277 \t 1.0309714902558629\n",
      "88     \t [ 0.04727424 -0.72727814]. \t  1.022104003357587 \t 1.0309714902558629\n",
      "89     \t [-2.33414382 -0.67004497]. \t  -13.939537240574026 \t 1.0309714902558629\n",
      "90     \t [ 0.10286414 -0.73642038]. \t  1.0264996125608052 \t 1.0309714902558629\n",
      "91     \t [-0.09347063  0.66943619]. \t  1.01702981816735 \t 1.0309714902558629\n",
      "92     \t [-0.0105045   0.72133907]. \t  1.0054827109788351 \t 1.0309714902558629\n",
      "93     \t [ 0.04196695 -0.63001971]. \t  0.9769036562183315 \t 1.0309714902558629\n",
      "94     \t [-0.00111953 -0.76723036]. \t  0.9677061353336265 \t 1.0309714902558629\n",
      "95     \t [ 0.14121613 -0.7181516 ]. \t  1.0214878905997056 \t 1.0309714902558629\n",
      "96     \t [-0.15625578  0.77577753]. \t  0.9833252456566308 \t 1.0309714902558629\n",
      "97     \t [-0.11686178  0.69891708]. \t  1.026910430745872 \t 1.0309714902558629\n",
      "98     \t [ 0.00280749 -0.78001664]. \t  0.9551336128958129 \t 1.0309714902558629\n",
      "99     \t [ 0.02864664 -0.70927402]. \t  1.0169995215461387 \t 1.0309714902558629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.13389289 -0.68482471]. \t  1.016809181397658 \t 1.0309714902558629\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.46729115 -0.97799468]. \t  -0.15324775214966774 \t -0.10477079024230418\n",
      "init   \t [-1.48483878  0.83961742]. \t  -0.10477079024230418 \t -0.10477079024230418\n",
      "init   \t [-0.31468582 -1.09222817]. \t  -1.6403350770531386 \t -0.10477079024230418\n",
      "init   \t [-0.58974261  1.52903094]. \t  -12.761482581115207 \t -0.10477079024230418\n",
      "init   \t [-0.3767192   1.51370072]. \t  -11.790964688915137 \t -0.10477079024230418\n",
      "1      \t [-2.32351982  0.24401532]. \t  -12.047879630437892 \t -0.10477079024230418\n",
      "2      \t [1.60331857 1.01087507]. \t  -3.777929898000541 \t -0.10477079024230418\n",
      "3      \t [ 2.48192459 -2.        ]. \t  -65.9045473449595 \t -0.10477079024230418\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t -0.10477079024230418\n",
      "5      \t [-3.  2.]. \t  -150.89999999999998 \t -0.10477079024230418\n",
      "6      \t [3. 2.]. \t  -162.89999999999998 \t -0.10477079024230418\n",
      "7      \t [0.74730006 0.25831192]. \t  -1.5808942589070887 \t -0.10477079024230418\n",
      "8      \t [ 2.29910865 -0.15842848]. \t  -11.236581757486842 \t -0.10477079024230418\n",
      "9      \t [ 0.1997671 -2.       ]. \t  -47.756770186255494 \t -0.10477079024230418\n",
      "10     \t [-1.03841009 -0.09749923]. \t  -2.3529677785847616 \t -0.10477079024230418\n",
      "11     \t [0.88516331 2.        ]. \t  -51.77553650286502 \t -0.10477079024230418\n",
      "12     \t [1.55911437 0.22302878]. \t  -2.26108405833139 \t -0.10477079024230418\n",
      "13     \t [-0.40746738  0.52043719]. \t  \u001b[92m0.3942752578809966\u001b[0m \t 0.3942752578809966\n",
      "14     \t [3.         0.23026908]. \t  -109.38995796279679 \t 0.3942752578809966\n",
      "15     \t [ 1.81782539 -0.8524264 ]. \t  0.029468299691856603 \t 0.3942752578809966\n",
      "16     \t [-1.2367324 -2.       ]. \t  -52.8714756130079 \t 0.3942752578809966\n",
      "17     \t [-0.03429752 -0.42277676]. \t  \u001b[92m0.5679659639374242\u001b[0m \t 0.5679659639374242\n",
      "18     \t [0.8369308  0.96461144]. \t  -2.434584158249856 \t 0.5679659639374242\n",
      "19     \t [ 3.         -1.05903829]. \t  -106.26824292297094 \t 0.5679659639374242\n",
      "20     \t [-1.7501484  -0.69154928]. \t  -2.3410732940601857 \t 0.5679659639374242\n",
      "21     \t [-1.71242356  0.04196032]. \t  -1.9980841321965648 \t 0.5679659639374242\n",
      "22     \t [-3.         -0.35865342]. \t  -109.52761620578221 \t 0.5679659639374242\n",
      "23     \t [-1.1592484  -0.88712233]. \t  -2.7497488315496335 \t 0.5679659639374242\n",
      "24     \t [ 1.42444922 -2.        ]. \t  -47.40606975209904 \t 0.5679659639374242\n",
      "25     \t [ 1.75990065 -0.43816833]. \t  -0.7560628206273292 \t 0.5679659639374242\n",
      "26     \t [-1.62131139  2.        ]. \t  -46.81587806336189 \t 0.5679659639374242\n",
      "27     \t [-2.09281341  0.84290867]. \t  -2.6545338443793067 \t 0.5679659639374242\n",
      "28     \t [0.24913357 0.61577185]. \t  0.547935961841734 \t 0.5679659639374242\n",
      "29     \t [-0.36456417  2.        ]. \t  -47.766187376220344 \t 0.5679659639374242\n",
      "30     \t [-0.49942245  0.94823938]. \t  -0.03595498529115493 \t 0.5679659639374242\n",
      "31     \t [-3.         0.7616447]. \t  -105.64073163990541 \t 0.5679659639374242\n",
      "32     \t [ 1.24825909 -1.04615274]. \t  -1.5026739474428437 \t 0.5679659639374242\n",
      "33     \t [1.82700995 2.        ]. \t  -54.00487209407526 \t 0.5679659639374242\n",
      "34     \t [-1.87569962  0.62302653]. \t  -0.47684124011817397 \t 0.5679659639374242\n",
      "35     \t [-1.68713561  1.13390532]. \t  -1.615095018072974 \t 0.5679659639374242\n",
      "36     \t [1.99461392 0.50454855]. \t  -3.9126851394272886 \t 0.5679659639374242\n",
      "37     \t [ 0.53923944 -0.53922447]. \t  0.12189889345341764 \t 0.5679659639374242\n",
      "38     \t [0.18984394 0.05459044]. \t  -0.13992945478572955 \t 0.5679659639374242\n",
      "39     \t [1.49274043 0.6743144 ]. \t  -2.1888959716368817 \t 0.5679659639374242\n",
      "40     \t [-1.58872421 -0.46348441]. \t  -2.139222527327358 \t 0.5679659639374242\n",
      "41     \t [-2.02492628 -2.        ]. \t  -56.12369134858848 \t 0.5679659639374242\n",
      "42     \t [ 0.07802445 -0.78361295]. \t  \u001b[92m0.984838530360328\u001b[0m \t 0.984838530360328\n",
      "43     \t [-0.50562707 -0.71247865]. \t  -0.25142719297880145 \t 0.984838530360328\n",
      "44     \t [0.24148274 1.01713187]. \t  -0.6148062553970138 \t 0.984838530360328\n",
      "45     \t [-0.84724358  0.62371455]. \t  -0.43334240706312466 \t 0.984838530360328\n",
      "46     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.984838530360328\n",
      "47     \t [ 1.81607193 -1.37944341]. \t  -6.675007431035027 \t 0.984838530360328\n",
      "48     \t [-1.61258834 -1.30720623]. \t  -9.01525656634297 \t 0.984838530360328\n",
      "49     \t [-0.39567042 -0.03401611]. \t  -0.5848655509208235 \t 0.984838530360328\n",
      "50     \t [3.        1.1956221]. \t  -114.9428379694356 \t 0.984838530360328\n",
      "51     \t [-2.50104322 -1.275946  ]. \t  -31.717861895226225 \t 0.984838530360328\n",
      "52     \t [2.2250663  1.34543734]. \t  -17.641174057692428 \t 0.984838530360328\n",
      "53     \t [-0.46810045 -2.        ]. \t  -49.71535313143103 \t 0.984838530360328\n",
      "54     \t [ 0.26889605 -0.71634746]. \t  0.9135634364396865 \t 0.984838530360328\n",
      "55     \t [-2.30618193  2.        ]. \t  -55.407007232657826 \t 0.984838530360328\n",
      "56     \t [ 2.27272706 -1.15351544]. \t  -9.707652214099799 \t 0.984838530360328\n",
      "57     \t [ 2.05030175 -0.63888635]. \t  -2.1907953994748803 \t 0.984838530360328\n",
      "58     \t [ 1.143851   -0.74517918]. \t  -0.5450629018123032 \t 0.984838530360328\n",
      "59     \t [-1.11533603  1.08764828]. \t  -2.020624464552057 \t 0.984838530360328\n",
      "60     \t [-2.16071896 -0.45727019]. \t  -7.1490211597998075 \t 0.984838530360328\n",
      "61     \t [-3.         -1.19021165]. \t  -114.83128489687178 \t 0.984838530360328\n",
      "62     \t [-2.44491933  1.473547  ]. \t  -26.641695872896122 \t 0.984838530360328\n",
      "63     \t [-0.08427974  0.72293533]. \t  \u001b[92m1.03057287493288\u001b[0m \t 1.03057287493288\n",
      "64     \t [-0.1413933   0.66405821]. \t  1.0008251868878104 \t 1.03057287493288\n",
      "65     \t [ 0.07699427 -0.7974844 ]. \t  0.9637992548502257 \t 1.03057287493288\n",
      "66     \t [ 0.12089526 -0.74353412]. \t  1.0207051264883897 \t 1.03057287493288\n",
      "67     \t [ 0.04697141 -0.63331253]. \t  0.981795644896404 \t 1.03057287493288\n",
      "68     \t [0.02332665 0.73436319]. \t  0.9745192657565575 \t 1.03057287493288\n",
      "69     \t [2.39414633 2.        ]. \t  -69.49478057486823 \t 1.03057287493288\n",
      "70     \t [ 0.12701166 -0.6928846 ]. \t  1.0224358997333647 \t 1.03057287493288\n",
      "71     \t [ 0.15508939 -0.72964683]. \t  1.0139648655430902 \t 1.03057287493288\n",
      "72     \t [ 0.13879527 -0.70246934]. \t  1.0210489335135589 \t 1.03057287493288\n",
      "73     \t [ 0.08999805 -0.71454137]. \t  \u001b[92m1.0315994795679801\u001b[0m \t 1.0315994795679801\n",
      "74     \t [ 0.08566508 -0.69892801]. \t  1.030103694018014 \t 1.0315994795679801\n",
      "75     \t [-0.14107001  0.67302003]. \t  1.0073164415439548 \t 1.0315994795679801\n",
      "76     \t [ 0.0238107  -0.69091003]. \t  1.0121330472544663 \t 1.0315994795679801\n",
      "77     \t [-0.07424349  0.77500149]. \t  0.9950507551192272 \t 1.0315994795679801\n",
      "78     \t [0.02802638 0.67149198]. \t  0.9683972357461593 \t 1.0315994795679801\n",
      "79     \t [-0.10651331  0.68537503]. \t  1.0242280991792154 \t 1.0315994795679801\n",
      "80     \t [-0.07241862  0.72821843]. \t  1.0281435932615275 \t 1.0315994795679801\n",
      "81     \t [-0.06491263  0.70117432]. \t  1.0284185348803845 \t 1.0315994795679801\n",
      "82     \t [ 0.08322729 -0.70631432]. \t  1.0311731294772966 \t 1.0315994795679801\n",
      "83     \t [-0.05641653  0.69393376]. \t  1.0250768128164107 \t 1.0315994795679801\n",
      "84     \t [ 0.15742372 -0.7149102 ]. \t  1.0142070359135422 \t 1.0315994795679801\n",
      "85     \t [ 0.1161056  -0.70376726]. \t  1.0280812846524872 \t 1.0315994795679801\n",
      "86     \t [ 0.11849887 -0.74520711]. \t  1.0203041767590804 \t 1.0315994795679801\n",
      "87     \t [ 0.93819195 -1.60659588]. \t  -16.93858869114524 \t 1.0315994795679801\n",
      "88     \t [ 0.09965865 -0.69043708]. \t  1.0271165280804246 \t 1.0315994795679801\n",
      "89     \t [-0.14392201  0.68687248]. \t  1.0137175727486392 \t 1.0315994795679801\n",
      "90     \t [-0.03690923  0.7478504 ]. \t  1.0081007322024378 \t 1.0315994795679801\n",
      "91     \t [-0.09335627  0.65939455]. \t  1.0098527826079393 \t 1.0315994795679801\n",
      "92     \t [ 0.19658662 -0.73713003]. \t  0.9859212967493852 \t 1.0315994795679801\n",
      "93     \t [-0.17146614  0.61418025]. \t  0.9292130478404965 \t 1.0315994795679801\n",
      "94     \t [2.45022742 0.77605087]. \t  -21.396869286839834 \t 1.0315994795679801\n",
      "95     \t [ 0.16658614 -0.70308033]. \t  1.0076008253774187 \t 1.0315994795679801\n",
      "96     \t [-0.02937372  0.6180745 ]. \t  0.9590246371198865 \t 1.0315994795679801\n",
      "97     \t [-0.03324243  0.72751373]. \t  1.0163382638741083 \t 1.0315994795679801\n",
      "98     \t [-0.00974123 -0.68238389]. \t  0.9882528937985718 \t 1.0315994795679801\n",
      "99     \t [-0.09510926  0.66766167]. \t  1.0157266538297154 \t 1.0315994795679801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.08838807  0.70064181]. \t  1.0304752701954767 \t 1.0315994795679801\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.18368999 -0.73453599]. \t  -4.868217712547965 \t 0.15481417016151777\n",
      "init   \t [1.03290467 0.03164667]. \t  -2.310710708702905 \t 0.15481417016151777\n",
      "init   \t [ 1.68996256 -0.86527329]. \t  0.15481417016151777 \t 0.15481417016151777\n",
      "init   \t [-1.5977473   0.25519875]. \t  -1.420015353077075 \t 0.15481417016151777\n",
      "init   \t [2.25014619 0.87577168]. \t  -10.939300534748709 \t 0.15481417016151777\n",
      "1      \t [ 0.86068927 -1.53224454]. \t  -13.284524015138361 \t 0.15481417016151777\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "4      \t [0.13130272 2.        ]. \t  -48.330944576227665 \t 0.15481417016151777\n",
      "5      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "6      \t [-0.6741605  -0.80338779]. \t  -1.041694446005319 \t 0.15481417016151777\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "8      \t [-0.63293078 -2.        ]. \t  -50.55268568763917 \t 0.15481417016151777\n",
      "9      \t [2.11322887 0.06953081]. \t  -5.797183109054952 \t 0.15481417016151777\n",
      "10     \t [-0.38829508  0.32189791]. \t  -0.059979016686557085 \t 0.15481417016151777\n",
      "11     \t [1.28604244 1.22233553]. \t  -6.904255518428418 \t 0.15481417016151777\n",
      "12     \t [-3.         -0.01138174]. \t  -108.93362711304943 \t 0.15481417016151777\n",
      "13     \t [3.         0.04751807]. \t  -109.0335427324433 \t 0.15481417016151777\n",
      "14     \t [-1.19743223  2.        ]. \t  -48.00572085967627 \t 0.15481417016151777\n",
      "15     \t [-1.04240425 -0.12900066]. \t  -2.3635982614440336 \t 0.15481417016151777\n",
      "16     \t [ 0.12825092 -0.68107104]. \t  \u001b[92m1.0168964116624\u001b[0m \t 1.0168964116624\n",
      "17     \t [1.66597037 0.67351176]. \t  -2.1824423976987855 \t 1.0168964116624\n",
      "18     \t [1.54354114 2.        ]. \t  -53.20476601082719 \t 1.0168964116624\n",
      "19     \t [-1.18350749  0.89245727]. \t  -0.6940931741225393 \t 1.0168964116624\n",
      "20     \t [0.34164141 0.77646182]. \t  0.2535841322393658 \t 1.0168964116624\n",
      "21     \t [ 1.4537279 -2.       ]. \t  -47.31308183293595 \t 1.0168964116624\n",
      "22     \t [-1.62653671 -1.00886982]. \t  -3.7699465823824343 \t 1.0168964116624\n",
      "23     \t [0.266828  0.0644422]. \t  -0.27491681474958296 \t 1.0168964116624\n",
      "24     \t [ 1.75500145 -0.43697432]. \t  -0.753112783710532 \t 1.0168964116624\n",
      "25     \t [-0.54059787  0.93821964]. \t  -0.06912721230840752 \t 1.0168964116624\n",
      "26     \t [-1.60824526 -2.        ]. \t  -53.281415677131896 \t 1.0168964116624\n",
      "27     \t [ 0.74911058 -0.74943808]. \t  -0.09605964756452057 \t 1.0168964116624\n",
      "28     \t [-0.99726512  0.55758877]. \t  -0.8158941270179909 \t 1.0168964116624\n",
      "29     \t [-1.58528781 -0.49968613]. \t  -2.122893032963426 \t 1.0168964116624\n",
      "30     \t [-2.08699564  0.92819759]. \t  -2.7120010180849814 \t 1.0168964116624\n",
      "31     \t [-1.74892637  0.76640092]. \t  0.18317378869595924 \t 1.0168964116624\n",
      "32     \t [-1.24531894 -0.98040701]. \t  -3.4676867184397198 \t 1.0168964116624\n",
      "33     \t [ 0.048495   -1.24236888]. \t  -3.3045523596557342 \t 1.0168964116624\n",
      "34     \t [-3.          0.92993051]. \t  -105.64243928796697 \t 1.0168964116624\n",
      "35     \t [-1.86493621  1.47283785]. \t  -9.93207283181755 \t 1.0168964116624\n",
      "36     \t [ 0.38563563 -2.        ]. \t  -47.77824061127122 \t 1.0168964116624\n",
      "37     \t [0.77302779 0.59822503]. \t  -1.2547682832508813 \t 1.0168964116624\n",
      "38     \t [0.67863587 1.36667727]. \t  -8.840388498926888 \t 1.0168964116624\n",
      "39     \t [-3.         -1.02778065]. \t  -112.22136781663679 \t 1.0168964116624\n",
      "40     \t [ 3.         -1.06722933]. \t  -106.33148580576949 \t 1.0168964116624\n",
      "41     \t [-2.07904476 -0.1611148 ]. \t  -5.207624077514709 \t 1.0168964116624\n",
      "42     \t [ 0.32225605 -0.91197678]. \t  0.4606646314494655 \t 1.0168964116624\n",
      "43     \t [1.94669179 1.30339425]. \t  -10.427223256206442 \t 1.0168964116624\n",
      "44     \t [ 2.07739853 -1.43781933]. \t  -10.782008481717853 \t 1.0168964116624\n",
      "45     \t [-0.32239852 -0.33740257]. \t  -0.0987045271817662 \t 1.0168964116624\n",
      "46     \t [ 1.8953667  -0.97665925]. \t  -0.6949697394419556 \t 1.0168964116624\n",
      "47     \t [-0.12386617 -0.81492378]. \t  0.7304640976775346 \t 1.0168964116624\n",
      "48     \t [-2.020575    0.43436284]. \t  -2.5212787911280112 \t 1.0168964116624\n",
      "49     \t [0.83556472 2.        ]. \t  -51.55361907393652 \t 1.0168964116624\n",
      "50     \t [ 1.56817005 -1.25362048]. \t  -3.7212986495628546 \t 1.0168964116624\n",
      "51     \t [-1.72176533  1.0651423 ]. \t  -0.8634898439353489 \t 1.0168964116624\n",
      "52     \t [3.         1.17998612]. \t  -114.62523546656425 \t 1.0168964116624\n",
      "53     \t [-2.05428333  2.        ]. \t  -48.42460270219529 \t 1.0168964116624\n",
      "54     \t [-0.66123398  1.42788245]. \t  -8.903377410423467 \t 1.0168964116624\n",
      "55     \t [-2.30323369 -2.        ]. \t  -64.4912343782357 \t 1.0168964116624\n",
      "56     \t [-2.15537281 -0.84724999]. \t  -7.697005011611287 \t 1.0168964116624\n",
      "57     \t [ 2.19523143 -2.        ]. \t  -51.421520585521186 \t 1.0168964116624\n",
      "58     \t [0.0115225  1.10201261]. \t  -1.0548801343719327 \t 1.0168964116624\n",
      "59     \t [-0.75226755 -1.32408612]. \t  -7.929659773694272 \t 1.0168964116624\n",
      "60     \t [2.32700347 2.        ]. \t  -65.66337030505036 \t 1.0168964116624\n",
      "61     \t [-2.24608664 -1.44871433]. \t  -22.01002699895237 \t 1.0168964116624\n",
      "62     \t [-0.05058021  0.54953534]. \t  0.8607426346037456 \t 1.0168964116624\n",
      "63     \t [ 0.03340648 -0.71108139]. \t  \u001b[92m1.0191662790008926\u001b[0m \t 1.0191662790008926\n",
      "64     \t [ 0.07224227 -0.66250489]. \t  1.0121156106335114 \t 1.0191662790008926\n",
      "65     \t [ 0.05323343 -0.70264071]. \t  \u001b[92m1.0259270790269173\u001b[0m \t 1.0259270790269173\n",
      "66     \t [ 0.048639   -0.66331495]. \t  1.0084053437264147 \t 1.0259270790269173\n",
      "67     \t [ 1.07974068 -0.93562781]. \t  -0.8907358854659939 \t 1.0259270790269173\n",
      "68     \t [-0.25697934  0.78199953]. \t  0.8961166809194224 \t 1.0259270790269173\n",
      "69     \t [-0.04944115  0.73645389]. \t  1.0194670117194908 \t 1.0259270790269173\n",
      "70     \t [ 0.04850903 -0.64755623]. \t  0.995980246496195 \t 1.0259270790269173\n",
      "71     \t [ 0.10123315 -0.76638215]. \t  1.006296644357985 \t 1.0259270790269173\n",
      "72     \t [-0.01236451  0.69525265]. \t  1.0068795897779932 \t 1.0259270790269173\n",
      "73     \t [ 0.17528848 -0.77513006]. \t  0.974276063533094 \t 1.0259270790269173\n",
      "74     \t [-0.09402776  0.69750556]. \t  \u001b[92m1.0296564396002956\u001b[0m \t 1.0296564396002956\n",
      "75     \t [-0.06560404  0.7643564 ]. \t  1.0045821912705533 \t 1.0296564396002956\n",
      "76     \t [2.50243806 1.47414463]. \t  -38.44066719563339 \t 1.0296564396002956\n",
      "77     \t [-0.1052763   0.70787383]. \t  \u001b[92m1.0304427284477888\u001b[0m \t 1.0304427284477888\n",
      "78     \t [-0.07262594  0.73755913]. \t  1.0247844884095276 \t 1.0304427284477888\n",
      "79     \t [ 0.05084352 -0.6204655 ]. \t  0.968299659873906 \t 1.0304427284477888\n",
      "80     \t [ 0.09508656 -0.65842759]. \t  1.0089385263071986 \t 1.0304427284477888\n",
      "81     \t [ 0.19055661 -0.7042246 ]. \t  0.9916341761832087 \t 1.0304427284477888\n",
      "82     \t [-0.07057965  0.70209915]. \t  1.0294808415647447 \t 1.0304427284477888\n",
      "83     \t [ 0.08954723 -0.746877  ]. \t  1.0215657587682183 \t 1.0304427284477888\n",
      "84     \t [ 0.0296333  -0.67214674]. \t  1.007106823052624 \t 1.0304427284477888\n",
      "85     \t [ 0.09846386 -0.69214621]. \t  1.027815081140319 \t 1.0304427284477888\n",
      "86     \t [-0.07912289  0.75729468]. \t  1.0133536469534323 \t 1.0304427284477888\n",
      "87     \t [-0.10925495  0.75629086]. \t  1.0144584896246882 \t 1.0304427284477888\n",
      "88     \t [-0.00862199 -0.62916425]. \t  0.9508871605760972 \t 1.0304427284477888\n",
      "89     \t [-2.41967004  1.57163983]. \t  -29.05385155244032 \t 1.0304427284477888\n",
      "90     \t [ 0.06677972 -0.74359048]. \t  1.020655389924522 \t 1.0304427284477888\n",
      "91     \t [-0.04200971  0.66244548]. \t  1.0058113073888189 \t 1.0304427284477888\n",
      "92     \t [ 0.0940484  -0.71120776]. \t  \u001b[92m1.0315362848550111\u001b[0m \t 1.0315362848550111\n",
      "93     \t [-0.0714927   0.66680119]. \t  1.0150153295658053 \t 1.0315362848550111\n",
      "94     \t [ 0.06817367 -0.68409834]. \t  1.0239938019707906 \t 1.0315362848550111\n",
      "95     \t [-0.09996432  0.73410886]. \t  1.0275648344519146 \t 1.0315362848550111\n",
      "96     \t [-0.12002276  0.71557057]. \t  1.0281177261715644 \t 1.0315362848550111\n",
      "97     \t [ 1.27112523 -0.55637605]. \t  -0.8245435663550674 \t 1.0315362848550111\n",
      "98     \t [-0.0226737   0.67316327]. \t  1.0044271286317061 \t 1.0315362848550111\n",
      "99     \t [ 0.08241127 -0.73383705]. \t  1.027472551973198 \t 1.0315362848550111\n",
      "100    \t [ 0.06054733 -0.71538032]. \t  1.0281246215376647 \t 1.0315362848550111\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.91094417 0.9220785 ]. \t  -2.3942909876906837 \t -0.5838248233399961\n",
      "init   \t [-1.83979583  0.07870895]. \t  -2.2368016331814964 \t -0.5838248233399961\n",
      "init   \t [1.0582996  1.21777949]. \t  -6.46789670014418 \t -0.5838248233399961\n",
      "init   \t [0.20107719 1.63213672]. \t  -18.215840822341526 \t -0.5838248233399961\n",
      "init   \t [-0.33911323 -0.97611116]. \t  -0.5838248233399961 \t -0.5838248233399961\n",
      "1      \t [ 1.30830276 -0.13870529]. \t  -2.1087542553073133 \t -0.5838248233399961\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -0.5838248233399961\n",
      "3      \t [-3.          1.96112072]. \t  -146.7993538695235 \t -0.5838248233399961\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.5838248233399961\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t -0.5838248233399961\n",
      "6      \t [ 0.52542348 -2.        ]. \t  -47.900395324868 \t -0.5838248233399961\n",
      "7      \t [-0.69486879  0.20135114]. \t  -1.1837985868562728 \t -0.5838248233399961\n",
      "8      \t [ 3.00000000e+00 -6.20085699e-04]. \t  -108.8981382048783 \t -0.5838248233399961\n",
      "9      \t [-1.01113054 -2.        ]. \t  -52.272954916698986 \t -0.5838248233399961\n",
      "10     \t [ 0.41204136 -0.35320331]. \t  \u001b[92m-0.037920079466529644\u001b[0m \t -0.037920079466529644\n",
      "11     \t [-1.09962973  2.        ]. \t  -48.1563390807533 \t -0.037920079466529644\n",
      "12     \t [-3.         -0.01284888]. \t  -108.93788638446422 \t -0.037920079466529644\n",
      "13     \t [-1.24124863 -0.49562076]. \t  -2.270978513771488 \t -0.037920079466529644\n",
      "14     \t [-1.40336091  0.57738447]. \t  -0.579574567880479 \t -0.037920079466529644\n",
      "15     \t [ 1.30689554 -1.08007124]. \t  -1.7322833934640296 \t -0.037920079466529644\n",
      "16     \t [-1.32257838  0.08032315]. \t  -2.2235598300357284 \t -0.037920079466529644\n",
      "17     \t [-0.36742973  0.90934357]. \t  \u001b[92m0.4040869396725452\u001b[0m \t 0.4040869396725452\n",
      "18     \t [1.11228831 2.        ]. \t  -52.59022340774388 \t 0.4040869396725452\n",
      "19     \t [1.72914728 0.71379062]. \t  -2.330680564522487 \t 0.4040869396725452\n",
      "20     \t [ 1.57603173 -2.        ]. \t  -46.93540346196779 \t 0.4040869396725452\n",
      "21     \t [-0.29183149 -0.47257649]. \t  0.2302622146236072 \t 0.4040869396725452\n",
      "22     \t [ 0.76625656 -0.91168602]. \t  -0.4322185434772404 \t 0.4040869396725452\n",
      "23     \t [-1.92688535 -0.82593026]. \t  -3.687510321523571 \t 0.4040869396725452\n",
      "24     \t [1.36099898 0.71141854]. \t  -2.290874284325002 \t 0.4040869396725452\n",
      "25     \t [-0.03818768  0.5236479 ]. \t  \u001b[92m0.8102384728457657\u001b[0m \t 0.8102384728457657\n",
      "26     \t [ 1.85789987 -0.5488341 ]. \t  -0.633537374063138 \t 0.8102384728457657\n",
      "27     \t [ 1.4656022 -0.6559662]. \t  -0.2644125244543992 \t 0.8102384728457657\n",
      "28     \t [1.83145334 0.11288608]. \t  -2.5258511424436274 \t 0.8102384728457657\n",
      "29     \t [-1.45329387 -1.17616734]. \t  -6.051739081990253 \t 0.8102384728457657\n",
      "30     \t [-1.70727082 -0.55223018]. \t  -2.1672120184684682 \t 0.8102384728457657\n",
      "31     \t [-2.16052952  0.93014546]. \t  -4.341215764610069 \t 0.8102384728457657\n",
      "32     \t [-1.88373399  0.62413314]. \t  -0.5181976635535444 \t 0.8102384728457657\n",
      "33     \t [-1.85044047  1.50587157]. \t  -11.168790167743117 \t 0.8102384728457657\n",
      "34     \t [-1.93194769 -2.        ]. \t  -54.870629024263124 \t 0.8102384728457657\n",
      "35     \t [0.18645254 0.93924035]. \t  0.10411897509413337 \t 0.8102384728457657\n",
      "36     \t [-1.33826402  1.17209035]. \t  -2.8283810035138726 \t 0.8102384728457657\n",
      "37     \t [ 3.         -1.01177385]. \t  -105.96166789661623 \t 0.8102384728457657\n",
      "38     \t [ 1.97651591 -1.21827971]. \t  -3.9174919236531163 \t 0.8102384728457657\n",
      "39     \t [ 0.1840597  -0.83532506]. \t  \u001b[92m0.8641863790262617\u001b[0m \t 0.8641863790262617\n",
      "40     \t [ 1.76812758 -0.92003729]. \t  -0.018950800735180295 \t 0.8641863790262617\n",
      "41     \t [2.01745424 2.        ]. \t  -56.00208822526625 \t 0.8641863790262617\n",
      "42     \t [-1.80163858  1.00891881]. \t  -0.5129835274387654 \t 0.8641863790262617\n",
      "43     \t [3.         1.04257057]. \t  -112.4057689018458 \t 0.8641863790262617\n",
      "44     \t [-3.         -1.02645326]. \t  -112.20527952261017 \t 0.8641863790262617\n",
      "45     \t [1.72991127 1.25118136]. \t  -7.802294912849581 \t 0.8641863790262617\n",
      "46     \t [-0.65833053  0.6989787 ]. \t  0.09335740837517892 \t 0.8641863790262617\n",
      "47     \t [ 0.25775374 -1.2760059 ]. \t  -4.018941448088942 \t 0.8641863790262617\n",
      "48     \t [-3.          1.01688625]. \t  -105.99021149965684 \t 0.8641863790262617\n",
      "49     \t [0.32740916 0.35229738]. \t  -0.08557443019363198 \t 0.8641863790262617\n",
      "50     \t [-0.2135938 -2.       ]. \t  -48.60533756325247 \t 0.8641863790262617\n",
      "51     \t [ 0.00630673 -0.01471127]. \t  0.000799182031537442 \t 0.8641863790262617\n",
      "52     \t [-0.30729459  2.        ]. \t  -47.74468561944379 \t 0.8641863790262617\n",
      "53     \t [-2.13271049  2.        ]. \t  -49.84948063231755 \t 0.8641863790262617\n",
      "54     \t [-0.74981645 -0.80250317]. \t  -1.3290221822533326 \t 0.8641863790262617\n",
      "55     \t [ 2.25894088 -2.        ]. \t  -53.50231348569004 \t 0.8641863790262617\n",
      "56     \t [2.34635322 0.39438932]. \t  -14.393324622070336 \t 0.8641863790262617\n",
      "57     \t [-0.06735004 -0.7993345 ]. \t  0.850851396263704 \t 0.8641863790262617\n",
      "58     \t [-2.26127115 -1.48127824]. \t  -23.941813563301604 \t 0.8641863790262617\n",
      "59     \t [2.39038848 1.37403943]. \t  -26.46835376618324 \t 0.8641863790262617\n",
      "60     \t [-0.1116804   0.75105853]. \t  \u001b[92m1.0178849501893075\u001b[0m \t 1.0178849501893075\n",
      "61     \t [ 2.33268842 -0.41422346]. \t  -11.757112734265695 \t 1.0178849501893075\n",
      "62     \t [-0.08374379  0.68206823]. \t  \u001b[92m1.024330648344262\u001b[0m \t 1.024330648344262\n",
      "63     \t [-0.07410324  0.70817749]. \t  \u001b[92m1.0305671694807501\u001b[0m \t 1.0305671694807501\n",
      "64     \t [-0.01209183  0.75599461]. \t  0.9880916204564629 \t 1.0305671694807501\n",
      "65     \t [-0.13904327  0.73288709]. \t  1.0198407795692401 \t 1.0305671694807501\n",
      "66     \t [-0.06397439  0.68591495]. \t  1.0240593702907934 \t 1.0305671694807501\n",
      "67     \t [-0.16387139  0.77632728]. \t  0.9791343597881345 \t 1.0305671694807501\n",
      "68     \t [-2.31724802 -0.44668944]. \t  -12.933058600698248 \t 1.0305671694807501\n",
      "69     \t [-0.04260336  0.73949828]. \t  1.0154692652914599 \t 1.0305671694807501\n",
      "70     \t [-0.06601473  0.77999125]. \t  0.9871085814731773 \t 1.0305671694807501\n",
      "71     \t [ 0.08365147 -0.73308349]. \t  1.0278374989902879 \t 1.0305671694807501\n",
      "72     \t [ 0.08271228 -0.71905061]. \t  \u001b[92m1.0310466123491342\u001b[0m \t 1.0310466123491342\n",
      "73     \t [-2.33669036  0.37921827]. \t  -12.115457394321535 \t 1.0310466123491342\n",
      "74     \t [ 0.08830048 -0.76730752]. \t  1.0051792891494518 \t 1.0310466123491342\n",
      "75     \t [-0.08837864  0.76690081]. \t  1.0055900520131738 \t 1.0310466123491342\n",
      "76     \t [ 0.00078784 -0.66055802]. \t  0.9843059693907671 \t 1.0310466123491342\n",
      "77     \t [ 0.05588341 -0.72950703]. \t  1.024153650315443 \t 1.0310466123491342\n",
      "78     \t [-0.10123509  0.69996035]. \t  1.0296821321089031 \t 1.0310466123491342\n",
      "79     \t [-0.15055277  0.7635749 ]. \t  0.9977818034343615 \t 1.0310466123491342\n",
      "80     \t [-0.03307602  0.75075052]. \t  1.0042650140857743 \t 1.0310466123491342\n",
      "81     \t [-0.05384109  0.71510133]. \t  1.0264069222531502 \t 1.0310466123491342\n",
      "82     \t [ 2.31273818 -1.04018513]. \t  -10.272753466543069 \t 1.0310466123491342\n",
      "83     \t [ 0.01705681 -0.68446421]. \t  1.0065400088105274 \t 1.0310466123491342\n",
      "84     \t [ 0.05459795 -0.68886526]. \t  1.0231117423764082 \t 1.0310466123491342\n",
      "85     \t [-0.18653455  0.76346109]. \t  0.9782881216325578 \t 1.0310466123491342\n",
      "86     \t [ 0.08752875 -0.69981185]. \t  1.0303102912463762 \t 1.0310466123491342\n",
      "87     \t [-0.00881726  0.7037377 ]. \t  1.0058037002747942 \t 1.0310466123491342\n",
      "88     \t [-0.04459953  0.72393962]. \t  1.0220181630762326 \t 1.0310466123491342\n",
      "89     \t [-0.14402034  0.65986111]. \t  0.9962824683441865 \t 1.0310466123491342\n",
      "90     \t [-0.10967941  0.78984376]. \t  0.9774561956304667 \t 1.0310466123491342\n",
      "91     \t [-0.08694321  0.71975327]. \t  \u001b[92m1.0311585683591444\u001b[0m \t 1.0311585683591444\n",
      "92     \t [-0.21588327  0.74803896]. \t  0.9654037148415132 \t 1.0311585683591444\n",
      "93     \t [0.00266098 0.61586005]. \t  0.9400430839121477 \t 1.0311585683591444\n",
      "94     \t [-0.09241762  0.7623073 ]. \t  1.010122649876515 \t 1.0311585683591444\n",
      "95     \t [-0.07648225  0.71120877]. \t  1.0309331054027377 \t 1.0311585683591444\n",
      "96     \t [-0.20938482  0.78389845]. \t  0.9403387329117661 \t 1.0311585683591444\n",
      "97     \t [-0.81676868 -1.47655393]. \t  -13.33127520598867 \t 1.0311585683591444\n",
      "98     \t [0.01967557 0.77466574]. \t  0.9431241628269527 \t 1.0311585683591444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.07983721  0.68805205]. \t  1.0266945521834698 \t 1.0311585683591444\n",
      "100    \t [-0.00300818  0.74691197]. \t  0.9888114289826113 \t 1.0311585683591444\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.24057642 1.87416265]. \t  -48.82879577505094 \t -0.03190064766213015\n",
      "init   \t [2.21516724 0.12342277]. \t  -8.660623551418242 \t -0.03190064766213015\n",
      "init   \t [-1.60363003 -1.95440478]. \t  -48.28332373939171 \t -0.03190064766213015\n",
      "init   \t [-0.41718709 -0.39059456]. \t  -0.2801229037700297 \t -0.03190064766213015\n",
      "init   \t [ 0.13604803 -0.08643282]. \t  -0.03190064766213015 \t -0.03190064766213015\n",
      "1      \t [-1.57325339  1.20401448]. \t  -2.802921326312851 \t -0.03190064766213015\n",
      "2      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "4      \t [-0.29941428  2.        ]. \t  -47.74312970464237 \t -0.03190064766213015\n",
      "5      \t [-1.98892409 -0.00677659]. \t  -3.6089286920451014 \t -0.03190064766213015\n",
      "6      \t [ 0.47000471 -2.        ]. \t  -47.84472416723246 \t -0.03190064766213015\n",
      "7      \t [-0.95858101  0.51724208]. \t  -0.8813518415232754 \t -0.03190064766213015\n",
      "8      \t [-3.        -1.1312834]. \t  -113.72621572460534 \t -0.03190064766213015\n",
      "9      \t [3.         0.63979942]. \t  -109.85227281877631 \t -0.03190064766213015\n",
      "10     \t [ 1.3710538  -0.16755064]. \t  -1.9738657380816573 \t -0.03190064766213015\n",
      "11     \t [1.183492 0.9014  ]. \t  -2.856198216471313 \t -0.03190064766213015\n",
      "12     \t [1.10605396 2.        ]. \t  -52.57296339689554 \t -0.03190064766213015\n",
      "13     \t [-1.75358732  0.55466886]. \t  -0.3105623518741871 \t -0.03190064766213015\n",
      "14     \t [1.57724878 0.3735362 ]. \t  -2.195380706328395 \t -0.03190064766213015\n",
      "15     \t [0.24984462 0.73432267]. \t  \u001b[92m0.5687898621728367\u001b[0m \t 0.5687898621728367\n",
      "16     \t [ 2.10415688 -0.54542114]. \t  -3.490892516910097 \t 0.5687898621728367\n",
      "17     \t [-1.33451496 -0.44574336]. \t  -2.303999868307612 \t 0.5687898621728367\n",
      "18     \t [-1.35478722  2.        ]. \t  -47.61871993993006 \t 0.5687898621728367\n",
      "19     \t [-3.          0.37331887]. \t  -107.30026810485059 \t 0.5687898621728367\n",
      "20     \t [-0.00704051 -1.00899138]. \t  -0.08086170079990926 \t 0.5687898621728367\n",
      "21     \t [-0.58165444 -2.        ]. \t  -50.28913517821139 \t 0.5687898621728367\n",
      "22     \t [-1.1134773   1.05926978]. \t  -1.7348501038311004 \t 0.5687898621728367\n",
      "23     \t [ 1.08966237 -1.07707769]. \t  -1.9160589725188237 \t 0.5687898621728367\n",
      "24     \t [ 0.44323496 -0.79138374]. \t  \u001b[92m0.5796681522581768\u001b[0m \t 0.5796681522581768\n",
      "25     \t [ 1.83862912 -0.26582272]. \t  -1.6495113947646902 \t 0.5796681522581768\n",
      "26     \t [ 3.         -0.50064112]. \t  -106.64679521127213 \t 0.5796681522581768\n",
      "27     \t [ 1.67780075 -2.        ]. \t  -46.69909200966863 \t 0.5796681522581768\n",
      "28     \t [0.74766117 0.4561299 ]. \t  -1.319969431930844 \t 0.5796681522581768\n",
      "29     \t [ 1.66668708 -0.91018412]. \t  0.03361262109441443 \t 0.5796681522581768\n",
      "30     \t [-0.79862621 -1.04777495]. \t  -3.049827973532927 \t 0.5796681522581768\n",
      "31     \t [-3. -2.]. \t  -162.89999999999998 \t 0.5796681522581768\n",
      "32     \t [1.90924454 0.98304385]. \t  -4.569186906563059 \t 0.5796681522581768\n",
      "33     \t [-1.88265591 -0.8360549 ]. \t  -3.3706527441276615 \t 0.5796681522581768\n",
      "34     \t [3. 2.]. \t  -162.89999999999998 \t 0.5796681522581768\n",
      "35     \t [-0.30471593  0.66118007]. \t  \u001b[92m0.832107209266252\u001b[0m \t 0.832107209266252\n",
      "36     \t [-1.45684604  0.79196172]. \t  -0.12777607222897414 \t 0.832107209266252\n",
      "37     \t [-0.11701035  1.1693225 ]. \t  -1.9264920946463662 \t 0.832107209266252\n",
      "38     \t [ 0.89703235 -0.57166434]. \t  -0.6398035814202734 \t 0.832107209266252\n",
      "39     \t [-1.46485266 -1.11620956]. \t  -5.067948515114445 \t 0.832107209266252\n",
      "40     \t [-0.04312385 -0.67837209]. \t  \u001b[92m0.9569748186984283\u001b[0m \t 0.9569748186984283\n",
      "41     \t [1.61899648 1.41108672]. \t  -12.238355035877085 \t 0.9569748186984283\n",
      "42     \t [0.50831042 1.14721764]. \t  -3.1463456853404987 \t 0.9569748186984283\n",
      "43     \t [-1.61987745  0.05045956]. \t  -1.9672218221954005 \t 0.9569748186984283\n",
      "44     \t [-2.1594584  2.       ]. \t  -50.46994740036878 \t 0.9569748186984283\n",
      "45     \t [-2.16948411  1.12519331]. \t  -5.967383924154915 \t 0.9569748186984283\n",
      "46     \t [-1.75846311 -0.52277303]. \t  -2.2696515185213046 \t 0.9569748186984283\n",
      "47     \t [ 2.01879946 -1.27603613]. \t  -5.501981709523873 \t 0.9569748186984283\n",
      "48     \t [-0.55610869  0.84606916]. \t  0.2381278628127056 \t 0.9569748186984283\n",
      "49     \t [ 1.59844668 -1.29211295]. \t  -4.476912387683459 \t 0.9569748186984283\n",
      "50     \t [ 3.         -1.27897311]. \t  -109.22299484128484 \t 0.9569748186984283\n",
      "51     \t [-2.25880155 -2.        ]. \t  -62.53238157521534 \t 0.9569748186984283\n",
      "52     \t [-1.83884356  0.90546441]. \t  -0.14615507299910335 \t 0.9569748186984283\n",
      "53     \t [-0.40038854 -0.74388569]. \t  0.10211694079907074 \t 0.9569748186984283\n",
      "54     \t [-3.         -0.39905416]. \t  -109.56162047817976 \t 0.9569748186984283\n",
      "55     \t [0.36493751 2.        ]. \t  -49.2261327664525 \t 0.9569748186984283\n",
      "56     \t [2.53758183 1.32473641]. \t  -36.34392561872456 \t 0.9569748186984283\n",
      "57     \t [-0.11190355  0.80307205]. \t  0.9560929519328973 \t 0.9569748186984283\n",
      "58     \t [-3.         1.2362555]. \t  -108.4210645547899 \t 0.9569748186984283\n",
      "59     \t [ 2.26249767 -2.        ]. \t  -53.634392080142376 \t 0.9569748186984283\n",
      "60     \t [-0.0659072   0.58513204]. \t  0.9218520896210308 \t 0.9569748186984283\n",
      "61     \t [-0.09629965  0.69006861]. \t  \u001b[92m1.0272724384237262\u001b[0m \t 1.0272724384237262\n",
      "62     \t [ 0.04522997 -0.6839479 ]. \t  1.0186094341527352 \t 1.0272724384237262\n",
      "63     \t [-2.09551691 -1.46404342]. \t  -18.167143702004832 \t 1.0272724384237262\n",
      "64     \t [ 0.10069995 -0.75083898]. \t  1.0190021906221616 \t 1.0272724384237262\n",
      "65     \t [ 0.10624253 -0.68870162]. \t  1.0256466572455953 \t 1.0272724384237262\n",
      "66     \t [-0.13825303  0.76000156]. \t  1.0052932922602804 \t 1.0272724384237262\n",
      "67     \t [ 0.58333095 -1.40793436]. \t  -8.098394584415928 \t 1.0272724384237262\n",
      "68     \t [-0.08550315  0.74156342]. \t  1.024308411542484 \t 1.0272724384237262\n",
      "69     \t [-0.00236239 -0.64460954]. \t  0.9699083625938508 \t 1.0272724384237262\n",
      "70     \t [ 0.08092457 -0.68773902]. \t  1.0266305569824725 \t 1.0272724384237262\n",
      "71     \t [ 0.0074997  -0.64479626]. \t  0.9762267861682926 \t 1.0272724384237262\n",
      "72     \t [-0.13591375  0.65926674]. \t  0.9993361469746563 \t 1.0272724384237262\n",
      "73     \t [-0.08019459  0.76568859]. \t  1.0059898875059734 \t 1.0272724384237262\n",
      "74     \t [ 0.03638385 -0.65036261]. \t  0.9946376951113518 \t 1.0272724384237262\n",
      "75     \t [ 0.09711697 -0.71882864]. \t  \u001b[92m1.0311526526210966\u001b[0m \t 1.0311526526210966\n",
      "76     \t [-0.04056383  0.68071721]. \t  1.0156711908380995 \t 1.0311526526210966\n",
      "77     \t [2.19569053 0.68512691]. \t  -8.33414399351848 \t 1.0311526526210966\n",
      "78     \t [ 0.01526672 -0.75007408]. \t  0.9948383871155538 \t 1.0311526526210966\n",
      "79     \t [1.75369447 2.        ]. \t  -53.64283246128477 \t 1.0311526526210966\n",
      "80     \t [-0.09425475  0.72170484]. \t  1.0309135814319876 \t 1.0311526526210966\n",
      "81     \t [-2.36558533 -0.69271009]. \t  -15.6755806779307 \t 1.0311526526210966\n",
      "82     \t [-0.05876591  0.77417612]. \t  0.9922258386954276 \t 1.0311526526210966\n",
      "83     \t [ 0.16868908 -0.75299456]. \t  0.9969342831873017 \t 1.0311526526210966\n",
      "84     \t [ 0.08621174 -0.7571856 ]. \t  1.0141551272812512 \t 1.0311526526210966\n",
      "85     \t [ 0.02480687 -0.70518814]. \t  1.0150034149097018 \t 1.0311526526210966\n",
      "86     \t [-0.08225496  0.66784723]. \t  1.0163108643274408 \t 1.0311526526210966\n",
      "87     \t [-0.06074544  0.77421561]. \t  0.9927693700767459 \t 1.0311526526210966\n",
      "88     \t [ 0.03024811 -0.78698176]. \t  0.963178253932215 \t 1.0311526526210966\n",
      "89     \t [-0.01339584 -0.63240895]. \t  0.9507634743208235 \t 1.0311526526210966\n",
      "90     \t [-0.07308231  0.69606264]. \t  1.0286050204146022 \t 1.0311526526210966\n",
      "91     \t [-2.32560083  0.65488187]. \t  -10.437711594676886 \t 1.0311526526210966\n",
      "92     \t [ 0.14803007 -0.77453925]. \t  0.9880794821315801 \t 1.0311526526210966\n",
      "93     \t [-0.08445013  0.70927607]. \t  \u001b[92m1.0314400876247753\u001b[0m \t 1.0314400876247753\n",
      "94     \t [ 0.00399233 -0.6544964 ]. \t  0.9820232224275948 \t 1.0314400876247753\n",
      "95     \t [-0.05916234  0.72790773]. \t  1.0255256996379583 \t 1.0314400876247753\n",
      "96     \t [-0.08856746  0.69175033]. \t  1.028173029314279 \t 1.0314400876247753\n",
      "97     \t [ 0.15443807 -0.62503802]. \t  0.9545053263215307 \t 1.0314400876247753\n",
      "98     \t [ 0.09015185 -0.72122877]. \t  1.031021750774701 \t 1.0314400876247753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.16412417 -0.78238338]. \t  0.9718913336630965 \t 1.0314400876247753\n",
      "100    \t [ 0.19876235 -0.78240516]. \t  0.9504263232284851 \t 1.0314400876247753\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.35667346 1.04596796]. \t  -1.260209694806778 \t -1.260209694806778\n",
      "init   \t [-1.53281067  1.08675788]. \t  -1.318361654110369 \t -1.260209694806778\n",
      "init   \t [-0.87316751  1.1867896 ]. \t  -3.241714295197782 \t -1.260209694806778\n",
      "init   \t [-0.92399987 -0.22450226]. \t  -2.107788234046822 \t -1.260209694806778\n",
      "init   \t [-1.18017846 -0.23154432]. \t  -2.468366013151972 \t -1.260209694806778\n",
      "1      \t [-2.88325144  1.59592108]. \t  -90.78621502734951 \t -1.260209694806778\n",
      "2      \t [ 1.47406719 -0.32331975]. \t  -1.3452237810694556 \t -1.260209694806778\n",
      "3      \t [1.94579017 1.98345773]. \t  -53.16421047498187 \t -1.260209694806778\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.260209694806778\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -1.260209694806778\n",
      "6      \t [ 0.11085325 -2.        ]. \t  -47.82713078289238 \t -1.260209694806778\n",
      "7      \t [3.         0.46093431]. \t  -109.61351892597119 \t -1.260209694806778\n",
      "8      \t [ 0.60155071 -0.11154683]. \t  \u001b[92m-1.0720106398838007\u001b[0m \t -1.0720106398838007\n",
      "9      \t [0.23956475 2.        ]. \t  -48.70184068007716 \t -1.0720106398838007\n",
      "10     \t [1.18335977 0.55720659]. \t  -2.2017422335419083 \t -1.0720106398838007\n",
      "11     \t [-1.10929686  0.48225797]. \t  -1.1144880002937172 \t -1.0720106398838007\n",
      "12     \t [-3.         -0.01693168]. \t  -108.94964863459077 \t -1.0720106398838007\n",
      "13     \t [-1.50788749  2.        ]. \t  -47.1407455818699 \t -1.0720106398838007\n",
      "14     \t [-1.10875552 -1.53874341]. \t  -17.022739850089458 \t -1.0720106398838007\n",
      "15     \t [ 1.23894383 -1.32702936]. \t  -6.113969844433521 \t -1.0720106398838007\n",
      "16     \t [3. 2.]. \t  -162.89999999999998 \t -1.0720106398838007\n",
      "17     \t [ 0.6410518  -0.89457315]. \t  \u001b[92m-0.09943884155643246\u001b[0m \t -0.09943884155643246\n",
      "18     \t [1.08473185 1.41237665]. \t  -11.812013513360084 \t -0.09943884155643246\n",
      "19     \t [-0.68446609 -0.95400616]. \t  -1.773133688582614 \t -0.09943884155643246\n",
      "20     \t [ 1.10671086 -0.69701702]. \t  -0.5907812723919827 \t -0.09943884155643246\n",
      "21     \t [-0.03087283  0.53107547]. \t  \u001b[92m0.8225609487585431\u001b[0m \t 0.8225609487585431\n",
      "22     \t [ 2.39371592 -0.70000465]. \t  -14.004952221543256 \t 0.8225609487585431\n",
      "23     \t [0.59010857 0.62974403]. \t  -0.566738444778825 \t 0.8225609487585431\n",
      "24     \t [-0.00302454 -0.51302109]. \t  0.7740970548196849 \t 0.8225609487585431\n",
      "25     \t [-1.38511304 -0.93229381]. \t  -3.134891034104057 \t 0.8225609487585431\n",
      "26     \t [ 1.24073475 -2.        ]. \t  -47.915647296932114 \t 0.8225609487585431\n",
      "27     \t [ 1.79817885 -0.87116319]. \t  0.051670498607851445 \t 0.8225609487585431\n",
      "28     \t [1.94358022 0.28511368]. \t  -3.3671474860213646 \t 0.8225609487585431\n",
      "29     \t [ 1.93554885 -0.38643477]. \t  -1.7823641688195035 \t 0.8225609487585431\n",
      "30     \t [-1.22622113  0.91287203]. \t  -0.7248780793267922 \t 0.8225609487585431\n",
      "31     \t [-1.85501928  0.65128065]. \t  -0.29502036826806477 \t 0.8225609487585431\n",
      "32     \t [1.76014192 1.05062121]. \t  -4.455779722567868 \t 0.8225609487585431\n",
      "33     \t [-1.59654418  0.71854322]. \t  0.07402999105445895 \t 0.8225609487585431\n",
      "34     \t [-1.84269368 -2.        ]. \t  -54.10501232787185 \t 0.8225609487585431\n",
      "35     \t [-0.36269512  0.85438075]. \t  0.6077317294513593 \t 0.8225609487585431\n",
      "36     \t [-2.12324567 -0.64905102]. \t  -6.29677170918903 \t 0.8225609487585431\n",
      "37     \t [-1.85926278 -0.25864806]. \t  -2.733576910752553 \t 0.8225609487585431\n",
      "38     \t [-0.07426356 -0.99437587]. \t  -0.05147909798364528 \t 0.8225609487585431\n",
      "39     \t [ 2.03413954 -2.        ]. \t  -48.142679634359894 \t 0.8225609487585431\n",
      "40     \t [-0.78707792 -2.        ]. \t  -51.32545304371882 \t 0.8225609487585431\n",
      "41     \t [-1.94009535 -1.1903645 ]. \t  -7.752242689699628 \t 0.8225609487585431\n",
      "42     \t [-2.34776908  2.        ]. \t  -57.372305882784055 \t 0.8225609487585431\n",
      "43     \t [ 3.         -0.53821906]. \t  -106.4622812218536 \t 0.8225609487585431\n",
      "44     \t [-3.         -1.04918802]. \t  -112.49138504330931 \t 0.8225609487585431\n",
      "45     \t [1.14366486 2.        ]. \t  -52.672445365707055 \t 0.8225609487585431\n",
      "46     \t [ 2.01670859 -1.22133975]. \t  -4.427312881448755 \t 0.8225609487585431\n",
      "47     \t [-1.95467373  0.98894697]. \t  -1.1997890375555402 \t 0.8225609487585431\n",
      "48     \t [2.43434795 1.20578412]. \t  -24.90184825812798 \t 0.8225609487585431\n",
      "49     \t [-0.41517011  0.42331   ]. \t  0.13529227855636666 \t 0.8225609487585431\n",
      "50     \t [-0.18520662 -0.73058332]. \t  0.72538577083831 \t 0.8225609487585431\n",
      "51     \t [-0.61452399  2.        ]. \t  -47.999978202947524 \t 0.8225609487585431\n",
      "52     \t [-3.          0.83277973]. \t  -105.55146398809737 \t 0.8225609487585431\n",
      "53     \t [-0.09690694 -0.0579582 ]. \t  -0.02960397376160965 \t 0.8225609487585431\n",
      "54     \t [-1.78769012 -0.75034682]. \t  -2.5726283463967743 \t 0.8225609487585431\n",
      "55     \t [ 0.12023214 -0.67729368]. \t  \u001b[92m1.0172332984093952\u001b[0m \t 1.0172332984093952\n",
      "56     \t [ 3.         -1.28300732]. \t  -109.30523020464997 \t 1.0172332984093952\n",
      "57     \t [3.         1.26678528]. \t  -116.58220131853932 \t 1.0172332984093952\n",
      "58     \t [-3.  2.]. \t  -150.89999999999998 \t 1.0172332984093952\n",
      "59     \t [0.91385494 0.96225175]. \t  -2.6750628110465735 \t 1.0172332984093952\n",
      "60     \t [-0.10648915  0.72229919]. \t  \u001b[92m1.029940473654206\u001b[0m \t 1.029940473654206\n",
      "61     \t [2.14007521 0.76605357]. \t  -6.962658023195413 \t 1.029940473654206\n",
      "62     \t [ 0.067551   -0.76208913]. \t  1.0071694945404706 \t 1.029940473654206\n",
      "63     \t [-0.09632035  0.66668626]. \t  1.0149514351735909 \t 1.029940473654206\n",
      "64     \t [ 0.0600944  -0.64617366]. \t  0.9972151021889751 \t 1.029940473654206\n",
      "65     \t [-0.16409577  0.68441606]. \t  1.0021284265867305 \t 1.029940473654206\n",
      "66     \t [-0.15642977  0.64622661]. \t  0.9773073521774103 \t 1.029940473654206\n",
      "67     \t [-2.00226762  1.46611727]. \t  -10.71037181683095 \t 1.029940473654206\n",
      "68     \t [-2.41998168 -2.        ]. \t  -71.19290342575246 \t 1.029940473654206\n",
      "69     \t [ 0.6342188  -1.53772118]. \t  -13.22234249758661 \t 1.029940473654206\n",
      "70     \t [-0.01556448  0.72235786]. \t  1.0083731218173648 \t 1.029940473654206\n",
      "71     \t [-0.10399835  0.64007493]. \t  0.990930031433724 \t 1.029940473654206\n",
      "72     \t [ 0.1170498  -0.76590302]. \t  1.0052361071534106 \t 1.029940473654206\n",
      "73     \t [ 0.01810317 -0.70796424]. \t  1.0114998345456907 \t 1.029940473654206\n",
      "74     \t [-0.0724378   0.77788274]. \t  0.9912315958063695 \t 1.029940473654206\n",
      "75     \t [ 0.08972257 -0.73256791]. \t  1.0282886596318537 \t 1.029940473654206\n",
      "76     \t [-0.05987706  0.72274934]. \t  1.0269609750785464 \t 1.029940473654206\n",
      "77     \t [-0.04092686  0.64151229]. \t  0.9882588569931421 \t 1.029940473654206\n",
      "78     \t [ 0.06931108 -0.68068367]. \t  1.0226325637488853 \t 1.029940473654206\n",
      "79     \t [2.36647731 0.05799717]. \t  -15.209050207895377 \t 1.029940473654206\n",
      "80     \t [ 0.04103182 -0.76176294]. \t  0.9987467289197645 \t 1.029940473654206\n",
      "81     \t [ 0.09333899 -0.72601373]. \t  \u001b[92m1.0301391349648283\u001b[0m \t 1.0301391349648283\n",
      "82     \t [ 0.06980882 -0.70046062]. \t  1.0291050195877018 \t 1.0301391349648283\n",
      "83     \t [-0.09032827  0.68189073]. \t  1.024189833401917 \t 1.0301391349648283\n",
      "84     \t [ 0.0459031  -0.70959806]. \t  1.024103865512604 \t 1.0301391349648283\n",
      "85     \t [-0.15527143  1.38370662]. \t  -6.885241509706223 \t 1.0301391349648283\n",
      "86     \t [-0.01655579  0.68327067]. \t  1.0058225160979364 \t 1.0301391349648283\n",
      "87     \t [ 0.04671684 -0.70582985]. \t  1.0242412646963588 \t 1.0301391349648283\n",
      "88     \t [0.04915406 0.8001761 ]. \t  0.8722999289603834 \t 1.0301391349648283\n",
      "89     \t [-0.25589566  0.77219128]. \t  0.9075023085797713 \t 1.0301391349648283\n",
      "90     \t [-0.06473727  0.73987792]. \t  1.0221765055378071 \t 1.0301391349648283\n",
      "91     \t [-0.17092332  0.76686544]. \t  0.9849659269640842 \t 1.0301391349648283\n",
      "92     \t [-0.01709918 -0.62670819]. \t  0.9421154937902884 \t 1.0301391349648283\n",
      "93     \t [ 0.03349873 -0.68929822]. \t  1.016130938131337 \t 1.0301391349648283\n",
      "94     \t [-0.16401209  0.6517104 ]. \t  0.9781371799852842 \t 1.0301391349648283\n",
      "95     \t [-0.05382096  0.72491876]. \t  1.0248441828747001 \t 1.0301391349648283\n",
      "96     \t [-2.54432369 -1.45983059]. \t  -41.67542987899243 \t 1.0301391349648283\n",
      "97     \t [ 0.0562591  -0.71067563]. \t  1.0272402455557654 \t 1.0301391349648283\n",
      "98     \t [-0.12063059  0.74781879]. \t  1.0184123506553233 \t 1.0301391349648283\n",
      "99     \t [-0.19005316  0.79932048]. \t  0.9329691197710107 \t 1.0301391349648283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.12882035  0.58819145]. \t  0.9150671156057304 \t 1.0301391349648283\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.42310371 0.25811502]. \t  -0.5111512856704337 \t 0.9146183273252478\n",
      "init   \t [-0.069349   -0.65408899]. \t  0.9146183273252478 \t 0.9146183273252478\n",
      "init   \t [-0.74479093  0.12814348]. \t  -1.4695211123708676 \t 0.9146183273252478\n",
      "init   \t [-2.59136227  0.33811624]. \t  -31.819735064764004 \t 0.9146183273252478\n",
      "init   \t [-1.57261342 -1.35697367]. \t  -10.421507729581124 \t 0.9146183273252478\n",
      "1      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "2      \t [3. 2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "3      \t [-0.59070905  2.        ]. \t  -47.972802467374706 \t 0.9146183273252478\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "5      \t [-0.38611543 -2.        ]. \t  -49.323000515409404 \t 0.9146183273252478\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "7      \t [ 1.73364608 -0.18317544]. \t  -1.6549679932987365 \t 0.9146183273252478\n",
      "8      \t [0.9268859 2.       ]. \t  -51.95163437754319 \t 0.9146183273252478\n",
      "9      \t [-1.54539641 -0.46018008]. \t  -2.1592775352491835 \t 0.9146183273252478\n",
      "10     \t [3.00000000e+00 2.94576216e-03]. \t  -108.90880257672794 \t 0.9146183273252478\n",
      "11     \t [ 1.09778022 -0.90768454]. \t  -0.7772089856004172 \t 0.9146183273252478\n",
      "12     \t [ 1.05305303 -0.20639054]. \t  -1.927380155315301 \t 0.9146183273252478\n",
      "13     \t [ 1.19010905 -2.        ]. \t  -48.01957145135075 \t 0.9146183273252478\n",
      "14     \t [-0.93100483 -0.87807456]. \t  -2.2177238013169074 \t 0.9146183273252478\n",
      "15     \t [-3.         -0.39908461]. \t  -109.56164557870906 \t 0.9146183273252478\n",
      "16     \t [-1.62937224  0.81168118]. \t  0.16617802737890874 \t 0.9146183273252478\n",
      "17     \t [1.44284135 0.82252606]. \t  -2.544916602655105 \t 0.9146183273252478\n",
      "18     \t [-0.10579824  1.01477994]. \t  -0.05980881621286244 \t 0.9146183273252478\n",
      "19     \t [-1.67595192  0.32192892]. \t  -1.1429663522107212 \t 0.9146183273252478\n",
      "20     \t [0.74282914 0.92431119]. \t  -1.8126444507996766 \t 0.9146183273252478\n",
      "21     \t [-0.89813348  0.90272877]. \t  -0.6210389314267739 \t 0.9146183273252478\n",
      "22     \t [ 0.47283138 -1.20688949]. \t  -2.882581125503747 \t 0.9146183273252478\n",
      "23     \t [-0.17169674  0.56557898]. \t  0.851232009005201 \t 0.9146183273252478\n",
      "24     \t [1.48996611 0.34270249]. \t  -2.273384815854201 \t 0.9146183273252478\n",
      "25     \t [ 0.49427608 -0.66110903]. \t  0.45417346545188275 \t 0.9146183273252478\n",
      "26     \t [-1.53382854 -2.        ]. \t  -53.195453230296735 \t 0.9146183273252478\n",
      "27     \t [ 1.92739538 -0.89720763]. \t  -0.6104023618293661 \t 0.9146183273252478\n",
      "28     \t [ 1.65203184 -0.67855413]. \t  0.06367194898662054 \t 0.9146183273252478\n",
      "29     \t [-1.64938385  2.        ]. \t  -46.752463149647355 \t 0.9146183273252478\n",
      "30     \t [1.81776716 2.        ]. \t  -53.949962083411435 \t 0.9146183273252478\n",
      "31     \t [ 3.         -0.92530768]. \t  -105.6315744779709 \t 0.9146183273252478\n",
      "32     \t [2.21942298 0.90496576]. \t  -10.004638986368114 \t 0.9146183273252478\n",
      "33     \t [-3.          0.88558287]. \t  -105.56645960184807 \t 0.9146183273252478\n",
      "34     \t [-0.28222312 -0.25174362]. \t  -0.13905932268328622 \t 0.9146183273252478\n",
      "35     \t [ 1.70892421 -1.28812012]. \t  -4.2478747684839036 \t 0.9146183273252478\n",
      "36     \t [0.21974119 0.77993245]. \t  0.5934197946166838 \t 0.9146183273252478\n",
      "37     \t [-1.96634324  0.5972462 ]. \t  -1.2469630984439615 \t 0.9146183273252478\n",
      "38     \t [1.96263715 0.5055628 ]. \t  -3.531296985134554 \t 0.9146183273252478\n",
      "39     \t [-2.01467398 -0.62192157]. \t  -4.23274986653706 \t 0.9146183273252478\n",
      "40     \t [-1.62789082 -0.86510968]. \t  -2.711098948592639 \t 0.9146183273252478\n",
      "41     \t [-1.98628745 -0.06572379]. \t  -3.6773796067009523 \t 0.9146183273252478\n",
      "42     \t [-0.41505186 -1.20097216]. \t  -3.6788962156748184 \t 0.9146183273252478\n",
      "43     \t [-2.12608765 -1.43141638]. \t  -17.599653594701834 \t 0.9146183273252478\n",
      "44     \t [ 2.06259318 -2.        ]. \t  -48.55021911406979 \t 0.9146183273252478\n",
      "45     \t [-0.53758679 -0.60530814]. \t  -0.3854541774585093 \t 0.9146183273252478\n",
      "46     \t [-1.18725502  1.38521964]. \t  -7.807113225785869 \t 0.9146183273252478\n",
      "47     \t [3.         0.97828938]. \t  -111.67044734379601 \t 0.9146183273252478\n",
      "48     \t [1.77879643 1.21846772]. \t  -7.237016650559234 \t 0.9146183273252478\n",
      "49     \t [-2.07568718  1.41221559]. \t  -9.912090143594304 \t 0.9146183273252478\n",
      "50     \t [-1.77302201  1.07359089]. \t  -0.9770239394859308 \t 0.9146183273252478\n",
      "51     \t [0.2639605  1.51258006]. \t  -12.454203375397544 \t 0.9146183273252478\n",
      "52     \t [ 0.21622553 -0.86099607]. \t  0.7707814666442722 \t 0.9146183273252478\n",
      "53     \t [ 0.33715369 -2.        ]. \t  -47.75373759617585 \t 0.9146183273252478\n",
      "54     \t [-3.         -1.29946593]. \t  -117.4495889850421 \t 0.9146183273252478\n",
      "55     \t [ 2.29977433 -1.4165818 ]. \t  -16.551361780489014 \t 0.9146183273252478\n",
      "56     \t [0.00219591 0.10473722]. \t  0.04314890579331633 \t 0.9146183273252478\n",
      "57     \t [-2.27180274  2.        ]. \t  -53.988408923481146 \t 0.9146183273252478\n",
      "58     \t [0.15151686 2.        ]. \t  -48.393760396911745 \t 0.9146183273252478\n",
      "59     \t [-2.19194779 -2.        ]. \t  -60.095843265418566 \t 0.9146183273252478\n",
      "60     \t [2.38197557 1.65443939]. \t  -38.936296682298206 \t 0.9146183273252478\n",
      "61     \t [-0.11706099  0.74822503]. \t  \u001b[92m1.0188447042785873\u001b[0m \t 1.0188447042785873\n",
      "62     \t [0.00108877 0.59624729]. \t  0.915837597807952 \t 1.0188447042785873\n",
      "63     \t [ 0.09954376 -0.71004513]. \t  \u001b[92m1.03118123253397\u001b[0m \t 1.03118123253397\n",
      "64     \t [ 0.11891142 -0.69673177]. \t  1.0258600490590215 \t 1.03118123253397\n",
      "65     \t [-0.05350513  0.74785189]. \t  1.0145222852758629 \t 1.03118123253397\n",
      "66     \t [ 2.27410709 -0.2034179 ]. \t  -10.00495198937824 \t 1.03118123253397\n",
      "67     \t [-0.08064217  0.74461987]. \t  1.0222608213823798 \t 1.03118123253397\n",
      "68     \t [ 0.10842026 -0.71097128]. \t  1.0302333853106496 \t 1.03118123253397\n",
      "69     \t [ 0.13219193 -0.702631  ]. \t  1.0234635618445007 \t 1.03118123253397\n",
      "70     \t [-0.0855971   0.70951835]. \t  \u001b[92m1.0314911776236342\u001b[0m \t 1.0314911776236342\n",
      "71     \t [ 0.05664677 -0.70045732]. \t  1.0265144276039107 \t 1.0314911776236342\n",
      "72     \t [-0.07126129  0.68140653]. \t  1.023205617279376 \t 1.0314911776236342\n",
      "73     \t [-0.07649655  0.62827032]. \t  0.9803929842815494 \t 1.0314911776236342\n",
      "74     \t [-0.06236798  0.73425254]. \t  1.0241429206333545 \t 1.0314911776236342\n",
      "75     \t [ 0.10790536 -0.75059758]. \t  1.018626832406605 \t 1.0314911776236342\n",
      "76     \t [-2.28489084  1.03498474]. \t  -9.017569008462537 \t 1.0314911776236342\n",
      "77     \t [-0.0014532   0.76772978]. \t  0.9691313352394418 \t 1.0314911776236342\n",
      "78     \t [ 0.03382771 -0.69028666]. \t  1.0165664970963229 \t 1.0314911776236342\n",
      "79     \t [ 0.16544924 -0.72894534]. \t  1.0087422243661905 \t 1.0314911776236342\n",
      "80     \t [ 0.0961055 -0.7461484]. \t  1.0220662387732542 \t 1.0314911776236342\n",
      "81     \t [-0.02613556 -0.66001825]. \t  0.9634417313222318 \t 1.0314911776236342\n",
      "82     \t [-0.01053452  0.64242281]. \t  0.9758435094013466 \t 1.0314911776236342\n",
      "83     \t [0.00788233 0.75175833]. \t  0.9768526899303086 \t 1.0314911776236342\n",
      "84     \t [ 0.0782652  -0.73822102]. \t  1.0252646664327931 \t 1.0314911776236342\n",
      "85     \t [0.03651517 0.67714281]. \t  0.9630627442830326 \t 1.0314911776236342\n",
      "86     \t [-0.05941466  0.74774652]. \t  1.016349860691483 \t 1.0314911776236342\n",
      "87     \t [2.46845602 0.41014772]. \t  -22.26741402282821 \t 1.0314911776236342\n",
      "88     \t [0.85525998 0.57244654]. \t  -1.5410818036167435 \t 1.0314911776236342\n",
      "89     \t [-0.07735512  0.71746013]. \t  1.0307689408495904 \t 1.0314911776236342\n",
      "90     \t [-0.03824239  0.7392825 ]. \t  1.0137631301727852 \t 1.0314911776236342\n",
      "91     \t [ 0.12863837 -0.68454013]. \t  1.0184952923705628 \t 1.0314911776236342\n",
      "92     \t [-0.00660215  0.73126503]. \t  0.9998237261564363 \t 1.0314911776236342\n",
      "93     \t [-0.09910086  0.72706372]. \t  1.0296941548228393 \t 1.0314911776236342\n",
      "94     \t [-0.07355097  0.69870851]. \t  1.0292555461830393 \t 1.0314911776236342\n",
      "95     \t [ 0.08175493 -0.5620206 ]. \t  0.8836865769990463 \t 1.0314911776236342\n",
      "96     \t [ 0.02364334 -0.65694721]. \t  0.9945716676002693 \t 1.0314911776236342\n",
      "97     \t [ 0.11870524 -0.74614831]. \t  1.0197475250622376 \t 1.0314911776236342\n",
      "98     \t [-0.03184777  0.73571654]. \t  1.0125602026146983 \t 1.0314911776236342\n",
      "99     \t [ 0.16676478 -0.67377917]. \t  0.9942657306488631 \t 1.0314911776236342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.00980733 -0.63124974]. \t  0.9645778609519519 \t 1.0314911776236342\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.970904200660691, -7.858899506465735)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.699693111723787, -7.957505436074646)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.216499649930707, -7.912574131400304)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.948454147079031, -7.43530645774878)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.655941354427242, -8.136338815310955)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.679629852384771, -6.7456323338565465)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.032257683360543, -6.597867840027261)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.031280240575756, -6.6115847244845565)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.347088372812169, -8.63604880766433)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.676652568596023, -6.037391708799952)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.80460924679021, -9.220077960183275)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.982088148036611, -6.1754649685376855)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.825641965209888, -8.741745887915192)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.04607920243526, -7.372159026255912)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.278743571125627, -14.468606562754937)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.607771789777762, -9.661088268717132)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.339251078685901, -7.72548738375732)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.403375308397165, -8.740884547593609)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.741692009347558, -6.528726528871048)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.208763597837319, -9.125793579560927)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.763750478984005, -7.372159026255912)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
