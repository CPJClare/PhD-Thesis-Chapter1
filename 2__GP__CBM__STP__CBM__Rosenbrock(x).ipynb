{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock synthetic function:\n",
    "\n",
    "GP CBM versus STP nu = 3 CBM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/rosen.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Rosenbrock'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'CBMinimized'\n",
    "util_winner = 'tCBMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Rosenbrock':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = 0\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = -2.048 \n",
    "    ub = +2.048 \n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 2\n",
    "\n",
    "# 2-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test)\n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training):\n",
    "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 44\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 10000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 166\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1999\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - CBM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'CBMinimized': self.CBMinimized,\n",
    "            'tCBMinimized': self.tCBMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def CBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    \n",
    "    def tCBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [-0.14193382 -0.19540202]. \t  \u001b[92m-5.950073563706454\u001b[0m \t -5.950073563706454\n",
      "2      \t [-0.4427397  -0.43381045]. \t  -41.74994134652668 \t -5.950073563706454\n",
      "3      \t [ 1.48734751 -0.11836167]. \t  -543.3904970970373 \t -5.950073563706454\n",
      "4      \t [ 1.43239979 -2.048     ]. \t  -1680.9976825616914 \t -5.950073563706454\n",
      "5      \t [ 0.20587254 -0.56050519]. \t  -36.978115474632276 \t -5.950073563706454\n",
      "6      \t [-0.05243791  0.09173427]. \t  \u001b[92m-1.8994502242776696\u001b[0m \t -1.8994502242776696\n",
      "7      \t [-0.05312265  0.08768902]. \t  \u001b[92m-1.829308163195874\u001b[0m \t -1.829308163195874\n",
      "8      \t [-0.05333052  0.08649944]. \t  \u001b[92m-1.8093260842864538\u001b[0m \t -1.8093260842864538\n",
      "9      \t [-0.05349197  0.08558476]. \t  \u001b[92m-1.7941608789100003\u001b[0m \t -1.7941608789100003\n",
      "10     \t [-0.05363468  0.08478254]. \t  \u001b[92m-1.7810031071915735\u001b[0m \t -1.7810031071915735\n",
      "11     \t [-0.05377078  0.08402287]. \t  \u001b[92m-1.7686661114430957\u001b[0m \t -1.7686661114430957\n",
      "12     \t [-0.05390779  0.08326314]. \t  \u001b[92m-1.7564477520506907\u001b[0m \t -1.7564477520506907\n",
      "13     \t [-0.05405213  0.08246765]. \t  \u001b[92m-1.743782721616291\u001b[0m \t -1.743782721616291\n",
      "14     \t [-0.05421068  0.08159964]. \t  \u001b[92m-1.7301130365695503\u001b[0m \t -1.7301130365695503\n",
      "15     \t [-0.05439211  0.08061273]. \t  \u001b[92m-1.7147606161704072\u001b[0m \t -1.7147606161704072\n",
      "16     \t [-0.05460967  0.0794385 ]. \t  \u001b[92m-1.6967578399402674\u001b[0m \t -1.6967578399402674\n",
      "17     \t [-0.05488499  0.07796493]. \t  \u001b[92m-1.6745710236799622\u001b[0m \t -1.6745710236799622\n",
      "18     \t [-0.0552587   0.07598603]. \t  \u001b[92m-1.6454860117580834\u001b[0m \t -1.6454860117580834\n",
      "19     \t [-0.05581793  0.07306669]. \t  \u001b[92m-1.6040664252374337\u001b[0m \t -1.6040664252374337\n",
      "20     \t [-0.05679014  0.0680992 ]. \t  \u001b[92m-1.5376700599998263\u001b[0m \t -1.5376700599998263\n",
      "21     \t [-0.05892534  0.05762412]. \t  \u001b[92m-1.4145659325863027\u001b[0m \t -1.4145659325863027\n",
      "22     \t [-0.06349026  0.03708678]. \t  \u001b[92m-1.2402799198832826\u001b[0m \t -1.2402799198832826\n",
      "23     \t [-0.06746927  0.02146623]. \t  \u001b[92m-1.1680994142444912\u001b[0m \t -1.1680994142444912\n",
      "24     \t [-0.06930789  0.01551613]. \t  \u001b[92m-1.1548952218210427\u001b[0m \t -1.1548952218210427\n",
      "25     \t [-0.07025886  0.01303905]. \t  \u001b[92m-1.152019467636916\u001b[0m \t -1.152019467636916\n",
      "26     \t [-0.0708597   0.01176545]. \t  \u001b[92m-1.1512891387969393\u001b[0m \t -1.1512891387969393\n",
      "27     \t [-0.07128884  0.01101137]. \t  \u001b[92m-1.1511754129374758\u001b[0m \t -1.1511754129374758\n",
      "28     \t [-0.07161943  0.01052037]. \t  -1.1512745300976046 \t -1.1511754129374758\n",
      "29     \t [-0.07188572  0.0101816 ]. \t  -1.1514530736884405 \t -1.1511754129374758\n",
      "30     \t [-0.07210822  0.00993449]. \t  -1.1516579601885848 \t -1.1511754129374758\n",
      "31     \t [-0.07229794  0.00974945]. \t  -1.1518681457319677 \t -1.1511754129374758\n",
      "32     \t [-0.07246265  0.00960768]. \t  -1.1520743391174055 \t -1.1511754129374758\n",
      "33     \t [-0.0726081   0.00949392]. \t  -1.1522706580863757 \t -1.1511754129374758\n",
      "34     \t [-0.07273772  0.00940306]. \t  -1.1524572978790615 \t -1.1511754129374758\n",
      "35     \t [-0.0728541   0.00932897]. \t  -1.152632966514544 \t -1.1511754129374758\n",
      "36     \t [-0.07295963  0.00926748]. \t  -1.1527981655943471 \t -1.1511754129374758\n",
      "37     \t [-0.07305585  0.00921646]. \t  -1.152953763656217 \t -1.1511754129374758\n",
      "38     \t [-0.07314418  0.00917314]. \t  -1.1531000117653654 \t -1.1511754129374758\n",
      "39     \t [-0.07322535  0.00913654]. \t  -1.1532373911510714 \t -1.1511754129374758\n",
      "40     \t [-0.07330085  0.00910394]. \t  -1.153366686445703 \t -1.1511754129374758\n",
      "41     \t [-0.07337046  0.00907752]. \t  -1.1534889318200523 \t -1.1511754129374758\n",
      "42     \t [-0.07343546  0.00905352]. \t  -1.1536038062988991 \t -1.1511754129374758\n",
      "43     \t [-0.07349617  0.00903355]. \t  -1.153713072299077 \t -1.1511754129374758\n",
      "44     \t [-0.07355295  0.00901571]. \t  -1.1538160202591479 \t -1.1511754129374758\n",
      "45     \t [-0.07360658  0.00899889]. \t  -1.1539134194234566 \t -1.1511754129374758\n",
      "46     \t [-0.0736568   0.00898491]. \t  -1.1540059886351204 \t -1.1511754129374758\n",
      "47     \t [-0.07370414  0.0089728 ]. \t  -1.154094101001617 \t -1.1511754129374758\n",
      "48     \t [-0.07374874  0.0089618 ]. \t  -1.1541774612544549 \t -1.1511754129374758\n",
      "49     \t [-0.07379092  0.00895223]. \t  -1.1542569294075944 \t -1.1511754129374758\n",
      "50     \t [-0.073831    0.00894356]. \t  -1.1543328140931168 \t -1.1511754129374758\n",
      "51     \t [-0.07386874  0.00893616]. \t  -1.15440479911207 \t -1.1511754129374758\n",
      "52     \t [-0.07390522  0.00892781]. \t  -1.154473626519006 \t -1.1511754129374758\n",
      "53     \t [-0.07393923  0.00892256]. \t  -1.1545395467772053 \t -1.1511754129374758\n",
      "54     \t [-0.07397191  0.00891702]. \t  -1.1546025774243467 \t -1.1511754129374758\n",
      "55     \t [-0.0740028   0.00891261]. \t  -1.1546627634247897 \t -1.1511754129374758\n",
      "56     \t [-0.07403272  0.00890729]. \t  -1.1547203380287165 \t -1.1511754129374758\n",
      "57     \t [-0.07406086  0.00890376]. \t  -1.1547755161810591 \t -1.1511754129374758\n",
      "58     \t [-0.07408803  0.00890016]. \t  -1.1548286658331508 \t -1.1511754129374758\n",
      "59     \t [-0.07411432  0.00889463]. \t  -1.1548787161247054 \t -1.1511754129374758\n",
      "60     \t [-0.07413881  0.00889282]. \t  -1.1549276299341205 \t -1.1511754129374758\n",
      "61     \t [-0.07416259  0.00889019]. \t  -1.1549745530113151 \t -1.1511754129374758\n",
      "62     \t [-0.07418536  0.00888791]. \t  -1.155019620996035 \t -1.1511754129374758\n",
      "63     \t [-0.0742073  0.0088854]. \t  -1.15506286799576 \t -1.1511754129374758\n",
      "64     \t [-0.07422829  0.00888403]. \t  -1.1551049405466265 \t -1.1511754129374758\n",
      "65     \t [-0.07424856  0.00888152]. \t  -1.1551447657322633 \t -1.1511754129374758\n",
      "66     \t [-0.07426785  0.00888021]. \t  -1.1551834027903045 \t -1.1511754129374758\n",
      "67     \t [-0.07428681  0.00887813]. \t  -1.1552208370825099 \t -1.1511754129374758\n",
      "68     \t [-0.07430483  0.00887701]. \t  -1.155256997006066 \t -1.1511754129374758\n",
      "69     \t [-0.07432198  0.00887688]. \t  -1.1552920607592718 \t -1.1511754129374758\n",
      "70     \t [-0.07433862  0.00887545]. \t  -1.155325192790208 \t -1.1511754129374758\n",
      "71     \t [-0.07435493  0.00887356]. \t  -1.1553573473967624 \t -1.1511754129374758\n",
      "72     \t [-0.07437027  0.00887343]. \t  -1.1553887067361635 \t -1.1511754129374758\n",
      "73     \t [-0.07438558  0.00887188]. \t  -1.1554190330457095 \t -1.1511754129374758\n",
      "74     \t [-0.07439989  0.0088716 ]. \t  -1.1554481777465022 \t -1.1511754129374758\n",
      "75     \t [-0.07441427  0.00886991]. \t  -1.1554765350429381 \t -1.1511754129374758\n",
      "76     \t [-0.07442738  0.0088709 ]. \t  -1.1555040655601563 \t -1.1511754129374758\n",
      "77     \t [-0.0744406   0.00887017]. \t  -1.1555306695275929 \t -1.1511754129374758\n",
      "78     \t [-0.07445343  0.00886905]. \t  -1.1555562213989616 \t -1.1511754129374758\n",
      "79     \t [-0.07446567  0.00886844]. \t  -1.155580913120474 \t -1.1511754129374758\n",
      "80     \t [-0.07447736  0.00886844]. \t  -1.1556048864057857 \t -1.1511754129374758\n",
      "81     \t [-0.07448901  0.00886816]. \t  -1.1556285833409654 \t -1.1511754129374758\n",
      "82     \t [-0.07450019  0.0088679 ]. \t  -1.155651325158068 \t -1.1511754129374758\n",
      "83     \t [-0.07451107  0.00886748]. \t  -1.1556733558418326 \t -1.1511754129374758\n",
      "84     \t [-0.0745215   0.00886759]. \t  -1.1556947932764685 \t -1.1511754129374758\n",
      "85     \t [-0.07453198  0.00886683]. \t  -1.1557157862313048 \t -1.1511754129374758\n",
      "86     \t [-0.0745414   0.00886826]. \t  -1.1557360403384656 \t -1.1511754129374758\n",
      "87     \t [-0.07455113  0.00886691]. \t  -1.155755101891017 \t -1.1511754129374758\n",
      "88     \t [-0.07456028  0.00886727]. \t  -1.1557741012180012 \t -1.1511754129374758\n",
      "89     \t [-0.07456932  0.00886724]. \t  -1.1557926243838277 \t -1.1511754129374758\n",
      "90     \t [-0.07457794  0.00886781]. \t  -1.1558106660874021 \t -1.1511754129374758\n",
      "91     \t [-0.07458696  0.0088665 ]. \t  -1.1558283113885113 \t -1.1511754129374758\n",
      "92     \t [-0.07459498  0.00886696]. \t  -1.1558450536991354 \t -1.1511754129374758\n",
      "93     \t [-0.07460325  0.00886685]. \t  -1.1558619352726611 \t -1.1511754129374758\n",
      "94     \t [-0.07461092  0.00886703]. \t  -1.155877788942251 \t -1.1511754129374758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95     \t [-0.07461861  0.00886691]. \t  -1.1558934793733635 \t -1.1511754129374758\n",
      "96     \t [-0.07462584  0.00886788]. \t  -1.1559089472911555 \t -1.1511754129374758\n",
      "97     \t [-0.07463339  0.00886746]. \t  -1.155924153866844 \t -1.1511754129374758\n",
      "98     \t [-0.07464034  0.00886756]. \t  -1.1559384799706707 \t -1.1511754129374758\n",
      "99     \t [-0.07464757  0.00886666]. \t  -1.155952709245856 \t -1.1511754129374758\n",
      "100    \t [-0.07465401  0.00886779]. \t  -1.1559666535881494 \t -1.1511754129374758\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.25191278 -0.26933549]. \t  -11.63492174223243 \t -1.3013277264983028\n",
      "2      \t [0.68033338 0.10113341]. \t  -13.186329397775957 \t -1.3013277264983028\n",
      "3      \t [0.96343453 1.98895665]. \t  -112.5205107109817 \t -1.3013277264983028\n",
      "4      \t [ 0.27238606 -0.83307626]. \t  -82.84338424585411 \t -1.3013277264983028\n",
      "5      \t [0.69495202 1.33201364]. \t  -72.18254902035635 \t -1.3013277264983028\n",
      "6      \t [1.41835429 1.3727838 ]. \t  -41.00010479560696 \t -1.3013277264983028\n",
      "7      \t [1.06770591 1.01733853]. \t  -1.5090671171279442 \t -1.3013277264983028\n",
      "8      \t [1.95467954 2.048     ]. \t  -315.1835129725652 \t -1.3013277264983028\n",
      "9      \t [-0.50128322 -0.6192719 ]. \t  -78.04075974689782 \t -1.3013277264983028\n",
      "10     \t [ 0.56575684 -0.35347332]. \t  -45.55608315857834 \t -1.3013277264983028\n",
      "11     \t [-0.34195984  0.51249925]. \t  -17.44784208263534 \t -1.3013277264983028\n",
      "12     \t [-0.14427573  0.08066906]. \t  -1.6676119878522964 \t -1.3013277264983028\n",
      "13     \t [-0.0578739  -0.32187566]. \t  -11.696230488665723 \t -1.3013277264983028\n",
      "14     \t [-0.89734587 -2.048     ]. \t  -817.6918389951727 \t -1.3013277264983028\n",
      "15     \t [0.04979271 0.4848866 ]. \t  -24.17457251686364 \t -1.3013277264983028\n",
      "16     \t [1.09658663 1.28753843]. \t  \u001b[92m-0.7324446178922864\u001b[0m \t -0.7324446178922864\n",
      "17     \t [1.01875355 1.09262237]. \t  \u001b[92m-0.3002566013566517\u001b[0m \t -0.3002566013566517\n",
      "18     \t [0.84740859 0.63894583]. \t  -0.6498433205758481 \t -0.3002566013566517\n",
      "19     \t [1.05120689 1.12035356]. \t  \u001b[92m-0.02608511324707418\u001b[0m \t -0.02608511324707418\n",
      "20     \t [1.04054542 1.0988854 ]. \t  -0.02772821508080672 \t -0.02608511324707418\n",
      "21     \t [1.0401502  1.09769958]. \t  -0.026535410049173005 \t -0.02608511324707418\n",
      "22     \t [1.0401342  1.09747139]. \t  \u001b[92m-0.02592252631816139\u001b[0m \t -0.02592252631816139\n",
      "23     \t [0.87283752 0.75481224]. \t  \u001b[92m-0.021116740452460935\u001b[0m \t -0.021116740452460935\n",
      "24     \t [1.04916724 1.11610169]. \t  -0.025979024087812552 \t -0.021116740452460935\n",
      "25     \t [1.04725021 1.11216352]. \t  -0.026042659551752098 \t -0.021116740452460935\n",
      "26     \t [1.04608296 1.10974701]. \t  -0.026016929337084188 \t -0.021116740452460935\n",
      "27     \t [0.85852452 0.7278967 ]. \t  -0.028419907625828467 \t -0.021116740452460935\n",
      "28     \t [1.04730352 1.11236033]. \t  -0.02631125449364651 \t -0.021116740452460935\n",
      "29     \t [1.04658195 1.11085828]. \t  -0.026270861139588802 \t -0.021116740452460935\n",
      "30     \t [1.04608445 1.10982634]. \t  -0.026253224044239225 \t -0.021116740452460935\n",
      "31     \t [0.85901871 0.72878602]. \t  -0.028206181259568888 \t -0.021116740452460935\n",
      "32     \t [1.04694916 1.11166501]. \t  -0.026423272455482576 \t -0.021116740452460935\n",
      "33     \t [1.0465651 1.1108671]. \t  -0.02640643371254771 \t -0.021116740452460935\n",
      "34     \t [1.04627802 1.1102567 ]. \t  -0.02634990667802268 \t -0.021116740452460935\n",
      "35     \t [0.8590558  0.72883472]. \t  -0.02822314264959722 \t -0.021116740452460935\n",
      "36     \t [1.04683424 1.11145804]. \t  -0.026517346180771787 \t -0.021116740452460935\n",
      "37     \t [1.04660999 1.11098975]. \t  -0.026499998671102767 \t -0.021116740452460935\n",
      "38     \t [1.04640759 1.11055588]. \t  -0.026449229204831524 \t -0.021116740452460935\n",
      "39     \t [0.85912692 0.7289562 ]. \t  -0.028204418221086484 \t -0.021116740452460935\n",
      "40     \t [1.04681675 1.11143729]. \t  -0.026565179441646235 \t -0.021116740452460935\n",
      "41     \t [1.04664468 1.11107347]. \t  -0.026537856099857177 \t -0.021116740452460935\n",
      "42     \t [1.04651347 1.11080117]. \t  -0.02653298734279376 \t -0.021116740452460935\n",
      "43     \t [0.85920904 0.72909235]. \t  -0.02819036658737383 \t -0.021116740452460935\n",
      "44     \t [1.04682028 1.11144907]. \t  -0.026579219748751336 \t -0.021116740452460935\n",
      "45     \t [1.04670704 1.11122016]. \t  -0.02659414287714325 \t -0.021116740452460935\n",
      "46     \t [1.04658799 1.11097135]. \t  -0.026584292569731497 \t -0.021116740452460935\n",
      "47     \t [0.85926429 0.72918405]. \t  -0.028180751570891442 \t -0.021116740452460935\n",
      "48     \t [1.04684082 1.11150654]. \t  -0.026626369876560415 \t -0.021116740452460935\n",
      "49     \t [1.04673452 1.11129261]. \t  -0.02664335196772341 \t -0.021116740452460935\n",
      "50     \t [1.04665721 1.11112682]. \t  -0.026623788842289185 \t -0.021116740452460935\n",
      "51     \t [1.04657432 1.11095963]. \t  -0.026635813070118877 \t -0.021116740452460935\n",
      "52     \t [0.85942199 0.72945352]. \t  -0.028139254209168046 \t -0.021116740452460935\n",
      "53     \t [1.0467791  1.11138661]. \t  -0.02664963746262962 \t -0.021116740452460935\n",
      "54     \t [1.04669986 1.11122547]. \t  -0.02665710225059853 \t -0.021116740452460935\n",
      "55     \t [1.04663967 1.11108643]. \t  -0.026610716347429245 \t -0.021116740452460935\n",
      "56     \t [0.85937417 0.72936727]. \t  -0.02816014657109564 \t -0.021116740452460935\n",
      "57     \t [1.04680768 1.11145667]. \t  -0.02668430890315201 \t -0.021116740452460935\n",
      "58     \t [1.04676151 1.11135693]. \t  -0.026670384354365383 \t -0.021116740452460935\n",
      "59     \t [1.04671527 1.11126433]. \t  -0.02667915393062509 \t -0.021116740452460935\n",
      "60     \t [0.85940205 0.72940806]. \t  -0.028165359634284626 \t -0.021116740452460935\n",
      "61     \t [1.04683856 1.11152721]. \t  -0.026705617102268184 \t -0.021116740452460935\n",
      "62     \t [1.04679085 1.11141551]. \t  -0.026664178709876807 \t -0.021116740452460935\n",
      "63     \t [1.04674462 1.11132989]. \t  -0.02669482385231238 \t -0.021116740452460935\n",
      "64     \t [0.85940451 0.72941463]. \t  -0.028160355074556283 \t -0.021116740452460935\n",
      "65     \t [1.04685315 1.11156533]. \t  -0.026730709227910945 \t -0.021116740452460935\n",
      "66     \t [1.0468362  1.11152407]. \t  -0.026711072693851937 \t -0.021116740452460935\n",
      "67     \t [1.04679051 1.11142655]. \t  -0.02670094121100848 \t -0.021116740452460935\n",
      "68     \t [1.04674388 1.11133464]. \t  -0.026714460494315626 \t -0.021116740452460935\n",
      "69     \t [0.85947774 0.72953753]. \t  -0.02814521862203201 \t -0.021116740452460935\n",
      "70     \t [1.04682287 1.11151152]. \t  -0.02675793767586951 \t -0.021116740452460935\n",
      "71     \t [1.04680371 1.11146203]. \t  -0.02672676498361742 \t -0.021116740452460935\n",
      "72     \t [1.04677128 1.11139563]. \t  -0.02672842045766858 \t -0.021116740452460935\n",
      "73     \t [0.85948033 0.7295422 ]. \t  -0.028144098332924805 \t -0.021116740452460935\n",
      "74     \t [1.04684358 1.11154629]. \t  -0.02673297277246675 \t -0.021116740452460935\n",
      "75     \t [1.04681632 1.11148953]. \t  -0.02673138514032697 \t -0.021116740452460935\n",
      "76     \t [1.04680371 1.11146817]. \t  -0.02674596754495 \t -0.021116740452460935\n",
      "77     \t [0.8594858  0.72954827]. \t  -0.028148676618088328 \t -0.021116740452460935\n",
      "78     \t [1.04686273 1.1115895 ]. \t  -0.026744504915427722 \t -0.021116740452460935\n",
      "79     \t [1.04684772 1.11155915]. \t  -0.02674646177158794 \t -0.021116740452460935\n",
      "80     \t [1.04682445 1.11151293]. \t  -0.026752111895320254 \t -0.021116740452460935\n",
      "81     \t [1.04680087 1.11148142]. \t  -0.026805941664159477 \t -0.021116740452460935\n",
      "82     \t [0.8595499  0.72965884]. \t  -0.02812998099946071 \t -0.021116740452460935\n",
      "83     \t [1.04688332 1.11165458]. \t  -0.026815319721220116 \t -0.021116740452460935\n",
      "84     \t [1.04686525 1.11159962]. \t  -0.02675992850980352 \t -0.021116740452460935\n",
      "85     \t [1.04683222 1.11152803]. \t  -0.02674921375821305 \t -0.021116740452460935\n",
      "86     \t [0.85955132 0.72966581]. \t  -0.02812125963133047 \t -0.021116740452460935\n",
      "87     \t [1.04688655 1.11164816]. \t  -0.02677430120316897 \t -0.021116740452460935\n",
      "88     \t [1.04687266 1.11161855]. \t  -0.02677128607951598 \t -0.021116740452460935\n",
      "89     \t [1.04685628 1.11158617]. \t  -0.026775808677256374 \t -0.021116740452460935\n",
      "90     \t [1.04683704 1.11154327]. \t  -0.026765766969468703 \t -0.021116740452460935\n",
      "91     \t [0.85958164 0.72971402]. \t  -0.02811991482092492 \t -0.021116740452460935\n",
      "92     \t [1.04688569 1.11164904]. \t  -0.026782599062578533 \t -0.021116740452460935\n",
      "93     \t [1.04687882 1.11162611]. \t  -0.02675516782038955 \t -0.021116740452460935\n",
      "94     \t [1.04685751 1.11158458]. \t  -0.026762859690886577 \t -0.021116740452460935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95     \t [0.85958322 0.72971195]. \t  -0.028128266897085107 \t -0.021116740452460935\n",
      "96     \t [1.04690993 1.11170584]. \t  -0.026803847041948475 \t -0.021116740452460935\n",
      "97     \t [1.046878   1.11163531]. \t  -0.026789323594217387 \t -0.021116740452460935\n",
      "98     \t [1.04688767 1.1116488 ]. \t  -0.026769031831406977 \t -0.021116740452460935\n",
      "99     \t [1.04684619 1.11156539]. \t  -0.026775911135529937 \t -0.021116740452460935\n",
      "100    \t [0.85959979 0.72974831]. \t  -0.028109179529126962 \t -0.021116740452460935\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.59078358 -1.05985081]. \t  -201.02376610784617 \t -1.118465165857483\n",
      "2      \t [ 0.18930005 -0.42444684]. \t  -21.843126511204556 \t -1.118465165857483\n",
      "3      \t [-0.48321417  1.23197812]. \t  -101.89659119355446 \t -1.118465165857483\n",
      "4      \t [-0.02894714  0.40798065]. \t  -17.635251410148022 \t -1.118465165857483\n",
      "5      \t [-0.877176    0.41687181]. \t  -15.954063473940352 \t -1.118465165857483\n",
      "6      \t [-0.39090409  0.2712598 ]. \t  -3.337744265955508 \t -1.118465165857483\n",
      "7      \t [-2.048       0.83226182]. \t  -1139.6230648302358 \t -1.118465165857483\n",
      "8      \t [1.33618124 1.60762452]. \t  -3.272729346030333 \t -1.118465165857483\n",
      "9      \t [-0.85338319  0.03918421]. \t  -50.9179694819037 \t -1.118465165857483\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.118465165857483\n",
      "11     \t [1.12560421 0.973546  ]. \t  -8.626411975910516 \t -1.118465165857483\n",
      "12     \t [0.69037349 2.048     ]. \t  -247.0207768587215 \t -1.118465165857483\n",
      "13     \t [1.35015914 1.29865696]. \t  -27.608801047597026 \t -1.118465165857483\n",
      "14     \t [1.03236621 1.3601895 ]. \t  -8.668743138946356 \t -1.118465165857483\n",
      "15     \t [0.52365229 0.2015124 ]. \t  \u001b[92m-0.7554263131741444\u001b[0m \t -0.7554263131741444\n",
      "16     \t [1.24377626 1.77697159]. \t  -5.349067691627571 \t -0.7554263131741444\n",
      "17     \t [0.21799054 0.0958127 ]. \t  -0.8447584120528671 \t -0.7554263131741444\n",
      "18     \t [0.72731205 0.5944548 ]. \t  \u001b[92m-0.5030167210114544\u001b[0m \t -0.5030167210114544\n",
      "19     \t [0.87408846 0.47963675]. \t  -8.103842058011422 \t -0.5030167210114544\n",
      "20     \t [-0.72135965  0.66721958]. \t  -5.11986013631805 \t -0.5030167210114544\n",
      "21     \t [1.20282268 1.45806269]. \t  \u001b[92m-0.05386155221865788\u001b[0m \t -0.05386155221865788\n",
      "22     \t [-0.64590915  0.45805458]. \t  -2.8759377370023183 \t -0.05386155221865788\n",
      "23     \t [1.25102564 1.5825959 ]. \t  -0.09374662143699306 \t -0.05386155221865788\n",
      "24     \t [0.96281927 0.95243118]. \t  -0.06595033309149334 \t -0.05386155221865788\n",
      "25     \t [1.25152164 1.57415971]. \t  -0.06943055092113226 \t -0.05386155221865788\n",
      "26     \t [1.25175209 1.57304875]. \t  -0.06718040980297638 \t -0.05386155221865788\n",
      "27     \t [1.25186942 1.57258687]. \t  -0.06636481900817993 \t -0.05386155221865788\n",
      "28     \t [0.97709848 0.9752941 ]. \t  \u001b[92m-0.04284788848266186\u001b[0m \t -0.04284788848266186\n",
      "29     \t [1.25278882 1.57486759]. \t  -0.06680499085174973 \t -0.04284788848266186\n",
      "30     \t [1.25280089 1.5746303 ]. \t  -0.06652996637539185 \t -0.04284788848266186\n",
      "31     \t [1.25281301 1.57447017]. \t  -0.06634464297851055 \t -0.04284788848266186\n",
      "32     \t [0.98369354 0.98804329]. \t  \u001b[92m-0.041842380763814566\u001b[0m \t -0.041842380763814566\n",
      "33     \t [1.25336461 1.57592459]. \t  -0.06669538144300383 \t -0.041842380763814566\n",
      "34     \t [1.25334223 1.57576003]. \t  -0.06657670616205265 \t -0.041842380763814566\n",
      "35     \t [0.98439199 0.98941763]. \t  \u001b[92m-0.0418189706377938\u001b[0m \t -0.0418189706377938\n",
      "36     \t [1.25362703 1.57650168]. \t  -0.06674824313335284 \t -0.0418189706377938\n",
      "37     \t [1.25359852 1.57636284]. \t  -0.0666679407849714 \t -0.0418189706377938\n",
      "38     \t [0.98527688 0.99113547]. \t  \u001b[92m-0.04168982853775144\u001b[0m \t -0.04168982853775144\n",
      "39     \t [1.25377773 1.5768238 ]. \t  -0.06677015053104772 \t -0.04168982853775144\n",
      "40     \t [0.98512397 0.99085265]. \t  -0.04176962876607513 \t -0.04168982853775144\n",
      "41     \t [1.25387924 1.57707716]. \t  -0.06682053508144525 \t -0.04168982853775144\n",
      "42     \t [0.98528164 0.99115737]. \t  -0.04174070034348387 \t -0.04168982853775144\n",
      "43     \t [1.25395644 1.57726374]. \t  -0.0668529144348531 \t -0.04168982853775144\n",
      "44     \t [0.98550415 0.99158804]. \t  -0.04170220829327942 \t -0.04168982853775144\n",
      "45     \t [0.98506304 0.99072898]. \t  -0.04175668812210479 \t -0.04168982853775144\n",
      "46     \t [1.25405138 1.57751164]. \t  -0.06691065711331347 \t -0.04168982853775144\n",
      "47     \t [0.9854629  0.99152186]. \t  -0.04176504475221944 \t -0.04168982853775144\n",
      "48     \t [1.25406647 1.57753351]. \t  -0.0669028001336773 \t -0.04168982853775144\n",
      "49     \t [0.98580726 0.9921674 ]. \t  \u001b[92m-0.04161954173064981\u001b[0m \t -0.04161954173064981\n",
      "50     \t [0.98559539 0.99176437]. \t  -0.04168530136495092 \t -0.04161954173064981\n",
      "51     \t [0.98532978 0.9912485 ]. \t  -0.04172409189177662 \t -0.04161954173064981\n",
      "52     \t [1.25414659 1.57774375]. \t  -0.06695252956247402 \t -0.04161954173064981\n",
      "53     \t [0.98580355 0.99214949]. \t  \u001b[92m-0.0415765355397527\u001b[0m \t -0.0415765355397527\n",
      "54     \t [0.98560167 0.99178466]. \t  -0.04171730332737644 \t -0.0415765355397527\n",
      "55     \t [0.98559847 0.99176596]. \t  -0.04166690394565246 \t -0.0415765355397527\n",
      "56     \t [1.25418638 1.57784802]. \t  -0.0669770927240153 \t -0.0415765355397527\n",
      "57     \t [0.98592935 0.99243116]. \t  -0.04170992436065391 \t -0.0415765355397527\n",
      "58     \t [0.98581599 0.99219649]. \t  -0.04166767739324094 \t -0.0415765355397527\n",
      "59     \t [0.98574196 0.99205992]. \t  -0.041708045637658404 \t -0.0415765355397527\n",
      "60     \t [0.98560529 0.99178025]. \t  -0.04167018722746697 \t -0.0415765355397527\n",
      "61     \t [0.9856183  0.99180786]. \t  -0.04167783126461401 \t -0.0415765355397527\n",
      "62     \t [1.25422638 1.57794937]. \t  -0.06699841179537468 \t -0.0415765355397527\n",
      "63     \t [0.98598235 0.99251968]. \t  -0.041643244326010145 \t -0.0415765355397527\n",
      "64     \t [0.9858904  0.99234734]. \t  -0.041682411838161446 \t -0.0415765355397527\n",
      "65     \t [0.98585042 0.99225974]. \t  -0.04164783142922108 \t -0.0415765355397527\n",
      "66     \t [0.98577691 0.99212297]. \t  -0.041683195812928585 \t -0.0415765355397527\n",
      "67     \t [0.98578461 0.99214567]. \t  -0.0417135788379845 \t -0.0415765355397527\n",
      "68     \t [0.98570565 0.99198025]. \t  -0.04167615367355558 \t -0.0415765355397527\n",
      "69     \t [0.98567749 0.99192703]. \t  -0.041686246727810954 \t -0.0415765355397527\n",
      "70     \t [1.25426565 1.57803593]. \t  -0.06700676919956788 \t -0.0415765355397527\n",
      "71     \t [0.98607786 0.99271213]. \t  -0.041657364636647494 \t -0.0415765355397527\n",
      "72     \t [0.98597075 0.99251598]. \t  -0.041721738506739665 \t -0.0415765355397527\n",
      "73     \t [0.98597823 0.99251395]. \t  -0.041653132077508 \t -0.0415765355397527\n",
      "74     \t [0.98590797 0.99237616]. \t  -0.04165816563792509 \t -0.0415765355397527\n",
      "75     \t [0.98588835 0.99234254]. \t  -0.04167937733782246 \t -0.0415765355397527\n",
      "76     \t [0.98588818 0.99233791]. \t  -0.04166192819069639 \t -0.0415765355397527\n",
      "77     \t [0.98586232 0.99228989]. \t  -0.041674757781788134 \t -0.0415765355397527\n",
      "78     \t [0.98584734 0.9922624 ]. \t  -0.04168351190526758 \t -0.0415765355397527\n",
      "79     \t [0.98583429 0.99223245]. \t  -0.041666635144326135 \t -0.0415765355397527\n",
      "80     \t [0.98579848 0.99216425]. \t  -0.04167747350843166 \t -0.0415765355397527\n",
      "81     \t [0.98575106 0.99207928]. \t  -0.041713569109571656 \t -0.0415765355397527\n",
      "82     \t [0.98571715 0.99200777]. \t  -0.04169551321034839 \t -0.0415765355397527\n",
      "83     \t [0.98575422 0.99207778]. \t  -0.04168195580351458 \t -0.0415765355397527\n",
      "84     \t [0.98572244 0.99201918]. \t  -0.04169934820979996 \t -0.0415765355397527\n",
      "85     \t [1.25429872 1.57812861]. \t  -0.06703303928209155 \t -0.0415765355397527\n",
      "86     \t [0.98609619 0.99274167]. \t  -0.04162989526879386 \t -0.0415765355397527\n",
      "87     \t [0.98608166 0.99271407]. \t  -0.04163459728277745 \t -0.0415765355397527\n",
      "88     \t [0.98604404 0.9926371 ]. \t  -0.04162428910583985 \t -0.0415765355397527\n",
      "89     \t [0.98602733 0.99260622]. \t  -0.04163320327552075 \t -0.0415765355397527\n",
      "90     \t [0.98604019 0.99262998]. \t  -0.04162639360199762 \t -0.0415765355397527\n",
      "91     \t [0.9860088  0.99257425]. \t  -0.04165235165690167 \t -0.0415765355397527\n",
      "92     \t [0.98599003 0.99253747]. \t  -0.041653814070686135 \t -0.0415765355397527\n",
      "93     \t [0.98596895 0.99249631]. \t  -0.04165611812394654 \t -0.0415765355397527\n",
      "94     \t [0.985948   0.99244713]. \t  -0.04162468710185001 \t -0.0415765355397527\n",
      "95     \t [0.98593799 0.99242943]. \t  -0.04163321465734832 \t -0.0415765355397527\n",
      "96     \t [0.98594223 0.99244117]. \t  -0.04164686409183178 \t -0.0415765355397527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.98591478 0.99239705]. \t  -0.04168840911399313 \t -0.0415765355397527\n",
      "98     \t [0.98582463 0.9921746 ]. \t  \u001b[92m-0.04150908057667356\u001b[0m \t -0.04150908057667356\n",
      "99     \t [0.9859098  0.99238033]. \t  -0.04166042375944524 \t -0.04150908057667356\n",
      "100    \t [0.98591825 0.99240083]. \t  -0.04167580750394446 \t -0.04150908057667356\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.37151344 -1.61875516]. \t  -1225.0010186831207 \t -3.8154339181361143\n",
      "init   \t [ 1.00204741 -0.57138857]. \t  -248.216119608043 \t -3.8154339181361143\n",
      "init   \t [-0.57626281  0.44744041]. \t  -3.8154339181361143 \t -3.8154339181361143\n",
      "init   \t [-0.43507896 -0.37243859]. \t  -33.61376822003779 \t -3.8154339181361143\n",
      "init   \t [0.04056027 0.86076618]. \t  -74.72942162517022 \t -3.8154339181361143\n",
      "1      \t [1.44439392 0.415879  ]. \t  -279.2193656393819 \t -3.8154339181361143\n",
      "2      \t [-1.58942359  1.20251283]. \t  -181.9377156612223 \t -3.8154339181361143\n",
      "3      \t [ 0.16700236 -0.02921687]. \t  \u001b[92m-1.0200020669805454\u001b[0m \t -1.0200020669805454\n",
      "4      \t [-0.25152316  0.10802583]. \t  -1.766673254616738 \t -1.0200020669805454\n",
      "5      \t [ 0.06898497 -0.07587468]. \t  -1.5169668061020714 \t -1.0200020669805454\n",
      "6      \t [-0.58305554  0.23935346]. \t  -3.518107064062648 \t -1.0200020669805454\n",
      "7      \t [-2.048      -1.82709362]. \t  -3635.013235470355 \t -1.0200020669805454\n",
      "8      \t [ 0.37361061 -0.01024125]. \t  -2.637150944479983 \t -1.0200020669805454\n",
      "9      \t [ 0.15616983 -0.02877023]. \t  \u001b[92m-0.9946398789731798\u001b[0m \t -0.9946398789731798\n",
      "10     \t [ 0.16157545 -0.02454224]. \t  \u001b[92m-0.9594864722395542\u001b[0m \t -0.9594864722395542\n",
      "11     \t [ 0.16322288 -0.02173394]. \t  \u001b[92m-0.9342162389231127\u001b[0m \t -0.9342162389231127\n",
      "12     \t [ 0.16358281 -0.01940577]. \t  \u001b[92m-0.9127154111911798\u001b[0m \t -0.9127154111911798\n",
      "13     \t [ 0.16331663 -0.01725275]. \t  \u001b[92m-0.8929802239991481\u001b[0m \t -0.8929802239991481\n",
      "14     \t [ 0.16266373 -0.01514454]. \t  \u001b[92m-0.8742215561361297\u001b[0m \t -0.8742215561361297\n",
      "15     \t [ 0.16175116 -0.0130226 ]. \t  \u001b[92m-0.8562156566825639\u001b[0m \t -0.8562156566825639\n",
      "16     \t [ 0.16067875 -0.01087475]. \t  \u001b[92m-0.839093426572054\u001b[0m \t -0.839093426572054\n",
      "17     \t [ 0.15954987 -0.00872873]. \t  \u001b[92m-0.8232171285915835\u001b[0m \t -0.8232171285915835\n",
      "18     \t [ 0.15847659 -0.00663617]. \t  \u001b[92m-0.8089742469797965\u001b[0m \t -0.8089742469797965\n",
      "19     \t [ 0.15755356 -0.00465767]. \t  \u001b[92m-0.7966277363725569\u001b[0m \t -0.7966277363725569\n",
      "20     \t [ 0.15684224 -0.00284178]. \t  \u001b[92m-0.7862173316912929\u001b[0m \t -0.7862173316912929\n",
      "21     \t [ 0.15636361 -0.00121236]. \t  \u001b[92m-0.7775758509251809\u001b[0m \t -0.7775758509251809\n",
      "22     \t [0.15611466 0.00022794]. \t  \u001b[92m-0.770435025049714\u001b[0m \t -0.770435025049714\n",
      "23     \t [0.15605612 0.00149459]. \t  \u001b[92m-0.7644942880585128\u001b[0m \t -0.7644942880585128\n",
      "24     \t [0.15615332 0.00260653]. \t  \u001b[92m-0.7595024397620724\u001b[0m \t -0.7595024397620724\n",
      "25     \t [0.156376   0.00358761]. \t  \u001b[92m-0.7552398063020096\u001b[0m \t -0.7552398063020096\n",
      "26     \t [0.15669542 0.00445943]. \t  \u001b[92m-0.7515395801321514\u001b[0m \t -0.7515395801321514\n",
      "27     \t [0.15708729 0.00524079]. \t  \u001b[92m-0.7482761960158707\u001b[0m \t -0.7482761960158707\n",
      "28     \t [0.1575352  0.00594641]. \t  \u001b[92m-0.7453581214045278\u001b[0m \t -0.7453581214045278\n",
      "29     \t [0.15802334 0.00658935]. \t  \u001b[92m-0.7427145822920965\u001b[0m \t -0.7427145822920965\n",
      "30     \t [0.15854995 0.00718067]. \t  \u001b[92m-0.7402850764023401\u001b[0m \t -0.7402850764023401\n",
      "31     \t [0.15909874 0.00772723]. \t  \u001b[92m-0.7380387731599639\u001b[0m \t -0.7380387731599639\n",
      "32     \t [0.15967258 0.00823821]. \t  \u001b[92m-0.7359310115236717\u001b[0m \t -0.7359310115236717\n",
      "33     \t [0.1602636  0.00871877]. \t  \u001b[92m-0.7339405520896257\u001b[0m \t -0.7339405520896257\n",
      "34     \t [0.16086685 0.00917249]. \t  \u001b[92m-0.7320523383515912\u001b[0m \t -0.7320523383515912\n",
      "35     \t [0.16148346 0.00960445]. \t  \u001b[92m-0.73024416639103\u001b[0m \t -0.73024416639103\n",
      "36     \t [0.16210865 0.01001702]. \t  \u001b[92m-0.7285078109197101\u001b[0m \t -0.7285078109197101\n",
      "37     \t [0.16274736 0.01041304]. \t  \u001b[92m-0.7268282530998432\u001b[0m \t -0.7268282530998432\n",
      "38     \t [0.16339688 0.01079639]. \t  \u001b[92m-0.7251926183926244\u001b[0m \t -0.7251926183926244\n",
      "39     \t [0.16405427 0.011167  ]. \t  \u001b[92m-0.7236014533123345\u001b[0m \t -0.7236014533123345\n",
      "40     \t [0.16471901 0.01152675]. \t  \u001b[92m-0.7220478026831727\u001b[0m \t -0.7220478026831727\n",
      "41     \t [0.16538727 0.0118779 ]. \t  \u001b[92m-0.7205261220673965\u001b[0m \t -0.7205261220673965\n",
      "42     \t [0.16606784 0.01222149]. \t  \u001b[92m-0.7190267185079211\u001b[0m \t -0.7190267185079211\n",
      "43     \t [0.16675804 0.01255856]. \t  \u001b[92m-0.7175474509791411\u001b[0m \t -0.7175474509791411\n",
      "44     \t [0.16745078 0.01289011]. \t  \u001b[92m-0.7160893997109458\u001b[0m \t -0.7160893997109458\n",
      "45     \t [0.16815177 0.01321688]. \t  \u001b[92m-0.7146462402984122\u001b[0m \t -0.7146462402984122\n",
      "46     \t [0.16886018 0.01353851]. \t  \u001b[92m-0.7132192062472916\u001b[0m \t -0.7132192062472916\n",
      "47     \t [0.16957555 0.01385741]. \t  \u001b[92m-0.7118011685689656\u001b[0m \t -0.7118011685689656\n",
      "48     \t [0.1702962  0.01417283]. \t  \u001b[92m-0.7103952614731562\u001b[0m \t -0.7103952614731562\n",
      "49     \t [0.17102023 0.01448452]. \t  \u001b[92m-0.7090032442289781\u001b[0m \t -0.7090032442289781\n",
      "50     \t [0.17175278 0.01479403]. \t  \u001b[92m-0.7076171247078017\u001b[0m \t -0.7076171247078017\n",
      "51     \t [0.17248648 0.0151012 ]. \t  \u001b[92m-0.7062420013674604\u001b[0m \t -0.7062420013674604\n",
      "52     \t [0.17322594 0.01540662]. \t  \u001b[92m-0.7048731011326583\u001b[0m \t -0.7048731011326583\n",
      "53     \t [0.1739679  0.01570975]. \t  \u001b[92m-0.7035140576463688\u001b[0m \t -0.7035140576463688\n",
      "54     \t [0.17471069 0.01601028]. \t  \u001b[92m-0.7021667536591668\u001b[0m \t -0.7021667536591668\n",
      "55     \t [0.17545996 0.01631027]. \t  \u001b[92m-0.7008215195146607\u001b[0m \t -0.7008215195146607\n",
      "56     \t [0.17620998 0.01660735]. \t  \u001b[92m-0.699488880657762\u001b[0m \t -0.699488880657762\n",
      "57     \t [0.17695116 0.01690224]. \t  \u001b[92m-0.698172684716635\u001b[0m \t -0.698172684716635\n",
      "58     \t [0.17769976 0.01719458]. \t  \u001b[92m-0.6968636621680963\u001b[0m \t -0.6968636621680963\n",
      "59     \t [0.17843981 0.01748478]. \t  \u001b[92m-0.695570591505336\u001b[0m \t -0.695570591505336\n",
      "60     \t [0.17918253 0.01777373]. \t  \u001b[92m-0.6942837998485203\u001b[0m \t -0.6942837998485203\n",
      "61     \t [0.17991938 0.01805928]. \t  \u001b[92m-0.6930147239132172\u001b[0m \t -0.6930147239132172\n",
      "62     \t [0.18064622 0.01834198]. \t  \u001b[92m-0.6917641017214092\u001b[0m \t -0.6917641017214092\n",
      "63     \t [0.18137328 0.01862246]. \t  \u001b[92m-0.6905238632749761\u001b[0m \t -0.6905238632749761\n",
      "64     \t [0.18209105 0.01889944]. \t  \u001b[92m-0.6893032850482639\u001b[0m \t -0.6893032850482639\n",
      "65     \t [0.18280135 0.01917256]. \t  \u001b[92m-0.688102134507177\u001b[0m \t -0.688102134507177\n",
      "66     \t [0.18350127 0.01944285]. \t  \u001b[92m-0.6869190773086916\u001b[0m \t -0.6869190773086916\n",
      "67     \t [0.18419267 0.01970908]. \t  \u001b[92m-0.6857563590121742\u001b[0m \t -0.6857563590121742\n",
      "68     \t [0.18487458 0.01997338]. \t  \u001b[92m-0.6846083161518739\u001b[0m \t -0.6846083161518739\n",
      "69     \t [0.18554146 0.02023265]. \t  \u001b[92m-0.6834867871712833\u001b[0m \t -0.6834867871712833\n",
      "70     \t [0.18620014 0.0204854 ]. \t  \u001b[92m-0.6823918899933403\u001b[0m \t -0.6823918899933403\n",
      "71     \t [0.186847   0.02073593]. \t  \u001b[92m-0.681313333954123\u001b[0m \t -0.681313333954123\n",
      "72     \t [0.18747939 0.02098251]. \t  \u001b[92m-0.6802573310655641\u001b[0m \t -0.6802573310655641\n",
      "73     \t [0.18810059 0.02122405]. \t  \u001b[92m-0.6792249235472557\u001b[0m \t -0.6792249235472557\n",
      "74     \t [0.18870573 0.02146005]. \t  \u001b[92m-0.6782200873721411\u001b[0m \t -0.6782200873721411\n",
      "75     \t [0.18929931 0.02169238]. \t  \u001b[92m-0.6772347981149516\u001b[0m \t -0.6772347981149516\n",
      "76     \t [0.18988177 0.02191963]. \t  \u001b[92m-0.6762726670381256\u001b[0m \t -0.6762726670381256\n",
      "77     \t [0.19044767 0.02214214]. \t  \u001b[92m-0.6753354968064392\u001b[0m \t -0.6753354968064392\n",
      "78     \t [0.19099876 0.02235991]. \t  \u001b[92m-0.6744221998262905\u001b[0m \t -0.6744221998262905\n",
      "79     \t [0.19153707 0.02257141]. \t  \u001b[92m-0.6735357410653926\u001b[0m \t -0.6735357410653926\n",
      "80     \t [0.19206207 0.02278046]. \t  \u001b[92m-0.6726654998044228\u001b[0m \t -0.6726654998044228\n",
      "81     \t [0.19257228 0.02298311]. \t  \u001b[92m-0.6718232662278173\u001b[0m \t -0.6718232662278173\n",
      "82     \t [0.19307359 0.02318128]. \t  \u001b[92m-0.6710003310259082\u001b[0m \t -0.6710003310259082\n",
      "83     \t [0.1935563  0.02337575]. \t  \u001b[92m-0.6701994229245154\u001b[0m \t -0.6701994229245154\n",
      "84     \t [0.19402832 0.02356628]. \t  \u001b[92m-0.669416995812189\u001b[0m \t -0.669416995812189\n",
      "85     \t [0.19448541 0.02374977]. \t  \u001b[92m-0.668663773498942\u001b[0m \t -0.668663773498942\n",
      "86     \t [0.19493506 0.023929  ]. \t  \u001b[92m-0.667927968184528\u001b[0m \t -0.667927968184528\n",
      "87     \t [0.19536532 0.02410524]. \t  \u001b[92m-0.667212000353133\u001b[0m \t -0.667212000353133\n",
      "88     \t [0.19578879 0.02427678]. \t  \u001b[92m-0.6665140979023926\u001b[0m \t -0.6665140979023926\n",
      "89     \t [0.19619742 0.02444507]. \t  \u001b[92m-0.6658342162181986\u001b[0m \t -0.6658342162181986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.19659652 0.02460783]. \t  \u001b[92m-0.6651759549139744\u001b[0m \t -0.6651759549139744\n",
      "91     \t [0.196986   0.02476677]. \t  \u001b[92m-0.6645344251672279\u001b[0m \t -0.6645344251672279\n",
      "92     \t [0.19736143 0.02492109]. \t  \u001b[92m-0.6639140149339855\u001b[0m \t -0.6639140149339855\n",
      "93     \t [0.19772908 0.02507292]. \t  \u001b[92m-0.6633055345175085\u001b[0m \t -0.6633055345175085\n",
      "94     \t [0.19808441 0.02522034]. \t  \u001b[92m-0.6627165018028667\u001b[0m \t -0.6627165018028667\n",
      "95     \t [0.19843256 0.02536356]. \t  \u001b[92m-0.6621437651191033\u001b[0m \t -0.6621437651191033\n",
      "96     \t [0.19876774 0.02550447]. \t  \u001b[92m-0.6615847328195462\u001b[0m \t -0.6615847328195462\n",
      "97     \t [0.19909648 0.02564142]. \t  \u001b[92m-0.661040822296984\u001b[0m \t -0.661040822296984\n",
      "98     \t [0.19941554 0.02577496]. \t  \u001b[92m-0.6605119624007507\u001b[0m \t -0.6605119624007507\n",
      "99     \t [0.19972785 0.02590606]. \t  \u001b[92m-0.6599939622629024\u001b[0m \t -0.6599939622629024\n",
      "100    \t [0.20002923 0.02603254]. \t  \u001b[92m-0.6594949108123463\u001b[0m \t -0.6594949108123463\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-0.30078537  0.35230477]. \t  -8.547691189448665 \t -1.9278091788796494\n",
      "2      \t [0.05043917 0.46345842]. \t  -22.14586628652677 \t -1.9278091788796494\n",
      "3      \t [-0.51359727 -0.11382417]. \t  -16.549630551211276 \t -1.9278091788796494\n",
      "4      \t [-0.00361681  0.03849727]. \t  \u001b[92m-1.1553499761831185\u001b[0m \t -1.1553499761831185\n",
      "5      \t [ 0.5529392  -0.12702581]. \t  -18.928640266834112 \t -1.1553499761831185\n",
      "6      \t [-1.54630998  0.19804393]. \t  -487.42203318716025 \t -1.1553499761831185\n",
      "7      \t [-0.41010106 -1.93225191]. \t  -443.1710156038331 \t -1.1553499761831185\n",
      "8      \t [-1.28755844  2.048     ]. \t  -20.45800238062276 \t -1.1553499761831185\n",
      "9      \t [-0.2031184   0.04730781]. \t  -1.4511550110324414 \t -1.1553499761831185\n",
      "10     \t [0.23005464 0.08708434]. \t  \u001b[92m-0.7095009986861723\u001b[0m \t -0.7095009986861723\n",
      "11     \t [0.23704869 0.07651957]. \t  \u001b[92m-0.6234153852957512\u001b[0m \t -0.6234153852957512\n",
      "12     \t [0.24243722 0.07366883]. \t  \u001b[92m-0.5960815760474026\u001b[0m \t -0.5960815760474026\n",
      "13     \t [0.24656489 0.07274523]. \t  \u001b[92m-0.5819470639038155\u001b[0m \t -0.5819470639038155\n",
      "14     \t [0.24996076 0.07254302]. \t  \u001b[92m-0.5726845409626597\u001b[0m \t -0.5726845409626597\n",
      "15     \t [0.2529114  0.07268334]. \t  \u001b[92m-0.5657437509145014\u001b[0m \t -0.5657437509145014\n",
      "16     \t [0.25557983 0.07301373]. \t  \u001b[92m-0.5600791190220722\u001b[0m \t -0.5600791190220722\n",
      "17     \t [0.25806186 0.07346618]. \t  \u001b[92m-0.5551922430016\u001b[0m \t -0.5551922430016\n",
      "18     \t [0.26041996 0.07400541]. \t  \u001b[92m-0.5508063545526658\u001b[0m \t -0.5508063545526658\n",
      "19     \t [0.26269311 0.07461159]. \t  \u001b[92m-0.5467618367491087\u001b[0m \t -0.5467618367491087\n",
      "20     \t [0.26490989 0.07527107]. \t  \u001b[92m-0.5429521717106582\u001b[0m \t -0.5429521717106582\n",
      "21     \t [0.267088   0.07597997]. \t  \u001b[92m-0.5393166438648852\u001b[0m \t -0.5393166438648852\n",
      "22     \t [0.26923942 0.07672781]. \t  \u001b[92m-0.5358070414655847\u001b[0m \t -0.5358070414655847\n",
      "23     \t [0.27137097 0.07751229]. \t  \u001b[92m-0.5323980135100482\u001b[0m \t -0.5323980135100482\n",
      "24     \t [0.27348201 0.07832757]. \t  \u001b[92m-0.5290781283215054\u001b[0m \t -0.5290781283215054\n",
      "25     \t [0.27557171 0.07916805]. \t  \u001b[92m-0.5258385310627164\u001b[0m \t -0.5258385310627164\n",
      "26     \t [0.27763376 0.0800251 ]. \t  \u001b[92m-0.5226800471262348\u001b[0m \t -0.5226800471262348\n",
      "27     \t [0.27966296 0.08089441]. \t  \u001b[92m-0.5196053170816036\u001b[0m \t -0.5196053170816036\n",
      "28     \t [0.28164913 0.08176764]. \t  \u001b[92m-0.516624017018005\u001b[0m \t -0.516624017018005\n",
      "29     \t [0.28358415 0.08263795]. \t  \u001b[92m-0.5137436088460734\u001b[0m \t -0.5137436088460734\n",
      "30     \t [0.28546054 0.08349823]. \t  \u001b[92m-0.5109708618522061\u001b[0m \t -0.5109708618522061\n",
      "31     \t [0.28726658 0.08434147]. \t  \u001b[92m-0.5083199436082555\u001b[0m \t -0.5083199436082555\n",
      "32     \t [0.28899925 0.08516412]. \t  \u001b[92m-0.5057921908765902\u001b[0m \t -0.5057921908765902\n",
      "33     \t [0.29065165 0.0859588 ]. \t  \u001b[92m-0.5033942383102431\u001b[0m \t -0.5033942383102431\n",
      "34     \t [0.29222054 0.08672332]. \t  \u001b[92m-0.5011287828367073\u001b[0m \t -0.5011287828367073\n",
      "35     \t [0.29370263 0.08745326]. \t  \u001b[92m-0.49899806943941516\u001b[0m \t -0.49899806943941516\n",
      "36     \t [0.29510305 0.0881493 ]. \t  \u001b[92m-0.4969928056414606\u001b[0m \t -0.4969928056414606\n",
      "37     \t [0.29641644 0.08880893]. \t  \u001b[92m-0.4951193660939562\u001b[0m \t -0.4951193660939562\n",
      "38     \t [0.29765211 0.08943357]. \t  \u001b[92m-0.49336258306662806\u001b[0m \t -0.49336258306662806\n",
      "39     \t [0.29881073 0.09002424]. \t  \u001b[92m-0.49172061891064595\u001b[0m \t -0.49172061891064595\n",
      "40     \t [0.29989174 0.09058011]. \t  \u001b[92m-0.4901931894464547\u001b[0m \t -0.4901931894464547\n",
      "41     \t [0.30090759 0.09110261]. \t  \u001b[92m-0.48876125296337397\u001b[0m \t -0.48876125296337397\n",
      "42     \t [0.30185265 0.09159481]. \t  \u001b[92m-0.48743274592334657\u001b[0m \t -0.48743274592334657\n",
      "43     \t [0.30274183 0.09205833]. \t  \u001b[92m-0.48618541243249164\u001b[0m \t -0.48618541243249164\n",
      "44     \t [0.30357028 0.09249353]. \t  \u001b[92m-0.4850258262960724\u001b[0m \t -0.4850258262960724\n",
      "45     \t [0.30434908 0.09290383]. \t  \u001b[92m-0.4839377952637463\u001b[0m \t -0.4839377952637463\n",
      "46     \t [0.30507706 0.09328895]. \t  \u001b[92m-0.4829226038794941\u001b[0m \t -0.4829226038794941\n",
      "47     \t [0.30576488 0.09365442]. \t  \u001b[92m-0.4819650333023754\u001b[0m \t -0.4819650333023754\n",
      "48     \t [0.30640679 0.09399591]. \t  \u001b[92m-0.4810727649500416\u001b[0m \t -0.4810727649500416\n",
      "49     \t [0.307012   0.09432065]. \t  \u001b[92m-0.4802327774248724\u001b[0m \t -0.4802327774248724\n",
      "50     \t [0.30757569 0.09462356]. \t  \u001b[92m-0.47945147398800536\u001b[0m \t -0.47945147398800536\n",
      "51     \t [0.30811694 0.09491508]. \t  \u001b[92m-0.47870221537677254\u001b[0m \t -0.47870221537677254\n",
      "52     \t [0.30862623 0.0951902 ]. \t  \u001b[92m-0.4779980477731972\u001b[0m \t -0.4779980477731972\n",
      "53     \t [0.30910907 0.09544989]. \t  \u001b[92m-0.47733124284039663\u001b[0m \t -0.47733124284039663\n",
      "54     \t [0.30956221 0.09569928]. \t  \u001b[92m-0.476706013661638\u001b[0m \t -0.476706013661638\n",
      "55     \t [0.30999348 0.09593353]. \t  \u001b[92m-0.47611163943167445\u001b[0m \t -0.47611163943167445\n",
      "56     \t [0.31040246 0.09615694]. \t  \u001b[92m-0.4755484794589596\u001b[0m \t -0.4755484794589596\n",
      "57     \t [0.31079203 0.09637108]. \t  \u001b[92m-0.4750124947545559\u001b[0m \t -0.4750124947545559\n",
      "58     \t [0.31116209 0.09657088]. \t  \u001b[92m-0.47450395894744934\u001b[0m \t -0.47450395894744934\n",
      "59     \t [0.31151426 0.09676601]. \t  \u001b[92m-0.47402017740078334\u001b[0m \t -0.47402017740078334\n",
      "60     \t [0.31185263 0.0969518 ]. \t  \u001b[92m-0.47355582252708883\u001b[0m \t -0.47355582252708883\n",
      "61     \t [0.31217585 0.09712969]. \t  \u001b[92m-0.4731125617656189\u001b[0m \t -0.4731125617656189\n",
      "62     \t [0.31247997 0.09729826]. \t  \u001b[92m-0.47269573224402955\u001b[0m \t -0.47269573224402955\n",
      "63     \t [0.31277574 0.09746156]. \t  \u001b[92m-0.4722906606802789\u001b[0m \t -0.4722906606802789\n",
      "64     \t [0.3130612 0.0976207]. \t  \u001b[92m-0.4718998640895056\u001b[0m \t -0.4718998640895056\n",
      "65     \t [0.31333023 0.09776911]. \t  \u001b[92m-0.47153191299527103\u001b[0m \t -0.47153191299527103\n",
      "66     \t [0.31358926 0.09791279]. \t  \u001b[92m-0.47117780389255653\u001b[0m \t -0.47117780389255653\n",
      "67     \t [0.31384047 0.09805445]. \t  \u001b[92m-0.47083437827715136\u001b[0m \t -0.47083437827715136\n",
      "68     \t [0.31408009 0.09818613]. \t  \u001b[92m-0.4705072946699684\u001b[0m \t -0.4705072946699684\n",
      "69     \t [0.31431321 0.09831788]. \t  \u001b[92m-0.4701889229415905\u001b[0m \t -0.4701889229415905\n",
      "70     \t [0.31453913 0.09844219]. \t  \u001b[92m-0.46988087894400266\u001b[0m \t -0.46988087894400266\n",
      "71     \t [0.31475052 0.09856184]. \t  \u001b[92m-0.46959246353640954\u001b[0m \t -0.46959246353640954\n",
      "72     \t [0.3149592  0.09867827]. \t  \u001b[92m-0.4693080484824508\u001b[0m \t -0.4693080484824508\n",
      "73     \t [0.31515925 0.09879279]. \t  \u001b[92m-0.46903521472571846\u001b[0m \t -0.46903521472571846\n",
      "74     \t [0.31535423 0.09890275]. \t  \u001b[92m-0.46876959678276037\u001b[0m \t -0.46876959678276037\n",
      "75     \t [0.31554387 0.09900887]. \t  \u001b[92m-0.468511447588549\u001b[0m \t -0.468511447588549\n",
      "76     \t [0.31572689 0.09911194]. \t  \u001b[92m-0.4682623501326443\u001b[0m \t -0.4682623501326443\n",
      "77     \t [0.31590233 0.0992113 ]. \t  \u001b[92m-0.46802361434257156\u001b[0m \t -0.46802361434257156\n",
      "78     \t [0.31607458 0.09930811]. \t  \u001b[92m-0.46778938635883655\u001b[0m \t -0.46778938635883655\n",
      "79     \t [0.31624218 0.09940279]. \t  \u001b[92m-0.4675615165896677\u001b[0m \t -0.4675615165896677\n",
      "80     \t [0.31640169 0.09949387]. \t  \u001b[92m-0.467344614896658\u001b[0m \t -0.467344614896658\n",
      "81     \t [0.31655763 0.09958072]. \t  \u001b[92m-0.4671329177263374\u001b[0m \t -0.4671329177263374\n",
      "82     \t [0.31671129 0.09966983]. \t  \u001b[92m-0.4669239373030372\u001b[0m \t -0.4669239373030372\n",
      "83     \t [0.31685894 0.09975584]. \t  \u001b[92m-0.4667231533279295\u001b[0m \t -0.4667231533279295\n",
      "84     \t [0.31700248 0.09983645]. \t  \u001b[92m-0.46652839670699764\u001b[0m \t -0.46652839670699764\n",
      "85     \t [0.31714628 0.0999168 ]. \t  \u001b[92m-0.46633341809800516\u001b[0m \t -0.46633341809800516\n",
      "86     \t [0.31728129 0.09999414]. \t  \u001b[92m-0.46615017159009664\u001b[0m \t -0.46615017159009664\n",
      "87     \t [0.31741533 0.10007159]. \t  \u001b[92m-0.4659681932298317\u001b[0m \t -0.4659681932298317\n",
      "88     \t [0.31754453 0.10014605]. \t  \u001b[92m-0.4657928662897822\u001b[0m \t -0.4657928662897822\n",
      "89     \t [0.31766908 0.10021716]. \t  \u001b[92m-0.46562399313319\u001b[0m \t -0.46562399313319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.31779713 0.10029   ]. \t  \u001b[92m-0.4654504675146198\u001b[0m \t -0.4654504675146198\n",
      "91     \t [0.31791984 0.10035994]. \t  \u001b[92m-0.4652841969278457\u001b[0m \t -0.4652841969278457\n",
      "92     \t [0.31803502 0.10042595]. \t  \u001b[92m-0.465128121746599\u001b[0m \t -0.465128121746599\n",
      "93     \t [0.31815111 0.10049398]. \t  \u001b[92m-0.4649706326205552\u001b[0m \t -0.4649706326205552\n",
      "94     \t [0.31826391 0.10055924]. \t  \u001b[92m-0.4648177788526713\u001b[0m \t -0.4648177788526713\n",
      "95     \t [0.31837328 0.10062052]. \t  \u001b[92m-0.4646698913349861\u001b[0m \t -0.4646698913349861\n",
      "96     \t [0.31848214 0.10068305]. \t  \u001b[92m-0.4645225233326512\u001b[0m \t -0.4645225233326512\n",
      "97     \t [0.31859123 0.10074673]. \t  \u001b[92m-0.46437471244387146\u001b[0m \t -0.46437471244387146\n",
      "98     \t [0.31869356 0.10080545]. \t  \u001b[92m-0.46423624056938806\u001b[0m \t -0.46423624056938806\n",
      "99     \t [0.31879481 0.10086441]. \t  \u001b[92m-0.46409914796704477\u001b[0m \t -0.46409914796704477\n",
      "100    \t [0.31889337 0.10092296]. \t  \u001b[92m-0.4639655313981982\u001b[0m \t -0.4639655313981982\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [0.13711698 0.84640674]. \t  -69.2376827299732 \t -3.0269049669752817\n",
      "2      \t [-0.45729878 -0.33168592]. \t  -31.371059118194484 \t -3.0269049669752817\n",
      "3      \t [-0.13587026  0.18795312]. \t  -4.162968498372732 \t -3.0269049669752817\n",
      "4      \t [-0.73962211  1.72672949]. \t  -142.19280949424734 \t -3.0269049669752817\n",
      "5      \t [-0.48972698  0.89652512]. \t  -45.34380401885947 \t -3.0269049669752817\n",
      "6      \t [1.297758   1.82984107]. \t  \u001b[92m-2.2104960895250776\u001b[0m \t -2.2104960895250776\n",
      "7      \t [1.07273798 1.43914475]. \t  -8.32147598044044 \t -2.2104960895250776\n",
      "8      \t [0.95102965 2.048     ]. \t  -130.77136726786637 \t -2.2104960895250776\n",
      "9      \t [2.048      1.83499462]. \t  -557.7323769139438 \t -2.2104960895250776\n",
      "10     \t [-1.45392144  2.048     ]. \t  -6.455847454549707 \t -2.2104960895250776\n",
      "11     \t [-0.4705609   0.36947533]. \t  -4.354363779570168 \t -2.2104960895250776\n",
      "12     \t [0.29708607 0.30659509]. \t  -5.261103440677725 \t -2.2104960895250776\n",
      "13     \t [1.2234579  1.74712446]. \t  -6.31370186841672 \t -2.2104960895250776\n",
      "14     \t [1.35495217 2.048     ]. \t  -4.624827515849431 \t -2.2104960895250776\n",
      "15     \t [-0.1855033  -0.80431107]. \t  -71.75096803568783 \t -2.2104960895250776\n",
      "16     \t [0.90461147 0.87868799]. \t  \u001b[92m-0.3735053570955349\u001b[0m \t -0.3735053570955349\n",
      "17     \t [1.14737042 1.27504776]. \t  \u001b[92m-0.1932061558226327\u001b[0m \t -0.1932061558226327\n",
      "18     \t [0.83717696 0.92770283]. \t  -5.17204002432198 \t -0.1932061558226327\n",
      "19     \t [ 0.02579739 -0.08202341]. \t  -1.6328163814237096 \t -0.1932061558226327\n",
      "20     \t [1.03652233 1.0701675 ]. \t  \u001b[92m-0.0031071613114685504\u001b[0m \t -0.0031071613114685504\n",
      "21     \t [1.07669686 1.16896018]. \t  -0.015260472178226073 \t -0.0031071613114685504\n",
      "22     \t [1.0865142  1.18880878]. \t  -0.014366515914641775 \t -0.0031071613114685504\n",
      "23     \t [1.08558822 1.18586286]. \t  -0.012743876640252307 \t -0.0031071613114685504\n",
      "24     \t [1.08601387 1.18640925]. \t  -0.012274790952314228 \t -0.0031071613114685504\n",
      "25     \t [1.0860385  1.18622542]. \t  -0.011953191479643993 \t -0.0031071613114685504\n",
      "26     \t [1.08619281 1.18643297]. \t  -0.011809196285253235 \t -0.0031071613114685504\n",
      "27     \t [1.08626278 1.18646898]. \t  -0.011669064156987041 \t -0.0031071613114685504\n",
      "28     \t [1.08626908 1.18640424]. \t  -0.011568767381979246 \t -0.0031071613114685504\n",
      "29     \t [1.08627669 1.18635319]. \t  -0.011483725636523757 \t -0.0031071613114685504\n",
      "30     \t [1.0863485  1.18646486]. \t  -0.011439928965908102 \t -0.0031071613114685504\n",
      "31     \t [1.0864611  1.18666725]. \t  -0.011406213977004162 \t -0.0031071613114685504\n",
      "32     \t [1.08633424 1.1863596 ]. \t  -0.011344274897955472 \t -0.0031071613114685504\n",
      "33     \t [1.08637254 1.18641483]. \t  -0.011316059193918546 \t -0.0031071613114685504\n",
      "34     \t [1.08641803 1.18649025]. \t  -0.01129488964521134 \t -0.0031071613114685504\n",
      "35     \t [1.08645702 1.18656873]. \t  -0.011293904246383208 \t -0.0031071613114685504\n",
      "36     \t [1.08642704 1.18647191]. \t  -0.011249659656540793 \t -0.0031071613114685504\n",
      "37     \t [1.08644159 1.18649362]. \t  -0.011240013119261106 \t -0.0031071613114685504\n",
      "38     \t [1.08644084 1.1864735 ]. \t  -0.011217231067073875 \t -0.0031071613114685504\n",
      "39     \t [1.08644865 1.18648288]. \t  -0.011209290127192002 \t -0.0031071613114685504\n",
      "40     \t [1.08638221 1.18631644]. \t  -0.011170855772516216 \t -0.0031071613114685504\n",
      "41     \t [1.0864388  1.18644222]. \t  -0.011184075925610288 \t -0.0031071613114685504\n",
      "42     \t [1.08651046 1.18659261]. \t  -0.011189979336943379 \t -0.0031071613114685504\n",
      "43     \t [1.08647141 1.18649621]. \t  -0.011169178092618876 \t -0.0031071613114685504\n",
      "44     \t [1.08647714 1.18650413]. \t  -0.011164667101748777 \t -0.0031071613114685504\n",
      "45     \t [1.08642421 1.18637578]. \t  -0.011139340692152614 \t -0.0031071613114685504\n",
      "46     \t [1.08644608 1.18642976]. \t  -0.011150938365463308 \t -0.0031071613114685504\n",
      "47     \t [1.08651436 1.18656948]. \t  -0.011152273470164847 \t -0.0031071613114685504\n",
      "48     \t [1.08649411 1.18651759]. \t  -0.011139231282942764 \t -0.0031071613114685504\n",
      "49     \t [1.08648502 1.18649743]. \t  -0.01113717657856839 \t -0.0031071613114685504\n",
      "50     \t [1.08650173 1.18652452]. \t  -0.011128906613592827 \t -0.0031071613114685504\n",
      "51     \t [1.08652223 1.18656765]. \t  -0.01113075616056289 \t -0.0031071613114685504\n",
      "52     \t [1.08652563 1.18657369]. \t  -0.011129712750329989 \t -0.0031071613114685504\n",
      "53     \t [1.08648799 1.18647861]. \t  -0.011107164000694555 \t -0.0031071613114685504\n",
      "54     \t [1.08654395 1.18661244]. \t  -0.011131606264915259 \t -0.0031071613114685504\n",
      "55     \t [1.08645295 1.18640197]. \t  -0.011100512741575247 \t -0.0031071613114685504\n",
      "56     \t [1.08659549 1.18671438]. \t  -0.011128373436764902 \t -0.0031071613114685504\n",
      "57     \t [1.08650064 1.18650027]. \t  -0.01110233798360848 \t -0.0031071613114685504\n",
      "58     \t [1.08648224 1.18644891]. \t  -0.011085481802725502 \t -0.0031071613114685504\n",
      "59     \t [1.08653561 1.18657803]. \t  -0.011110521434006542 \t -0.0031071613114685504\n",
      "60     \t [1.08655558 1.18662966]. \t  -0.011123899092852753 \t -0.0031071613114685504\n",
      "61     \t [1.08648677 1.18645628]. \t  -0.011083293833276699 \t -0.0031071613114685504\n",
      "62     \t [1.08654942 1.18658514]. \t  -0.011085387323600584 \t -0.0031071613114685504\n",
      "63     \t [1.08643909 1.18635587]. \t  -0.011078882332642096 \t -0.0031071613114685504\n",
      "64     \t [1.08650879 1.18649819]. \t  -0.011079972781050945 \t -0.0031071613114685504\n",
      "65     \t [1.08650282 1.18648521]. \t  -0.011078937605208972 \t -0.0031071613114685504\n",
      "66     \t [1.08653406 1.18653929]. \t  -0.011067791320858531 \t -0.0031071613114685504\n",
      "67     \t [1.08652198 1.18652335]. \t  -0.011078066169182737 \t -0.0031071613114685504\n",
      "68     \t [1.08650122 1.1864795 ]. \t  -0.01107597298001757 \t -0.0031071613114685504\n",
      "69     \t [1.08650068 1.18646455]. \t  -0.01105938611214199 \t -0.0031071613114685504\n",
      "70     \t [1.08650283 1.18647713]. \t  -0.011069234369270249 \t -0.0031071613114685504\n",
      "71     \t [1.08649696 1.18646778]. \t  -0.011072293350482171 \t -0.0031071613114685504\n",
      "72     \t [1.08659499 1.18668311]. \t  -0.011092029120595658 \t -0.0031071613114685504\n",
      "73     \t [1.08653386 1.18653887]. \t  -0.011067781061910942 \t -0.0031071613114685504\n",
      "74     \t [1.08655159 1.18657564]. \t  -0.011068755617079502 \t -0.0031071613114685504\n",
      "75     \t [1.08654962 1.18657537]. \t  -0.011073200260217768 \t -0.0031071613114685504\n",
      "76     \t [1.08655672 1.18658745]. \t  -0.0110704379330972 \t -0.0031071613114685504\n",
      "77     \t [1.08656533 1.18661452]. \t  -0.011081943960835314 \t -0.0031071613114685504\n",
      "78     \t [1.08654175 1.18654369]. \t  -0.01105441348690406 \t -0.0031071613114685504\n",
      "79     \t [1.0865308  1.18652693]. \t  -0.011060916730751567 \t -0.0031071613114685504\n",
      "80     \t [1.08652156 1.18650524]. \t  -0.011057405394805936 \t -0.0031071613114685504\n",
      "81     \t [1.08651466 1.18649032]. \t  -0.011056286917579539 \t -0.0031071613114685504\n",
      "82     \t [1.08651039 1.18647981]. \t  -0.011054088342388024 \t -0.0031071613114685504\n",
      "83     \t [1.0864714  1.18638556]. \t  -0.011035969436558421 \t -0.0031071613114685504\n",
      "84     \t [1.08654121 1.18655066]. \t  -0.011064051702310212 \t -0.0031071613114685504\n",
      "85     \t [1.08655219 1.18657478]. \t  -0.011066272109251282 \t -0.0031071613114685504\n",
      "86     \t [1.08649872 1.18647995]. \t  -0.011082607041723914 \t -0.0031071613114685504\n",
      "87     \t [1.08651376 1.18647941]. \t  -0.01104544246430865 \t -0.0031071613114685504\n",
      "88     \t [1.0865175  1.18648929]. \t  -0.011048185478467216 \t -0.0031071613114685504\n",
      "89     \t [1.08658459 1.18664422]. \t  -0.011070720372352797 \t -0.0031071613114685504\n",
      "90     \t [1.08658982 1.18664312]. \t  -0.011056739204116465 \t -0.0031071613114685504\n",
      "91     \t [1.08659262 1.18667502]. \t  -0.011088087526568117 \t -0.0031071613114685504\n",
      "92     \t [1.0865267  1.18649466]. \t  -0.011032362379934742 \t -0.0031071613114685504\n",
      "93     \t [1.08654455 1.18654847]. \t  -0.01105335360038183 \t -0.0031071613114685504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     \t [1.0865611  1.18659059]. \t  -0.011063571115430224 \t -0.0031071613114685504\n",
      "95     \t [1.08658624 1.18663877]. \t  -0.0110602119471654 \t -0.0031071613114685504\n",
      "96     \t [1.08649508 1.18644094]. \t  -0.011044745301387918 \t -0.0031071613114685504\n",
      "97     \t [1.08654462 1.18655019]. \t  -0.011055227396233287 \t -0.0031071613114685504\n",
      "98     \t [1.08656702 1.18660331]. \t  -0.011064426334265898 \t -0.0031071613114685504\n",
      "99     \t [1.08654376 1.18653573]. \t  -0.011040069284652813 \t -0.0031071613114685504\n",
      "100    \t [1.08646989 1.18636723]. \t  -0.01101776943240162 \t -0.0031071613114685504\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.5478517   1.44840324]. \t  -134.24635095805354 \t -2.0077595729598063\n",
      "2      \t [0.01346655 0.10594777]. \t  -2.0919018602418165 \t -2.0077595729598063\n",
      "3      \t [1.89098985 0.86778937]. \t  -734.1490948269187 \t -2.0077595729598063\n",
      "4      \t [0.88745084 2.048     ]. \t  -158.8812997965131 \t -2.0077595729598063\n",
      "5      \t [-0.22338967  0.71101828]. \t  -45.20403113462293 \t -2.0077595729598063\n",
      "6      \t [-1.58370908  1.61847198]. \t  -85.82548521148476 \t -2.0077595729598063\n",
      "7      \t [ 0.2224586  -1.25307417]. \t  -170.27134688115515 \t -2.0077595729598063\n",
      "8      \t [-1.00936597  1.05728021]. \t  -4.185473008190416 \t -2.0077595729598063\n",
      "9      \t [-0.40867632  0.10611859]. \t  -2.355222425682723 \t -2.0077595729598063\n",
      "10     \t [1.02888345 1.4557935 ]. \t  -15.777010625034421 \t -2.0077595729598063\n",
      "11     \t [1.78723364 2.048     ]. \t  -131.99812110145982 \t -2.0077595729598063\n",
      "12     \t [0.65890205 0.32666695]. \t  \u001b[92m-1.2716493120172978\u001b[0m \t -1.2716493120172978\n",
      "13     \t [-1.2318921  2.048    ]. \t  -33.11819739613159 \t -1.2716493120172978\n",
      "14     \t [-0.65053556  0.58435254]. \t  -5.321393843234125 \t -1.2716493120172978\n",
      "15     \t [0.50350558 0.58405086]. \t  -11.17171257647206 \t -1.2716493120172978\n",
      "16     \t [-0.14242552 -0.4485349 ]. \t  -23.28434865311336 \t -1.2716493120172978\n",
      "17     \t [-1.10177235  0.92942695]. \t  -12.510069977274021 \t -1.2716493120172978\n",
      "18     \t [ 1.10606238 -0.83828773]. \t  -425.05615811432233 \t -1.2716493120172978\n",
      "19     \t [-0.34366205 -2.048     ]. \t  -471.00590911403026 \t -1.2716493120172978\n",
      "20     \t [1.37634383 2.048     ]. \t  -2.503316851878986 \t -1.2716493120172978\n",
      "21     \t [ 0.13401706 -0.07710312]. \t  -1.6536370755949443 \t -1.2716493120172978\n",
      "22     \t [1.30616367 1.7983152 ]. \t  \u001b[92m-0.9447731518673398\u001b[0m \t -0.9447731518673398\n",
      "23     \t [1.19740184 1.42440056]. \t  \u001b[92m-0.047748301438289335\u001b[0m \t -0.047748301438289335\n",
      "24     \t [-0.13850542 -0.05046618]. \t  -1.7813059381761553 \t -0.047748301438289335\n",
      "25     \t [0.44495352 0.17160951]. \t  -0.3776360491144407 \t -0.047748301438289335\n",
      "26     \t [1.25286337 1.5881269 ]. \t  -0.09801811038913599 \t -0.047748301438289335\n",
      "27     \t [1.25955332 1.60117729]. \t  -0.08898492621482879 \t -0.047748301438289335\n",
      "28     \t [1.26245052 1.60733275]. \t  -0.08724443332208517 \t -0.047748301438289335\n",
      "29     \t [1.26399658 1.61067161]. \t  -0.08655329383082314 \t -0.047748301438289335\n",
      "30     \t [1.26492142 1.61267886]. \t  -0.08619233872773814 \t -0.047748301438289335\n",
      "31     \t [1.26551462 1.61396785]. \t  -0.08597484928977195 \t -0.047748301438289335\n",
      "32     \t [1.26592977 1.61486325]. \t  -0.08581095038234478 \t -0.047748301438289335\n",
      "33     \t [1.26619663 1.61544098]. \t  -0.0857131590577045 \t -0.047748301438289335\n",
      "34     \t [1.26641494 1.61591134]. \t  -0.08562893325087499 \t -0.047748301438289335\n",
      "35     \t [1.26657938 1.61625852]. \t  -0.08554917135464925 \t -0.047748301438289335\n",
      "36     \t [1.26668639 1.61649295]. \t  -0.08551815715276574 \t -0.047748301438289335\n",
      "37     \t [1.26679343 1.61671616]. \t  -0.08546033128267952 \t -0.047748301438289335\n",
      "38     \t [1.26686045 1.61685886]. \t  -0.08543141196704591 \t -0.047748301438289335\n",
      "39     \t [1.26693962 1.61702042]. \t  -0.08538071328242006 \t -0.047748301438289335\n",
      "40     \t [1.26697652 1.61710314]. \t  -0.08537481005790601 \t -0.047748301438289335\n",
      "41     \t [1.26701861 1.61719935]. \t  -0.08537246696095573 \t -0.047748301438289335\n",
      "42     \t [1.26707495 1.61730521]. \t  -0.08531511456615569 \t -0.047748301438289335\n",
      "43     \t [1.26708733 1.61732907]. \t  -0.08530395881488115 \t -0.047748301438289335\n",
      "44     \t [1.267114   1.61738294]. \t  -0.0852858019274836 \t -0.047748301438289335\n",
      "45     \t [1.26713853 1.61743241]. \t  -0.08526896311683954 \t -0.047748301438289335\n",
      "46     \t [1.26716633 1.61748755]. \t  -0.0852477338467059 \t -0.047748301438289335\n",
      "47     \t [1.26717634 1.61751089]. \t  -0.08524828541353659 \t -0.047748301438289335\n",
      "48     \t [1.26719153 1.61753943]. \t  -0.0852329874201269 \t -0.047748301438289335\n",
      "49     \t [1.26720621 1.61756342]. \t  -0.0852097230544574 \t -0.047748301438289335\n",
      "50     \t [1.26721701 1.61758766]. \t  -0.08520816259934305 \t -0.047748301438289335\n",
      "51     \t [1.26723147 1.61762138]. \t  -0.08520897492808027 \t -0.047748301438289335\n",
      "52     \t [1.26724046 1.61763495]. \t  -0.08519213454473276 \t -0.047748301438289335\n",
      "53     \t [1.26725237 1.61766205]. \t  -0.08519129883696641 \t -0.047748301438289335\n",
      "54     \t [1.2672598  1.61766932]. \t  -0.08516812517969027 \t -0.047748301438289335\n",
      "55     \t [1.2672361  1.61762414]. \t  -0.0851903952303542 \t -0.047748301438289335\n",
      "56     \t [1.26726937 1.61769033]. \t  -0.08516567635050581 \t -0.047748301438289335\n",
      "57     \t [1.26727251 1.61769869]. \t  -0.08516824906200238 \t -0.047748301438289335\n",
      "58     \t [1.26724432 1.61763551]. \t  -0.08517257257040792 \t -0.047748301438289335\n",
      "59     \t [1.26730217 1.61776249]. \t  -0.0851574707949881 \t -0.047748301438289335\n",
      "60     \t [1.26730497 1.61777135]. \t  -0.08516307763113584 \t -0.047748301438289335\n",
      "61     \t [1.26730167 1.61775109]. \t  -0.08513350344867127 \t -0.047748301438289335\n",
      "62     \t [1.26728311 1.61771016]. \t  -0.08513789212164577 \t -0.047748301438289335\n",
      "63     \t [1.26731104 1.61776865]. \t  -0.08512402933763587 \t -0.047748301438289335\n",
      "64     \t [1.26730019 1.61775113]. \t  -0.08514156404481416 \t -0.047748301438289335\n",
      "65     \t [1.26732524 1.61780033]. \t  -0.0851215379674533 \t -0.047748301438289335\n",
      "66     \t [1.2673093 1.6177664]. \t  -0.08512813094426008 \t -0.047748301438289335\n",
      "67     \t [1.26731127 1.61776737]. \t  -0.08511977948844421 \t -0.047748301438289335\n",
      "68     \t [1.26731824 1.61777333]. \t  -0.08509617210870463 \t -0.047748301438289335\n",
      "69     \t [1.26731786 1.61777924]. \t  -0.08511204042967455 \t -0.047748301438289335\n",
      "70     \t [1.26731746 1.61777746]. \t  -0.08511001597933447 \t -0.047748301438289335\n",
      "71     \t [1.26732887 1.6178037 ]. \t  -0.0851098767643054 \t -0.047748301438289335\n",
      "72     \t [1.26734621 1.61784187]. \t  -0.08510558631173859 \t -0.047748301438289335\n",
      "73     \t [1.26730822 1.61775484]. \t  -0.08510695139667421 \t -0.047748301438289335\n",
      "74     \t [1.26733064 1.61780697]. \t  -0.08510797848654296 \t -0.047748301438289335\n",
      "75     \t [1.26732996 1.61780243]. \t  -0.08510101535138931 \t -0.047748301438289335\n",
      "76     \t [1.26733327 1.61780624]. \t  -0.08509206582630513 \t -0.047748301438289335\n",
      "77     \t [1.26733078 1.61780289]. \t  -0.08509765004448971 \t -0.047748301438289335\n",
      "78     \t [1.2673398  1.61782652]. \t  -0.08510427961300382 \t -0.047748301438289335\n",
      "79     \t [1.26732875 1.61779124]. \t  -0.08508141510037272 \t -0.047748301438289335\n",
      "80     \t [1.26733057 1.61779975]. \t  -0.08509148723961711 \t -0.047748301438289335\n",
      "81     \t [1.26734332 1.61782794]. \t  -0.08508867648900091 \t -0.047748301438289335\n",
      "82     \t [1.2673327  1.61780172]. \t  -0.08508461702259515 \t -0.047748301438289335\n",
      "83     \t [1.2673377  1.61781409]. \t  -0.08508654801153663 \t -0.047748301438289335\n",
      "84     \t [1.26735032 1.61784558]. \t  -0.08509214472190635 \t -0.047748301438289335\n",
      "85     \t [1.26733941 1.61781486]. \t  -0.08507919621071537 \t -0.047748301438289335\n",
      "86     \t [1.26734717 1.61782794]. \t  -0.08506792581649114 \t -0.047748301438289335\n",
      "87     \t [1.26737239 1.61788949]. \t  -0.08507586231997283 \t -0.047748301438289335\n",
      "88     \t [1.26736049 1.61786056]. \t  -0.08507241605954016 \t -0.047748301438289335\n",
      "89     \t [1.26734852 1.61783666]. \t  -0.08508104338020425 \t -0.047748301438289335\n",
      "90     \t [1.26735519 1.61784908]. \t  -0.08507413831423541 \t -0.047748301438289335\n",
      "91     \t [1.26734676 1.61782921]. \t  -0.08507310413310651 \t -0.047748301438289335\n",
      "92     \t [1.26733977 1.61780999]. \t  -0.08506589855525992 \t -0.047748301438289335\n",
      "93     \t [1.26734367 1.6178192 ]. \t  -0.08506639917992086 \t -0.047748301438289335\n",
      "94     \t [1.26735072 1.61783514]. \t  -0.08506565598948324 \t -0.047748301438289335\n",
      "95     \t [1.26734015 1.61781095]. \t  -0.08506604983627625 \t -0.047748301438289335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.2673476  1.61782959]. \t  -0.08506950652994964 \t -0.047748301438289335\n",
      "97     \t [1.26735225 1.61783643]. \t  -0.08506044640399246 \t -0.047748301438289335\n",
      "98     \t [1.26735224 1.61783536]. \t  -0.08505798747893935 \t -0.047748301438289335\n",
      "99     \t [1.26734912 1.61782782]. \t  -0.08505719415761324 \t -0.047748301438289335\n",
      "100    \t [1.26736299 1.61785895]. \t  -0.08505518719432523 \t -0.047748301438289335\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.65776375 0.51003966]. \t  \u001b[92m-0.715992963237076\u001b[0m \t -0.715992963237076\n",
      "2      \t [ 0.40369109 -0.12235146]. \t  -8.496218147561772 \t -0.715992963237076\n",
      "3      \t [-1.47129209  1.02291474]. \t  -136.47473463333077 \t -0.715992963237076\n",
      "4      \t [1.56388942 0.92474112]. \t  -231.6648139129838 \t -0.715992963237076\n",
      "5      \t [-0.609178    0.82731706]. \t  -23.403052401809752 \t -0.715992963237076\n",
      "6      \t [0.84483259 0.15571185]. \t  -31.163853307051276 \t -0.715992963237076\n",
      "7      \t [0.5260928  1.78187917]. \t  -226.7588572237721 \t -0.715992963237076\n",
      "8      \t [0.10954991 0.6804971 ]. \t  -45.481580582647666 \t -0.715992963237076\n",
      "9      \t [-1.01668905  0.49612151]. \t  -32.96143401986912 \t -0.715992963237076\n",
      "10     \t [0.78316532 0.87688813]. \t  -6.9923617529865005 \t -0.715992963237076\n",
      "11     \t [-0.65911161  0.48094525]. \t  -2.969035694527746 \t -0.715992963237076\n",
      "12     \t [-1.26344112  2.048     ]. \t  -25.527949059644857 \t -0.715992963237076\n",
      "13     \t [0.68926626 0.64628442]. \t  -3.0273775108585674 \t -0.715992963237076\n",
      "14     \t [ 0.58407295 -0.03876145]. \t  -14.605598762301112 \t -0.715992963237076\n",
      "15     \t [-0.19908794  0.14208979]. \t  -2.4874896460832088 \t -0.715992963237076\n",
      "16     \t [1.35145702 1.92316004]. \t  -1.0590744302161377 \t -0.715992963237076\n",
      "17     \t [1.25137354 1.72575718]. \t  -2.6174778258074367 \t -0.715992963237076\n",
      "18     \t [0.6509621  0.48342587]. \t  \u001b[92m-0.4779287288888122\u001b[0m \t -0.4779287288888122\n",
      "19     \t [0.57718182 0.3855681 ]. \t  \u001b[92m-0.45365778291361464\u001b[0m \t -0.45365778291361464\n",
      "20     \t [0.57039816 0.37806599]. \t  -0.4624124303886135 \t -0.45365778291361464\n",
      "21     \t [0.56729991 0.37466998]. \t  -0.4664443008209189 \t -0.45365778291361464\n",
      "22     \t [0.56588776 0.37312217]. \t  -0.4682226262382573 \t -0.45365778291361464\n",
      "23     \t [0.56508844 0.37225028]. \t  -0.46925717483124463 \t -0.45365778291361464\n",
      "24     \t [0.56457326 0.37168688]. \t  -0.46990224315466717 \t -0.45365778291361464\n",
      "25     \t [0.56421639 0.37129623]. \t  -0.4703421825769607 \t -0.45365778291361464\n",
      "26     \t [0.56394644 0.37099717]. \t  -0.4706356464459441 \t -0.45365778291361464\n",
      "27     \t [0.56374643 0.37078221]. \t  -0.47092231515447536 \t -0.45365778291361464\n",
      "28     \t [0.56359944 0.37062124]. \t  -0.47110073495875526 \t -0.45365778291361464\n",
      "29     \t [0.56347022 0.37047844]. \t  -0.4712436002467596 \t -0.45365778291361464\n",
      "30     \t [0.56336341 0.37036069]. \t  -0.47136453554341473 \t -0.45365778291361464\n",
      "31     \t [0.56326856 0.37026027]. \t  -0.47151565379855587 \t -0.45365778291361464\n",
      "32     \t [0.56320246 0.37018523]. \t  -0.4715671968869478 \t -0.45365778291361464\n",
      "33     \t [0.56313353 0.37010369]. \t  -0.4715860699840526 \t -0.45365778291361464\n",
      "34     \t [0.56308599 0.37006061]. \t  -0.4717385925667332 \t -0.45365778291361464\n",
      "35     \t [0.56304085 0.37001118]. \t  -0.4717928094572725 \t -0.45365778291361464\n",
      "36     \t [0.56299973 0.3699643 ]. \t  -0.47182274110816447 \t -0.45365778291361464\n",
      "37     \t [0.56295969 0.36992215]. \t  -0.4718886233689302 \t -0.45365778291361464\n",
      "38     \t [0.56292964 0.36988727]. \t  -0.47190388765724267 \t -0.45365778291361464\n",
      "39     \t [0.56289746 0.36985305]. \t  -0.4719533359898857 \t -0.45365778291361464\n",
      "40     \t [0.56287239 0.36982702]. \t  -0.47199856375346916 \t -0.45365778291361464\n",
      "41     \t [0.56284988 0.36979956]. \t  -0.4719957122524423 \t -0.45365778291361464\n",
      "42     \t [0.56282884 0.36978146]. \t  -0.4720731975567103 \t -0.45365778291361464\n",
      "43     \t [0.56280568 0.36975103]. \t  -0.4720473098981883 \t -0.45365778291361464\n",
      "44     \t [0.56278627 0.36973169]. \t  -0.47209089473157906 \t -0.45365778291361464\n",
      "45     \t [0.56276551 0.36970701]. \t  -0.4720950895075616 \t -0.45365778291361464\n",
      "46     \t [0.56275036 0.36968807]. \t  -0.47208834395291177 \t -0.45365778291361464\n",
      "47     \t [0.56273837 0.36968013]. \t  -0.47215769738099 \t -0.45365778291361464\n",
      "48     \t [0.562727   0.36966449]. \t  -0.47213750668866306 \t -0.45365778291361464\n",
      "49     \t [0.5627143  0.36965168]. \t  -0.47216440947278615 \t -0.45365778291361464\n",
      "50     \t [0.56270044 0.36963717]. \t  -0.47218801795679866 \t -0.45365778291361464\n",
      "51     \t [0.56268961 0.36962977]. \t  -0.47224821561519237 \t -0.45365778291361464\n",
      "52     \t [0.56268542 0.36962173]. \t  -0.4722166880882362 \t -0.45365778291361464\n",
      "53     \t [0.56265486 0.36958353]. \t  -0.47220307338973255 \t -0.45365778291361464\n",
      "54     \t [0.56265801 0.36958859]. \t  -0.4722163206342366 \t -0.45365778291361464\n",
      "55     \t [0.56265    0.36957789]. \t  -0.47220546689790943 \t -0.45365778291361464\n",
      "56     \t [0.56264547 0.36957502]. \t  -0.4722330410797153 \t -0.45365778291361464\n",
      "57     \t [0.56264633 0.369577  ]. \t  -0.47224293576477333 \t -0.45365778291361464\n",
      "58     \t [0.56263751 0.3695681 ]. \t  -0.47226159924250866 \t -0.45365778291361464\n",
      "59     \t [0.56262335 0.36955137]. \t  -0.47226548113855926 \t -0.45365778291361464\n",
      "60     \t [0.5626222  0.36955077]. \t  -0.47227392790159406 \t -0.45365778291361464\n",
      "61     \t [0.56260755 0.36953449]. \t  -0.47228882461940297 \t -0.45365778291361464\n",
      "62     \t [0.56260858 0.36953267]. \t  -0.47225630549997577 \t -0.45365778291361464\n",
      "63     \t [0.56260307 0.36952984]. \t  -0.4722969027340075 \t -0.45365778291361464\n",
      "64     \t [0.56260038 0.36953021]. \t  -0.4723352439222301 \t -0.45365778291361464\n",
      "65     \t [0.56259318 0.36951587]. \t  -0.47227537415306575 \t -0.45365778291361464\n",
      "66     \t [0.56258796 0.36951088]. \t  -0.4722893363683034 \t -0.45365778291361464\n",
      "67     \t [0.56258579 0.36951067]. \t  -0.4723149117830809 \t -0.45365778291361464\n",
      "68     \t [0.56258839 0.36950402]. \t  -0.4722111740952943 \t -0.45365778291361464\n",
      "69     \t [0.56257607 0.36950144]. \t  -0.4723415397229398 \t -0.45365778291361464\n",
      "70     \t [0.5625851  0.36950733]. \t  -0.47228840048851806 \t -0.45365778291361464\n",
      "71     \t [0.56256198 0.36948414]. \t  -0.4723385521644863 \t -0.45365778291361464\n",
      "72     \t [0.56255855 0.36948539]. \t  -0.4723956445516568 \t -0.45365778291361464\n",
      "73     \t [0.56256437 0.36948845]. \t  -0.47235363971321304 \t -0.45365778291361464\n",
      "74     \t [0.56256658 0.36949328]. \t  -0.4723765254350206 \t -0.45365778291361464\n",
      "75     \t [0.56254613 0.36946806]. \t  -0.47237104378086814 \t -0.45365778291361464\n",
      "76     \t [0.56254838 0.369474  ]. \t  -0.4724052196331412 \t -0.45365778291361464\n",
      "77     \t [0.56255373 0.36947315]. \t  -0.47232763585884585 \t -0.45365778291361464\n",
      "78     \t [0.56255542 0.36947593]. \t  -0.47233543911457426 \t -0.45365778291361464\n",
      "79     \t [0.56255268 0.36947032]. \t  -0.472311029790743 \t -0.45365778291361464\n",
      "80     \t [0.56253224 0.3694604 ]. \t  -0.4724675002561012 \t -0.45365778291361464\n",
      "81     \t [0.56253693 0.36945693]. \t  -0.47237077437735 \t -0.45365778291361464\n",
      "82     \t [0.56253445 0.36945387]. \t  -0.4723700179486584 \t -0.45365778291361464\n",
      "83     \t [0.56253534 0.36945403]. \t  -0.4723603861174167 \t -0.45365778291361464\n",
      "84     \t [0.56253117 0.3694494 ]. \t  -0.47236460608188324 \t -0.45365778291361464\n",
      "85     \t [0.56252593 0.36944111]. \t  -0.4723439146583062 \t -0.45365778291361464\n",
      "86     \t [0.56252607 0.36944325]. \t  -0.47236477992313025 \t -0.45365778291361464\n",
      "87     \t [0.56252635 0.36944082]. \t  -0.4723354239124563 \t -0.45365778291361464\n",
      "88     \t [0.56251768 0.36943433]. \t  -0.47237752330554356 \t -0.45365778291361464\n",
      "89     \t [0.56251009 0.36942388]. \t  -0.47236398082186326 \t -0.45365778291361464\n",
      "90     \t [0.56251521 0.36943244]. \t  -0.4723891114828637 \t -0.45365778291361464\n",
      "91     \t [0.56252254 0.36943931]. \t  -0.4723682128179603 \t -0.45365778291361464\n",
      "92     \t [0.56250763 0.36942615]. \t  -0.472419540073512 \t -0.45365778291361464\n",
      "93     \t [0.56251798 0.3694346 ]. \t  -0.47237660488430233 \t -0.45365778291361464\n",
      "94     \t [0.56250965 0.36942481]. \t  -0.47237944116081143 \t -0.45365778291361464\n",
      "95     \t [0.56251022 0.36942501]. \t  -0.47237436870438543 \t -0.45365778291361464\n",
      "96     \t [0.56250064 0.36941895]. \t  -0.47243259623475864 \t -0.45365778291361464\n",
      "97     \t [0.56250788 0.36942584]. \t  -0.47241314056131894 \t -0.45365778291361464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.56251033 0.36942356]. \t  -0.47235753383520923 \t -0.45365778291361464\n",
      "99     \t [0.56249108 0.36940898]. \t  -0.4724494211851539 \t -0.45365778291361464\n",
      "100    \t [0.56249191 0.36941376]. \t  -0.4724893468792395 \t -0.45365778291361464\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [ 0.40971713 -1.59139579]. \t  -309.84938445113715 \t -29.9831488845538\n",
      "2      \t [-0.15291437  0.77817227]. \t  -58.29992559216384 \t -29.9831488845538\n",
      "3      \t [ 0.70560349 -0.04880809]. \t  \u001b[92m-29.973050984662514\u001b[0m \t -29.973050984662514\n",
      "4      \t [ 0.70558508 -0.04880178]. \t  \u001b[92m-29.969529795305224\u001b[0m \t -29.969529795305224\n",
      "5      \t [ 0.70557279 -0.04879757]. \t  \u001b[92m-29.96718090614811\u001b[0m \t -29.96718090614811\n",
      "6      \t [ 0.70556354 -0.0487944 ]. \t  \u001b[92m-29.965412910907762\u001b[0m \t -29.965412910907762\n",
      "7      \t [ 0.70555612 -0.04879187]. \t  \u001b[92m-29.96399583905666\u001b[0m \t -29.96399583905666\n",
      "8      \t [ 0.70554993 -0.04878975]. \t  \u001b[92m-29.962813339013064\u001b[0m \t -29.962813339013064\n",
      "9      \t [ 0.7055446  -0.04878793]. \t  \u001b[92m-29.961795655312866\u001b[0m \t -29.961795655312866\n",
      "10     \t [ 0.70553995 -0.04878634]. \t  \u001b[92m-29.960906436512065\u001b[0m \t -29.960906436512065\n",
      "11     \t [ 0.70553587 -0.04878489]. \t  \u001b[92m-29.96012161927289\u001b[0m \t -29.96012161927289\n",
      "12     \t [ 0.7055321  -0.04878363]. \t  \u001b[92m-29.959403796982848\u001b[0m \t -29.959403796982848\n",
      "13     \t [ 0.70552865 -0.04878247]. \t  \u001b[92m-29.95874775794497\u001b[0m \t -29.95874775794497\n",
      "14     \t [ 0.70552554 -0.04878139]. \t  \u001b[92m-29.958151859604097\u001b[0m \t -29.958151859604097\n",
      "15     \t [ 0.70552264 -0.04878041]. \t  \u001b[92m-29.957600024245515\u001b[0m \t -29.957600024245515\n",
      "16     \t [ 0.70551998 -0.04877947]. \t  \u001b[92m-29.95708748026672\u001b[0m \t -29.95708748026672\n",
      "17     \t [ 0.70551745 -0.04877859]. \t  \u001b[92m-29.956602960492173\u001b[0m \t -29.956602960492173\n",
      "18     \t [ 0.70551508 -0.04877779]. \t  \u001b[92m-29.956151370542905\u001b[0m \t -29.956151370542905\n",
      "19     \t [ 0.70551285 -0.04877705]. \t  \u001b[92m-29.95572659401868\u001b[0m \t -29.95572659401868\n",
      "20     \t [ 0.70551075 -0.0487763 ]. \t  \u001b[92m-29.955323153691484\u001b[0m \t -29.955323153691484\n",
      "21     \t [ 0.70550874 -0.04877564]. \t  \u001b[92m-29.954941920796475\u001b[0m \t -29.954941920796475\n",
      "22     \t [ 0.70550685 -0.04877496]. \t  \u001b[92m-29.954576564402807\u001b[0m \t -29.954576564402807\n",
      "23     \t [ 0.70550503 -0.04877434]. \t  \u001b[92m-29.95423008730926\u001b[0m \t -29.95423008730926\n",
      "24     \t [ 0.7055033  -0.04877374]. \t  \u001b[92m-29.953899333461163\u001b[0m \t -29.953899333461163\n",
      "25     \t [ 0.7055016  -0.04877316]. \t  \u001b[92m-29.953573815127942\u001b[0m \t -29.953573815127942\n",
      "26     \t [ 0.70550004 -0.04877261]. \t  \u001b[92m-29.953274624201498\u001b[0m \t -29.953274624201498\n",
      "27     \t [ 0.70549849 -0.0487721 ]. \t  \u001b[92m-29.95298003719609\u001b[0m \t -29.95298003719609\n",
      "28     \t [ 0.70549697 -0.0487716 ]. \t  \u001b[92m-29.95269209745074\u001b[0m \t -29.95269209745074\n",
      "29     \t [ 0.70549557 -0.04877109]. \t  \u001b[92m-29.952420904193154\u001b[0m \t -29.952420904193154\n",
      "30     \t [ 0.70549418 -0.04877064]. \t  \u001b[92m-29.952158083925777\u001b[0m \t -29.952158083925777\n",
      "31     \t [ 0.70549285 -0.04877018]. \t  \u001b[92m-29.951903880742467\u001b[0m \t -29.951903880742467\n",
      "32     \t [ 0.70549155 -0.04876973]. \t  \u001b[92m-29.951655567044384\u001b[0m \t -29.951655567044384\n",
      "33     \t [ 0.70549033 -0.04876928]. \t  \u001b[92m-29.951417638249488\u001b[0m \t -29.951417638249488\n",
      "34     \t [-0.1529036   0.77815867]. \t  -58.29834472702657 \t -29.951417638249488\n",
      "35     \t [ 0.70548908 -0.04876886]. \t  \u001b[92m-29.951179980284564\u001b[0m \t -29.951179980284564\n",
      "36     \t [ 0.70548792 -0.04876844]. \t  \u001b[92m-29.950955553525258\u001b[0m \t -29.950955553525258\n",
      "37     \t [ 0.70548673 -0.04876808]. \t  \u001b[92m-29.950733675813197\u001b[0m \t -29.950733675813197\n",
      "38     \t [ 0.70548563 -0.04876767]. \t  \u001b[92m-29.95051993593165\u001b[0m \t -29.95051993593165\n",
      "39     \t [ 0.70548457 -0.04876728]. \t  \u001b[92m-29.95031463575538\u001b[0m \t -29.95031463575538\n",
      "40     \t [ 0.70548346 -0.04876696]. \t  \u001b[92m-29.95010909795403\u001b[0m \t -29.95010909795403\n",
      "41     \t [ 0.70548249 -0.04876659]. \t  \u001b[92m-29.949918846539497\u001b[0m \t -29.949918846539497\n",
      "42     \t [ 0.7054814  -0.04876624]. \t  \u001b[92m-29.94971457875413\u001b[0m \t -29.94971457875413\n",
      "43     \t [-0.15290014  0.77815429]. \t  -58.29783488423566 \t -29.94971457875413\n",
      "44     \t [ 0.70548046 -0.04876589]. \t  \u001b[92m-29.94953081731956\u001b[0m \t -29.94953081731956\n",
      "45     \t [ 0.70547947 -0.04876555]. \t  \u001b[92m-29.949342475877568\u001b[0m \t -29.949342475877568\n",
      "46     \t [ 0.70547854 -0.04876522]. \t  \u001b[92m-29.94916297575224\u001b[0m \t -29.94916297575224\n",
      "47     \t [ 0.7054776  -0.04876491]. \t  \u001b[92m-29.94898499415912\u001b[0m \t -29.94898499415912\n",
      "48     \t [ 0.70547669 -0.04876461]. \t  \u001b[92m-29.948812250486718\u001b[0m \t -29.948812250486718\n",
      "49     \t [ 0.7054758 -0.0487643]. \t  \u001b[92m-29.94864082412761\u001b[0m \t -29.94864082412761\n",
      "50     \t [ 0.70547496 -0.04876398]. \t  \u001b[92m-29.94847879023472\u001b[0m \t -29.94847879023472\n",
      "51     \t [ 0.70547408 -0.0487637 ]. \t  \u001b[92m-29.94831189293977\u001b[0m \t -29.94831189293977\n",
      "52     \t [ 0.70547331 -0.04876339]. \t  \u001b[92m-29.94815965128233\u001b[0m \t -29.94815965128233\n",
      "53     \t [ 0.70547247 -0.04876311]. \t  \u001b[92m-29.948000475916864\u001b[0m \t -29.948000475916864\n",
      "54     \t [ 0.70547159 -0.04876285]. \t  \u001b[92m-29.94783789140658\u001b[0m \t -29.94783789140658\n",
      "55     \t [ 0.70547079 -0.04876259]. \t  \u001b[92m-29.94768452690306\u001b[0m \t -29.94768452690306\n",
      "56     \t [ 0.70547002 -0.04876232]. \t  \u001b[92m-29.94753722274989\u001b[0m \t -29.94753722274989\n",
      "57     \t [ 0.70546929 -0.04876205]. \t  \u001b[92m-29.94739627606366\u001b[0m \t -29.94739627606366\n",
      "58     \t [ 0.70546851 -0.04876179]. \t  \u001b[92m-29.947248534071097\u001b[0m \t -29.947248534071097\n",
      "59     \t [ 0.70546776 -0.04876152]. \t  \u001b[92m-29.94710293486551\u001b[0m \t -29.94710293486551\n",
      "60     \t [ 0.70546704 -0.0487613 ]. \t  \u001b[92m-29.946967487636137\u001b[0m \t -29.946967487636137\n",
      "61     \t [ 0.70546632 -0.04876104]. \t  \u001b[92m-29.946829507141015\u001b[0m \t -29.946829507141015\n",
      "62     \t [ 0.70546562 -0.0487608 ]. \t  \u001b[92m-29.94669598454207\u001b[0m \t -29.94669598454207\n",
      "63     \t [ 0.70546495 -0.04876055]. \t  \u001b[92m-29.946565821848047\u001b[0m \t -29.946565821848047\n",
      "64     \t [ 0.70546425 -0.04876033]. \t  \u001b[92m-29.94643249241481\u001b[0m \t -29.94643249241481\n",
      "65     \t [ 0.70546358 -0.04876009]. \t  \u001b[92m-29.946304079477446\u001b[0m \t -29.946304079477446\n",
      "66     \t [ 0.70546289 -0.04875986]. \t  \u001b[92m-29.946172596319972\u001b[0m \t -29.946172596319972\n",
      "67     \t [ 0.70546224 -0.04875964]. \t  \u001b[92m-29.946049464005366\u001b[0m \t -29.946049464005366\n",
      "68     \t [ 0.70546155 -0.04875942]. \t  \u001b[92m-29.94592028413359\u001b[0m \t -29.94592028413359\n",
      "69     \t [ 0.70546092 -0.04875921]. \t  \u001b[92m-29.945800551290315\u001b[0m \t -29.945800551290315\n",
      "70     \t [ 0.70546029 -0.04875899]. \t  \u001b[92m-29.94567853857525\u001b[0m \t -29.94567853857525\n",
      "71     \t [ 0.70545966 -0.04875879]. \t  \u001b[92m-29.94556094311104\u001b[0m \t -29.94556094311104\n",
      "72     \t [ 0.70545907 -0.04875854]. \t  \u001b[92m-29.94544334050322\u001b[0m \t -29.94544334050322\n",
      "73     \t [ 0.70545846 -0.04875834]. \t  \u001b[92m-29.945325898459686\u001b[0m \t -29.945325898459686\n",
      "74     \t [ 0.70545784 -0.04875815]. \t  \u001b[92m-29.94521051911514\u001b[0m \t -29.94521051911514\n",
      "75     \t [ 0.70545734 -0.04875793]. \t  \u001b[92m-29.945109705100393\u001b[0m \t -29.945109705100393\n",
      "76     \t [ 0.70545667 -0.04875772]. \t  \u001b[92m-29.94498438175139\u001b[0m \t -29.94498438175139\n",
      "77     \t [ 0.70545612 -0.04875752]. \t  \u001b[92m-29.944877109699725\u001b[0m \t -29.944877109699725\n",
      "78     \t [ 0.70545552 -0.04875733]. \t  \u001b[92m-29.944764832235442\u001b[0m \t -29.944764832235442\n",
      "79     \t [-0.15289761  0.77815112]. \t  -58.29746710470378 \t -29.944764832235442\n",
      "80     \t [ 0.70545497 -0.04875715]. \t  \u001b[92m-29.94466089421665\u001b[0m \t -29.94466089421665\n",
      "81     \t [ 0.70545437 -0.04875695]. \t  \u001b[92m-29.94454721891139\u001b[0m \t -29.94454721891139\n",
      "82     \t [ 0.70545383 -0.04875676]. \t  \u001b[92m-29.94444381757124\u001b[0m \t -29.94444381757124\n",
      "83     \t [ 0.70545329 -0.04875656]. \t  \u001b[92m-29.94433903875902\u001b[0m \t -29.94433903875902\n",
      "84     \t [ 0.7054528  -0.04875636]. \t  \u001b[92m-29.944241407028294\u001b[0m \t -29.944241407028294\n",
      "85     \t [ 0.70545224 -0.0487562 ]. \t  \u001b[92m-29.944136560438782\u001b[0m \t -29.944136560438782\n",
      "86     \t [ 0.70545169 -0.04875601]. \t  \u001b[92m-29.944032019524194\u001b[0m \t -29.944032019524194\n",
      "87     \t [ 0.70545122 -0.04875582]. \t  \u001b[92m-29.94393914417933\u001b[0m \t -29.94393914417933\n",
      "88     \t [ 0.70545069 -0.04875562]. \t  \u001b[92m-29.943836450388257\u001b[0m \t -29.943836450388257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [ 0.70545015 -0.04875546]. \t  \u001b[92m-29.943735445718815\u001b[0m \t -29.943735445718815\n",
      "90     \t [ 0.70544967 -0.04875529]. \t  \u001b[92m-29.94364389938184\u001b[0m \t -29.94364389938184\n",
      "91     \t [ 0.70544912 -0.04875512]. \t  \u001b[92m-29.943539166118423\u001b[0m \t -29.943539166118423\n",
      "92     \t [ 0.70544862 -0.04875497]. \t  \u001b[92m-29.943446884803326\u001b[0m \t -29.943446884803326\n",
      "93     \t [ 0.7054481  -0.04875483]. \t  \u001b[92m-29.943352616788406\u001b[0m \t -29.943352616788406\n",
      "94     \t [ 0.70544759 -0.04875469]. \t  \u001b[92m-29.943258426223615\u001b[0m \t -29.943258426223615\n",
      "95     \t [ 0.70544715 -0.04875444]. \t  \u001b[92m-29.943164468960564\u001b[0m \t -29.943164468960564\n",
      "96     \t [ 0.70544666 -0.04875429]. \t  \u001b[92m-29.943072604169807\u001b[0m \t -29.943072604169807\n",
      "97     \t [ 0.70544623 -0.04875413]. \t  \u001b[92m-29.942988367142384\u001b[0m \t -29.942988367142384\n",
      "98     \t [ 0.70544572 -0.04875397]. \t  \u001b[92m-29.942892008049814\u001b[0m \t -29.942892008049814\n",
      "99     \t [ 0.70544525 -0.04875378]. \t  \u001b[92m-29.9427996588828\u001b[0m \t -29.9427996588828\n",
      "100    \t [ 0.70544482 -0.04875367]. \t  \u001b[92m-29.94272090594308\u001b[0m \t -29.94272090594308\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.18624574 -1.34403967]. \t  -190.75104997517573 \t -190.75104997517573\n",
      "init   \t [ 1.44560869 -1.30980491]. \t  -1155.919375128387 \t -190.75104997517573\n",
      "init   \t [ 0.36331541 -1.55330601]. \t  -284.4303550043376 \t -190.75104997517573\n",
      "init   \t [ 0.22404608 -1.71202077]. \t  -311.14312856964693 \t -190.75104997517573\n",
      "init   \t [ 2.0181204  -1.01286191]. \t  -2587.442372593159 \t -190.75104997517573\n",
      "1      \t [-0.85302395 -0.42819185]. \t  \u001b[92m-137.03070260521912\u001b[0m \t -137.03070260521912\n",
      "2      \t [-1.45054235 -2.048     ]. \t  -1729.9762742964454 \t -137.03070260521912\n",
      "3      \t [-0.07620255 -0.19988115]. \t  \u001b[92m-5.38896631794783\u001b[0m \t -5.38896631794783\n",
      "4      \t [-0.20253067 -0.58272636]. \t  -40.35186682238222 \t -5.38896631794783\n",
      "5      \t [-0.55157259  1.11713816]. \t  -68.4889675033586 \t -5.38896631794783\n",
      "6      \t [-2.03684395  0.56598372]. \t  -1292.831874388824 \t -5.38896631794783\n",
      "7      \t [0.30846427 1.09146605]. \t  -99.74274776632701 \t -5.38896631794783\n",
      "8      \t [-0.26390291  0.45498527]. \t  -16.44618226233421 \t -5.38896631794783\n",
      "9      \t [-0.46770904  2.048     ]. \t  -336.76908858478663 \t -5.38896631794783\n",
      "10     \t [0.19455835 0.01379327]. \t  \u001b[92m-0.706623082459493\u001b[0m \t -0.706623082459493\n",
      "11     \t [ 0.12394753 -0.12834323]. \t  -2.8326158351125557 \t -0.706623082459493\n",
      "12     \t [0.05782785 0.15630453]. \t  -3.2273787776889966 \t -0.706623082459493\n",
      "13     \t [0.15301742 0.02634783]. \t  -0.7182400274078692 \t -0.706623082459493\n",
      "14     \t [0.16061516 0.02847009]. \t  \u001b[92m-0.7052813339963746\u001b[0m \t -0.7052813339963746\n",
      "15     \t [0.16089821 0.02837839]. \t  \u001b[92m-0.704711912431036\u001b[0m \t -0.704711912431036\n",
      "16     \t [0.16097582 0.02834786]. \t  \u001b[92m-0.7045543341789124\u001b[0m \t -0.7045543341789124\n",
      "17     \t [0.16100562 0.02833649]. \t  \u001b[92m-0.704494150205884\u001b[0m \t -0.704494150205884\n",
      "18     \t [0.16101852 0.02833038]. \t  \u001b[92m-0.7044675716907897\u001b[0m \t -0.7044675716907897\n",
      "19     \t [0.16102379 0.02832715]. \t  \u001b[92m-0.7044563498745979\u001b[0m \t -0.7044563498745979\n",
      "20     \t [0.16102487 0.02832636]. \t  \u001b[92m-0.7044539921170694\u001b[0m \t -0.7044539921170694\n",
      "21     \t [0.16102473 0.02832552]. \t  \u001b[92m-0.7044538459398906\u001b[0m \t -0.7044538459398906\n",
      "22     \t [0.16102575 0.02832582]. \t  \u001b[92m-0.7044521277253887\u001b[0m \t -0.7044521277253887\n",
      "23     \t [0.16102617 0.02832677]. \t  \u001b[92m-0.7044518186811254\u001b[0m \t -0.7044518186811254\n",
      "24     \t [0.16102411 0.02832718]. \t  -0.7044557853502499 \t -0.7044518186811254\n",
      "25     \t [0.16102341 0.0283268 ]. \t  -0.7044568857268741 \t -0.7044518186811254\n",
      "26     \t [0.16102232 0.02832693]. \t  -0.7044589399934703 \t -0.7044518186811254\n",
      "27     \t [0.16102138 0.02832851]. \t  -0.7044614192772429 \t -0.7044518186811254\n",
      "28     \t [0.16102151 0.02832834]. \t  -0.7044611067189024 \t -0.7044518186811254\n",
      "29     \t [0.1610184  0.02833038]. \t  -0.7044677785502076 \t -0.7044518186811254\n",
      "30     \t [0.163309   0.02701583]. \t  \u001b[92m-0.7000637983652552\u001b[0m \t -0.7000637983652552\n",
      "31     \t [0.15593437 0.03233863]. \t  -0.7188837919522555 \t -0.7000637983652552\n",
      "32     \t [0.15949753 0.02428204]. \t  -0.7065783562271158 \t -0.7000637983652552\n",
      "33     \t [0.16108581 0.02779143]. \t  -0.7041165992423012 \t -0.7000637983652552\n",
      "34     \t [0.16133579 0.03142976]. \t  -0.7062742178485821 \t -0.7000637983652552\n",
      "35     \t [0.16304749 0.02896816]. \t  -0.7010576978677302 \t -0.7000637983652552\n",
      "36     \t [0.16185557 0.02850758]. \t  -0.7030198523098947 \t -0.7000637983652552\n",
      "37     \t [0.15404237 0.02292594]. \t  -0.7157088117738848 \t -0.7000637983652552\n",
      "38     \t [0.16118375 0.02906863]. \t  -0.7045665418993388 \t -0.7000637983652552\n",
      "39     \t [0.15747249 0.02492813]. \t  -0.7098543098430041 \t -0.7000637983652552\n",
      "40     \t [0.15724027 0.03005524]. \t  -0.7130856431457253 \t -0.7000637983652552\n",
      "41     \t [0.1605136  0.02975414]. \t  -0.7063290520805339 \t -0.7000637983652552\n",
      "42     \t [0.1562329  0.02909377]. \t  -0.7141378965982313 \t -0.7000637983652552\n",
      "43     \t [0.16283528 0.02654921]. \t  -0.700844885173961 \t -0.7000637983652552\n",
      "44     \t [0.16395365 0.02655723]. \t  \u001b[92m-0.6989839650064391\u001b[0m \t -0.6989839650064391\n",
      "45     \t [0.16117292 0.0245029 ]. \t  -0.7038480883172279 \t -0.6989839650064391\n",
      "46     \t [0.16427    0.02866406]. \t  \u001b[92m-0.6987266818388761\u001b[0m \t -0.6987266818388761\n",
      "47     \t [0.16137846 0.02775266]. \t  -0.7035783747597542 \t -0.6987266818388761\n",
      "48     \t [0.16223243 0.02836328]. \t  -0.7022722678276507 \t -0.6987266818388761\n",
      "49     \t [0.16185337 0.02865478]. \t  -0.7030940733426103 \t -0.6987266818388761\n",
      "50     \t [0.16450283 0.03230604]. \t  -0.7008063796416814 \t -0.6987266818388761\n",
      "51     \t [0.16127325 0.02849413]. \t  -0.704080119014179 \t -0.6987266818388761\n",
      "52     \t [0.17273805 0.02854974]. \t  \u001b[92m-0.6845284003292175\u001b[0m \t -0.6845284003292175\n",
      "53     \t [0.16591836 0.02873181]. \t  -0.6958368823361513 \t -0.6845284003292175\n",
      "54     \t [0.15939096 0.02832829]. \t  -0.7074778410849816 \t -0.6845284003292175\n",
      "55     \t [0.16042625 0.02249869]. \t  -0.7059324775862048 \t -0.6845284003292175\n",
      "56     \t [0.1584641  0.02629323]. \t  -0.7083224592311854 \t -0.6845284003292175\n",
      "57     \t [0.16041053 0.02784067]. \t  -0.7053553218917833 \t -0.6845284003292175\n",
      "58     \t [0.16340272 0.02920679]. \t  -0.7005231779815846 \t -0.6845284003292175\n",
      "59     \t [0.16251469 0.02845748]. \t  -0.701800434674447 \t -0.6845284003292175\n",
      "60     \t [0.16316415 0.0297794 ]. \t  -0.7012908130504091 \t -0.6845284003292175\n",
      "61     \t [0.16249371 0.02898101]. \t  -0.7020807861327745 \t -0.6845284003292175\n",
      "62     \t [0.16701725 0.03215239]. \t  -0.6956729965724255 \t -0.6845284003292175\n",
      "63     \t [0.15931261 0.02814661]. \t  -0.707520422870879 \t -0.6845284003292175\n",
      "64     \t [0.16277674 0.03023155]. \t  -0.7023380168904528 \t -0.6845284003292175\n",
      "65     \t [0.15667232 0.02982228]. \t  -0.7139852650302676 \t -0.6845284003292175\n",
      "66     \t [0.16336341 0.02778194]. \t  -0.7000805428359307 \t -0.6845284003292175\n",
      "67     \t [0.16316076 0.02996185]. \t  -0.7014157485266718 \t -0.6845284003292175\n",
      "68     \t [0.1631661  0.02981012]. \t  -0.7013066308099826 \t -0.6845284003292175\n",
      "69     \t [0.16514851 0.02982477]. \t  -0.6976276305476805 \t -0.6845284003292175\n",
      "70     \t [0.15872145 0.02861782]. \t  -0.7089228885026668 \t -0.6845284003292175\n",
      "71     \t [0.16230907 0.0282655 ]. \t  -0.702095225382383 \t -0.6845284003292175\n",
      "72     \t [0.16270114 0.02837022]. \t  -0.7014298355716407 \t -0.6845284003292175\n",
      "73     \t [0.16350635 0.02962619]. \t  -0.7005579122537552 \t -0.6845284003292175\n",
      "74     \t [0.15603448 0.02825555]. \t  -0.7138056719361245 \t -0.6845284003292175\n",
      "75     \t [0.16285922 0.02934273]. \t  -0.7015997048975875 \t -0.6845284003292175\n",
      "76     \t [0.16463978 0.02707755]. \t  -0.6978267850783217 \t -0.6845284003292175\n",
      "77     \t [0.16138028 0.02587372]. \t  -0.7032859259141581 \t -0.6845284003292175\n",
      "78     \t [0.16414648 0.02923727]. \t  -0.6991769912860476 \t -0.6845284003292175\n",
      "79     \t [0.15836906 0.03128567]. \t  -0.7121927368106451 \t -0.6845284003292175\n",
      "80     \t [0.164383   0.02668142]. \t  -0.6982673564162832 \t -0.6845284003292175\n",
      "81     \t [0.16356657 0.02968305]. \t  -0.7004787917684406 \t -0.6845284003292175\n",
      "82     \t [0.15668142 0.02507458]. \t  -0.7112138381096815 \t -0.6845284003292175\n",
      "83     \t [0.16357455 0.02965218]. \t  -0.7004459543461296 \t -0.6845284003292175\n",
      "84     \t [0.16279923 0.02542747]. \t  -0.701020930221752 \t -0.6845284003292175\n",
      "85     \t [0.15937567 0.02816054]. \t  -0.707410986631446 \t -0.6845284003292175\n",
      "86     \t [0.16296734 0.03024373]. \t  -0.701981874653245 \t -0.6845284003292175\n",
      "87     \t [0.16349649 0.02959159]. \t  -0.7005563547447416 \t -0.6845284003292175\n",
      "88     \t [0.16298747 0.03027996]. \t  -0.7019701292648967 \t -0.6845284003292175\n",
      "89     \t [0.1603035  0.02646656]. \t  -0.7051493989368391 \t -0.6845284003292175\n",
      "90     \t [0.16473395 0.02898243]. \t  -0.6980098388878129 \t -0.6845284003292175\n",
      "91     \t [0.16372264 0.03002277]. \t  -0.7003951506553624 \t -0.6845284003292175\n",
      "92     \t [0.16604058 0.02654645]. \t  -0.6955929706158351 \t -0.6845284003292175\n",
      "93     \t [0.1636326  0.02958857]. \t  -0.7003016934755926 \t -0.6845284003292175\n",
      "94     \t [0.16294512 0.02904218]. \t  -0.7012814168302623 \t -0.6845284003292175\n",
      "95     \t [0.16195413 0.02798814]. \t  -0.702630291560764 \t -0.6845284003292175\n",
      "96     \t [0.16402635 0.03000458]. \t  -0.6998129049412658 \t -0.6845284003292175\n",
      "97     \t [0.16077045 0.02563505]. \t  -0.7043107358309194 \t -0.6845284003292175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.16440157 0.02821159]. \t  -0.6983648480878407 \t -0.6845284003292175\n",
      "99     \t [0.16223109 0.02975003]. \t  -0.7030339859698479 \t -0.6845284003292175\n",
      "100    \t [0.16394273 0.0298152 ]. \t  -0.6998549359789477 \t -0.6845284003292175\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.59615619  0.88802982]. \t  \u001b[92m-30.91693235158791\u001b[0m \t -30.91693235158791\n",
      "2      \t [-0.34049868  0.06411137]. \t  \u001b[92m-2.0655505956228666\u001b[0m \t -2.0655505956228666\n",
      "3      \t [-1.24180279  0.84652576]. \t  -53.404439597583355 \t -2.0655505956228666\n",
      "4      \t [ 1.42616086 -1.15669448]. \t  -1018.1931386035618 \t -2.0655505956228666\n",
      "5      \t [-0.81490605  0.448126  ]. \t  -7.957146028489332 \t -2.0655505956228666\n",
      "6      \t [-0.38370934  2.048     ]. \t  -363.20622544617527 \t -2.0655505956228666\n",
      "7      \t [0.4920827  0.11666021]. \t  \u001b[92m-1.8326328720002625\u001b[0m \t -1.8326328720002625\n",
      "8      \t [-0.01588957 -0.34151418]. \t  -12.712476471700711 \t -1.8326328720002625\n",
      "9      \t [ 0.32210606 -0.08256665]. \t  -3.931015859757417 \t -1.8326328720002625\n",
      "10     \t [-0.49046169 -0.06524787]. \t  -11.572872747584844 \t -1.8326328720002625\n",
      "11     \t [0.15331125 0.84013662]. \t  -67.40571037834265 \t -1.8326328720002625\n",
      "12     \t [-0.55410432  0.22013938]. \t  -3.1702661159399783 \t -1.8326328720002625\n",
      "13     \t [0.84004848 0.11729894]. \t  -34.64498230271825 \t -1.8326328720002625\n",
      "14     \t [0.39156009 0.08423784]. \t  \u001b[92m-0.8474239377949653\u001b[0m \t -0.8474239377949653\n",
      "15     \t [0.39681173 0.08444401]. \t  -0.8969629518703593 \t -0.8474239377949653\n",
      "16     \t [0.39644883 0.08436725]. \t  -0.8943224150955783 \t -0.8474239377949653\n",
      "17     \t [0.39610196 0.08429589]. \t  -0.891781505413666 \t -0.8474239377949653\n",
      "18     \t [0.39584594 0.08424703]. \t  -0.8898586341940501 \t -0.8474239377949653\n",
      "19     \t [0.39565062 0.08421273]. \t  -0.8883530774470298 \t -0.8474239377949653\n",
      "20     \t [0.3955     0.08418892]. \t  -0.8871567813723273 \t -0.8474239377949653\n",
      "21     \t [0.39537788 0.08417078]. \t  -0.8861718182565058 \t -0.8474239377949653\n",
      "22     \t [0.39527755 0.08415713]. \t  -0.885345756340635 \t -0.8474239377949653\n",
      "23     \t [0.39519215 0.08414578]. \t  -0.8846397770001917 \t -0.8474239377949653\n",
      "24     \t [0.39512252 0.08413752]. \t  -0.8840505040328479 \t -0.8474239377949653\n",
      "25     \t [0.39505551 0.08413145]. \t  -0.8834569182363174 \t -0.8474239377949653\n",
      "26     \t [0.39499978 0.08412575]. \t  -0.8829729184961113 \t -0.8474239377949653\n",
      "27     \t [0.39494949 0.08412267]. \t  -0.8825069532771059 \t -0.8474239377949653\n",
      "28     \t [0.39490111 0.08411823]. \t  -0.8820802484825998 \t -0.8474239377949653\n",
      "29     \t [0.39486191 0.08411576]. \t  -0.8817184863775664 \t -0.8474239377949653\n",
      "30     \t [0.39482476 0.08411331]. \t  -0.8813774434218953 \t -0.8474239377949653\n",
      "31     \t [0.39478848 0.08411234]. \t  -0.8810241774443814 \t -0.8474239377949653\n",
      "32     \t [0.39475626 0.08411111]. \t  -0.8807158146473164 \t -0.8474239377949653\n",
      "33     \t [0.39472748 0.08411135]. \t  -0.8804214080831729 \t -0.8474239377949653\n",
      "34     \t [0.39469785 0.08410924]. \t  -0.8801520783087979 \t -0.8474239377949653\n",
      "35     \t [0.39467186 0.08410864]. \t  -0.8798980598482933 \t -0.8474239377949653\n",
      "36     \t [0.39464753 0.08410944]. \t  -0.8796409261600973 \t -0.8474239377949653\n",
      "37     \t [0.39462356 0.08410904]. \t  -0.8794046164571532 \t -0.8474239377949653\n",
      "38     \t [0.39460384 0.08410966]. \t  -0.8791967928698474 \t -0.8474239377949653\n",
      "39     \t [0.39458135 0.08410994]. \t  -0.8789657869701206 \t -0.8474239377949653\n",
      "40     \t [0.39456204 0.08411105]. \t  -0.8787551676033791 \t -0.8474239377949653\n",
      "41     \t [0.39454147 0.08411119]. \t  -0.8785457208462562 \t -0.8474239377949653\n",
      "42     \t [0.39452325 0.08411135]. \t  -0.8783598217097577 \t -0.8474239377949653\n",
      "43     \t [0.39450678 0.0841131 ]. \t  -0.878168833033212 \t -0.8474239377949653\n",
      "44     \t [0.39448901 0.08411348]. \t  -0.8779844241056559 \t -0.8474239377949653\n",
      "45     \t [0.3944738  0.08411544]. \t  -0.8778032071914628 \t -0.8474239377949653\n",
      "46     \t [0.39445903 0.084117  ]. \t  -0.8776321952929705 \t -0.8474239377949653\n",
      "47     \t [0.39444323 0.08411724]. \t  -0.8774696557477679 \t -0.8474239377949653\n",
      "48     \t [0.39442817 0.08411873]. \t  -0.8772968238861302 \t -0.8474239377949653\n",
      "49     \t [0.3944144  0.08412023]. \t  -0.8771367857293257 \t -0.8474239377949653\n",
      "50     \t [0.39440075 0.0841203 ]. \t  -0.8769986029704115 \t -0.8474239377949653\n",
      "51     \t [0.39438762 0.08412381]. \t  -0.8768163343147573 \t -0.8474239377949653\n",
      "52     \t [0.39437607 0.08412539]. \t  -0.8766776834244797 \t -0.8474239377949653\n",
      "53     \t [0.39436232 0.08412467]. \t  -0.8765497407057663 \t -0.8474239377949653\n",
      "54     \t [0.39435071 0.08412796]. \t  -0.8763861391097945 \t -0.8474239377949653\n",
      "55     \t [0.39434033 0.08413138]. \t  -0.8762329837988677 \t -0.8474239377949653\n",
      "56     \t [0.39432802 0.08413159]. \t  -0.8761063177293725 \t -0.8474239377949653\n",
      "57     \t [0.39431466 0.08413087]. \t  -0.8759822608548362 \t -0.8474239377949653\n",
      "58     \t [0.3943033  0.08413462]. \t  -0.8758146880915778 \t -0.8474239377949653\n",
      "59     \t [0.39429347 0.08413629]. \t  -0.8756923299558645 \t -0.8474239377949653\n",
      "60     \t [0.39428252 0.08413869]. \t  -0.8755480867111961 \t -0.8474239377949653\n",
      "61     \t [0.3942719  0.08414028]. \t  -0.8754188346981857 \t -0.8474239377949653\n",
      "62     \t [0.39426206 0.08414217]. \t  -0.8752931478418136 \t -0.8474239377949653\n",
      "63     \t [0.39425265 0.08414386]. \t  -0.8751746442110315 \t -0.8474239377949653\n",
      "64     \t [0.3942393  0.08414654]. \t  -0.8750025890757751 \t -0.8474239377949653\n",
      "65     \t [0.39423038 0.08414829]. \t  -0.8748881667537873 \t -0.8474239377949653\n",
      "66     \t [0.39422257 0.08415015]. \t  -0.8747833424951896 \t -0.8474239377949653\n",
      "67     \t [0.39421362 0.08415083]. \t  -0.8746838756191138 \t -0.8474239377949653\n",
      "68     \t [0.39420311 0.0841538 ]. \t  -0.8745363753886375 \t -0.8474239377949653\n",
      "69     \t [0.39419324 0.08415595]. \t  -0.8744066481697141 \t -0.8474239377949653\n",
      "70     \t [0.39418393 0.08415857]. \t  -0.8742761681426366 \t -0.8474239377949653\n",
      "71     \t [0.39417627 0.08416085]. \t  -0.8741668755424676 \t -0.8474239377949653\n",
      "72     \t [0.39416531 0.08416269]. \t  -0.8740309250595293 \t -0.8474239377949653\n",
      "73     \t [0.39415965 0.08416563]. \t  -0.8739324013383944 \t -0.8474239377949653\n",
      "74     \t [0.3941484  0.08416818]. \t  -0.8737834707937279 \t -0.8474239377949653\n",
      "75     \t [0.39414021 0.08417011]. \t  -0.8736740542801549 \t -0.8474239377949653\n",
      "76     \t [0.39413212 0.08417193]. \t  -0.8735670678667979 \t -0.8474239377949653\n",
      "77     \t [0.39412466 0.08417562]. \t  -0.8734399103693407 \t -0.8474239377949653\n",
      "78     \t [0.39411434 0.08417693]. \t  -0.8733180541013428 \t -0.8474239377949653\n",
      "79     \t [0.39410564 0.0841806 ]. \t  -0.8731787924499768 \t -0.8474239377949653\n",
      "80     \t [0.39409879 0.08418222]. \t  -0.8730872162846994 \t -0.8474239377949653\n",
      "81     \t [0.39408992 0.08418521]. \t  -0.8729559685541346 \t -0.8474239377949653\n",
      "82     \t [0.39408148 0.08418716]. \t  -0.8728438869226334 \t -0.8474239377949653\n",
      "83     \t [0.39407359 0.0841904 ]. \t  -0.872718889568618 \t -0.8474239377949653\n",
      "84     \t [0.39406473 0.0841924 ]. \t  -0.8726019531094449 \t -0.8474239377949653\n",
      "85     \t [0.3940568  0.08419605]. \t  -0.8724707550382951 \t -0.8474239377949653\n",
      "86     \t [0.39404974 0.08419944]. \t  -0.8723521442497055 \t -0.8474239377949653\n",
      "87     \t [0.39404024 0.08420142]. \t  -0.8722289036538693 \t -0.8474239377949653\n",
      "88     \t [0.39403255 0.08420442]. \t  -0.8721095287313282 \t -0.8474239377949653\n",
      "89     \t [0.39402543 0.08420755]. \t  -0.8719939644583701 \t -0.8474239377949653\n",
      "90     \t [0.39401709 0.08420962]. \t  -0.8718812001582181 \t -0.8474239377949653\n",
      "91     \t [0.39400505 0.08421187]. \t  -0.8717290359849 \t -0.8474239377949653\n",
      "92     \t [0.39400003 0.08421607]. \t  -0.8716194193938922 \t -0.8474239377949653\n",
      "93     \t [0.3939941  0.08422037]. \t  -0.871499031420656 \t -0.8474239377949653\n",
      "94     \t [0.39398592 0.08422341]. \t  -0.8713742682748719 \t -0.8474239377949653\n",
      "95     \t [0.39397493 0.08422513]. \t  -0.8712401937893712 \t -0.8474239377949653\n",
      "96     \t [0.39396857 0.08423102]. \t  -0.8710931544368781 \t -0.8474239377949653\n",
      "97     \t [0.3939599  0.08423307]. \t  -0.8709775807475478 \t -0.8474239377949653\n",
      "98     \t [0.39395238 0.08423556]. \t  -0.8708673323888418 \t -0.8474239377949653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [0.39394628 0.08424089]. \t  -0.8707307780653231 \t -0.8474239377949653\n",
      "100    \t [0.39393347 0.08424043]. \t  -0.8706096782681043 \t -0.8474239377949653\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-1.50615787  2.048     ]. \t  \u001b[92m-11.143361307811109\u001b[0m \t -11.143361307811109\n",
      "2      \t [-1.02358192  1.79472254]. \t  -59.8961696291556 \t -11.143361307811109\n",
      "3      \t [-2.048       1.68388822]. \t  -639.5090414271456 \t -11.143361307811109\n",
      "4      \t [-0.61899272  0.53631279]. \t  \u001b[92m-4.966960521126012\u001b[0m \t -4.966960521126012\n",
      "5      \t [-0.75359363  0.83039077]. \t  -9.965054552549175 \t -4.966960521126012\n",
      "6      \t [-0.91450392 -0.97175973]. \t  -330.5796226721671 \t -4.966960521126012\n",
      "7      \t [-0.01774827  0.23751491]. \t  -6.662191284671878 \t -4.966960521126012\n",
      "8      \t [-1.34953637  2.048     ]. \t  -10.661948913419874 \t -4.966960521126012\n",
      "9      \t [-0.29820525  0.25227769]. \t  \u001b[92m-4.3537022089196205\u001b[0m \t -4.3537022089196205\n",
      "10     \t [-0.30777293  0.25688301]. \t  \u001b[92m-4.339818875808186\u001b[0m \t -4.339818875808186\n",
      "11     \t [-0.31038003  0.25817181]. \t  \u001b[92m-4.336186447198082\u001b[0m \t -4.336186447198082\n",
      "12     \t [-0.31216032  0.25906056]. \t  \u001b[92m-4.33375390494737\u001b[0m \t -4.33375390494737\n",
      "13     \t [-0.31352187  0.25974531]. \t  \u001b[92m-4.331929016738432\u001b[0m \t -4.331929016738432\n",
      "14     \t [-0.31463426  0.26030734]. \t  \u001b[92m-4.330439554108079\u001b[0m \t -4.330439554108079\n",
      "15     \t [-0.31557765  0.26078619]. \t  \u001b[92m-4.32918984410542\u001b[0m \t -4.32918984410542\n",
      "16     \t [-0.31640419  0.26120714]. \t  \u001b[92m-4.328096947038898\u001b[0m \t -4.328096947038898\n",
      "17     \t [-0.31713878  0.2615826 ]. \t  \u001b[92m-4.327134586183886\u001b[0m \t -4.327134586183886\n",
      "18     \t [-0.31780399  0.26192316]. \t  \u001b[92m-4.326253850496192\u001b[0m \t -4.326253850496192\n",
      "19     \t [-0.31841508  0.26223736]. \t  \u001b[92m-4.325464732898272\u001b[0m \t -4.325464732898272\n",
      "20     \t [-0.31897971  0.26252815]. \t  \u001b[92m-4.324731329753247\u001b[0m \t -4.324731329753247\n",
      "21     \t [-0.31950709  0.2628004 ]. \t  \u001b[92m-4.3240499539502935\u001b[0m \t -4.3240499539502935\n",
      "22     \t [-0.32000236  0.26305646]. \t  \u001b[92m-4.323407625091968\u001b[0m \t -4.323407625091968\n",
      "23     \t [-0.32047121  0.26329933]. \t  \u001b[92m-4.3228010487932735\u001b[0m \t -4.3228010487932735\n",
      "24     \t [-0.32091813  0.26353153]. \t  \u001b[92m-4.322232822236689\u001b[0m \t -4.322232822236689\n",
      "25     \t [-0.32134265  0.26375229]. \t  \u001b[92m-4.321688578268303\u001b[0m \t -4.321688578268303\n",
      "26     \t [-0.32175113  0.26396504]. \t  \u001b[92m-4.3211655971925484\u001b[0m \t -4.3211655971925484\n",
      "27     \t [-0.32214453  0.26417021]. \t  \u001b[92m-4.320661219299377\u001b[0m \t -4.320661219299377\n",
      "28     \t [-0.3225233   0.26436844]. \t  \u001b[92m-4.320189162905064\u001b[0m \t -4.320189162905064\n",
      "29     \t [-0.32288804  0.26455928]. \t  \u001b[92m-4.319724953698285\u001b[0m \t -4.319724953698285\n",
      "30     \t [-0.32324391  0.26474573]. \t  \u001b[92m-4.319272392362055\u001b[0m \t -4.319272392362055\n",
      "31     \t [-0.3235918   0.26492822]. \t  \u001b[92m-4.318830198916995\u001b[0m \t -4.318830198916995\n",
      "32     \t [-0.32392659  0.26510429]. \t  \u001b[92m-4.318412115138438\u001b[0m \t -4.318412115138438\n",
      "33     \t [-0.32425856  0.26527891]. \t  \u001b[92m-4.317992115813671\u001b[0m \t -4.317992115813671\n",
      "34     \t [-0.32458109  0.26544862]. \t  \u001b[92m-4.317579497437496\u001b[0m \t -4.317579497437496\n",
      "35     \t [-0.32489934  0.26561678]. \t  \u001b[92m-4.3171889549551175\u001b[0m \t -4.3171889549551175\n",
      "36     \t [-0.3252083   0.26577998]. \t  \u001b[92m-4.316802465019524\u001b[0m \t -4.316802465019524\n",
      "37     \t [-0.32551694  0.26594309]. \t  \u001b[92m-4.3164130015009095\u001b[0m \t -4.3164130015009095\n",
      "38     \t [-0.32581663  0.26610182]. \t  \u001b[92m-4.316040934987344\u001b[0m \t -4.316040934987344\n",
      "39     \t [-0.32611218  0.26625838]. \t  \u001b[92m-4.315669572985851\u001b[0m \t -4.315669572985851\n",
      "40     \t [-0.32640743  0.26641513]. \t  \u001b[92m-4.31530451310544\u001b[0m \t -4.31530451310544\n",
      "41     \t [-0.32670085  0.2665712 ]. \t  \u001b[92m-4.314945976471961\u001b[0m \t -4.314945976471961\n",
      "42     \t [-0.32698724  0.26672384]. \t  \u001b[92m-4.314600725362412\u001b[0m \t -4.314600725362412\n",
      "43     \t [-0.3272691   0.26687371]. \t  \u001b[92m-4.314245386690139\u001b[0m \t -4.314245386690139\n",
      "44     \t [-0.32755269  0.26702474]. \t  \u001b[92m-4.313890456864827\u001b[0m \t -4.313890456864827\n",
      "45     \t [-0.32783424  0.26717541]. \t  \u001b[92m-4.313556718286056\u001b[0m \t -4.313556718286056\n",
      "46     \t [-0.32811286  0.26732421]. \t  \u001b[92m-4.313212134143888\u001b[0m \t -4.313212134143888\n",
      "47     \t [-0.32838699  0.26747074]. \t  \u001b[92m-4.312872730414352\u001b[0m \t -4.312872730414352\n",
      "48     \t [-0.32867006  0.26762238]. \t  \u001b[92m-4.312527968994049\u001b[0m \t -4.312527968994049\n",
      "49     \t [-0.32894331  0.26776913]. \t  \u001b[92m-4.312202947949917\u001b[0m \t -4.312202947949917\n",
      "50     \t [-0.32922012  0.26791758]. \t  \u001b[92m-4.311862097664118\u001b[0m \t -4.311862097664118\n",
      "51     \t [-0.32949615  0.26806582]. \t  \u001b[92m-4.311524714080877\u001b[0m \t -4.311524714080877\n",
      "52     \t [-0.32977183  0.26821408]. \t  \u001b[92m-4.311189604892193\u001b[0m \t -4.311189604892193\n",
      "53     \t [-0.33004514  0.26836144]. \t  \u001b[92m-4.3108649086064705\u001b[0m \t -4.3108649086064705\n",
      "54     \t [-0.33032293  0.2685113 ]. \t  \u001b[92m-4.310533241212904\u001b[0m \t -4.310533241212904\n",
      "55     \t [-0.33059845  0.26865988]. \t  \u001b[92m-4.310198234619368\u001b[0m \t -4.310198234619368\n",
      "56     \t [-0.33087747  0.26881064]. \t  \u001b[92m-4.309863441946476\u001b[0m \t -4.309863441946476\n",
      "57     \t [-0.33115547  0.26896103]. \t  \u001b[92m-4.309531227098404\u001b[0m \t -4.309531227098404\n",
      "58     \t [-0.33143423  0.26911207]. \t  \u001b[92m-4.309200974831379\u001b[0m \t -4.309200974831379\n",
      "59     \t [-0.33171514  0.26926447]. \t  \u001b[92m-4.308869584414821\u001b[0m \t -4.308869584414821\n",
      "60     \t [-0.33199743  0.2694176 ]. \t  \u001b[92m-4.3085315427409645\u001b[0m \t -4.3085315427409645\n",
      "61     \t [-0.33228298  0.26957275]. \t  \u001b[92m-4.308192956400651\u001b[0m \t -4.308192956400651\n",
      "62     \t [-0.33256968  0.2697287 ]. \t  \u001b[92m-4.307853675401089\u001b[0m \t -4.307853675401089\n",
      "63     \t [-0.33285898  0.26988633]. \t  \u001b[92m-4.3075150875763\u001b[0m \t -4.3075150875763\n",
      "64     \t [-0.3331501   0.27004509]. \t  \u001b[92m-4.3071734144512135\u001b[0m \t -4.3071734144512135\n",
      "65     \t [-0.33344354  0.27020542]. \t  \u001b[92m-4.306834088490168\u001b[0m \t -4.306834088490168\n",
      "66     \t [-0.33374128  0.27036817]. \t  \u001b[92m-4.3064865623407815\u001b[0m \t -4.3064865623407815\n",
      "67     \t [-0.33404172  0.27053248]. \t  \u001b[92m-4.306133203654658\u001b[0m \t -4.306133203654658\n",
      "68     \t [-0.33434784  0.27070033]. \t  \u001b[92m-4.305781735325935\u001b[0m \t -4.305781735325935\n",
      "69     \t [-0.33465633  0.27086963]. \t  \u001b[92m-4.305426969948025\u001b[0m \t -4.305426969948025\n",
      "70     \t [-0.3349664   0.27103986]. \t  \u001b[92m-4.305066535818891\u001b[0m \t -4.305066535818891\n",
      "71     \t [-0.33528666  0.27121613]. \t  \u001b[92m-4.30470219811867\u001b[0m \t -4.30470219811867\n",
      "72     \t [-0.33560894  0.27139355]. \t  \u001b[92m-4.304330850856861\u001b[0m \t -4.304330850856861\n",
      "73     \t [-0.33593447  0.27157318]. \t  \u001b[92m-4.30396310105237\u001b[0m \t -4.30396310105237\n",
      "74     \t [-0.3362657   0.27175591]. \t  \u001b[92m-4.303581020089461\u001b[0m \t -4.303581020089461\n",
      "75     \t [-0.33660392  0.27194291]. \t  \u001b[92m-4.303197627278815\u001b[0m \t -4.303197627278815\n",
      "76     \t [-0.3369492   0.27213411]. \t  \u001b[92m-4.302808264876201\u001b[0m \t -4.302808264876201\n",
      "77     \t [-0.33729774  0.27232754]. \t  \u001b[92m-4.302421966361091\u001b[0m \t -4.302421966361091\n",
      "78     \t [-0.33765809  0.27252754]. \t  \u001b[92m-4.302015732428565\u001b[0m \t -4.302015732428565\n",
      "79     \t [-0.33802359  0.27273073]. \t  \u001b[92m-4.301606062895025\u001b[0m \t -4.301606062895025\n",
      "80     \t [-0.33839698  0.27293883]. \t  \u001b[92m-4.301196084358963\u001b[0m \t -4.301196084358963\n",
      "81     \t [-0.33878094  0.27315284]. \t  \u001b[92m-4.300766939499743\u001b[0m \t -4.300766939499743\n",
      "82     \t [-0.33917176  0.27337124]. \t  \u001b[92m-4.30033884718764\u001b[0m \t -4.30033884718764\n",
      "83     \t [-0.33957364  0.27359616]. \t  \u001b[92m-4.299900479694908\u001b[0m \t -4.299900479694908\n",
      "84     \t [-0.33998244  0.27382515]. \t  \u001b[92m-4.299450671928422\u001b[0m \t -4.299450671928422\n",
      "85     \t [-0.34040364  0.27406187]. \t  \u001b[92m-4.2990021066766975\u001b[0m \t -4.2990021066766975\n",
      "86     \t [-0.34084166  0.27430818]. \t  \u001b[92m-4.298528832718621\u001b[0m \t -4.298528832718621\n",
      "87     \t [-0.34128696  0.27455885]. \t  \u001b[92m-4.298044356988016\u001b[0m \t -4.298044356988016\n",
      "88     \t [-0.34174817  0.27481942]. \t  \u001b[92m-4.2975607162039555\u001b[0m \t -4.2975607162039555\n",
      "89     \t [-0.34222351  0.27508826]. \t  \u001b[92m-4.297058102715629\u001b[0m \t -4.297058102715629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [-0.34271267  0.27536558]. \t  \u001b[92m-4.296548100372338\u001b[0m \t -4.296548100372338\n",
      "91     \t [-0.34322305  0.27565524]. \t  \u001b[92m-4.296010792331295\u001b[0m \t -4.296010792331295\n",
      "92     \t [-0.34375106  0.27595565]. \t  \u001b[92m-4.295462469251531\u001b[0m \t -4.295462469251531\n",
      "93     \t [-0.34429395  0.27626538]. \t  \u001b[92m-4.294908833264353\u001b[0m \t -4.294908833264353\n",
      "94     \t [-0.3448614   0.27658968]. \t  \u001b[92m-4.2943290758135895\u001b[0m \t -4.2943290758135895\n",
      "95     \t [-0.34545666  0.27693053]. \t  \u001b[92m-4.293721531774876\u001b[0m \t -4.293721531774876\n",
      "96     \t [-0.34606838  0.27728199]. \t  \u001b[92m-4.293113121156109\u001b[0m \t -4.293113121156109\n",
      "97     \t [-0.34671389  0.27765351]. \t  \u001b[92m-4.29246789566439\u001b[0m \t -4.29246789566439\n",
      "98     \t [-0.34738538  0.27804127]. \t  \u001b[92m-4.2918111156759835\u001b[0m \t -4.2918111156759835\n",
      "99     \t [-0.34809019  0.27844918]. \t  \u001b[92m-4.2911224554830705\u001b[0m \t -4.2911224554830705\n",
      "100    \t [-0.34882747  0.2788773 ]. \t  \u001b[92m-4.290415523233158\u001b[0m \t -4.290415523233158\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.27224766 0.94496041]. \t  -76.36613704622805 \t -62.0309701572776\n",
      "2      \t [1.5909853  1.72063139]. \t  -66.05695818099366 \t -62.0309701572776\n",
      "3      \t [0.35539481 0.16801966]. \t  \u001b[92m-0.5895232305615902\u001b[0m \t -0.5895232305615902\n",
      "4      \t [0.93710448 1.09376284]. \t  -4.65220745962002 \t -0.5895232305615902\n",
      "5      \t [0.69722759 0.47980091]. \t  \u001b[92m-0.0956721992899204\u001b[0m \t -0.0956721992899204\n",
      "6      \t [-1.03562666  2.048     ]. \t  -99.29939408764791 \t -0.0956721992899204\n",
      "7      \t [ 0.35995367 -0.23149537]. \t  -13.446236992658328 \t -0.0956721992899204\n",
      "8      \t [1.40600198 1.17919458]. \t  -63.788907908016625 \t -0.0956721992899204\n",
      "9      \t [0.63081713 2.048     ]. \t  -272.4093128222167 \t -0.0956721992899204\n",
      "10     \t [-1.1790493   0.99081094]. \t  -20.696003565273884 \t -0.0956721992899204\n",
      "11     \t [0.93374431 0.83468435]. \t  -0.14272983753796187 \t -0.0956721992899204\n",
      "12     \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.0956721992899204\n",
      "13     \t [-0.79228979  1.09779712]. \t  -25.309259774094695 \t -0.0956721992899204\n",
      "14     \t [0.51086427 0.06834821]. \t  -3.9500431779286873 \t -0.0956721992899204\n",
      "15     \t [-0.73801546  0.54397985]. \t  -3.020744935965769 \t -0.0956721992899204\n",
      "16     \t [-0.74549232  0.72389639]. \t  -5.8737683303499795 \t -0.0956721992899204\n",
      "17     \t [-0.47951873  0.40136292]. \t  -5.127618718711332 \t -0.0956721992899204\n",
      "18     \t [0.8307351 0.7337665]. \t  -0.2191452377417425 \t -0.0956721992899204\n",
      "19     \t [0.84861152 0.71236722]. \t  \u001b[92m-0.028962439143223388\u001b[0m \t -0.028962439143223388\n",
      "20     \t [0.85332991 0.72364152]. \t  \u001b[92m-0.023564578910022414\u001b[0m \t -0.023564578910022414\n",
      "21     \t [0.85475235 0.72679075]. \t  \u001b[92m-0.022549118296720457\u001b[0m \t -0.022549118296720457\n",
      "22     \t [0.4787216  0.23825696]. \t  -0.27998052216181996 \t -0.022549118296720457\n",
      "23     \t [0.85123685 0.71692339]. \t  -0.02802992581700818 \t -0.022549118296720457\n",
      "24     \t [0.85198207 0.71860367]. \t  -0.027194273993138857 \t -0.022549118296720457\n",
      "25     \t [0.85243961 0.71963069]. \t  -0.02670576610801386 \t -0.022549118296720457\n",
      "26     \t [0.8527285  0.72027938]. \t  -0.0264038026695397 \t -0.022549118296720457\n",
      "27     \t [0.85295188 0.72077272]. \t  -0.02618505389629749 \t -0.022549118296720457\n",
      "28     \t [0.85308461 0.72108088]. \t  -0.02603631543712075 \t -0.022549118296720457\n",
      "29     \t [0.85319669 0.72133412]. \t  -0.0259210430168221 \t -0.022549118296720457\n",
      "30     \t [0.85328681 0.72153656]. \t  -0.02583050619377624 \t -0.022549118296720457\n",
      "31     \t [0.85334968 0.72168188]. \t  -0.025762314532427628 \t -0.022549118296720457\n",
      "32     \t [0.85340432 0.7218047 ]. \t  -0.025707792833566376 \t -0.022549118296720457\n",
      "33     \t [0.85344756 0.72190224]. \t  -0.025664347213637508 \t -0.022549118296720457\n",
      "34     \t [0.85348083 0.72198282]. \t  -0.025623865858599874 \t -0.022549118296720457\n",
      "35     \t [0.85351126 0.72205435]. \t  -0.02558973809674533 \t -0.022549118296720457\n",
      "36     \t [0.85354386 0.72212645]. \t  -0.025559069872652097 \t -0.022549118296720457\n",
      "37     \t [0.85356122 0.72216782]. \t  -0.02553896500116768 \t -0.022549118296720457\n",
      "38     \t [0.85358402 0.7222194 ]. \t  -0.02551609153434406 \t -0.022549118296720457\n",
      "39     \t [0.85360091 0.72225906]. \t  -0.02549733427496361 \t -0.022549118296720457\n",
      "40     \t [0.85361708 0.7222945 ]. \t  -0.025482621775149383 \t -0.022549118296720457\n",
      "41     \t [0.85362716 0.72231906]. \t  -0.025470308841003316 \t -0.022549118296720457\n",
      "42     \t [0.85364135 0.7223487 ]. \t  -0.02545927787151714 \t -0.022549118296720457\n",
      "43     \t [0.85364631 0.72236776]. \t  -0.02544436830679634 \t -0.022549118296720457\n",
      "44     \t [0.85365878 0.7223897 ]. \t  -0.0254398936269549 \t -0.022549118296720457\n",
      "45     \t [0.85366927 0.72241575]. \t  -0.025426496643858154 \t -0.022549118296720457\n",
      "46     \t [0.85367388 0.722429  ]. \t  -0.025418350967722644 \t -0.022549118296720457\n",
      "47     \t [0.85368079 0.72244395]. \t  -0.025412325862053907 \t -0.022549118296720457\n",
      "48     \t [0.85368513 0.72245712]. \t  -0.025403775963664322 \t -0.022549118296720457\n",
      "49     \t [0.85369111 0.72246992]. \t  -0.025398752072655482 \t -0.022549118296720457\n",
      "50     \t [0.85369529 0.72248101]. \t  -0.0253925269894774 \t -0.022549118296720457\n",
      "51     \t [0.85369871 0.72248657]. \t  -0.025391889979814487 \t -0.022549118296720457\n",
      "52     \t [0.85370285 0.72250061]. \t  -0.02538187840591263 \t -0.022549118296720457\n",
      "53     \t [0.85370898 0.72251438]. \t  -0.02537592402794337 \t -0.022549118296720457\n",
      "54     \t [0.85370987 0.72251887]. \t  -0.0253719081739326 \t -0.022549118296720457\n",
      "55     \t [0.85371929 0.72253645]. \t  -0.025367256369679625 \t -0.022549118296720457\n",
      "56     \t [0.85371636 0.72253415]. \t  -0.02536471560162503 \t -0.022549118296720457\n",
      "57     \t [0.85372371 0.72254779]. \t  -0.02536120842271048 \t -0.022549118296720457\n",
      "58     \t [0.85372606 0.72255277]. \t  -0.02535928191956041 \t -0.022549118296720457\n",
      "59     \t [0.85372303 0.7225512 ]. \t  -0.025355643095778133 \t -0.022549118296720457\n",
      "60     \t [0.8537241  0.72255401]. \t  -0.025354099113456145 \t -0.022549118296720457\n",
      "61     \t [0.85372908 0.72256411]. \t  -0.02535062161187667 \t -0.022549118296720457\n",
      "62     \t [0.85373788 0.72257805]. \t  -0.025349428177990817 \t -0.022549118296720457\n",
      "63     \t [0.85373491 0.72257734]. \t  -0.0253448049507522 \t -0.022549118296720457\n",
      "64     \t [0.85373437 0.72258   ]. \t  -0.025340458279943745 \t -0.022549118296720457\n",
      "65     \t [0.85373875 0.7225876 ]. \t  -0.025339026016373143 \t -0.022549118296720457\n",
      "66     \t [0.85373609 0.72258481]. \t  -0.025337589294437822 \t -0.022549118296720457\n",
      "67     \t [0.85374062 0.7225865 ]. \t  -0.025343867919178906 \t -0.022549118296720457\n",
      "68     \t [0.85374317 0.72259606]. \t  -0.025336585529782945 \t -0.022549118296720457\n",
      "69     \t [0.85374115 0.72259588]. \t  -0.02533307889215084 \t -0.022549118296720457\n",
      "70     \t [0.85374382 0.72260294]. \t  -0.025329149988449474 \t -0.022549118296720457\n",
      "71     \t [0.8537434  0.72259744]. \t  -0.02533527334532554 \t -0.022549118296720457\n",
      "72     \t [0.85374666 0.72260913]. \t  -0.025326640706666036 \t -0.022549118296720457\n",
      "73     \t [0.85374773 0.72261039]. \t  -0.025327033150915463 \t -0.022549118296720457\n",
      "74     \t [0.85374905 0.72261143]. \t  -0.02532817861319909 \t -0.022549118296720457\n",
      "75     \t [0.85374887 0.72261305]. \t  -0.02532580574318802 \t -0.022549118296720457\n",
      "76     \t [0.85375922 0.72262821]. \t  -0.025325920417771527 \t -0.022549118296720457\n",
      "77     \t [0.85375204 0.72262051]. \t  -0.02532230926914742 \t -0.022549118296720457\n",
      "78     \t [0.85375429 0.72262658]. \t  -0.02531884329685383 \t -0.022549118296720457\n",
      "79     \t [0.85375415 0.72262594]. \t  -0.02531939068962498 \t -0.022549118296720457\n",
      "80     \t [0.85375356 0.7226236 ]. \t  -0.02532124492280792 \t -0.022549118296720457\n",
      "81     \t [0.85375642 0.72262664]. \t  -0.025322726736240267 \t -0.022549118296720457\n",
      "82     \t [0.85375556 0.72262742]. \t  -0.02532015466378409 \t -0.022549118296720457\n",
      "83     \t [0.85375655 0.72263048]. \t  -0.02531814827616475 \t -0.022549118296720457\n",
      "84     \t [0.85375121 0.72262534]. \t  -0.02531472269552565 \t -0.022549118296720457\n",
      "85     \t [0.85376035 0.72263508]. \t  -0.025319386642797925 \t -0.022549118296720457\n",
      "86     \t [0.85375804 0.72263478]. \t  -0.025315510970090483 \t -0.022549118296720457\n",
      "87     \t [0.85376037 0.72263729]. \t  -0.02531667624007599 \t -0.022549118296720457\n",
      "88     \t [0.85375027 0.7226259 ]. \t  -0.02531228849574948 \t -0.022549118296720457\n",
      "89     \t [0.85375695 0.72263548]. \t  -0.025312617916420456 \t -0.022549118296720457\n",
      "90     \t [0.85375707 0.72264017]. \t  -0.02530696726250248 \t -0.022549118296720457\n",
      "91     \t [0.85376445 0.72265384]. \t  -0.025303472290534874 \t -0.022549118296720457\n",
      "92     \t [0.85376157 0.72264475]. \t  -0.02530953859485643 \t -0.022549118296720457\n",
      "93     \t [0.85376941 0.72265896]. \t  -0.02530620859997063 \t -0.022549118296720457\n",
      "94     \t [0.8537631  0.72264672]. \t  -0.02530988946075681 \t -0.022549118296720457\n",
      "95     \t [0.85376216 0.72264806]. \t  -0.025306479393210537 \t -0.022549118296720457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.85376422 0.72265041]. \t  -0.025307326536572586 \t -0.022549118296720457\n",
      "97     \t [0.85376354 0.72265084]. \t  -0.025305546795011846 \t -0.022549118296720457\n",
      "98     \t [0.85375484 0.7226388 ]. \t  -0.02530456761293235 \t -0.022549118296720457\n",
      "99     \t [0.85376565 0.72265112]. \t  -0.025309095284600184 \t -0.022549118296720457\n",
      "100    \t [0.85376331 0.72265103]. \t  -0.025304877215636748 \t -0.022549118296720457\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [1.51129244 0.53614644]. \t  -305.7623191685012 \t -4.306489127802793\n",
      "2      \t [0.29162456 0.47434286]. \t  -15.657087228926631 \t -4.306489127802793\n",
      "3      \t [0.61452449 0.48084942]. \t  \u001b[92m-1.2138024752315941\u001b[0m \t -1.2138024752315941\n",
      "4      \t [1.41223364 2.02081738]. \t  \u001b[92m-0.23970402036945168\u001b[0m \t -0.23970402036945168\n",
      "5      \t [1.09719271 1.57511001]. \t  -13.794194161704713 \t -0.23970402036945168\n",
      "6      \t [ 0.19402663 -0.22774983]. \t  -7.693105096954872 \t -0.23970402036945168\n",
      "7      \t [2.01205266 2.048     ]. \t  -401.1666190371779 \t -0.23970402036945168\n",
      "8      \t [1.01955893 2.048     ]. \t  -101.7075257255709 \t -0.23970402036945168\n",
      "9      \t [-1.54569214  2.048     ]. \t  -18.119849791755467 \t -0.23970402036945168\n",
      "10     \t [0.39806945 0.05037773]. \t  -1.5304827125030858 \t -0.23970402036945168\n",
      "11     \t [1.36365425 2.048     ]. \t  -3.6834746509471157 \t -0.23970402036945168\n",
      "12     \t [1.35559358 1.61452875]. \t  -5.104040379259226 \t -0.23970402036945168\n",
      "13     \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.23970402036945168\n",
      "14     \t [-0.98786162  2.048     ]. \t  -118.89774381716006 \t -0.23970402036945168\n",
      "15     \t [-1.32351318  1.61032742]. \t  -7.396970857785169 \t -0.23970402036945168\n",
      "16     \t [-1.35932308  1.91436939]. \t  -6.010096668964739 \t -0.23970402036945168\n",
      "17     \t [-1.2752763   1.19842023]. \t  -23.487528734768453 \t -0.23970402036945168\n",
      "18     \t [0.57265234 0.78786652]. \t  -21.336722147799826 \t -0.23970402036945168\n",
      "19     \t [1.33615169 1.83416628]. \t  -0.3517762034403464 \t -0.23970402036945168\n",
      "20     \t [-1.39427158  2.048     ]. \t  -6.814276795303412 \t -0.23970402036945168\n",
      "21     \t [-1.45800087  1.59657359]. \t  -34.046286741010725 \t -0.23970402036945168\n",
      "22     \t [-0.91170083  0.94427454]. \t  -4.93322131629457 \t -0.23970402036945168\n",
      "23     \t [1.18256563 1.42117309]. \t  \u001b[92m-0.08491196519028466\u001b[0m \t -0.08491196519028466\n",
      "24     \t [-0.08858245  0.01501531]. \t  -1.1901504344808032 \t -0.08491196519028466\n",
      "25     \t [0.10881139 0.025895  ]. \t  -0.8139716690671379 \t -0.08491196519028466\n",
      "26     \t [1.27138466 1.62462896]. \t  \u001b[92m-0.08039005631022353\u001b[0m \t -0.08039005631022353\n",
      "27     \t [-0.59937956 -1.57977013]. \t  -378.5401960122858 \t -0.08039005631022353\n",
      "28     \t [1.26775142 1.61414087]. \t  \u001b[92m-0.07651717344093344\u001b[0m \t -0.07651717344093344\n",
      "29     \t [1.26700372 1.61152106]. \t  \u001b[92m-0.07516310058567967\u001b[0m \t -0.07516310058567967\n",
      "30     \t [1.26672656 1.61047284]. \t  \u001b[92m-0.0745965764986864\u001b[0m \t -0.0745965764986864\n",
      "31     \t [1.2665884  1.60991563]. \t  \u001b[92m-0.07428364561933955\u001b[0m \t -0.07428364561933955\n",
      "32     \t [0.47100683 0.23636411]. \t  -0.3009071859342622 \t -0.07428364561933955\n",
      "33     \t [1.26819178 1.61373779]. \t  -0.07487249997160185 \t -0.07428364561933955\n",
      "34     \t [1.26793829 1.61302188]. \t  -0.07465787281924911 \t -0.07428364561933955\n",
      "35     \t [1.2677999  1.61262493]. \t  -0.07453464503397879 \t -0.07428364561933955\n",
      "36     \t [1.26771381 1.61235065]. \t  -0.07442939879781055 \t -0.07428364561933955\n",
      "37     \t [1.26766144 1.61217553]. \t  -0.07435705360009232 \t -0.07428364561933955\n",
      "38     \t [1.26761863 1.61203006]. \t  -0.07429580926269866 \t -0.07428364561933955\n",
      "39     \t [1.26758337 1.61191574]. \t  \u001b[92m-0.07425119193188902\u001b[0m \t -0.07425119193188902\n",
      "40     \t [1.26756035 1.61183716]. \t  \u001b[92m-0.07421809267086474\u001b[0m \t -0.07421809267086474\n",
      "41     \t [1.2675577  1.61181247]. \t  \u001b[92m-0.07419827842623317\u001b[0m \t -0.07419827842623317\n",
      "42     \t [1.26753225 1.61172506]. \t  \u001b[92m-0.07416132400152405\u001b[0m \t -0.07416132400152405\n",
      "43     \t [1.26753374 1.61170926]. \t  \u001b[92m-0.07414223063988275\u001b[0m \t -0.07414223063988275\n",
      "44     \t [1.26752137 1.6116518 ]. \t  \u001b[92m-0.07410922899452464\u001b[0m \t -0.07410922899452464\n",
      "45     \t [1.26753467 1.6116812 ]. \t  -0.07411199984121167 \t -0.07410922899452464\n",
      "46     \t [1.26751994 1.61162771]. \t  \u001b[92m-0.07408788032662583\u001b[0m \t -0.07408788032662583\n",
      "47     \t [1.2675193  1.61162705]. \t  -0.07408850232652399 \t -0.07408788032662583\n",
      "48     \t [1.26750631 1.61157791]. \t  \u001b[92m-0.07406530135249073\u001b[0m \t -0.07406530135249073\n",
      "49     \t [1.26751997 1.61160047]. \t  \u001b[92m-0.07406052471770307\u001b[0m \t -0.07406052471770307\n",
      "50     \t [1.26750055 1.6115503 ]. \t  \u001b[92m-0.07404920922165055\u001b[0m \t -0.07404920922165055\n",
      "51     \t [1.26752372 1.61159371]. \t  \u001b[92m-0.0740463247713874\u001b[0m \t -0.0740463247713874\n",
      "52     \t [1.26752586 1.61158799]. \t  \u001b[92m-0.07403639535858653\u001b[0m \t -0.07403639535858653\n",
      "53     \t [1.26752901 1.61159113]. \t  \u001b[92m-0.0740332503288869\u001b[0m \t -0.0740332503288869\n",
      "54     \t [1.26753268 1.61159631]. \t  \u001b[92m-0.07403112578134792\u001b[0m \t -0.07403112578134792\n",
      "55     \t [1.2675269  1.61157735]. \t  \u001b[92m-0.07402377676496104\u001b[0m \t -0.07402377676496104\n",
      "56     \t [1.26750774 1.61151735]. \t  \u001b[92m-0.07400221411495547\u001b[0m \t -0.07400221411495547\n",
      "57     \t [1.26751438 1.61153868]. \t  -0.07401020526528149 \t -0.07400221411495547\n",
      "58     \t [1.26752178 1.6115484 ]. \t  -0.07400522458129918 \t -0.07400221411495547\n",
      "59     \t [1.26752119 1.61154398]. \t  \u001b[92m-0.07400203121145302\u001b[0m \t -0.07400203121145302\n",
      "60     \t [1.26752623 1.61154856]. \t  \u001b[92m-0.0739966524977366\u001b[0m \t -0.0739966524977366\n",
      "61     \t [1.2675131  1.61152111]. \t  \u001b[92m-0.07399538495149571\u001b[0m \t -0.07399538495149571\n",
      "62     \t [1.26752606 1.61154142]. \t  \u001b[92m-0.07398994393170016\u001b[0m \t -0.07398994393170016\n",
      "63     \t [1.26752897 1.611542  ]. \t  \u001b[92m-0.07398483130502\u001b[0m \t -0.07398483130502\n",
      "64     \t [1.26752549 1.61153077]. \t  \u001b[92m-0.0739805958896588\u001b[0m \t -0.0739805958896588\n",
      "65     \t [1.26753021 1.61154217]. \t  -0.0739825784526512 \t -0.0739805958896588\n",
      "66     \t [1.26754041 1.61156444]. \t  -0.07398450420735408 \t -0.0739805958896588\n",
      "67     \t [1.2675223  1.61151897]. \t  \u001b[92m-0.07397526045111769\u001b[0m \t -0.07397526045111769\n",
      "68     \t [1.26753997 1.61156424]. \t  -0.07398517604139111 \t -0.07397526045111769\n",
      "69     \t [1.26750332 1.61146366]. \t  \u001b[92m-0.07395803437842616\u001b[0m \t -0.07395803437842616\n",
      "70     \t [1.26751722 1.61149831]. \t  -0.07396489935236897 \t -0.07395803437842616\n",
      "71     \t [1.26751497 1.61149676]. \t  -0.0739677567572887 \t -0.07395803437842616\n",
      "72     \t [1.26753084 1.61152686]. \t  -0.07396634151060216 \t -0.07395803437842616\n",
      "73     \t [1.2675386  1.61154191]. \t  -0.07396597406723127 \t -0.07395803437842616\n",
      "74     \t [1.26752766 1.61151434]. \t  -0.07396028463178345 \t -0.07395803437842616\n",
      "75     \t [1.26753777 1.61153716]. \t  -0.07396293550773105 \t -0.07395803437842616\n",
      "76     \t [1.26752249 1.61149091]. \t  \u001b[92m-0.0739474268565307\u001b[0m \t -0.0739474268565307\n",
      "77     \t [1.26752922 1.61151376]. \t  -0.0739566765780841 \t -0.0739474268565307\n",
      "78     \t [1.26752957 1.61151089]. \t  -0.07395319154939235 \t -0.0739474268565307\n",
      "79     \t [1.2675314  1.61151321]. \t  -0.07395192421202827 \t -0.0739474268565307\n",
      "80     \t [1.26753344 1.61152289]. \t  -0.07395740324817683 \t -0.0739474268565307\n",
      "81     \t [1.26753064 1.61150957]. \t  -0.0739498311829048 \t -0.0739474268565307\n",
      "82     \t [1.26753588 1.61151935]. \t  -0.07394922271815818 \t -0.0739474268565307\n",
      "83     \t [1.26753443 1.61151699]. \t  -0.07394972765007343 \t -0.0739474268565307\n",
      "84     \t [1.26753968 1.61152891]. \t  -0.07395117775220181 \t -0.0739474268565307\n",
      "85     \t [1.26754254 1.6115325 ]. \t  -0.07394914793473831 \t -0.0739474268565307\n",
      "86     \t [1.26753254 1.61151148]. \t  -0.07394801161964118 \t -0.0739474268565307\n",
      "87     \t [1.26753317 1.61150849]. \t  \u001b[92m-0.07394388359337738\u001b[0m \t -0.07394388359337738\n",
      "88     \t [1.26753412 1.61150788]. \t  \u001b[92m-0.07394145605090913\u001b[0m \t -0.07394145605090913\n",
      "89     \t [1.26751625 1.61147341]. \t  -0.07394245164036953 \t -0.07394145605090913\n",
      "90     \t [1.26754317 1.61152776]. \t  -0.07394332202050213 \t -0.07394145605090913\n",
      "91     \t [1.26753592 1.61151178]. \t  -0.07394178930591891 \t -0.07394145605090913\n",
      "92     \t [1.26754639 1.61153112]. \t  \u001b[92m-0.07394038072916322\u001b[0m \t -0.07394038072916322\n",
      "93     \t [1.26754052 1.61151885]. \t  \u001b[92m-0.07393977712390024\u001b[0m \t -0.07393977712390024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     \t [1.26754854 1.61153775]. \t  -0.07394266472884138 \t -0.07393977712390024\n",
      "95     \t [1.26753949 1.61151506]. \t  \u001b[92m-0.07393807521589847\u001b[0m \t -0.07393807521589847\n",
      "96     \t [1.26754062 1.61151568]. \t  \u001b[92m-0.07393650843020627\u001b[0m \t -0.07393650843020627\n",
      "97     \t [1.26754278 1.61151954]. \t  \u001b[92m-0.07393607559577028\u001b[0m \t -0.07393607559577028\n",
      "98     \t [1.26754035 1.61151663]. \t  -0.0739379391531692 \t -0.07393607559577028\n",
      "99     \t [1.26753991 1.61150527]. \t  \u001b[92m-0.07392776567205865\u001b[0m \t -0.07392776567205865\n",
      "100    \t [1.26753289 1.61149937]. \t  -0.07393554838644924 \t -0.07392776567205865\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-1.98800207  0.72912528]. \t  -1047.718432081089 \t -6.867717811955245\n",
      "2      \t [-0.35955497  0.27951486]. \t  \u001b[92m-4.105447783320992\u001b[0m \t -4.105447783320992\n",
      "3      \t [-0.58608294  0.78186479]. \t  -21.732623317033294 \t -4.105447783320992\n",
      "4      \t [1.40302617 2.048     ]. \t  \u001b[92m-0.7947344243711687\u001b[0m \t -0.7947344243711687\n",
      "5      \t [-0.81147885  0.00468558]. \t  -46.02851392526419 \t -0.7947344243711687\n",
      "6      \t [1.49322395 0.39985631]. \t  -335.08256379891264 \t -0.7947344243711687\n",
      "7      \t [0.96196967 2.048     ]. \t  -126.02774297430138 \t -0.7947344243711687\n",
      "8      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.7947344243711687\n",
      "9      \t [1.19342651 1.50162118]. \t  \u001b[92m-0.6357833365257884\u001b[0m \t -0.6357833365257884\n",
      "10     \t [-0.03414516 -0.26527962]. \t  -8.168777147622194 \t -0.6357833365257884\n",
      "11     \t [1.29476811 1.77377647]. \t  -1.0346294928681294 \t -0.6357833365257884\n",
      "12     \t [-0.80017651  0.62050126]. \t  -3.2797649960846114 \t -0.6357833365257884\n",
      "13     \t [-0.33168142 -0.04217276]. \t  -4.089412901531907 \t -0.6357833365257884\n",
      "14     \t [0.50345839 0.51051781]. \t  -6.853893291900087 \t -0.6357833365257884\n",
      "15     \t [0.15038425 0.08541827]. \t  -1.1162667173735965 \t -0.6357833365257884\n",
      "16     \t [0.73843075 1.06455739]. \t  -27.033321731995645 \t -0.6357833365257884\n",
      "17     \t [1.2781676  1.48972096]. \t  -2.1507311496966977 \t -0.6357833365257884\n",
      "18     \t [-0.96520685  1.01765995]. \t  -4.602251982269667 \t -0.6357833365257884\n",
      "19     \t [0.07593188 0.27353761]. \t  -8.024084230925949 \t -0.6357833365257884\n",
      "20     \t [ 0.32375712 -0.03095978]. \t  -2.3008832349148873 \t -0.6357833365257884\n",
      "21     \t [-1.88210729 -2.048     ]. \t  -3133.483099565445 \t -0.6357833365257884\n",
      "22     \t [1.36484011 2.048     ]. \t  -3.5634373164965583 \t -0.6357833365257884\n",
      "23     \t [1.29063859 1.65195742]. \t  \u001b[92m-0.10348874548248496\u001b[0m \t -0.10348874548248496\n",
      "24     \t [1.06523479 1.17061004]. \t  -0.13302808263069132 \t -0.10348874548248496\n",
      "25     \t [1.16844425 1.3742984 ]. \t  \u001b[92m-0.03653919647753083\u001b[0m \t -0.03653919647753083\n",
      "26     \t [1.1798427  1.39762702]. \t  \u001b[92m-0.035477406436521516\u001b[0m \t -0.035477406436521516\n",
      "27     \t [1.29840745 1.67716801]. \t  -0.09660538417180162 \t -0.035477406436521516\n",
      "28     \t [1.16920701 1.37505669]. \t  \u001b[92m-0.0350496690023932\u001b[0m \t -0.0350496690023932\n",
      "29     \t [1.17205534 1.38089092]. \t  \u001b[92m-0.034754251200706715\u001b[0m \t -0.034754251200706715\n",
      "30     \t [1.17383323 1.38453293]. \t  \u001b[92m-0.03463821374457157\u001b[0m \t -0.03463821374457157\n",
      "31     \t [1.29357832 1.66506672]. \t  -0.09304099306409858 \t -0.03463821374457157\n",
      "32     \t [1.16675869 1.36968911]. \t  -0.034802871675825686 \t -0.03463821374457157\n",
      "33     \t [1.16794497 1.37212686]. \t  -0.03465586412359951 \t -0.03463821374457157\n",
      "34     \t [1.16876423 1.37381503]. \t  \u001b[92m-0.034573497734586574\u001b[0m \t -0.034573497734586574\n",
      "35     \t [1.16938272 1.37507424]. \t  \u001b[92m-0.03449433798889193\u001b[0m \t -0.03449433798889193\n",
      "36     \t [1.16984616 1.37603229]. \t  \u001b[92m-0.03446109548725398\u001b[0m \t -0.03446109548725398\n",
      "37     \t [1.17019506 1.37674992]. \t  \u001b[92m-0.034432666290415886\u001b[0m \t -0.034432666290415886\n",
      "38     \t [1.17045874 1.37728694]. \t  \u001b[92m-0.03440458443506712\u001b[0m \t -0.03440458443506712\n",
      "39     \t [1.29540786 1.67029471]. \t  -0.09332923953434001 \t -0.03440458443506712\n",
      "40     \t [1.16629365 1.36857689]. \t  -0.034602488241095214 \t -0.03440458443506712\n",
      "41     \t [1.16679867 1.36961673]. \t  -0.0345418466058698 \t -0.03440458443506712\n",
      "42     \t [1.16710665 1.37025406]. \t  -0.034511779881857926 \t -0.03440458443506712\n",
      "43     \t [1.16742122 1.3709039 ]. \t  -0.03448050936514682 \t -0.03440458443506712\n",
      "44     \t [1.16757667 1.37122612]. \t  -0.03446727097321357 \t -0.03440458443506712\n",
      "45     \t [1.16773758 1.37155288]. \t  -0.03444313248491819 \t -0.03440458443506712\n",
      "46     \t [1.16792565 1.37197875]. \t  -0.034485010617192315 \t -0.03440458443506712\n",
      "47     \t [1.16808934 1.37226814]. \t  \u001b[92m-0.03439343342601591\u001b[0m \t -0.03439343342601591\n",
      "48     \t [1.16820629 1.37251798]. \t  -0.03439614224125642 \t -0.03439343342601591\n",
      "49     \t [1.1682771  1.37265563]. \t  \u001b[92m-0.034376651742106024\u001b[0m \t -0.034376651742106024\n",
      "50     \t [1.16843306 1.37297839]. \t  \u001b[92m-0.03436444572623259\u001b[0m \t -0.03436444572623259\n",
      "51     \t [1.16847389 1.37306711]. \t  -0.03436783839125872 \t -0.03436444572623259\n",
      "52     \t [1.16860316 1.37332659]. \t  \u001b[92m-0.034345621644754486\u001b[0m \t -0.034345621644754486\n",
      "53     \t [1.1686481  1.37341216]. \t  \u001b[92m-0.034330863325914486\u001b[0m \t -0.034330863325914486\n",
      "54     \t [1.16866022 1.37344723]. \t  -0.03434529852893839 \t -0.034330863325914486\n",
      "55     \t [1.16866951 1.37346162]. \t  -0.034337203269552474 \t -0.034330863325914486\n",
      "56     \t [1.16879461 1.37373264]. \t  -0.03434661496867073 \t -0.034330863325914486\n",
      "57     \t [1.16881417 1.37374757]. \t  \u001b[92m-0.034306197061963285\u001b[0m \t -0.034306197061963285\n",
      "58     \t [1.1688604  1.37386106]. \t  -0.03433007103794998 \t -0.034306197061963285\n",
      "59     \t [1.1689804  1.37410817]. \t  -0.03431973126103001 \t -0.034306197061963285\n",
      "60     \t [1.16890646 1.37395193]. \t  -0.034320029325842265 \t -0.034306197061963285\n",
      "61     \t [1.16894891 1.37404168]. \t  -0.03431991551407338 \t -0.034306197061963285\n",
      "62     \t [1.16898601 1.37411312]. \t  -0.0343092292517385 \t -0.034306197061963285\n",
      "63     \t [1.29511857 1.66972896]. \t  -0.09287573866385404 \t -0.034306197061963285\n",
      "64     \t [1.16619668 1.36828337]. \t  -0.0344584439643509 \t -0.034306197061963285\n",
      "65     \t [1.16606225 1.36800673]. \t  -0.034474887746774124 \t -0.034306197061963285\n",
      "66     \t [1.16636387 1.36863117]. \t  -0.034444467674680825 \t -0.034306197061963285\n",
      "67     \t [1.16648372 1.36888496]. \t  -0.034441975457177 \t -0.034306197061963285\n",
      "68     \t [1.16652442 1.36896529]. \t  -0.034431535275875204 \t -0.034306197061963285\n",
      "69     \t [1.16659864 1.36911647]. \t  -0.034420346857467386 \t -0.034306197061963285\n",
      "70     \t [1.16665117 1.36924932]. \t  -0.034454648539951394 \t -0.034306197061963285\n",
      "71     \t [1.16664438 1.3692122 ]. \t  -0.03441765371841923 \t -0.034306197061963285\n",
      "72     \t [1.16678111 1.36949401]. \t  -0.03440265546894507 \t -0.034306197061963285\n",
      "73     \t [1.16695846 1.36983738]. \t  -0.03434785542571261 \t -0.034306197061963285\n",
      "74     \t [1.16684417 1.36963103]. \t  -0.034407244538280146 \t -0.034306197061963285\n",
      "75     \t [1.16691059 1.36976409]. \t  -0.034393859802847634 \t -0.034306197061963285\n",
      "76     \t [1.16693206 1.36980441]. \t  -0.03438523772663762 \t -0.034306197061963285\n",
      "77     \t [1.16699867 1.36993632]. \t  -0.03436948892186475 \t -0.034306197061963285\n",
      "78     \t [1.16697508 1.36988538]. \t  -0.034368242409951164 \t -0.034306197061963285\n",
      "79     \t [1.16714292 1.37023667]. \t  -0.034359288117542526 \t -0.034306197061963285\n",
      "80     \t [1.16709378 1.37013811]. \t  -0.03436877818368808 \t -0.034306197061963285\n",
      "81     \t [1.1671335  1.37022995]. \t  -0.03438063358917265 \t -0.034306197061963285\n",
      "82     \t [1.16715536 1.37026602]. \t  -0.03436394271021706 \t -0.034306197061963285\n",
      "83     \t [1.16716056 1.37027999]. \t  -0.034368646052102776 \t -0.034306197061963285\n",
      "84     \t [1.1672834  1.37056114]. \t  -0.03440071288977166 \t -0.034306197061963285\n",
      "85     \t [1.1672446  1.37045346]. \t  -0.03436037109174464 \t -0.034306197061963285\n",
      "86     \t [1.16721395 1.37038828]. \t  -0.034360294221221575 \t -0.034306197061963285\n",
      "87     \t [1.16737033 1.37071034]. \t  -0.03434398796201825 \t -0.034306197061963285\n",
      "88     \t [1.16729047 1.37053092]. \t  -0.03432845287817874 \t -0.034306197061963285\n",
      "89     \t [1.16766134 1.37126223]. \t  \u001b[92m-0.03424000164324374\u001b[0m \t -0.03424000164324374\n",
      "90     \t [1.16739958 1.37078022]. \t  -0.034356279957910296 \t -0.03424000164324374\n",
      "91     \t [1.16743385 1.37084143]. \t  -0.03433789348526601 \t -0.03424000164324374\n",
      "92     \t [1.16730637 1.37057813]. \t  -0.03434986157510448 \t -0.03424000164324374\n",
      "93     \t [1.16750973 1.37099565]. \t  -0.03432689497352346 \t -0.03424000164324374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     \t [1.16738147 1.37073443]. \t  -0.034344639943155335 \t -0.03424000164324374\n",
      "95     \t [1.16743861 1.37085742]. \t  -0.03434723713751554 \t -0.03424000164324374\n",
      "96     \t [1.16742334 1.37083964]. \t  -0.03437052690070647 \t -0.03424000164324374\n",
      "97     \t [1.16739299 1.37076903]. \t  -0.034360748215052285 \t -0.03424000164324374\n",
      "98     \t [1.16745724 1.37090294]. \t  -0.034356678040545915 \t -0.03424000164324374\n",
      "99     \t [1.16749075 1.37096713]. \t  -0.03434556697963116 \t -0.03424000164324374\n",
      "100    \t [1.16758069 1.37113408]. \t  -0.03430757731930623 \t -0.03424000164324374\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-0.82846093  1.34052693]. \t  -46.13833945095503 \t -43.94591987332933\n",
      "init   \t [ 1.08702204 -1.93893911]. \t  -973.794564983344 \t -43.94591987332933\n",
      "init   \t [1.16845539 0.39302383]. \t  -94.55813856061427 \t -43.94591987332933\n",
      "init   \t [0.40748384 0.826307  ]. \t  -43.94591987332933 \t -43.94591987332933\n",
      "init   \t [2.04295072 1.46997105]. \t  -732.0744672178031 \t -43.94591987332933\n",
      "1      \t [0.26112902 0.29438742]. \t  \u001b[92m-5.662531658030973\u001b[0m \t -5.662531658030973\n",
      "2      \t [0.26128637 0.29470534]. \t  -5.672968643407232 \t -5.662531658030973\n",
      "3      \t [0.26133588 0.29480533]. \t  -5.676252095635939 \t -5.662531658030973\n",
      "4      \t [0.2613686  0.29487139]. \t  -5.678422076318943 \t -5.662531658030973\n",
      "5      \t [0.2613929 0.2949204]. \t  -5.6800317615646065 \t -5.662531658030973\n",
      "6      \t [0.26141213 0.29495925]. \t  -5.681308001316287 \t -5.662531658030973\n",
      "7      \t [0.26142801 0.29499128]. \t  -5.6823603579994995 \t -5.662531658030973\n",
      "8      \t [0.26144149 0.29501845]. \t  -5.683252281723788 \t -5.662531658030973\n",
      "9      \t [0.26145314 0.29504205]. \t  -5.684028714130357 \t -5.662531658030973\n",
      "10     \t [0.26146341 0.29506275]. \t  -5.684708985920468 \t -5.662531658030973\n",
      "11     \t [0.26147259 0.2950813 ]. \t  -5.685318406685143 \t -5.662531658030973\n",
      "12     \t [0.26148085 0.29509793]. \t  -5.685864848944979 \t -5.662531658030973\n",
      "13     \t [0.26148835 0.29511305]. \t  -5.686361368777904 \t -5.662531658030973\n",
      "14     \t [0.2614952 0.2951269]. \t  -5.686816715827584 \t -5.662531658030973\n",
      "15     \t [0.2615015  0.29513963]. \t  -5.687235104033131 \t -5.662531658030973\n",
      "16     \t [0.26150734 0.29515137]. \t  -5.6876207414845785 \t -5.662531658030973\n",
      "17     \t [0.26151277 0.2951623 ]. \t  -5.687979627405364 \t -5.662531658030973\n",
      "18     \t [0.2615178  0.29517249]. \t  -5.688315146727716 \t -5.662531658030973\n",
      "19     \t [0.26152256 0.29518209]. \t  -5.688630357858728 \t -5.662531658030973\n",
      "20     \t [0.26152697 0.295191  ]. \t  -5.6889236210813525 \t -5.662531658030973\n",
      "21     \t [0.26153117 0.29519945]. \t  -5.689200633917967 \t -5.662531658030973\n",
      "22     \t [0.26153513 0.29520744]. \t  -5.689463410762963 \t -5.662531658030973\n",
      "23     \t [0.26153887 0.29521494]. \t  -5.689709591072045 \t -5.662531658030973\n",
      "24     \t [0.26154239 0.29522209]. \t  -5.689945191486829 \t -5.662531658030973\n",
      "25     \t [0.26154576 0.29522889]. \t  -5.6901684195339435 \t -5.662531658030973\n",
      "26     \t [0.26154896 0.29523537]. \t  -5.690381757242506 \t -5.662531658030973\n",
      "27     \t [0.26155204 0.29524154]. \t  -5.690584048109553 \t -5.662531658030973\n",
      "28     \t [0.26155494 0.29524739]. \t  -5.690776520185338 \t -5.662531658030973\n",
      "29     \t [0.26155772 0.29525302]. \t  -5.690961644268231 \t -5.662531658030973\n",
      "30     \t [0.26156038 0.29525838]. \t  -5.691138050561201 \t -5.662531658030973\n",
      "31     \t [0.26156296 0.29526358]. \t  -5.691308850198703 \t -5.662531658030973\n",
      "32     \t [0.26156541 0.29526853]. \t  -5.691471538717813 \t -5.662531658030973\n",
      "33     \t [0.26156776 0.29527325]. \t  -5.691626657244641 \t -5.662531658030973\n",
      "34     \t [0.26157004 0.29527789]. \t  -5.691779496454307 \t -5.662531658030973\n",
      "35     \t [0.26157221 0.29528223]. \t  -5.691921678328385 \t -5.662531658030973\n",
      "36     \t [0.26157431 0.29528647]. \t  -5.692061244746595 \t -5.662531658030973\n",
      "37     \t [0.26157634 0.29529059]. \t  -5.692197128970234 \t -5.662531658030973\n",
      "38     \t [0.2615783  0.29529451]. \t  -5.692325396681086 \t -5.662531658030973\n",
      "39     \t [0.2615802  0.29529834]. \t  -5.6924513068534655 \t -5.662531658030973\n",
      "40     \t [0.26158202 0.29530202]. \t  -5.692572324352385 \t -5.662531658030973\n",
      "41     \t [0.26158379 0.29530561]. \t  -5.692690710813079 \t -5.662531658030973\n",
      "42     \t [0.26158551 0.29530904]. \t  -5.692802685909586 \t -5.662531658030973\n",
      "43     \t [0.26158717 0.29531241]. \t  -5.692914156254637 \t -5.662531658030973\n",
      "44     \t [0.26158876 0.29531561]. \t  -5.693018965972404 \t -5.662531658030973\n",
      "45     \t [0.26159032 0.29531875]. \t  -5.693122168522905 \t -5.662531658030973\n",
      "46     \t [0.26159182 0.2953218 ]. \t  -5.6932228131583535 \t -5.662531658030973\n",
      "47     \t [0.2615933  0.29532474]. \t  -5.69331907052183 \t -5.662531658030973\n",
      "48     \t [0.26159471 0.29532763]. \t  -5.693414772167849 \t -5.662531658030973\n",
      "49     \t [0.26159612 0.29533043]. \t  -5.693506010426374 \t -5.662531658030973\n",
      "50     \t [0.26159744 0.29533313]. \t  -5.693595260174922 \t -5.662531658030973\n",
      "51     \t [0.26159874 0.29533577]. \t  -5.69368191019687 \t -5.662531658030973\n",
      "52     \t [0.26160005 0.29533838]. \t  -5.693767559289501 \t -5.662531658030973\n",
      "53     \t [0.26160127 0.29534085]. \t  -5.693848844408883 \t -5.662531658030973\n",
      "54     \t [0.26160247 0.29534327]. \t  -5.69392856904882 \t -5.662531658030973\n",
      "55     \t [0.26160364 0.29534564]. \t  -5.694006447636601 \t -5.662531658030973\n",
      "56     \t [0.26160479 0.29534791]. \t  -5.6940808164649015 \t -5.662531658030973\n",
      "57     \t [0.26160591 0.29535021]. \t  -5.694156625207708 \t -5.662531658030973\n",
      "58     \t [0.261607   0.29535238]. \t  -5.694227474931372 \t -5.662531658030973\n",
      "59     \t [0.26160806 0.29535453]. \t  -5.694298391672922 \t -5.662531658030973\n",
      "60     \t [0.26160909 0.29535661]. \t  -5.694367089769011 \t -5.662531658030973\n",
      "61     \t [0.2616101  0.29535867]. \t  -5.694434658967708 \t -5.662531658030973\n",
      "62     \t [0.26161109 0.29536061]. \t  -5.694498176817915 \t -5.662531658030973\n",
      "63     \t [0.26161205 0.29536256]. \t  -5.694562383719768 \t -5.662531658030973\n",
      "64     \t [0.26161298 0.29536447]. \t  -5.6946257725138985 \t -5.662531658030973\n",
      "65     \t [0.26161393 0.29536636]. \t  -5.694687499554487 \t -5.662531658030973\n",
      "66     \t [0.2616148  0.29536813]. \t  -5.694745657185321 \t -5.662531658030973\n",
      "67     \t [0.26161569 0.29536992]. \t  -5.694804326051272 \t -5.662531658030973\n",
      "68     \t [0.26161653 0.29537161]. \t  -5.694860202071685 \t -5.662531658030973\n",
      "69     \t [0.26161739 0.29537333]. \t  -5.694916224335626 \t -5.662531658030973\n",
      "70     \t [0.26161822 0.29537505]. \t  -5.694973534742489 \t -5.662531658030973\n",
      "71     \t [0.26161903 0.29537661]. \t  -5.695023890859947 \t -5.662531658030973\n",
      "72     \t [0.2616198  0.29537823]. \t  -5.695077756145837 \t -5.662531658030973\n",
      "73     \t [0.26162058 0.29537979]. \t  -5.695129361393508 \t -5.662531658030973\n",
      "74     \t [0.2616213  0.29538125]. \t  -5.695177360365927 \t -5.662531658030973\n",
      "75     \t [0.26162209 0.2953828 ]. \t  -5.695227826145437 \t -5.662531658030973\n",
      "76     \t [0.26162279 0.29538424]. \t  -5.695275281929371 \t -5.662531658030973\n",
      "77     \t [0.26162351 0.29538568]. \t  -5.695322744748167 \t -5.662531658030973\n",
      "78     \t [0.26162421 0.29538712]. \t  -5.695369953818078 \t -5.662531658030973\n",
      "79     \t [0.2616249  0.29538846]. \t  -5.695413957331604 \t -5.662531658030973\n",
      "80     \t [0.26162557 0.29538984]. \t  -5.695459223757893 \t -5.662531658030973\n",
      "81     \t [0.26162623 0.29539116]. \t  -5.695502592000566 \t -5.662531658030973\n",
      "82     \t [0.26162688 0.29539243]. \t  -5.695543719712808 \t -5.662531658030973\n",
      "83     \t [0.26162751 0.29539373]. \t  -5.6955869350253305 \t -5.662531658030973\n",
      "84     \t [0.26162813 0.295395  ]. \t  -5.695629166802522 \t -5.662531658030973\n",
      "85     \t [0.26162871 0.29539621]. \t  -5.695669045023678 \t -5.662531658030973\n",
      "86     \t [0.26162932 0.29539741]. \t  -5.695708548242209 \t -5.662531658030973\n",
      "87     \t [0.26162991 0.29539859]. \t  -5.695747101715646 \t -5.662531658030973\n",
      "88     \t [0.26163049 0.29539976]. \t  -5.695785518033145 \t -5.662531658030973\n",
      "89     \t [0.26163103 0.29540088]. \t  -5.695822755806327 \t -5.662531658030973\n",
      "90     \t [0.26163162 0.29540199]. \t  -5.6958583113009436 \t -5.662531658030973\n",
      "91     \t [0.26163215 0.29540312]. \t  -5.695896345641595 \t -5.662531658030973\n",
      "92     \t [0.26163269 0.2954042 ]. \t  -5.695931830667667 \t -5.662531658030973\n",
      "93     \t [0.26163322 0.29540526]. \t  -5.6959662253299586 \t -5.662531658030973\n",
      "94     \t [0.26163374 0.29540631]. \t  -5.696000851751706 \t -5.662531658030973\n",
      "95     \t [0.26163424 0.29540734]. \t  -5.696034861466712 \t -5.662531658030973\n",
      "96     \t [0.26163475 0.29540834]. \t  -5.696067632722239 \t -5.662531658030973\n",
      "97     \t [0.26163525 0.29540935]. \t  -5.696101032266592 \t -5.662531658030973\n",
      "98     \t [0.26163572 0.29541031]. \t  -5.696132487109665 \t -5.662531658030973\n",
      "99     \t [0.2616362  0.29541125]. \t  -5.696162911409489 \t -5.662531658030973\n",
      "100    \t [0.26163666 0.29541222]. \t  -5.696195458140122 \t -5.662531658030973\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.66097542 0.67072432]. \t  -436.45931870542523 \t -31.22188590191926\n",
      "2      \t [0.62178347 0.94407575]. \t  \u001b[92m-31.21933159391206\u001b[0m \t -31.21933159391206\n",
      "3      \t [0.6217444  0.94401069]. \t  \u001b[92m-31.217524325534605\u001b[0m \t -31.217524325534605\n",
      "4      \t [0.62171835 0.94396733]. \t  \u001b[92m-31.216321634220645\u001b[0m \t -31.216321634220645\n",
      "5      \t [0.62169877 0.94393473]. \t  \u001b[92m-31.215416212329128\u001b[0m \t -31.215416212329128\n",
      "6      \t [0.62168316 0.94390874]. \t  \u001b[92m-31.21469432021739\u001b[0m \t -31.21469432021739\n",
      "7      \t [0.62167011 0.94388702]. \t  \u001b[92m-31.21409190228158\u001b[0m \t -31.21409190228158\n",
      "8      \t [0.62165887 0.94386833]. \t  \u001b[92m-31.213574034113183\u001b[0m \t -31.213574034113183\n",
      "9      \t [0.62164912 0.9438521 ]. \t  \u001b[92m-31.213123891610532\u001b[0m \t -31.213123891610532\n",
      "10     \t [0.62164034 0.94383751]. \t  \u001b[92m-31.212720844235164\u001b[0m \t -31.212720844235164\n",
      "11     \t [0.62163256 0.94382452]. \t  \u001b[92m-31.21235801380473\u001b[0m \t -31.21235801380473\n",
      "12     \t [0.62162539 0.94381261]. \t  \u001b[92m-31.212027981271934\u001b[0m \t -31.212027981271934\n",
      "13     \t [0.62161882 0.94380167]. \t  \u001b[92m-31.211724365325452\u001b[0m \t -31.211724365325452\n",
      "14     \t [0.6216128  0.94379168]. \t  \u001b[92m-31.211449999494818\u001b[0m \t -31.211449999494818\n",
      "15     \t [0.62160716 0.94378227]. \t  \u001b[92m-31.211185756862353\u001b[0m \t -31.211185756862353\n",
      "16     \t [0.6216019  0.94377353]. \t  \u001b[92m-31.210944936773338\u001b[0m \t -31.210944936773338\n",
      "17     \t [0.62159696 0.94376532]. \t  \u001b[92m-31.210717822731752\u001b[0m \t -31.210717822731752\n",
      "18     \t [0.6215923  0.94375761]. \t  \u001b[92m-31.210507822534368\u001b[0m \t -31.210507822534368\n",
      "19     \t [0.62158793 0.94375032]. \t  \u001b[92m-31.210304348551716\u001b[0m \t -31.210304348551716\n",
      "20     \t [0.62158375 0.94374333]. \t  \u001b[92m-31.21010768031434\u001b[0m \t -31.21010768031434\n",
      "21     \t [0.62157984 0.94373682]. \t  \u001b[92m-31.209925953950535\u001b[0m \t -31.209925953950535\n",
      "22     \t [0.62157605 0.94373049]. \t  \u001b[92m-31.209749237318228\u001b[0m \t -31.209749237318228\n",
      "23     \t [0.62157247 0.94372455]. \t  \u001b[92m-31.209586433337897\u001b[0m \t -31.209586433337897\n",
      "24     \t [0.62156897 0.94371878]. \t  \u001b[92m-31.209430238187117\u001b[0m \t -31.209430238187117\n",
      "25     \t [0.62156572 0.94371334]. \t  \u001b[92m-31.20927626904521\u001b[0m \t -31.20927626904521\n",
      "26     \t [0.62156252 0.94370801]. \t  \u001b[92m-31.209128267105395\u001b[0m \t -31.209128267105395\n",
      "27     \t [0.62155942 0.94370292]. \t  \u001b[92m-31.208992486148833\u001b[0m \t -31.208992486148833\n",
      "28     \t [0.62155648 0.94369803]. \t  \u001b[92m-31.208856487718073\u001b[0m \t -31.208856487718073\n",
      "29     \t [0.62155365 0.94369333]. \t  \u001b[92m-31.20872815764555\u001b[0m \t -31.20872815764555\n",
      "30     \t [0.62155091 0.9436887 ]. \t  \u001b[92m-31.208593716960195\u001b[0m \t -31.208593716960195\n",
      "31     \t [0.62154819 0.94368424]. \t  \u001b[92m-31.2084765367538\u001b[0m \t -31.2084765367538\n",
      "32     \t [0.62154566 0.94367996]. \t  \u001b[92m-31.208351822962953\u001b[0m \t -31.208351822962953\n",
      "33     \t [0.62154315 0.94367581]. \t  \u001b[92m-31.208237766118526\u001b[0m \t -31.208237766118526\n",
      "34     \t [0.62154075 0.94367179]. \t  \u001b[92m-31.208123608084737\u001b[0m \t -31.208123608084737\n",
      "35     \t [0.62153837 0.94366788]. \t  \u001b[92m-31.208019391505303\u001b[0m \t -31.208019391505303\n",
      "36     \t [0.62153606 0.94366407]. \t  \u001b[92m-31.207916929629743\u001b[0m \t -31.207916929629743\n",
      "37     \t [0.62153383 0.94366025]. \t  \u001b[92m-31.207801271291665\u001b[0m \t -31.207801271291665\n",
      "38     \t [0.62153163 0.94365667]. \t  \u001b[92m-31.207709477908743\u001b[0m \t -31.207709477908743\n",
      "39     \t [0.62152955 0.9436532 ]. \t  \u001b[92m-31.20761201620747\u001b[0m \t -31.20761201620747\n",
      "40     \t [0.62152748 0.94364973]. \t  \u001b[92m-31.20751407361817\u001b[0m \t -31.20751407361817\n",
      "41     \t [0.6215255  0.94364645]. \t  \u001b[92m-31.207424672873838\u001b[0m \t -31.207424672873838\n",
      "42     \t [0.62152354 0.94364322]. \t  \u001b[92m-31.207337470163683\u001b[0m \t -31.207337470163683\n",
      "43     \t [0.62152157 0.94364001]. \t  \u001b[92m-31.207252986924875\u001b[0m \t -31.207252986924875\n",
      "44     \t [0.62151968 0.94363681]. \t  \u001b[92m-31.20715971568053\u001b[0m \t -31.20715971568053\n",
      "45     \t [0.62151786 0.94363374]. \t  \u001b[92m-31.207070980546227\u001b[0m \t -31.207070980546227\n",
      "46     \t [0.62151606 0.9436308 ]. \t  \u001b[92m-31.206994188289244\u001b[0m \t -31.206994188289244\n",
      "47     \t [0.62151429 0.94362783]. \t  \u001b[92m-31.206910738707027\u001b[0m \t -31.206910738707027\n",
      "48     \t [0.62151259 0.943625  ]. \t  \u001b[92m-31.206831277198397\u001b[0m \t -31.206831277198397\n",
      "49     \t [0.62151088 0.94362219]. \t  \u001b[92m-31.206757720893577\u001b[0m \t -31.206757720893577\n",
      "50     \t [0.62150924 0.94361946]. \t  \u001b[92m-31.2066809455855\u001b[0m \t -31.2066809455855\n",
      "51     \t [0.6215076  0.94361671]. \t  \u001b[92m-31.206603070568335\u001b[0m \t -31.206603070568335\n",
      "52     \t [0.62150603 0.94361403]. \t  \u001b[92m-31.206523467721205\u001b[0m \t -31.206523467721205\n",
      "53     \t [0.62150443 0.94361144]. \t  \u001b[92m-31.206457281640905\u001b[0m \t -31.206457281640905\n",
      "54     \t [0.62150294 0.94360893]. \t  \u001b[92m-31.206385112951292\u001b[0m \t -31.206385112951292\n",
      "55     \t [0.62150136 0.94360637]. \t  \u001b[92m-31.206319927201868\u001b[0m \t -31.206319927201868\n",
      "56     \t [0.62149991 0.94360392]. \t  \u001b[92m-31.206249143495\u001b[0m \t -31.206249143495\n",
      "57     \t [0.62149851 0.9436016 ]. \t  \u001b[92m-31.206185139262132\u001b[0m \t -31.206185139262132\n",
      "58     \t [0.62149701 0.94359914]. \t  \u001b[92m-31.206120645854057\u001b[0m \t -31.206120645854057\n",
      "59     \t [0.62149562 0.94359681]. \t  \u001b[92m-31.206054366373085\u001b[0m \t -31.206054366373085\n",
      "60     \t [0.6214942  0.94359446]. \t  \u001b[92m-31.205988549030177\u001b[0m \t -31.205988549030177\n",
      "61     \t [0.62149284 0.94359221]. \t  \u001b[92m-31.205928344059586\u001b[0m \t -31.205928344059586\n",
      "62     \t [0.62149148 0.94358997]. \t  \u001b[92m-31.20586860904584\u001b[0m \t -31.20586860904584\n",
      "63     \t [0.62149018 0.94358776]. \t  \u001b[92m-31.20580202717119\u001b[0m \t -31.20580202717119\n",
      "64     \t [0.62148889 0.94358564]. \t  \u001b[92m-31.20574566814785\u001b[0m \t -31.20574566814785\n",
      "65     \t [0.62148758 0.94358347]. \t  \u001b[92m-31.205686579401366\u001b[0m \t -31.205686579401366\n",
      "66     \t [0.62148631 0.94358131]. \t  \u001b[92m-31.205622841927052\u001b[0m \t -31.205622841927052\n",
      "67     \t [0.62148507 0.94357928]. \t  \u001b[92m-31.20556996774195\u001b[0m \t -31.20556996774195\n",
      "68     \t [0.62148385 0.94357725]. \t  \u001b[92m-31.205513625856444\u001b[0m \t -31.205513625856444\n",
      "69     \t [0.62148259 0.94357526]. \t  \u001b[92m-31.205466749948307\u001b[0m \t -31.205466749948307\n",
      "70     \t [0.62148149 0.94357329]. \t  \u001b[92m-31.205399611822838\u001b[0m \t -31.205399611822838\n",
      "71     \t [0.62148023 0.94357121]. \t  \u001b[92m-31.20534373525642\u001b[0m \t -31.20534373525642\n",
      "72     \t [0.62147906 0.94356929]. \t  \u001b[92m-31.20529223171641\u001b[0m \t -31.20529223171641\n",
      "73     \t [0.62147795 0.94356744]. \t  \u001b[92m-31.205241611552925\u001b[0m \t -31.205241611552925\n",
      "74     \t [0.62147675 0.94356546]. \t  \u001b[92m-31.205187769755995\u001b[0m \t -31.205187769755995\n",
      "75     \t [0.62147569 0.94356366]. \t  \u001b[92m-31.20513530590939\u001b[0m \t -31.20513530590939\n",
      "76     \t [0.62147452 0.94356173]. \t  \u001b[92m-31.20508289671479\u001b[0m \t -31.20508289671479\n",
      "77     \t [0.62147347 0.94355998]. \t  \u001b[92m-31.205034511670682\u001b[0m \t -31.205034511670682\n",
      "78     \t [0.62147237 0.94355816]. \t  \u001b[92m-31.204984834276313\u001b[0m \t -31.204984834276313\n",
      "79     \t [0.62147131 0.94355641]. \t  \u001b[92m-31.20493674968316\u001b[0m \t -31.20493674968316\n",
      "80     \t [0.62147022 0.94355466]. \t  \u001b[92m-31.204892841151356\u001b[0m \t -31.204892841151356\n",
      "81     \t [0.62146921 0.94355293]. \t  \u001b[92m-31.204841188733834\u001b[0m \t -31.204841188733834\n",
      "82     \t [0.62146823 0.94355121]. \t  \u001b[92m-31.20478632247963\u001b[0m \t -31.20478632247963\n",
      "83     \t [0.62146713 0.94354946]. \t  \u001b[92m-31.204743452465046\u001b[0m \t -31.204743452465046\n",
      "84     \t [0.62146615 0.94354785]. \t  \u001b[92m-31.204700535068937\u001b[0m \t -31.204700535068937\n",
      "85     \t [0.62146514 0.94354615]. \t  \u001b[92m-31.204653326321594\u001b[0m \t -31.204653326321594\n",
      "86     \t [0.62146416 0.94354454]. \t  \u001b[92m-31.204609394103837\u001b[0m \t -31.204609394103837\n",
      "87     \t [0.62146317 0.94354283]. \t  \u001b[92m-31.20455763268732\u001b[0m \t -31.20455763268732\n",
      "88     \t [0.62146222 0.94354129]. \t  \u001b[92m-31.20451776898407\u001b[0m \t -31.20451776898407\n",
      "89     \t [0.62146126 0.9435397 ]. \t  \u001b[92m-31.20447489283817\u001b[0m \t -31.20447489283817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.62146029 0.94353812]. \t  \u001b[92m-31.20443336981825\u001b[0m \t -31.20443336981825\n",
      "91     \t [0.62145941 0.94353657]. \t  \u001b[92m-31.204383364572983\u001b[0m \t -31.204383364572983\n",
      "92     \t [0.62145846 0.94353501]. \t  \u001b[92m-31.204341545753465\u001b[0m \t -31.204341545753465\n",
      "93     \t [0.62145753 0.94353346]. \t  \u001b[92m-31.204298617653652\u001b[0m \t -31.204298617653652\n",
      "94     \t [0.62145665 0.94353198]. \t  \u001b[92m-31.204256054335783\u001b[0m \t -31.204256054335783\n",
      "95     \t [0.62145571 0.94353049]. \t  \u001b[92m-31.204220774904208\u001b[0m \t -31.204220774904208\n",
      "96     \t [0.6214548  0.94352894]. \t  \u001b[92m-31.20417527236784\u001b[0m \t -31.20417527236784\n",
      "97     \t [0.62145395 0.9435275 ]. \t  \u001b[92m-31.204132800517282\u001b[0m \t -31.204132800517282\n",
      "98     \t [0.6214531  0.94352611]. \t  \u001b[92m-31.204096230597187\u001b[0m \t -31.204096230597187\n",
      "99     \t [0.62145224 0.94352468]. \t  \u001b[92m-31.204056608230843\u001b[0m \t -31.204056608230843\n",
      "100    \t [0.62145132 0.94352321]. \t  \u001b[92m-31.20401998240908\u001b[0m \t -31.20401998240908\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-0.68506997  0.53674828]. \t  -3.294106440972977 \t -1.7663579664225912\n",
      "2      \t [-0.32486919  0.113343  ]. \t  \u001b[92m-1.7613670658424039\u001b[0m \t -1.7613670658424039\n",
      "3      \t [-1.50422295  1.58707873]. \t  -51.91574163319671 \t -1.7613670658424039\n",
      "4      \t [0.52686813 2.048     ]. \t  -313.6590024060352 \t -1.7613670658424039\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7613670658424039\n",
      "6      \t [-1.76971868  0.6526317 ]. \t  -622.3505581853028 \t -1.7613670658424039\n",
      "7      \t [1.23025156 1.49138202]. \t  \u001b[92m-0.10201992616147108\u001b[0m \t -0.10201992616147108\n",
      "8      \t [ 0.44974867 -0.92777535]. \t  -128.0039004098994 \t -0.10201992616147108\n",
      "9      \t [-0.84866795  1.3162361 ]. \t  -38.939032166903715 \t -0.10201992616147108\n",
      "10     \t [-1.77336555  2.048     ]. \t  -127.99414963355662 \t -0.10201992616147108\n",
      "11     \t [1.23193058 2.048     ]. \t  -28.18058961645929 \t -0.10201992616147108\n",
      "12     \t [1.47541062 1.57555366]. \t  -36.3801219119911 \t -0.10201992616147108\n",
      "13     \t [1.24227307 1.75047612]. \t  -4.353278499642015 \t -0.10201992616147108\n",
      "14     \t [0.59667607 0.70497565]. \t  -12.339511976683484 \t -0.10201992616147108\n",
      "15     \t [-0.53863561  0.5953624 ]. \t  -11.684183546345707 \t -0.10201992616147108\n",
      "16     \t [1.36613911 1.83883076]. \t  -0.20971208448301099 \t -0.10201992616147108\n",
      "17     \t [0.7851815  1.11078974]. \t  -24.47739341942199 \t -0.10201992616147108\n",
      "18     \t [0.37587941 0.1858803 ]. \t  -0.5883976709037193 \t -0.10201992616147108\n",
      "19     \t [1.43396626 2.048     ]. \t  -0.1951482258613311 \t -0.10201992616147108\n",
      "20     \t [-1.00266374  1.12760665]. \t  -5.505708035400394 \t -0.10201992616147108\n",
      "21     \t [1.31120657 1.70275374]. \t  -0.1241039914167323 \t -0.10201992616147108\n",
      "22     \t [0.81950232 0.72576127]. \t  -0.3260965381699497 \t -0.10201992616147108\n",
      "23     \t [1.00640376 1.08973442]. \t  -0.5911849857332732 \t -0.10201992616147108\n",
      "24     \t [0.6216304  0.45592853]. \t  -0.626246729547094 \t -0.10201992616147108\n",
      "25     \t [1.37232925 1.87591053]. \t  -0.14407114893587053 \t -0.10201992616147108\n",
      "26     \t [1.35100479 1.81362877]. \t  -0.136625986717702 \t -0.10201992616147108\n",
      "27     \t [1.34608399 1.80077616]. \t  -0.1322419487894296 \t -0.10201992616147108\n",
      "28     \t [1.34545294 1.79952506]. \t  -0.13082647333469408 \t -0.10201992616147108\n",
      "29     \t [1.42350171 2.048     ]. \t  -0.2261950994532414 \t -0.10201992616147108\n",
      "30     \t [1.3376346  1.77884514]. \t  -0.12485724596138062 \t -0.10201992616147108\n",
      "31     \t [1.34105205 1.78815657]. \t  -0.12685150287730151 \t -0.10201992616147108\n",
      "32     \t [1.34223112 1.79148053]. \t  -0.12733089760772953 \t -0.10201992616147108\n",
      "33     \t [1.3426278  1.79266657]. \t  -0.1273595549491548 \t -0.10201992616147108\n",
      "34     \t [1.34274491 1.79307824]. \t  -0.12724671044580435 \t -0.10201992616147108\n",
      "35     \t [1.34275566 1.79319024]. \t  -0.12709039205538078 \t -0.10201992616147108\n",
      "36     \t [1.34279159 1.79335587]. \t  -0.12697993706562385 \t -0.10201992616147108\n",
      "37     \t [1.34279412 1.79340636]. \t  -0.1268967868104729 \t -0.10201992616147108\n",
      "38     \t [1.3427553 1.7933569]. \t  -0.12676435291174054 \t -0.10201992616147108\n",
      "39     \t [1.34275494 1.79338008]. \t  -0.12671760942586577 \t -0.10201992616147108\n",
      "40     \t [1.34276622 1.79348795]. \t  -0.1265768405055575 \t -0.10201992616147108\n",
      "41     \t [1.3427957  1.79355031]. \t  -0.12662909527570287 \t -0.10201992616147108\n",
      "42     \t [1.42374948 2.048     ]. \t  -0.22340122491746164 \t -0.10201992616147108\n",
      "43     \t [1.3385705  1.78239099]. \t  -0.12342841154497783 \t -0.10201992616147108\n",
      "44     \t [1.33966734 1.78530083]. \t  -0.12422446284868452 \t -0.10201992616147108\n",
      "45     \t [1.34044677 1.78739535]. \t  -0.12474412481930823 \t -0.10201992616147108\n",
      "46     \t [1.34095599 1.78874097]. \t  -0.12512839625837624 \t -0.10201992616147108\n",
      "47     \t [1.34122927 1.78950281]. \t  -0.12526051630663712 \t -0.10201992616147108\n",
      "48     \t [1.3415024  1.79024003]. \t  -0.12543856986363014 \t -0.10201992616147108\n",
      "49     \t [1.341636  1.7906262]. \t  -0.12547792440045702 \t -0.10201992616147108\n",
      "50     \t [1.34171341 1.7908497 ]. \t  -0.12550126792014998 \t -0.10201992616147108\n",
      "51     \t [1.34186754 1.79122978]. \t  -0.12566944953490006 \t -0.10201992616147108\n",
      "52     \t [1.34183336 1.79117935]. \t  -0.12556875055025235 \t -0.10201992616147108\n",
      "53     \t [1.34185525 1.79124735]. \t  -0.12556648331587983 \t -0.10201992616147108\n",
      "54     \t [1.34189171 1.79134845]. \t  -0.12558536364387166 \t -0.10201992616147108\n",
      "55     \t [1.34190351 1.79138012]. \t  -0.1255934011232359 \t -0.10201992616147108\n",
      "56     \t [1.34186881 1.79129245]. \t  -0.12555953622883054 \t -0.10201992616147108\n",
      "57     \t [1.34195024 1.79153457]. \t  -0.12557128035304563 \t -0.10201992616147108\n",
      "58     \t [1.34187837 1.79133963]. \t  -0.1255259836521019 \t -0.10201992616147108\n",
      "59     \t [1.34191173 1.79144385]. \t  -0.12552150596472586 \t -0.10201992616147108\n",
      "60     \t [1.34191124 1.79145871]. \t  -0.12549112566112555 \t -0.10201992616147108\n",
      "61     \t [1.34192246 1.79148247]. \t  -0.12551058794263784 \t -0.10201992616147108\n",
      "62     \t [1.34190386 1.7914453 ]. \t  -0.12547424324983672 \t -0.10201992616147108\n",
      "63     \t [1.34194221 1.7915456 ]. \t  -0.12550533430509095 \t -0.10201992616147108\n",
      "64     \t [1.34191569 1.79148832]. \t  -0.12546147937703775 \t -0.10201992616147108\n",
      "65     \t [1.34190706 1.79146968]. \t  -0.12544720843758203 \t -0.10201992616147108\n",
      "66     \t [1.34194718 1.79160244]. \t  -0.12542831865534926 \t -0.10201992616147108\n",
      "67     \t [1.34192662 1.79153315]. \t  -0.1254403030936315 \t -0.10201992616147108\n",
      "68     \t [1.34185261 1.79135374]. \t  -0.12535424863177275 \t -0.10201992616147108\n",
      "69     \t [1.34185421 1.79134658]. \t  -0.1253764898007715 \t -0.10201992616147108\n",
      "70     \t [1.34194591 1.79159112]. \t  -0.12544205888793403 \t -0.10201992616147108\n",
      "71     \t [1.34189453 1.79144075]. \t  -0.1254299823311854 \t -0.10201992616147108\n",
      "72     \t [1.34197824 1.79168376]. \t  -0.12545335331929997 \t -0.10201992616147108\n",
      "73     \t [1.34190529 1.79149354]. \t  -0.1253932047906924 \t -0.10201992616147108\n",
      "74     \t [1.34186127 1.79138083]. \t  -0.12535308308840487 \t -0.10201992616147108\n",
      "75     \t [1.34196024 1.79165045]. \t  -0.12541337811093428 \t -0.10201992616147108\n",
      "76     \t [1.3418438  1.79134615]. \t  -0.12531865603332595 \t -0.10201992616147108\n",
      "77     \t [1.34184119 1.79133888]. \t  -0.1253173916396162 \t -0.10201992616147108\n",
      "78     \t [1.34191157 1.7915108 ]. \t  -0.1253967125628285 \t -0.10201992616147108\n",
      "79     \t [1.34183641 1.79132162]. \t  -0.12532226691551296 \t -0.10201992616147108\n",
      "80     \t [1.34187977 1.79143468]. \t  -0.12535797980996696 \t -0.10201992616147108\n",
      "81     \t [1.34184681 1.79136211]. \t  -0.12530621806867065 \t -0.10201992616147108\n",
      "82     \t [1.34186232 1.79140829]. \t  -0.12530847534953124 \t -0.10201992616147108\n",
      "83     \t [1.34187741 1.7914328 ]. \t  -0.12534818404218415 \t -0.10201992616147108\n",
      "84     \t [1.3418442  1.79136047]. \t  -0.12529459237211107 \t -0.10201992616147108\n",
      "85     \t [1.34191185 1.79164305]. \t  -0.12515624782345533 \t -0.10201992616147108\n",
      "86     \t [1.34187722 1.7914491 ]. \t  -0.12531715875923144 \t -0.10201992616147108\n",
      "87     \t [1.34204321 1.79188741]. \t  -0.12544387520046263 \t -0.10201992616147108\n",
      "88     \t [1.3418205  1.79130622]. \t  -0.12526121054180522 \t -0.10201992616147108\n",
      "89     \t [1.34179386 1.79124276]. \t  -0.12522824912672698 \t -0.10201992616147108\n",
      "90     \t [1.34194021 1.79162513]. \t  -0.12534742545003838 \t -0.10201992616147108\n",
      "91     \t [1.34181762 1.79131256]. \t  -0.12523342376614005 \t -0.10201992616147108\n",
      "92     \t [1.34197169 1.79171835]. \t  -0.12535288916754042 \t -0.10201992616147108\n",
      "93     \t [1.34185314 1.79139847]. \t  -0.1252749835487662 \t -0.10201992616147108\n",
      "94     \t [1.34197754 1.79172842]. \t  -0.12536724343685635 \t -0.10201992616147108\n",
      "95     \t [1.3420146  1.79183877]. \t  -0.1253726398794905 \t -0.10201992616147108\n",
      "96     \t [1.34182369 1.79134185]. \t  -0.1252137885062094 \t -0.10201992616147108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.34185921 1.79142766]. \t  -0.12525548265945094 \t -0.10201992616147108\n",
      "98     \t [1.34189269 1.79150999]. \t  -0.12529213680298779 \t -0.10201992616147108\n",
      "99     \t [1.3419514  1.79167118]. \t  -0.12532569969551074 \t -0.10201992616147108\n",
      "100    \t [1.34183468 1.79136249]. \t  -0.12523749702706521 \t -0.10201992616147108\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.32923463 1.9850312 ]. \t  -4.868057505654614 \t -4.868057505654614\n",
      "init   \t [ 1.61031994 -0.73872624]. \t  -1110.4992966214563 \t -4.868057505654614\n",
      "init   \t [ 0.38197946 -1.24726347]. \t  -194.47470912529033 \t -4.868057505654614\n",
      "init   \t [1.47544785 1.49217491]. \t  -47.11724466380912 \t -4.868057505654614\n",
      "init   \t [-1.7259813   0.88847186]. \t  -444.4665457038575 \t -4.868057505654614\n",
      "1      \t [-0.53443798 -1.83227417]. \t  -450.90374812759075 \t -4.868057505654614\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -4.868057505654614\n",
      "3      \t [0.43565429 1.71132899]. \t  -231.82515555566138 \t -4.868057505654614\n",
      "4      \t [1.04458658 2.048     ]. \t  -91.55604977673191 \t -4.868057505654614\n",
      "5      \t [1.15025935 1.56953202]. \t  -6.095620686676037 \t -4.868057505654614\n",
      "6      \t [1.30944422 1.75444447]. \t  \u001b[92m-0.2541621824943111\u001b[0m \t -0.2541621824943111\n",
      "7      \t [-0.05038248  0.56419305]. \t  -32.64889877332216 \t -0.2541621824943111\n",
      "8      \t [1.27391115 1.65367469]. \t  \u001b[92m-0.17004577629682655\u001b[0m \t -0.17004577629682655\n",
      "9      \t [1.28369305 1.6929261 ]. \t  -0.2835064253247252 \t -0.17004577629682655\n",
      "10     \t [1.28489014 1.69143083]. \t  -0.24509153359522637 \t -0.17004577629682655\n",
      "11     \t [1.28519923 1.69147654]. \t  -0.23926115202892884 \t -0.17004577629682655\n",
      "12     \t [0.28546698 0.82863141]. \t  -56.33237787740959 \t -0.17004577629682655\n",
      "13     \t [-0.33716273 -0.24042427]. \t  -14.326895983219138 \t -0.17004577629682655\n",
      "14     \t [1.27751569 1.68777281]. \t  -0.3875589333843239 \t -0.17004577629682655\n",
      "15     \t [1.2779293  1.68541856]. \t  -0.35093328007365826 \t -0.17004577629682655\n",
      "16     \t [1.27823195 1.68509431]. \t  -0.33973503482436407 \t -0.17004577629682655\n",
      "17     \t [1.2784428  1.68505974]. \t  -0.3340092968936518 \t -0.17004577629682655\n",
      "18     \t [1.27859487 1.6850977 ]. \t  -0.33055231482682124 \t -0.17004577629682655\n",
      "19     \t [1.27870896 1.68515268]. \t  -0.32823991272415887 \t -0.17004577629682655\n",
      "20     \t [1.27879573 1.68520248]. \t  -0.32656793760966096 \t -0.17004577629682655\n",
      "21     \t [1.27886413 1.68525016]. \t  -0.3253381538535679 \t -0.17004577629682655\n",
      "22     \t [1.27891768 1.68528923]. \t  -0.32439473771581656 \t -0.17004577629682655\n",
      "23     \t [1.27896779 1.6853317 ]. \t  -0.3235722876031047 \t -0.17004577629682655\n",
      "24     \t [1.2790064  1.68536702]. \t  -0.3229650653288815 \t -0.17004577629682655\n",
      "25     \t [1.27903999 1.68539787]. \t  -0.32243879778743584 \t -0.17004577629682655\n",
      "26     \t [1.27906688 1.68541787]. \t  -0.32197157320258646 \t -0.17004577629682655\n",
      "27     \t [1.27908749 1.68543962]. \t  -0.32167697338652934 \t -0.17004577629682655\n",
      "28     \t [1.27911003 1.68545778]. \t  -0.3212996664850142 \t -0.17004577629682655\n",
      "29     \t [1.27913   1.6854797]. \t  -0.32102306057196117 \t -0.17004577629682655\n",
      "30     \t [1.27914658 1.68549315]. \t  -0.32074676758443704 \t -0.17004577629682655\n",
      "31     \t [1.27915905 1.68550681]. \t  -0.32057389234666606 \t -0.17004577629682655\n",
      "32     \t [1.27917268 1.68552096]. \t  -0.32037746189997063 \t -0.17004577629682655\n",
      "33     \t [1.27918241 1.68552839]. \t  -0.3202109573803409 \t -0.17004577629682655\n",
      "34     \t [1.27919287 1.68554031]. \t  -0.3200709022992468 \t -0.17004577629682655\n",
      "35     \t [1.27920286 1.68555094]. \t  -0.31992946307306636 \t -0.17004577629682655\n",
      "36     \t [1.2792107 1.6855586]. \t  -0.31981193283064435 \t -0.17004577629682655\n",
      "37     \t [1.27921972 1.68556817]. \t  -0.31968421770243954 \t -0.17004577629682655\n",
      "38     \t [1.27922487 1.6855737 ]. \t  -0.3196117518705866 \t -0.17004577629682655\n",
      "39     \t [1.27923367 1.6855834 ]. \t  -0.3194907286366408 \t -0.17004577629682655\n",
      "40     \t [1.27923955 1.68558787]. \t  -0.3193901258862226 \t -0.17004577629682655\n",
      "41     \t [1.27924577 1.68559516]. \t  -0.31930900072528323 \t -0.17004577629682655\n",
      "42     \t [1.27924991 1.68559878]. \t  -0.31924281398728505 \t -0.17004577629682655\n",
      "43     \t [1.27925521 1.68560664]. \t  -0.3191897932038047 \t -0.17004577629682655\n",
      "44     \t [1.27926005 1.68560871]. \t  -0.3190910521089314 \t -0.17004577629682655\n",
      "45     \t [1.27926337 1.68561361]. \t  -0.3190576898948908 \t -0.17004577629682655\n",
      "46     \t [1.27926736 1.68561807]. \t  -0.31900334064625036 \t -0.17004577629682655\n",
      "47     \t [1.27927162 1.68562356]. \t  -0.3189528232857759 \t -0.17004577629682655\n",
      "48     \t [1.2792673  1.68561808]. \t  -0.319004908194621 \t -0.17004577629682655\n",
      "49     \t [1.28052461 1.68839399]. \t  -0.31538321333304875 \t -0.17004577629682655\n",
      "50     \t [1.28215373 1.69340425]. \t  -0.3244978256889134 \t -0.17004577629682655\n",
      "51     \t [1.2745826  1.67663139]. \t  -0.3465301361427261 \t -0.17004577629682655\n",
      "52     \t [1.28146671 1.69704605]. \t  -0.380505028875677 \t -0.17004577629682655\n",
      "53     \t [1.27631635 1.67592419]. \t  -0.2966942457991088 \t -0.17004577629682655\n",
      "54     \t [1.28646225 1.69852275]. \t  -0.2716130709638756 \t -0.17004577629682655\n",
      "55     \t [1.27883542 1.68189597]. \t  -0.29375043857428085 \t -0.17004577629682655\n",
      "56     \t [1.27978709 1.68549347]. \t  -0.30522329175097884 \t -0.17004577629682655\n",
      "57     \t [1.27898619 1.68306332]. \t  -0.3011617489614701 \t -0.17004577629682655\n",
      "58     \t [1.27994065 1.68640995]. \t  -0.3103233833580859 \t -0.17004577629682655\n",
      "59     \t [1.27732204 1.68077592]. \t  -0.3192109591474269 \t -0.17004577629682655\n",
      "60     \t [1.27656616 1.6790254 ]. \t  -0.32056667649570203 \t -0.17004577629682655\n",
      "61     \t [1.27839603 1.68249193]. \t  -0.309785217101251 \t -0.17004577629682655\n",
      "62     \t [1.28344785 1.69494996]. \t  -0.3079821410205539 \t -0.17004577629682655\n",
      "63     \t [1.28108913 1.6922634 ]. \t  -0.3398667904478053 \t -0.17004577629682655\n",
      "64     \t [1.28279979 1.69222588]. \t  -0.2976034166796854 \t -0.17004577629682655\n",
      "65     \t [1.28334345 1.70472536]. \t  -0.41384696648821195 \t -0.17004577629682655\n",
      "66     \t [1.27278392 1.66668015]. \t  -0.292511798747497 \t -0.17004577629682655\n",
      "67     \t [1.27851134 1.68603152]. \t  -0.342178778727544 \t -0.17004577629682655\n",
      "68     \t [1.27904004 1.68131955]. \t  -0.28376275609244084 \t -0.17004577629682655\n",
      "69     \t [1.27788303 1.68232593]. \t  -0.3206712976682974 \t -0.17004577629682655\n",
      "70     \t [1.27867652 1.68015096]. \t  -0.2813983258303271 \t -0.17004577629682655\n",
      "71     \t [1.27844989 1.67961017]. \t  -0.2816219421012808 \t -0.17004577629682655\n",
      "72     \t [1.27943175 1.69369542]. \t  -0.40013623840302626 \t -0.17004577629682655\n",
      "73     \t [1.27393943 1.67146209]. \t  -0.3106601357265433 \t -0.17004577629682655\n",
      "74     \t [1.27314488 1.67052356]. \t  -0.32087893369045295 \t -0.17004577629682655\n",
      "75     \t [1.27701194 1.67425895]. \t  -0.2659557763750854 \t -0.17004577629682655\n",
      "76     \t [1.27526805 1.67054522]. \t  -0.27146032057510844 \t -0.17004577629682655\n",
      "77     \t [1.27930232 1.68118104]. \t  -0.276627948895476 \t -0.17004577629682655\n",
      "78     \t [1.27789588 1.67694489]. \t  -0.2701843308521777 \t -0.17004577629682655\n",
      "79     \t [1.27588476 1.66403066]. \t  -0.20678551694698527 \t -0.17004577629682655\n",
      "80     \t [1.27698913 1.67908804]. \t  -0.3108513784498618 \t -0.17004577629682655\n",
      "81     \t [1.30220435 1.7446936 ]. \t  -0.33101051271063947 \t -0.17004577629682655\n",
      "82     \t [1.28244734 1.68851911]. \t  -0.2720405935442616 \t -0.17004577629682655\n",
      "83     \t [1.27920054 1.67598999]. \t  -0.2350538666704408 \t -0.17004577629682655\n",
      "84     \t [1.27422794 1.67632176]. \t  -0.3525603173719657 \t -0.17004577629682655\n",
      "85     \t [1.28006962 1.68198703]. \t  -0.26687135280933166 \t -0.17004577629682655\n",
      "86     \t [1.28865735 1.70732187]. \t  -0.30126364215932727 \t -0.17004577629682655\n",
      "87     \t [1.2841644  1.70275454]. \t  -0.36886433598433865 \t -0.17004577629682655\n",
      "88     \t [1.27912242 1.67969292]. \t  -0.26747157614769207 \t -0.17004577629682655\n",
      "89     \t [1.27851061 1.67457816]. \t  -0.23747834583365485 \t -0.17004577629682655\n",
      "90     \t [1.28816459 1.72857677]. \t  -0.5620239141313825 \t -0.17004577629682655\n",
      "91     \t [1.27978731 1.67706388]. \t  -0.23201014989651808 \t -0.17004577629682655\n",
      "92     \t [1.27302344 1.6652119 ]. \t  -0.2736649699204972 \t -0.17004577629682655\n",
      "93     \t [1.27419965 1.66359807]. \t  -0.23529206018818416 \t -0.17004577629682655\n",
      "94     \t [1.27371277 1.67145201]. \t  -0.3160762308594854 \t -0.17004577629682655\n",
      "95     \t [1.27058188 1.64849666]. \t  -0.1896206680098631 \t -0.17004577629682655\n",
      "96     \t [1.28301635 1.67822918]. \t  -0.1831278155279693 \t -0.17004577629682655\n",
      "97     \t [1.29014576 1.72213664]. \t  -0.4166586363794784 \t -0.17004577629682655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [1.27718962 1.66714721]. \t  -0.20595852246010976 \t -0.17004577629682655\n",
      "99     \t [1.27796789 1.66890323]. \t  -0.20472449245728425 \t -0.17004577629682655\n",
      "100    \t [1.2774396  1.66745673]. \t  -0.20374289183172561 \t -0.17004577629682655\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [-0.1850592   0.44301427]. \t  -18.113440784342497 \t -3.777577453542735\n",
      "2      \t [-0.04752987 -0.09999115]. \t  \u001b[92m-2.142829977177247\u001b[0m \t -2.142829977177247\n",
      "3      \t [1.66486097 0.62011179]. \t  -463.40191814743474 \t -2.142829977177247\n",
      "4      \t [ 1.01835256 -1.62506704]. \t  -708.6827623616382 \t -2.142829977177247\n",
      "5      \t [0.49056354 0.9181957 ]. \t  -46.1659929450141 \t -2.142829977177247\n",
      "6      \t [-0.36620664 -0.14415599]. \t  -9.609566617077824 \t -2.142829977177247\n",
      "7      \t [-0.32917836  0.10557852]. \t  \u001b[92m-1.7674878783570191\u001b[0m \t -1.7674878783570191\n",
      "8      \t [ 0.48532388 -0.04915431]. \t  -8.369934576828205 \t -1.7674878783570191\n",
      "9      \t [0.2914614  0.03051483]. \t  \u001b[92m-0.7983430120197039\u001b[0m \t -0.7983430120197039\n",
      "10     \t [0.30124757 0.03901007]. \t  \u001b[92m-0.7559580438021152\u001b[0m \t -0.7559580438021152\n",
      "11     \t [0.30347408 0.04185468]. \t  \u001b[92m-0.7375725462426961\u001b[0m \t -0.7375725462426961\n",
      "12     \t [0.30546158 0.0443909 ]. \t  \u001b[92m-0.7216599429866597\u001b[0m \t -0.7216599429866597\n",
      "13     \t [0.30752008 0.04701624]. \t  \u001b[92m-0.7056511787150404\u001b[0m \t -0.7056511787150404\n",
      "14     \t [0.30994257 0.0501061 ]. \t  \u001b[92m-0.6873958181455556\u001b[0m \t -0.6873958181455556\n",
      "15     \t [0.31321118 0.05428191]. \t  \u001b[92m-0.6636922489541067\u001b[0m \t -0.6636922489541067\n",
      "16     \t [0.31853983 0.0610902 ]. \t  \u001b[92m-0.6274215855227712\u001b[0m \t -0.6274215855227712\n",
      "17     \t [0.33067317 0.07652489]. \t  \u001b[92m-0.5557127133172342\u001b[0m \t -0.5557127133172342\n",
      "18     \t [0.35605445 0.1077239 ]. \t  \u001b[92m-0.4509594462001769\u001b[0m \t -0.4509594462001769\n",
      "19     \t [0.36933523 0.12331527]. \t  \u001b[92m-0.41488134628393525\u001b[0m \t -0.41488134628393525\n",
      "20     \t [0.37460132 0.12942072]. \t  \u001b[92m-0.40301635387930224\u001b[0m \t -0.40301635387930224\n",
      "21     \t [0.37725697 0.13251301]. \t  \u001b[92m-0.3974321175391433\u001b[0m \t -0.3974321175391433\n",
      "22     \t [0.37883608 0.13436636]. \t  \u001b[92m-0.3942176287050854\u001b[0m \t -0.3942176287050854\n",
      "23     \t [0.37988316 0.13560169]. \t  \u001b[92m-0.39213048421325875\u001b[0m \t -0.39213048421325875\n",
      "24     \t [0.38063084 0.13649107]. \t  \u001b[92m-0.39065530363541295\u001b[0m \t -0.39065530363541295\n",
      "25     \t [0.38119311 0.13716285]. \t  \u001b[92m-0.3895566261786847\u001b[0m \t -0.3895566261786847\n",
      "26     \t [0.38163377 0.13769247]. \t  \u001b[92m-0.3887000127530775\u001b[0m \t -0.3887000127530775\n",
      "27     \t [0.38198997 0.13812155]. \t  \u001b[92m-0.38801227211775435\u001b[0m \t -0.38801227211775435\n",
      "28     \t [0.38228455 0.13847822]. \t  \u001b[92m-0.38744492356232163\u001b[0m \t -0.38744492356232163\n",
      "29     \t [0.38253299 0.13877988]. \t  \u001b[92m-0.3869681854218\u001b[0m \t -0.3869681854218\n",
      "30     \t [0.38274681 0.13904025]. \t  \u001b[92m-0.3865590064197492\u001b[0m \t -0.3865590064197492\n",
      "31     \t [0.38293098 0.13926566]. \t  \u001b[92m-0.3862065638607213\u001b[0m \t -0.3862065638607213\n",
      "32     \t [0.38309382 0.13946462]. \t  \u001b[92m-0.38589676980072174\u001b[0m \t -0.38589676980072174\n",
      "33     \t [0.38323827 0.13964284]. \t  \u001b[92m-0.3856204927340331\u001b[0m \t -0.3856204927340331\n",
      "34     \t [0.38336706 0.13980181]. \t  \u001b[92m-0.3853749099658622\u001b[0m \t -0.3853749099658622\n",
      "35     \t [0.38348287 0.13994461]. \t  \u001b[92m-0.38515498595296344\u001b[0m \t -0.38515498595296344\n",
      "36     \t [0.38358961 0.14007608]. \t  \u001b[92m-0.3849530604987116\u001b[0m \t -0.3849530604987116\n",
      "37     \t [0.38368514 0.14019518]. \t  \u001b[92m-0.3847707960880962\u001b[0m \t -0.3847707960880962\n",
      "38     \t [0.3837747  0.14030668]. \t  \u001b[92m-0.3846005569279017\u001b[0m \t -0.3846005569279017\n",
      "39     \t [0.38385584 0.14040782]. \t  \u001b[92m-0.38444649220766663\u001b[0m \t -0.38444649220766663\n",
      "40     \t [0.38393016 0.14050098]. \t  \u001b[92m-0.38430496532282604\u001b[0m \t -0.38430496532282604\n",
      "41     \t [0.384002   0.14058982]. \t  \u001b[92m-0.3841700911060572\u001b[0m \t -0.3841700911060572\n",
      "42     \t [0.38406671 0.14067089]. \t  \u001b[92m-0.3840473847561167\u001b[0m \t -0.3840473847561167\n",
      "43     \t [0.38412738 0.14074659]. \t  \u001b[92m-0.3839329600562071\u001b[0m \t -0.3839329600562071\n",
      "44     \t [0.38418566 0.14082046]. \t  \u001b[92m-0.38382165468282187\u001b[0m \t -0.38382165468282187\n",
      "45     \t [0.38423875 0.14088673]. \t  \u001b[92m-0.3837217984792576\u001b[0m \t -0.3837217984792576\n",
      "46     \t [0.38428994 0.14095047]. \t  \u001b[92m-0.38362586953757394\u001b[0m \t -0.38362586953757394\n",
      "47     \t [0.38433877 0.141013  ]. \t  \u001b[92m-0.38353216569750326\u001b[0m \t -0.38353216569750326\n",
      "48     \t [0.3843843  0.14107024]. \t  \u001b[92m-0.38344632729498623\u001b[0m \t -0.38344632729498623\n",
      "49     \t [0.38442753 0.14112584]. \t  \u001b[92m-0.38336327917605706\u001b[0m \t -0.38336327917605706\n",
      "50     \t [0.3844686  0.14117692]. \t  \u001b[92m-0.38328677904990244\u001b[0m \t -0.38328677904990244\n",
      "51     \t [0.38450899 0.14122734]. \t  \u001b[92m-0.38321139367873186\u001b[0m \t -0.38321139367873186\n",
      "52     \t [0.38454505 0.14127333]. \t  \u001b[92m-0.38314286362082917\u001b[0m \t -0.38314286362082917\n",
      "53     \t [0.38458081 0.14131881]. \t  \u001b[92m-0.383075149676725\u001b[0m \t -0.383075149676725\n",
      "54     \t [0.3846153  0.14136305]. \t  \u001b[92m-0.3830093999204251\u001b[0m \t -0.3830093999204251\n",
      "55     \t [0.38464795 0.14140449]. \t  \u001b[92m-0.38294781205284145\u001b[0m \t -0.38294781205284145\n",
      "56     \t [0.38467879 0.14144385]. \t  \u001b[92m-0.38288941032014623\u001b[0m \t -0.38288941032014623\n",
      "57     \t [0.38471049 0.14148329]. \t  \u001b[92m-0.3828307496727048\u001b[0m \t -0.3828307496727048\n",
      "58     \t [0.38473959 0.14152054]. \t  \u001b[92m-0.38277558599150185\u001b[0m \t -0.38277558599150185\n",
      "59     \t [0.38476675 0.14155558]. \t  \u001b[92m-0.38272379671040946\u001b[0m \t -0.38272379671040946\n",
      "60     \t [0.38479441 0.14159072]. \t  \u001b[92m-0.38267179380616223\u001b[0m \t -0.38267179380616223\n",
      "61     \t [0.38482023 0.14162406]. \t  \u001b[92m-0.3826226075906972\u001b[0m \t -0.3826226075906972\n",
      "62     \t [0.38484479 0.14165606]. \t  \u001b[92m-0.38257548150432724\u001b[0m \t -0.38257548150432724\n",
      "63     \t [0.3848696  0.14168737]. \t  \u001b[92m-0.38252921964494213\u001b[0m \t -0.38252921964494213\n",
      "64     \t [0.38489298 0.14171716]. \t  \u001b[92m-0.38248527592995807\u001b[0m \t -0.38248527592995807\n",
      "65     \t [0.38491633 0.14174698]. \t  \u001b[92m-0.3824413449540023\u001b[0m \t -0.3824413449540023\n",
      "66     \t [0.38493782 0.14177479]. \t  \u001b[92m-0.382400474955129\u001b[0m \t -0.382400474955129\n",
      "67     \t [0.38495906 0.14180175]. \t  \u001b[92m-0.38236077959396747\u001b[0m \t -0.38236077959396747\n",
      "68     \t [0.38498036 0.14182899]. \t  \u001b[92m-0.3823207330106097\u001b[0m \t -0.3823207330106097\n",
      "69     \t [0.38499952 0.14185407]. \t  \u001b[92m-0.38228399018814485\u001b[0m \t -0.38228399018814485\n",
      "70     \t [0.38502022 0.14188063]. \t  \u001b[92m-0.38224500752412693\u001b[0m \t -0.38224500752412693\n",
      "71     \t [0.38503975 0.14190632]. \t  \u001b[92m-0.3822074592868752\u001b[0m \t -0.3822074592868752\n",
      "72     \t [0.38505802 0.1419289 ]. \t  \u001b[92m-0.3821741890692465\u001b[0m \t -0.3821741890692465\n",
      "73     \t [0.38507624 0.14195318]. \t  \u001b[92m-0.3821387943409428\u001b[0m \t -0.3821387943409428\n",
      "74     \t [0.38509378 0.14197594]. \t  \u001b[92m-0.38210551694141337\u001b[0m \t -0.38210551694141337\n",
      "75     \t [0.38511062 0.14199698]. \t  \u001b[92m-0.3820746084022082\u001b[0m \t -0.3820746084022082\n",
      "76     \t [0.38512727 0.14201883]. \t  \u001b[92m-0.38204275386322806\u001b[0m \t -0.38204275386322806\n",
      "77     \t [0.38514434 0.14204053]. \t  \u001b[92m-0.3820109781067973\u001b[0m \t -0.3820109781067973\n",
      "78     \t [0.38515991 0.14206227]. \t  \u001b[92m-0.38197957713099734\u001b[0m \t -0.38197957713099734\n",
      "79     \t [0.38517537 0.14208121]. \t  \u001b[92m-0.38195172803611016\u001b[0m \t -0.38195172803611016\n",
      "80     \t [0.38519174 0.14210205]. \t  \u001b[92m-0.38192127695245703\u001b[0m \t -0.38192127695245703\n",
      "81     \t [0.38520622 0.14212124]. \t  \u001b[92m-0.3818934025394187\u001b[0m \t -0.3818934025394187\n",
      "82     \t [0.38522058 0.14214064]. \t  \u001b[92m-0.3818652994232621\u001b[0m \t -0.3818652994232621\n",
      "83     \t [0.38523499 0.14215865]. \t  \u001b[92m-0.3818389461519986\u001b[0m \t -0.3818389461519986\n",
      "84     \t [0.38524915 0.14217634]. \t  \u001b[92m-0.38181308068852515\u001b[0m \t -0.38181308068852515\n",
      "85     \t [0.38526214 0.14219327]. \t  \u001b[92m-0.3817884696042106\u001b[0m \t -0.3817884696042106\n",
      "86     \t [0.38527569 0.14221168]. \t  \u001b[92m-0.38176188477571166\u001b[0m \t -0.38176188477571166\n",
      "87     \t [0.38528845 0.14222829]. \t  \u001b[92m-0.3817377583451334\u001b[0m \t -0.3817377583451334\n",
      "88     \t [0.38530164 0.14224526]. \t  \u001b[92m-0.38171308664333453\u001b[0m \t -0.38171308664333453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.3853143  0.14226135]. \t  \u001b[92m-0.3816896623655296\u001b[0m \t -0.3816896623655296\n",
      "90     \t [0.38532668 0.14227798]. \t  \u001b[92m-0.38166563900753825\u001b[0m \t -0.38166563900753825\n",
      "91     \t [0.38533844 0.14229428]. \t  \u001b[92m-0.38164221266467885\u001b[0m \t -0.38164221266467885\n",
      "92     \t [0.38534992 0.14230842]. \t  \u001b[92m-0.3816215553909726\u001b[0m \t -0.3816215553909726\n",
      "93     \t [0.3853623  0.14232473]. \t  \u001b[92m-0.38159796493576104\u001b[0m \t -0.38159796493576104\n",
      "94     \t [0.38537354 0.14233899]. \t  \u001b[92m-0.3815772363503461\u001b[0m \t -0.3815772363503461\n",
      "95     \t [0.38538502 0.14235489]. \t  \u001b[92m-0.38155442098236225\u001b[0m \t -0.38155442098236225\n",
      "96     \t [0.3853956  0.14236884]. \t  \u001b[92m-0.38153426530470125\u001b[0m \t -0.38153426530470125\n",
      "97     \t [0.38540673 0.14238357]. \t  \u001b[92m-0.38151301465495935\u001b[0m \t -0.38151301465495935\n",
      "98     \t [0.38541754 0.14239749]. \t  \u001b[92m-0.38149286037074037\u001b[0m \t -0.38149286037074037\n",
      "99     \t [0.3854283  0.14241012]. \t  \u001b[92m-0.38147430220346557\u001b[0m \t -0.38147430220346557\n",
      "100    \t [0.38543945 0.14242581]. \t  \u001b[92m-0.38145188476572717\u001b[0m \t -0.38145188476572717\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [-0.14193359 -0.19540192]. \t  \u001b[92m-5.950065894671566\u001b[0m \t -5.950065894671566\n",
      "2      \t [-0.44273985 -0.43381218]. \t  -41.750176819727436 \t -5.950065894671566\n",
      "3      \t [ 1.48735563 -0.11837618]. \t  -543.4085275143516 \t -5.950065894671566\n",
      "4      \t [ 1.43238166 -2.048     ]. \t  -1680.955081155458 \t -5.950065894671566\n",
      "5      \t [ 0.20586663 -0.56050241]. \t  -36.977496229377685 \t -5.950065894671566\n",
      "6      \t [-0.05244573  0.09171604]. \t  \u001b[92m-1.899127756700905\u001b[0m \t -1.899127756700905\n",
      "7      \t [-0.05296599  0.08862988]. \t  \u001b[92m-1.8453216408304045\u001b[0m \t -1.8453216408304045\n",
      "8      \t [-0.05292046  0.08912403]. \t  -1.8538153892028273 \t -1.8453216408304045\n",
      "9      \t [-0.0522328   0.10002181]. \t  -2.0537971897959073 \t -1.8453216408304045\n",
      "10     \t [-0.05134349  0.0981286 ]. \t  -2.017203838469579 \t -1.8453216408304045\n",
      "11     \t [-0.0542068   0.07847575]. \t  \u001b[92m-1.6819414549235425\u001b[0m \t -1.6819414549235425\n",
      "12     \t [-0.05461955  0.0779342 ]. \t  \u001b[92m-1.6739862093836329\u001b[0m \t -1.6739862093836329\n",
      "13     \t [-0.05548472  0.07225681]. \t  \u001b[92m-1.5926110451956608\u001b[0m \t -1.5926110451956608\n",
      "14     \t [-0.05552827  0.07445279]. \t  -1.623499035183782 \t -1.5926110451956608\n",
      "15     \t [-0.06194608  0.03556333]. \t  \u001b[92m-1.2283834759350958\u001b[0m \t -1.2283834759350958\n",
      "16     \t [-0.06385687  0.02435991]. \t  \u001b[92m-1.1729282497426154\u001b[0m \t -1.1729282497426154\n",
      "17     \t [-0.06428026  0.02156483]. \t  \u001b[92m-1.1630830023605903\u001b[0m \t -1.1630830023605903\n",
      "18     \t [-0.06543052  0.01720221]. \t  \u001b[92m-1.151837567515048\u001b[0m \t -1.151837567515048\n",
      "19     \t [-0.06917243 -0.00071706]. \t  \u001b[92m-1.1461567651990163\u001b[0m \t -1.1461567651990163\n",
      "20     \t [-0.06413407  0.02333251]. \t  -1.1693195750753091 \t -1.1461567651990163\n",
      "21     \t [-0.06632437  0.01210663]. \t  \u001b[92m-1.1429885262138977\u001b[0m \t -1.1429885262138977\n",
      "22     \t [-0.06468107  0.02139858]. \t  -1.1631812146258047 \t -1.1429885262138977\n",
      "23     \t [-0.06906154  0.0021239 ]. \t  -1.1435924965494733 \t -1.1429885262138977\n",
      "24     \t [-0.06514594  0.02270443]. \t  -1.1686146494562117 \t -1.1429885262138977\n",
      "25     \t [-0.06661512  0.01075041]. \t  \u001b[92m-1.1416530025404206\u001b[0m \t -1.1416530025404206\n",
      "26     \t [-0.06826051  0.00433394]. \t  \u001b[92m-1.1411911232728302\u001b[0m \t -1.1411911232728302\n",
      "27     \t [-0.06832259  0.00679534]. \t  -1.1417657186303458 \t -1.1411911232728302\n",
      "28     \t [-0.06245712  0.02707935]. \t  -1.182539200475988 \t -1.1411911232728302\n",
      "29     \t [-0.06526332  0.01645345]. \t  -1.1496556698955536 \t -1.1411911232728302\n",
      "30     \t [-0.07108078 -0.01038587]. \t  -1.1710483121321318 \t -1.1411911232728302\n",
      "31     \t [-0.06347259  0.01773147]. \t  -1.149750339700492 \t -1.1411911232728302\n",
      "32     \t [-0.07174765 -0.00123455]. \t  -1.152716353487082 \t -1.1411911232728302\n",
      "33     \t [-0.06233744 -0.00255722]. \t  \u001b[92m-1.132712300898865\u001b[0m \t -1.132712300898865\n",
      "34     \t [-0.07431225 -0.01696279]. \t  -1.2047047811634968 \t -1.132712300898865\n",
      "35     \t [-0.07152289 -0.01313391]. \t  -1.1814654680289267 \t -1.132712300898865\n",
      "36     \t [-0.06302227  0.03818437]. \t  -1.2470663181099095 \t -1.132712300898865\n",
      "37     \t [-0.06527762  0.01849388]. \t  -1.155073400959885 \t -1.132712300898865\n",
      "38     \t [-0.06398626  0.01181725]. \t  -1.1380312426848773 \t -1.132712300898865\n",
      "39     \t [-0.06072548  0.05468212]. \t  -1.3851827967775736 \t -1.132712300898865\n",
      "40     \t [-0.06394829  0.0272517 ]. \t  -1.1856352538074155 \t -1.132712300898865\n",
      "41     \t [-0.08215389 -0.05218809]. \t  -1.5184181399458787 \t -1.132712300898865\n",
      "42     \t [-0.06897844  0.01010461]. \t  -1.14557350661111 \t -1.132712300898865\n",
      "43     \t [-0.06985353 -0.00073608]. \t  -1.1477400771727189 \t -1.132712300898865\n",
      "44     \t [-0.06640716  0.00947646]. \t  -1.1397912242915966 \t -1.132712300898865\n",
      "45     \t [-0.07765949 -0.00506689]. \t  -1.1736662911747016 \t -1.132712300898865\n",
      "46     \t [-0.07913266 -0.03922196]. \t  -1.3714061705310336 \t -1.132712300898865\n",
      "47     \t [-0.06508531  0.01306224]. \t  -1.1421967830636675 \t -1.132712300898865\n",
      "48     \t [-0.06827094 -0.00459166]. \t  -1.1497638267290657 \t -1.132712300898865\n",
      "49     \t [-0.06295764 -0.00019641]. \t  \u001b[92m-1.1316095636873724\u001b[0m \t -1.1316095636873724\n",
      "50     \t [-0.06615624  0.03818195]. \t  -1.2509689772426784 \t -1.1316095636873724\n",
      "51     \t [-0.07619028 -0.03797435]. \t  -1.34984834164189 \t -1.1316095636873724\n",
      "52     \t [-0.06899871 -0.00393393]. \t  -1.1503181212422677 \t -1.1316095636873724\n",
      "53     \t [-0.07494     0.00573368]. \t  -1.1554973871415988 \t -1.1316095636873724\n",
      "54     \t [-0.08922616 -0.05100097]. \t  -1.5340686034359525 \t -1.1316095636873724\n",
      "55     \t [-0.07122602 -0.0015167 ]. \t  -1.1518677861233748 \t -1.1316095636873724\n",
      "56     \t [-0.07356423 -0.01931778]. \t  -1.213694882322816 \t -1.1316095636873724\n",
      "57     \t [-0.06640429  0.01084769]. \t  -1.1413630961060197 \t -1.1316095636873724\n",
      "58     \t [-0.06885135 -0.00111544]. \t  -1.1458724206141238 \t -1.1316095636873724\n",
      "59     \t [-0.0781027   0.03296545]. \t  -1.234480521757543 \t -1.1316095636873724\n",
      "60     \t [-0.05516368 -0.05543609]. \t  -1.455351181239458 \t -1.1316095636873724\n",
      "61     \t [-0.07241058 -0.02789694]. \t  -1.2598919323938813 \t -1.1316095636873724\n",
      "62     \t [-0.04403961 -0.01036882]. \t  \u001b[92m-1.1051681561545823\u001b[0m \t -1.1051681561545823\n",
      "63     \t [-0.05145473  0.01497073]. \t  -1.1207430383526467 \t -1.1051681561545823\n",
      "64     \t [-0.0478613 -0.040029 ]. \t  -1.2771090316379425 \t -1.1051681561545823\n",
      "65     \t [-0.06303455 -0.02970499]. \t  -1.2434655629225362 \t -1.1051681561545823\n",
      "66     \t [-0.05997442  0.01031249]. \t  -1.1280556549082257 \t -1.1051681561545823\n",
      "67     \t [-0.06273592  0.02721433]. \t  -1.183596668775107 \t -1.1051681561545823\n",
      "68     \t [-0.05634586 -0.00880533]. \t  -1.1302190543402768 \t -1.1051681561545823\n",
      "69     \t [-0.07335598  0.04237538]. \t  -1.2889507176819854 \t -1.1051681561545823\n",
      "70     \t [-0.02906839 -0.00218313]. \t  \u001b[92m-1.05989870042512\u001b[0m \t -1.05989870042512\n",
      "71     \t [-0.05111092 -0.00611454]. \t  -1.1124499904854062 \t -1.05989870042512\n",
      "72     \t [-0.05395802  0.01891226]. \t  -1.1364300331262194 \t -1.05989870042512\n",
      "73     \t [-0.04195024  0.05950988]. \t  -1.419167242568397 \t -1.05989870042512\n",
      "74     \t [-0.04646347 -0.00561727]. \t  -1.1011326159808312 \t -1.05989870042512\n",
      "75     \t [-0.0911488  -0.01503679]. \t  -1.2451041344644715 \t -1.05989870042512\n",
      "76     \t [-0.02914832 -0.02527193]. \t  -1.1273798095883465 \t -1.05989870042512\n",
      "77     \t [-0.04957848  0.03568871]. \t  -1.2120428194227717 \t -1.05989870042512\n",
      "78     \t [-0.04272755 -0.02877277]. \t  -1.1809070623974738 \t -1.05989870042512\n",
      "79     \t [-0.05379357  0.01567434]. \t  -1.1268152394527178 \t -1.05989870042512\n",
      "80     \t [-0.06126618 -0.01153332]. \t  -1.1496547403513093 \t -1.05989870042512\n",
      "81     \t [-0.08261435  0.0119323 ]. \t  -1.1746621517034386 \t -1.05989870042512\n",
      "82     \t [-0.06346894  0.01377028]. \t  -1.1404567869004762 \t -1.05989870042512\n",
      "83     \t [-0.04470038 -0.01274439]. \t  -1.1131330521569114 \t -1.05989870042512\n",
      "84     \t [-0.050855   -0.00845303]. \t  -1.116482760502987 \t -1.05989870042512\n",
      "85     \t [-0.04637438  0.04665509]. \t  -1.2929644385730819 \t -1.05989870042512\n",
      "86     \t [-0.03874269  0.00460862]. \t  -1.079952097298736 \t -1.05989870042512\n",
      "87     \t [-0.04015093  0.02274538]. \t  -1.126575532639401 \t -1.05989870042512\n",
      "88     \t [-0.06466386 -0.02410695]. \t  -1.2135322904762482 \t -1.05989870042512\n",
      "89     \t [-0.06583961 -0.02931214]. \t  -1.2492260930311956 \t -1.05989870042512\n",
      "90     \t [-0.06234941 -0.0098772 ]. \t  -1.1475328364442148 \t -1.05989870042512\n",
      "91     \t [-0.04094839 -0.01739118]. \t  -1.1199322466009058 \t -1.05989870042512\n",
      "92     \t [-0.06413616  0.01240249]. \t  -1.1392565899791567 \t -1.05989870042512\n",
      "93     \t [-0.05668065  0.0041058 ]. \t  -1.116653766843564 \t -1.05989870042512\n",
      "94     \t [-0.04835926  0.02861008]. \t  -1.168076120866386 \t -1.05989870042512\n",
      "95     \t [-0.05170983  0.01763435]. \t  -1.1284750433813522 \t -1.05989870042512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-0.08009974  0.06228371]. \t  -1.4787358609186323 \t -1.05989870042512\n",
      "97     \t [-0.06457827  0.02477407]. \t  -1.1757782032276551 \t -1.05989870042512\n",
      "98     \t [-0.05463327  0.01292974]. \t  -1.1221415272507267 \t -1.05989870042512\n",
      "99     \t [-0.01431292  0.0238844 ]. \t  -1.0849027555023518 \t -1.05989870042512\n",
      "100    \t [-0.04102443  0.02296224]. \t  -1.1290124548029865 \t -1.05989870042512\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.25191253 -0.26933563]. \t  -11.634922647730086 \t -1.3013277264983028\n",
      "2      \t [0.68033271 0.10113502]. \t  -13.186147677362065 \t -1.3013277264983028\n",
      "3      \t [0.96343546 1.98895725]. \t  -112.52025725351962 \t -1.3013277264983028\n",
      "4      \t [ 0.27238963 -0.83307395]. \t  -82.84331307729389 \t -1.3013277264983028\n",
      "5      \t [0.69495204 1.33201468]. \t  -72.18272124969387 \t -1.3013277264983028\n",
      "6      \t [1.41835422 1.37278362]. \t  -41.0001014946172 \t -1.3013277264983028\n",
      "7      \t [1.06770957 1.01733652]. \t  -1.5093089040090504 \t -1.3013277264983028\n",
      "8      \t [1.95467407 2.048     ]. \t  -315.1759083252769 \t -1.3013277264983028\n",
      "9      \t [-0.50128857 -0.61927732]. \t  -78.04265346909345 \t -1.3013277264983028\n",
      "10     \t [ 0.56576046 -0.35347956]. \t  -45.55747312153701 \t -1.3013277264983028\n",
      "11     \t [-0.34195682  0.5124912 ]. \t  -17.44736094219574 \t -1.3013277264983028\n",
      "12     \t [-0.14428166  0.08066253]. \t  -1.6675269486788769 \t -1.3013277264983028\n",
      "13     \t [-0.05788613 -0.32190974]. \t  -11.698564874093904 \t -1.3013277264983028\n",
      "14     \t [-0.89734821 -2.048     ]. \t  -817.6942495154673 \t -1.3013277264983028\n",
      "15     \t [0.04977087 0.48492167]. \t  -24.178208261393483 \t -1.3013277264983028\n",
      "16     \t [1.09660342 1.28759064]. \t  \u001b[92m-0.7327092033490287\u001b[0m \t -0.7327092033490287\n",
      "17     \t [1.01819605 1.09270098]. \t  \u001b[92m-0.31368244711385473\u001b[0m \t -0.31368244711385473\n",
      "18     \t [0.84502942 0.63994351]. \t  -0.5735596751359258 \t -0.31368244711385473\n",
      "19     \t [1.04934011 1.1187365 ]. \t  \u001b[92m-0.03348732179577754\u001b[0m \t -0.03348732179577754\n",
      "20     \t [1.05107024 1.12045055]. \t  \u001b[92m-0.027263139611265922\u001b[0m \t -0.027263139611265922\n",
      "21     \t [1.03856234 1.09441287]. \t  \u001b[92m-0.026454652823453318\u001b[0m \t -0.026454652823453318\n",
      "22     \t [0.87817428 0.76886647]. \t  \u001b[92m-0.015381412545230969\u001b[0m \t -0.015381412545230969\n",
      "23     \t [1.03677514 1.09751235]. \t  -0.05247211354648358 \t -0.015381412545230969\n",
      "24     \t [1.04399356 1.10660292]. \t  -0.029758903039254485 \t -0.015381412545230969\n",
      "25     \t [0.8549661  0.72188676]. \t  -0.02927995752963699 \t -0.015381412545230969\n",
      "26     \t [0.85780867 0.72657359]. \t  -0.028797069349280807 \t -0.015381412545230969\n",
      "27     \t [1.05350835 1.12587369]. \t  -0.02844344316410401 \t -0.015381412545230969\n",
      "28     \t [0.87191522 0.74454372]. \t  -0.04103092338340004 \t -0.015381412545230969\n",
      "29     \t [0.85512613 0.72187847]. \t  -0.029753562524423557 \t -0.015381412545230969\n",
      "30     \t [1.06186709 1.14997085]. \t  -0.05404448532989628 \t -0.015381412545230969\n",
      "31     \t [1.04837082 1.10730856]. \t  \u001b[92m-0.00910837514711145\u001b[0m \t -0.00910837514711145\n",
      "32     \t [1.04592395 1.10830652]. \t  -0.022700156401788674 \t -0.00910837514711145\n",
      "33     \t [1.02343563 1.05892333]. \t  -0.013780767161303277 \t -0.00910837514711145\n",
      "34     \t [1.05628597 1.13075598]. \t  -0.02571592317777292 \t -0.00910837514711145\n",
      "35     \t [1.04093466 1.09232895]. \t  -0.009391485587656177 \t -0.00910837514711145\n",
      "36     \t [1.04936939 1.11732372]. \t  -0.028511832755551622 \t -0.00910837514711145\n",
      "37     \t [0.87297952 0.75912659]. \t  -0.01701430898149213 \t -0.00910837514711145\n",
      "38     \t [1.0451967  1.10633026]. \t  -0.02134740666539922 \t -0.00910837514711145\n",
      "39     \t [0.85627958 0.73166728]. \t  -0.020895018323275085 \t -0.00910837514711145\n",
      "40     \t [1.05096417 1.11905133]. \t  -0.023696785349984366 \t -0.00910837514711145\n",
      "41     \t [1.05235909 1.12397586]. \t  -0.030019993464559677 \t -0.00910837514711145\n",
      "42     \t [1.02247343 1.05483048]. \t  -0.009300779674561306 \t -0.00910837514711145\n",
      "43     \t [0.85877549 0.72814704]. \t  -0.02868343039812036 \t -0.00910837514711145\n",
      "44     \t [1.0564783  1.13500339]. \t  -0.03874839150766217 \t -0.00910837514711145\n",
      "45     \t [1.04268761 1.10692704]. \t  -0.04074792624602916 \t -0.00910837514711145\n",
      "46     \t [0.84936548 0.71132517]. \t  -0.03288478683775879 \t -0.00910837514711145\n",
      "47     \t [1.05036402 1.12019555]. \t  -0.031202306170641576 \t -0.00910837514711145\n",
      "48     \t [1.03855052 1.09395832]. \t  -0.02511332378383552 \t -0.00910837514711145\n",
      "49     \t [1.03889558 1.10839195]. \t  -0.08612356252741057 \t -0.00910837514711145\n",
      "50     \t [0.86439653 0.74164696]. \t  -0.021451253274053363 \t -0.00910837514711145\n",
      "51     \t [1.04406993 1.10560714]. \t  -0.026045068671564708 \t -0.00910837514711145\n",
      "52     \t [1.0337513  1.08284296]. \t  -0.02130657694782151 \t -0.00910837514711145\n",
      "53     \t [1.04940968 1.10664101]. \t  \u001b[92m-0.005336106815346353\u001b[0m \t -0.005336106815346353\n",
      "54     \t [0.84877625 0.69843948]. \t  -0.07118789577475834 \t -0.005336106815346353\n",
      "55     \t [1.0466109  1.11376628]. \t  -0.03592526431695034 \t -0.005336106815346353\n",
      "56     \t [1.08176437 1.18127814]. \t  -0.01892662146244956 \t -0.005336106815346353\n",
      "57     \t [1.04470557 1.1076017 ]. \t  -0.028216562558357736 \t -0.005336106815346353\n",
      "58     \t [0.84598411 0.68513248]. \t  -0.11709170687030299 \t -0.005336106815346353\n",
      "59     \t [0.88370802 0.78932599]. \t  -0.0205565370757331 \t -0.005336106815346353\n",
      "60     \t [0.84658874 0.70722215]. \t  -0.03254168272308579 \t -0.005336106815346353\n",
      "61     \t [0.867096   0.74631475]. \t  -0.020733440271885516 \t -0.005336106815346353\n",
      "62     \t [1.03997313 1.09703669]. \t  -0.025599861004058852 \t -0.005336106815346353\n",
      "63     \t [0.90018927 0.80813331]. \t  -0.010449448641326197 \t -0.005336106815346353\n",
      "64     \t [1.06131766 1.1429883 ]. \t  -0.03129306280354214 \t -0.005336106815346353\n",
      "65     \t [0.93918576 0.91404539]. \t  -0.10594156286661974 \t -0.005336106815346353\n",
      "66     \t [1.03270378 1.08009527]. \t  -0.019614987764241067 \t -0.005336106815346353\n",
      "67     \t [0.86362139 0.74217568]. \t  -0.01994324512948227 \t -0.005336106815346353\n",
      "68     \t [0.85923459 0.75546947]. \t  -0.04934869455811182 \t -0.005336106815346353\n",
      "69     \t [0.82088976 0.6538759 ]. \t  -0.0720169322400678 \t -0.005336106815346353\n",
      "70     \t [0.85743243 0.74033794]. \t  -0.022975269145292847 \t -0.005336106815346353\n",
      "71     \t [0.86002301 0.71639562]. \t  -0.07362172975398723 \t -0.005336106815346353\n",
      "72     \t [0.83757437 0.71383395]. \t  -0.04151879489558103 \t -0.005336106815346353\n",
      "73     \t [0.83761024 0.69039582]. \t  -0.03890345452355949 \t -0.005336106815346353\n",
      "74     \t [0.87896408 0.76818154]. \t  -0.016582452280351146 \t -0.005336106815346353\n",
      "75     \t [0.88202874 0.7715293 ]. \t  -0.018071537921980435 \t -0.005336106815346353\n",
      "76     \t [0.86714733 0.7394492 ]. \t  -0.03326307805988174 \t -0.005336106815346353\n",
      "77     \t [1.03489215 1.08371271]. \t  -0.017374291025161723 \t -0.005336106815346353\n",
      "78     \t [1.03666798 1.08689614]. \t  -0.016266743640293683 \t -0.005336106815346353\n",
      "79     \t [0.8672255  0.75191561]. \t  -0.017631772298528883 \t -0.005336106815346353\n",
      "80     \t [0.93015181 0.86239578]. \t  -0.005655293749100168 \t -0.005336106815346353\n",
      "81     \t [0.84597284 0.70841103]. \t  -0.028993703341631594 \t -0.005336106815346353\n",
      "82     \t [0.86083077 0.73821503]. \t  -0.020160263984899694 \t -0.005336106815346353\n",
      "83     \t [0.8473887  0.68190269]. \t  -0.1540803604162987 \t -0.005336106815346353\n",
      "84     \t [1.02827374 1.06472657]. \t  -0.0062453941562405895 \t -0.005336106815346353\n",
      "85     \t [1.07282528 1.14708459]. \t  -0.006800813436589022 \t -0.005336106815346353\n",
      "86     \t [0.90843291 0.826407  ]. \t  -0.008518315660943764 \t -0.005336106815346353\n",
      "87     \t [0.88121253 0.7725874 ]. \t  -0.01566923551060481 \t -0.005336106815346353\n",
      "88     \t [1.0309349 1.0684589]. \t  \u001b[92m-0.004129051957581238\u001b[0m \t -0.004129051957581238\n",
      "89     \t [1.05441649 1.12912856]. \t  -0.03300935956663419 \t -0.004129051957581238\n",
      "90     \t [0.87114005 0.76454416]. \t  -0.019807506163416685 \t -0.004129051957581238\n",
      "91     \t [0.87926409 0.77206307]. \t  -0.014685792179896805 \t -0.004129051957581238\n",
      "92     \t [0.84360919 0.68424177]. \t  -0.09972433790781643 \t -0.004129051957581238\n",
      "93     \t [0.89630822 0.79152106]. \t  -0.024787978488331884 \t -0.004129051957581238\n",
      "94     \t [1.05223455 1.12452756]. \t  -0.03276139743813227 \t -0.004129051957581238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95     \t [0.82724205 0.69179979]. \t  -0.0354259701077393 \t -0.004129051957581238\n",
      "96     \t [1.03264091 1.08113841]. \t  -0.02294323939535154 \t -0.004129051957581238\n",
      "97     \t [1.07129924 1.16748952]. \t  -0.044317108491449174 \t -0.004129051957581238\n",
      "98     \t [0.90818306 0.8168407 ]. \t  -0.014759773526095157 \t -0.004129051957581238\n",
      "99     \t [1.0493162  1.11790902]. \t  -0.030805915078558125 \t -0.004129051957581238\n",
      "100    \t [1.02718932 1.02241225]. \t  -0.10770526174755672 \t -0.004129051957581238\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.59078388 -1.05985098]. \t  -201.02391142232844 \t -1.118465165857483\n",
      "2      \t [ 0.18929989 -0.42444766]. \t  -21.84319668096658 \t -1.118465165857483\n",
      "3      \t [-0.4832137   1.23197831]. \t  -101.89671860809048 \t -1.118465165857483\n",
      "4      \t [-0.02894468  0.40798155]. \t  -17.63533124731379 \t -1.118465165857483\n",
      "5      \t [-0.87717308  0.41687208]. \t  -15.953671544605932 \t -1.118465165857483\n",
      "6      \t [-0.39090889  0.2712666 ]. \t  -3.3378298041941385 \t -1.118465165857483\n",
      "7      \t [-2.048       0.83225108]. \t  -1139.6302897097958 \t -1.118465165857483\n",
      "8      \t [1.33608132 1.60739178]. \t  -3.2714445748444736 \t -1.118465165857483\n",
      "9      \t [-0.8533829   0.03917884]. \t  -50.918639711466355 \t -1.118465165857483\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.118465165857483\n",
      "11     \t [1.12561596 0.97334107]. \t  -8.63999994688338 \t -1.118465165857483\n",
      "12     \t [0.69035285 2.048     ]. \t  -247.02974677159136 \t -1.118465165857483\n",
      "13     \t [1.35008757 1.29851893]. \t  -27.60296067260461 \t -1.118465165857483\n",
      "14     \t [1.03219542 1.35992187]. \t  -8.673737173799921 \t -1.118465165857483\n",
      "15     \t [0.52359594 0.20145254]. \t  \u001b[92m-0.755492366500458\u001b[0m \t -0.755492366500458\n",
      "16     \t [1.24362343 1.77643715]. \t  -5.34189859647979 \t -0.755492366500458\n",
      "17     \t [0.21802687 0.09577474]. \t  -0.8441822924771878 \t -0.755492366500458\n",
      "18     \t [0.72719004 0.59454506]. \t  \u001b[92m-0.5065960852941571\u001b[0m \t -0.5065960852941571\n",
      "19     \t [0.87404923 0.47956957]. \t  -8.103771880437629 \t -0.5065960852941571\n",
      "20     \t [-0.72138101  0.66713003]. \t  -5.116399950632349 \t -0.5065960852941571\n",
      "21     \t [1.20206639 1.45471153]. \t  \u001b[92m-0.05033301427051494\u001b[0m \t -0.05033301427051494\n",
      "22     \t [-0.64599463  0.45760637]. \t  -2.8716856443787133 \t -0.05033301427051494\n",
      "23     \t [1.25081999 1.59482672]. \t  -0.15457479449051142 \t -0.05033301427051494\n",
      "24     \t [0.94443117 0.91575344]. \t  -0.05974714221641332 \t -0.05033301427051494\n",
      "25     \t [1.02332035 1.06896857]. \t  \u001b[92m-0.04799823080424251\u001b[0m \t -0.04799823080424251\n",
      "26     \t [1.28121755 1.64015122]. \t  -0.07927023408088456 \t -0.04799823080424251\n",
      "27     \t [1.25639948 1.58905006]. \t  -0.07678756014606927 \t -0.04799823080424251\n",
      "28     \t [0.98629362 0.99308792]. \t  \u001b[92m-0.04144888585514873\u001b[0m \t -0.04144888585514873\n",
      "29     \t [0.92075643 0.86839122]. \t  -0.04871067251236496 \t -0.04144888585514873\n",
      "30     \t [1.1884826  1.42575873]. \t  -0.05312923584370551 \t -0.04144888585514873\n",
      "31     \t [1.22286492 1.52424048]. \t  -0.13285412243221154 \t -0.04144888585514873\n",
      "32     \t [1.21453935 1.48355329]. \t  -0.053163093871479786 \t -0.04144888585514873\n",
      "33     \t [0.93690963 0.8967794 ]. \t  \u001b[92m-0.04000349373108626\u001b[0m \t -0.04000349373108626\n",
      "34     \t [0.98929048 0.99817012]. \t  \u001b[92m-0.03804021103158116\u001b[0m \t -0.03804021103158116\n",
      "35     \t [0.91555075 0.86819976]. \t  -0.09693135412088737 \t -0.03804021103158116\n",
      "36     \t [0.90038098 0.8256266 ]. \t  \u001b[92m-0.032246365360190515\u001b[0m \t -0.032246365360190515\n",
      "37     \t [1.27487522 1.63505286]. \t  -0.08505492312705733 \t -0.032246365360190515\n",
      "38     \t [2.04799997 2.048     ]. \t  -461.76034234305394 \t -0.032246365360190515\n",
      "39     \t [1.90638756 1.400852  ]. \t  -499.6565761933034 \t -0.032246365360190515\n",
      "40     \t [-1.5669191   1.31830112]. \t  -135.8510443471627 \t -0.032246365360190515\n",
      "41     \t [-0.6308538   0.32751659]. \t  -3.156144136364855 \t -0.032246365360190515\n",
      "42     \t [-0.74531778  0.90043127]. \t  -14.943989108006267 \t -0.032246365360190515\n",
      "43     \t [0.37486958 0.12574433]. \t  -0.41264138062708633 \t -0.032246365360190515\n",
      "44     \t [-0.49013035  0.113146  ]. \t  -3.835465889381952 \t -0.032246365360190515\n",
      "45     \t [-0.60854958  0.55590324]. \t  -6.031078522192422 \t -0.032246365360190515\n",
      "46     \t [0.77723015 0.28129547]. \t  -10.46904464830926 \t -0.032246365360190515\n",
      "47     \t [-1.14389051  0.90007763]. \t  -21.275965558374203 \t -0.032246365360190515\n",
      "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.032246365360190515\n",
      "49     \t [0.51566321 0.61334818]. \t  -12.306012149552478 \t -0.032246365360190515\n",
      "50     \t [0.21186722 0.08804874]. \t  -0.8074406185287324 \t -0.032246365360190515\n",
      "51     \t [1.00469337 1.87833184]. \t  -75.50275398606166 \t -0.032246365360190515\n",
      "52     \t [ 1.47087737 -0.79711835]. \t  -876.7361204167271 \t -0.032246365360190515\n",
      "53     \t [-1.62081535  1.29941828]. \t  -183.12725700106427 \t -0.032246365360190515\n",
      "54     \t [1.87548978 1.44783844]. \t  -429.1006146679007 \t -0.032246365360190515\n",
      "55     \t [-0.95252395  0.07693675]. \t  -72.7629728343826 \t -0.032246365360190515\n",
      "56     \t [1.62745957 0.40127162]. \t  -505.4532696180515 \t -0.032246365360190515\n",
      "57     \t [-0.06663816  1.8800146 ]. \t  -352.915484102184 \t -0.032246365360190515\n",
      "58     \t [0.87472748 1.96633851]. \t  -144.3015198463999 \t -0.032246365360190515\n",
      "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.032246365360190515\n",
      "60     \t [0.10122637 1.28148254]. \t  -162.41183067541195 \t -0.032246365360190515\n",
      "61     \t [0.97098141 1.07564722]. \t  -1.7655505235114741 \t -0.032246365360190515\n",
      "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.032246365360190515\n",
      "63     \t [1.77371444 1.19321336]. \t  -381.9607676185476 \t -0.032246365360190515\n",
      "64     \t [0.81472395 0.50652259]. \t  -2.507163168891592 \t -0.032246365360190515\n",
      "65     \t [-0.56962728  0.77186033]. \t  -22.47907203764714 \t -0.032246365360190515\n",
      "66     \t [1.6660836  0.39597012]. \t  -566.8191464950605 \t -0.032246365360190515\n",
      "67     \t [-1.29162818  0.20955702]. \t  -218.04564925677252 \t -0.032246365360190515\n",
      "68     \t [-0.2392256  -0.03594745]. \t  -2.4038629848953867 \t -0.032246365360190515\n",
      "69     \t [0.13726616 0.89111239]. \t  -76.82987319556493 \t -0.032246365360190515\n",
      "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.032246365360190515\n",
      "71     \t [1.28690086 0.9224274 ]. \t  -53.911891272617204 \t -0.032246365360190515\n",
      "72     \t [1.36004558 1.85509004]. \t  -0.13251226977533717 \t -0.032246365360190515\n",
      "73     \t [1.16989133 1.1881658 ]. \t  -3.2861635530219657 \t -0.032246365360190515\n",
      "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.032246365360190515\n",
      "75     \t [0.07882742 0.07901573]. \t  -1.3785715680071087 \t -0.032246365360190515\n",
      "76     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.032246365360190515\n",
      "77     \t [1.86724692 1.93275361]. \t  -242.19941260834418 \t -0.032246365360190515\n",
      "78     \t [1.88493385 0.55999826]. \t  -896.5744506258867 \t -0.032246365360190515\n",
      "79     \t [-0.66359158  0.1419443 ]. \t  -11.672359020645331 \t -0.032246365360190515\n",
      "80     \t [-1.11729327 -0.0068485 ]. \t  -162.0338176682582 \t -0.032246365360190515\n",
      "81     \t [1.43012704 0.83279453]. \t  -147.1930756729963 \t -0.032246365360190515\n",
      "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.032246365360190515\n",
      "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.032246365360190515\n",
      "84     \t [0.59759045 1.25113413]. \t  -80.08907053588747 \t -0.032246365360190515\n",
      "85     \t [1.3791239  0.81616375]. \t  -118.04401966450507 \t -0.032246365360190515\n",
      "86     \t [-1.29488243  0.23170129]. \t  -214.07454278343977 \t -0.032246365360190515\n",
      "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.032246365360190515\n",
      "88     \t [1.43461294 0.78561241]. \t  -162.1149938824956 \t -0.032246365360190515\n",
      "89     \t [-0.42149914  1.71253973]. \t  -237.60577046989158 \t -0.032246365360190515\n",
      "90     \t [-0.09645539  0.81471556]. \t  -66.07105004352411 \t -0.032246365360190515\n",
      "91     \t [1.00166611 0.76267895]. \t  -5.791536090492289 \t -0.032246365360190515\n",
      "92     \t [ 1.88578136 -0.00540991]. \t  -1269.2706945134332 \t -0.032246365360190515\n",
      "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.032246365360190515\n",
      "94     \t [0.20671313 1.23995104]. \t  -143.96305008676384 \t -0.032246365360190515\n",
      "95     \t [-0.67784553  0.8152455 ]. \t  -15.472461212044728 \t -0.032246365360190515\n",
      "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.032246365360190515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [ 1.34080629 -0.92179771]. \t  -739.7163827482883 \t -0.032246365360190515\n",
      "98     \t [ 1.96201856 -0.58409861]. \t  -1966.62007213712 \t -0.032246365360190515\n",
      "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.032246365360190515\n",
      "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.032246365360190515\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.37151344 -1.61875516]. \t  -1225.0010186831207 \t -3.8154339181361143\n",
      "init   \t [ 1.00204741 -0.57138857]. \t  -248.216119608043 \t -3.8154339181361143\n",
      "init   \t [-0.57626281  0.44744041]. \t  -3.8154339181361143 \t -3.8154339181361143\n",
      "init   \t [-0.43507896 -0.37243859]. \t  -33.61376822003779 \t -3.8154339181361143\n",
      "init   \t [0.04056027 0.86076618]. \t  -74.72942162517022 \t -3.8154339181361143\n",
      "1      \t [1.44439417 0.4158791 ]. \t  -279.2195689248724 \t -3.8154339181361143\n",
      "2      \t [-1.5894237   1.20251294]. \t  -181.93778270267617 \t -3.8154339181361143\n",
      "3      \t [ 0.16700258 -0.02921685]. \t  \u001b[92m-1.020002367572181\u001b[0m \t -1.020002367572181\n",
      "4      \t [-0.25152601  0.1080278 ]. \t  -1.7666852004748752 \t -1.020002367572181\n",
      "5      \t [ 0.0689832  -0.07588243]. \t  -1.517091124268759 \t -1.020002367572181\n",
      "6      \t [-0.58304103  0.23935743]. \t  -3.5176406849826103 \t -1.020002367572181\n",
      "7      \t [-2.048     -1.8271054]. \t  -3635.027421412441 \t -1.020002367572181\n",
      "8      \t [ 0.37360878 -0.01023852]. \t  -2.6370302758918003 \t -1.020002367572181\n",
      "9      \t [ 0.16684521 -0.02934936]. \t  -1.0211785699915694 \t -1.020002367572181\n",
      "10     \t [ 0.2107329  -0.02709945]. \t  -1.1342792585088886 \t -1.020002367572181\n",
      "11     \t [ 0.16426087 -0.02678853]. \t  \u001b[92m-0.9875829278097977\u001b[0m \t -0.9875829278097977\n",
      "12     \t [ 0.16362316 -0.0288986 ]. \t  -1.0094537576784424 \t -0.9875829278097977\n",
      "13     \t [ 0.16637664 -0.02390425]. \t  \u001b[92m-0.9610335750262209\u001b[0m \t -0.9610335750262209\n",
      "14     \t [ 0.13737963 -0.02908627]. \t  -0.9741246640233421 \t -0.9610335750262209\n",
      "15     \t [ 0.15723753 -0.02085771]. \t  \u001b[92m-0.9180145539075899\u001b[0m \t -0.9180145539075899\n",
      "16     \t [ 0.17356397 -0.0193138 ]. \t  -0.927410555352815 \t -0.9180145539075899\n",
      "17     \t [ 0.09976795 -0.01570591]. \t  \u001b[92m-0.876259037104201\u001b[0m \t -0.876259037104201\n",
      "18     \t [ 0.06070931 -0.03134052]. \t  -1.004950052786116 \t -0.876259037104201\n",
      "19     \t [ 0.14722478 -0.0172893 ]. \t  -0.879048314582573 \t -0.876259037104201\n",
      "20     \t [ 0.16705619 -0.0162604 ]. \t  -0.888878089401055 \t -0.876259037104201\n",
      "21     \t [ 0.19802376 -0.00688448]. \t  \u001b[92m-0.8556674391862124\u001b[0m \t -0.8556674391862124\n",
      "22     \t [ 0.16286581 -0.01716332]. \t  -0.8916629357679523 \t -0.8556674391862124\n",
      "23     \t [ 0.15984048 -0.0102476 ]. \t  \u001b[92m-0.8340075495392165\u001b[0m \t -0.8340075495392165\n",
      "24     \t [ 0.18890191 -0.00466084]. \t  \u001b[92m-0.8206501364940194\u001b[0m \t -0.8206501364940194\n",
      "25     \t [ 0.12335705 -0.01218697]. \t  -0.8436003922962861 \t -0.8206501364940194\n",
      "26     \t [ 0.07517052 -0.01064893]. \t  -0.8818770747400854 \t -0.8206501364940194\n",
      "27     \t [ 0.14433021 -0.00409975]. \t  \u001b[92m-0.7943260452230474\u001b[0m \t -0.7943260452230474\n",
      "28     \t [0.19124066 0.0008113 ]. \t  \u001b[92m-0.7819815233909919\u001b[0m \t -0.7819815233909919\n",
      "29     \t [ 0.10838989 -0.00441468]. \t  -0.8210929978567626 \t -0.7819815233909919\n",
      "30     \t [ 0.17029206 -0.0025839 ]. \t  -0.7881656487561626 \t -0.7819815233909919\n",
      "31     \t [ 0.15175598 -0.00495427]. \t  -0.7978291626915223 \t -0.7819815233909919\n",
      "32     \t [0.16353477 0.00243339]. \t  \u001b[92m-0.7587727990810409\u001b[0m \t -0.7587727990810409\n",
      "33     \t [ 0.14416043 -0.00295942]. \t  -0.7888279708014594 \t -0.7587727990810409\n",
      "34     \t [ 0.19003141 -0.01304977]. \t  -0.89773645135822 \t -0.7587727990810409\n",
      "35     \t [ 0.1664577  -0.00799934]. \t  -0.8222953976712432 \t -0.7587727990810409\n",
      "36     \t [0.09654561 0.00341982]. \t  -0.8197122822188083 \t -0.7587727990810409\n",
      "37     \t [ 0.10163672 -0.03185681]. \t  -0.9850294811623911 \t -0.7587727990810409\n",
      "38     \t [ 0.10964539 -0.00078851]. \t  -0.8091425504433694 \t -0.7587727990810409\n",
      "39     \t [ 0.14177708 -0.01057748]. \t  -0.8306618857957367 \t -0.7587727990810409\n",
      "40     \t [0.17723817 0.00855866]. \t  \u001b[92m-0.7291708168231257\u001b[0m \t -0.7291708168231257\n",
      "41     \t [0.17215254 0.00927838]. \t  \u001b[92m-0.7267766910406985\u001b[0m \t -0.7267766910406985\n",
      "42     \t [0.18652352 0.01680589]. \t  \u001b[92m-0.6940904912640091\u001b[0m \t -0.6940904912640091\n",
      "43     \t [0.12430069 0.00297188]. \t  -0.7824212701942331 \t -0.6940904912640091\n",
      "44     \t [0.12144184 0.0053007 ]. \t  -0.7807898158917866 \t -0.6940904912640091\n",
      "45     \t [ 0.07447987 -0.01801548]. \t  -0.9121077433559026 \t -0.6940904912640091\n",
      "46     \t [0.26965944 0.00477709]. \t  -0.994969869991426 \t -0.6940904912640091\n",
      "47     \t [0.19446114 0.02060228]. \t  \u001b[92m-0.6785210993900119\u001b[0m \t -0.6785210993900119\n",
      "48     \t [0.16828802 0.00318668]. \t  -0.7549175175306149 \t -0.6785210993900119\n",
      "49     \t [ 0.13406753 -0.00590621]. \t  -0.8068659779452196 \t -0.6785210993900119\n",
      "50     \t [0.18165397 0.0189574 ]. \t  -0.6894045387465694 \t -0.6785210993900119\n",
      "51     \t [0.18399275 0.01360525]. \t  -0.7068663340949408 \t -0.6785210993900119\n",
      "52     \t [0.16689049 0.00962511]. \t  -0.7272950076053607 \t -0.6785210993900119\n",
      "53     \t [0.182327  0.0264079]. \t  \u001b[92m-0.6732611782413619\u001b[0m \t -0.6732611782413619\n",
      "54     \t [0.18018821 0.02771212]. \t  -0.6743530085734025 \t -0.6732611782413619\n",
      "55     \t [ 0.13837384 -0.01866001]. \t  -0.8853390540447486 \t -0.6732611782413619\n",
      "56     \t [0.21662645 0.0277741 ]. \t  \u001b[92m-0.6503575297827697\u001b[0m \t -0.6503575297827697\n",
      "57     \t [0.18965362 0.02319079]. \t  -0.6729882404759491 \t -0.6503575297827697\n",
      "58     \t [0.19082604 0.03587016]. \t  -0.6547921419225385 \t -0.6503575297827697\n",
      "59     \t [0.09927617 0.00153015]. \t  -0.8182349879121033 \t -0.6503575297827697\n",
      "60     \t [0.19608691 0.01121373]. \t  -0.7204580852125327 \t -0.6503575297827697\n",
      "61     \t [ 0.19002321 -0.00147674]. \t  -0.7973298627287386 \t -0.6503575297827697\n",
      "62     \t [0.20614067 0.05003924]. \t  \u001b[92m-0.6359057408926869\u001b[0m \t -0.6359057408926869\n",
      "63     \t [0.1497342  0.01904564]. \t  -0.7240907932274772 \t -0.6359057408926869\n",
      "64     \t [0.21543314 0.01635112]. \t  -0.7059074255137756 \t -0.6359057408926869\n",
      "65     \t [0.23183489 0.03723067]. \t  \u001b[92m-0.617357913228931\u001b[0m \t -0.617357913228931\n",
      "66     \t [0.15538675 0.02677749]. \t  -0.714064524269391 \t -0.617357913228931\n",
      "67     \t [0.11984499 0.00638309]. \t  -0.7810404465739744 \t -0.617357913228931\n",
      "68     \t [0.24035733 0.02491755]. \t  -0.6849961374301585 \t -0.617357913228931\n",
      "69     \t [0.22307874 0.04515205]. \t  \u001b[92m-0.6057337655988437\u001b[0m \t -0.6057337655988437\n",
      "70     \t [0.23060422 0.04154015]. \t  \u001b[92m-0.6055145309447443\u001b[0m \t -0.6055145309447443\n",
      "71     \t [0.23054466 0.04990505]. \t  \u001b[92m-0.5931150296908587\u001b[0m \t -0.5931150296908587\n",
      "72     \t [0.21071184 0.05083962]. \t  -0.6271233373023447 \t -0.5931150296908587\n",
      "73     \t [0.21435861 0.04280539]. \t  -0.6182210031299501 \t -0.5931150296908587\n",
      "74     \t [0.19706992 0.03523923]. \t  -0.6459907904605285 \t -0.5931150296908587\n",
      "75     \t [0.186847   0.00814438]. \t  -0.7328672987708991 \t -0.5931150296908587\n",
      "76     \t [0.21839517 0.0488513 ]. \t  -0.6110394827474525 \t -0.5931150296908587\n",
      "77     \t [0.21710204 0.04311184]. \t  -0.6145464278413789 \t -0.5931150296908587\n",
      "78     \t [0.22091123 0.05176905]. \t  -0.6078597765871808 \t -0.5931150296908587\n",
      "79     \t [0.23108371 0.03812169]. \t  -0.6145739545232382 \t -0.5931150296908587\n",
      "80     \t [0.21127407 0.04352417]. \t  -0.6222123712555057 \t -0.5931150296908587\n",
      "81     \t [0.19777294 0.02718468]. \t  -0.6577994366637828 \t -0.5931150296908587\n",
      "82     \t [0.23134556 0.03545098]. \t  -0.6234813717767244 \t -0.5931150296908587\n",
      "83     \t [0.17723783 0.02942086]. \t  -0.6773345472321208 \t -0.5931150296908587\n",
      "84     \t [0.20197269 0.00180612]. \t  -0.7888449993913712 \t -0.5931150296908587\n",
      "85     \t [0.21234773 0.0490205 ]. \t  -0.6219397489847912 \t -0.5931150296908587\n",
      "86     \t [0.11266671 0.04443382]. \t  -0.8881033475494442 \t -0.5931150296908587\n",
      "87     \t [0.13649375 0.02131773]. \t  -0.7463651297940165 \t -0.5931150296908587\n",
      "88     \t [0.18336429 0.02328719]. \t  -0.6775756745843168 \t -0.5931150296908587\n",
      "89     \t [0.17811643 0.03821284]. \t  -0.6797012016297361 \t -0.5931150296908587\n",
      "90     \t [0.22134887 0.05003126]. \t  -0.6064048972818203 \t -0.5931150296908587\n",
      "91     \t [0.21730697 0.03792926]. \t  -0.6212444644479502 \t -0.5931150296908587\n",
      "92     \t [0.25690977 0.04579534]. \t  \u001b[92m-0.5930165481518407\u001b[0m \t -0.5930165481518407\n",
      "93     \t [0.21221005 0.04409463]. \t  -0.6207010723966829 \t -0.5930165481518407\n",
      "94     \t [0.18594777 0.03261804]. \t  -0.6630646250525764 \t -0.5930165481518407\n",
      "95     \t [0.23877498 0.04370301]. \t  -0.5971804289665325 \t -0.5930165481518407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.21753886 0.04856888]. \t  -0.6124006110568514 \t -0.5930165481518407\n",
      "97     \t [0.20270017 0.03355571]. \t  -0.6413595878246099 \t -0.5930165481518407\n",
      "98     \t [0.20436138 0.04192725]. \t  -0.6330434933365039 \t -0.5930165481518407\n",
      "99     \t [0.22048047 0.04050829]. \t  -0.6142171219942729 \t -0.5930165481518407\n",
      "100    \t [0.17693552 0.02627973]. \t  -0.679961657191724 \t -0.5930165481518407\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-0.3007858   0.35230453]. \t  -8.54766574781701 \t -1.9278091788796494\n",
      "2      \t [0.05043877 0.46346229]. \t  -22.146227632849655 \t -1.9278091788796494\n",
      "3      \t [-0.51358757 -0.11382375]. \t  -16.54881693125303 \t -1.9278091788796494\n",
      "4      \t [-0.00362294  0.03849649]. \t  \u001b[92m-1.155355975101933\u001b[0m \t -1.155355975101933\n",
      "5      \t [ 0.55293391 -0.12702264]. \t  -18.927863958416626 \t -1.155355975101933\n",
      "6      \t [-1.54632787  0.19802638]. \t  -487.45407786075464 \t -1.155355975101933\n",
      "7      \t [-0.41010427 -1.9322449 ]. \t  -443.1691867745656 \t -1.155355975101933\n",
      "8      \t [-1.28756571  2.048     ]. \t  -20.456574915347357 \t -1.155355975101933\n",
      "9      \t [-0.20311728  0.04730445]. \t  -1.451148801759307 \t -1.155355975101933\n",
      "10     \t [0.22626742 0.08407997]. \t  \u001b[92m-0.7067914125450919\u001b[0m \t -0.7067914125450919\n",
      "11     \t [0.23132753 0.06836698]. \t  \u001b[92m-0.6129231320082315\u001b[0m \t -0.6129231320082315\n",
      "12     \t [0.28659604 0.06621435]. \t  \u001b[92m-0.5342992197979932\u001b[0m \t -0.5342992197979932\n",
      "13     \t [0.28281651 0.0785526 ]. \t  \u001b[92m-0.5145573812812198\u001b[0m \t -0.5145573812812198\n",
      "14     \t [0.27287318 0.07310298]. \t  -0.5288975041353974 \t -0.5145573812812198\n",
      "15     \t [0.28196097 0.07949485]. \t  -0.5155800559562799 \t -0.5145573812812198\n",
      "16     \t [0.28879862 0.07308604]. \t  -0.5164547649083194 \t -0.5145573812812198\n",
      "17     \t [0.28189476 0.08366898]. \t  -0.5174427638524607 \t -0.5145573812812198\n",
      "18     \t [0.27638828 0.0797406 ]. \t  -0.5247362555095632 \t -0.5145573812812198\n",
      "19     \t [0.26292028 0.08367064]. \t  -0.5644380553514078 \t -0.5145573812812198\n",
      "20     \t [0.30601091 0.09324394]. \t  \u001b[92m-0.4816367498037797\u001b[0m \t -0.4816367498037797\n",
      "21     \t [0.28061017 0.08193886]. \t  -0.5185436713850322 \t -0.4816367498037797\n",
      "22     \t [0.28994021 0.07906141]. \t  -0.5066888219308993 \t -0.4816367498037797\n",
      "23     \t [0.28489805 0.08541987]. \t  -0.5131795836478964 \t -0.4816367498037797\n",
      "24     \t [0.28083421 0.08385443]. \t  -0.5196860272953531 \t -0.4816367498037797\n",
      "25     \t [0.30310612 0.10726756]. \t  -0.5093593611611335 \t -0.4816367498037797\n",
      "26     \t [0.27896872 0.1218479 ]. \t  -0.7137004735680562 \t -0.4816367498037797\n",
      "27     \t [0.28542946 0.0727902 ]. \t  -0.5181449012400411 \t -0.4816367498037797\n",
      "28     \t [0.31088114 0.10238383]. \t  \u001b[92m-0.47817583918100814\u001b[0m \t -0.47817583918100814\n",
      "29     \t [0.27883657 0.09323884]. \t  -0.5440676281040282 \t -0.47817583918100814\n",
      "30     \t [0.32264934 0.10940605]. \t  \u001b[92m-0.46161658290026114\u001b[0m \t -0.46161658290026114\n",
      "31     \t [0.20547228 0.05865535]. \t  -0.6582901353357599 \t -0.46161658290026114\n",
      "32     \t [0.34704751 0.12439217]. \t  \u001b[92m-0.4279073559468492\u001b[0m \t -0.4279073559468492\n",
      "33     \t [0.2876726  0.09369855]. \t  -0.5193852871866163 \t -0.4279073559468492\n",
      "34     \t [0.37342896 0.10763315]. \t  -0.4938173327578478 \t -0.4279073559468492\n",
      "35     \t [-0.11273789 -0.00704009]. \t  -1.2771915315086586 \t -0.4279073559468492\n",
      "36     \t [0.29561004 0.09190486]. \t  -0.4982078578299224 \t -0.4279073559468492\n",
      "37     \t [0.29128909 0.08990631]. \t  -0.5048284520084082 \t -0.4279073559468492\n",
      "38     \t [0.35205616 0.13914365]. \t  -0.44293556925383043 \t -0.4279073559468492\n",
      "39     \t [0.30057332 0.09866225]. \t  -0.49611646372555956 \t -0.4279073559468492\n",
      "40     \t [0.3183473  0.09084471]. \t  -0.4756760137501444 \t -0.4279073559468492\n",
      "41     \t [0.31868005 0.08340169]. \t  -0.49715831235138336 \t -0.4279073559468492\n",
      "42     \t [0.32676339 0.10157343]. \t  -0.45595245051702654 \t -0.4279073559468492\n",
      "43     \t [0.33108184 0.10665349]. \t  -0.4483286706351546 \t -0.4279073559468492\n",
      "44     \t [0.30225801 0.07574301]. \t  -0.5112326147310429 \t -0.4279073559468492\n",
      "45     \t [0.32418694 0.12086874]. \t  -0.4815975112919227 \t -0.4279073559468492\n",
      "46     \t [0.33575627 0.09999517]. \t  -0.4574431195846363 \t -0.4279073559468492\n",
      "47     \t [0.32517883 0.12622606]. \t  -0.4973462765525312 \t -0.4279073559468492\n",
      "48     \t [0.33337507 0.10671227]. \t  -0.4463483339697599 \t -0.4279073559468492\n",
      "49     \t [0.3673518  0.13846556]. \t  \u001b[92m-0.4014815337463461\u001b[0m \t -0.4014815337463461\n",
      "50     \t [0.21963345 0.09146748]. \t  -0.7958434112409782 \t -0.4014815337463461\n",
      "51     \t [0.35780537 0.09737278]. \t  -0.5063678892100331 \t -0.4014815337463461\n",
      "52     \t [0.35921307 0.12007071]. \t  -0.41864199079355396 \t -0.4014815337463461\n",
      "53     \t [0.33324699 0.11455129]. \t  -0.44578298251402 \t -0.4014815337463461\n",
      "54     \t [0.35595537 0.12864605]. \t  -0.415170558581711 \t -0.4014815337463461\n",
      "55     \t [0.33549919 0.08174069]. \t  -0.5365425091111427 \t -0.4014815337463461\n",
      "56     \t [0.32528327 0.11300649]. \t  -0.4604227640221336 \t -0.4014815337463461\n",
      "57     \t [0.32832636 0.12882147]. \t  -0.4953432542628924 \t -0.4014815337463461\n",
      "58     \t [0.3266274  0.08823016]. \t  -0.4874904516995489 \t -0.4014815337463461\n",
      "59     \t [0.32638342 0.09093267]. \t  -0.47807490748961007 \t -0.4014815337463461\n",
      "60     \t [0.35455578 0.09015983]. \t  -0.5429782817421924 \t -0.4014815337463461\n",
      "61     \t [0.37402554 0.11110153]. \t  -0.4747510340150315 \t -0.4014815337463461\n",
      "62     \t [0.35146599 0.09180841]. \t  -0.5212117961255898 \t -0.4014815337463461\n",
      "63     \t [0.28243857 0.10336811]. \t  -0.5705742100299506 \t -0.4014815337463461\n",
      "64     \t [0.38567521 0.13723669]. \t  \u001b[92m-0.390639922256204\u001b[0m \t -0.390639922256204\n",
      "65     \t [0.3489564  0.12967137]. \t  -0.43010003806150005 \t -0.390639922256204\n",
      "66     \t [0.31050113 0.12557527]. \t  -0.5604644697244691 \t -0.390639922256204\n",
      "67     \t [0.37468639 0.12658038]. \t  -0.4100873787974921 \t -0.390639922256204\n",
      "68     \t [0.37496747 0.10767114]. \t  -0.49910060680338325 \t -0.390639922256204\n",
      "69     \t [0.30227756 0.0930577 ]. \t  -0.4871008593650246 \t -0.390639922256204\n",
      "70     \t [0.29117454 0.06091508]. \t  -0.5593994544211559 \t -0.390639922256204\n",
      "71     \t [0.27891606 0.09867961]. \t  -0.5635822222057099 \t -0.390639922256204\n",
      "72     \t [0.38558599 0.10702102]. \t  -0.5510229170180311 \t -0.390639922256204\n",
      "73     \t [0.35995366 0.13751908]. \t  -0.4159834391273073 \t -0.390639922256204\n",
      "74     \t [0.32968982 0.07695681]. \t  -0.5500494083331593 \t -0.390639922256204\n",
      "75     \t [0.3448675  0.11037203]. \t  -0.4365286309984353 \t -0.390639922256204\n",
      "76     \t [0.38664534 0.11469557]. \t  -0.4973013074182721 \t -0.390639922256204\n",
      "77     \t [0.29155446 0.09413025]. \t  -0.5102239260737078 \t -0.390639922256204\n",
      "78     \t [0.37577378 0.13225886]. \t  -0.3976633941024026 \t -0.390639922256204\n",
      "79     \t [0.28110993 0.0997941 ]. \t  -0.5599476595961957 \t -0.390639922256204\n",
      "80     \t [0.22870803 0.08256131]. \t  -0.6864214463842357 \t -0.390639922256204\n",
      "81     \t [0.35790505 0.10418757]. \t  -0.4694473466609207 \t -0.390639922256204\n",
      "82     \t [0.39201833 0.16152305]. \t  \u001b[92m-0.3757956209790093\u001b[0m \t -0.3757956209790093\n",
      "83     \t [0.2762248  0.10244317]. \t  -0.5921963180529193 \t -0.3757956209790093\n",
      "84     \t [0.38348775 0.19407401]. \t  -0.6010922223078229 \t -0.3757956209790093\n",
      "85     \t [0.22548676 0.09305573]. \t  -0.7780514495017065 \t -0.3757956209790093\n",
      "86     \t [0.35522147 0.15263923]. \t  -0.4857363526905456 \t -0.3757956209790093\n",
      "87     \t [0.31352986 0.10689751]. \t  -0.4786312909586801 \t -0.3757956209790093\n",
      "88     \t [0.33776403 0.12304519]. \t  -0.44658581217024174 \t -0.3757956209790093\n",
      "89     \t [0.16166858 0.04130456]. \t  -0.7258058749926614 \t -0.3757956209790093\n",
      "90     \t [0.36916099 0.13797884]. \t  -0.3982465189693206 \t -0.3757956209790093\n",
      "91     \t [0.39153862 0.10439367]. \t  -0.6094325167479429 \t -0.3757956209790093\n",
      "92     \t [0.31869586 0.09008892]. \t  -0.4773500813438005 \t -0.3757956209790093\n",
      "93     \t [0.34520534 0.13539767]. \t  -0.4551004048751667 \t -0.3757956209790093\n",
      "94     \t [0.34522181 0.11759269]. \t  -0.428985832678412 \t -0.3757956209790093\n",
      "95     \t [0.39349695 0.17960517]. \t  -0.4291780469575682 \t -0.3757956209790093\n",
      "96     \t [0.32186168 0.11816482]. \t  -0.48109970730383483 \t -0.3757956209790093\n",
      "97     \t [0.36753552 0.1498378 ]. \t  -0.4217836235374057 \t -0.3757956209790093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.30698506 0.0448341 ]. \t  -0.7243622823819955 \t -0.3757956209790093\n",
      "99     \t [0.40808216 0.15816301]. \t  \u001b[92m-0.35736913823086786\u001b[0m \t -0.35736913823086786\n",
      "100    \t [0.35030223 0.15512514]. \t  -0.5271706274871758 \t -0.35736913823086786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [0.13711721 0.84640717]. \t  -69.23774252762598 \t -3.0269049669752817\n",
      "2      \t [-0.45729935 -0.33168701]. \t  -31.371235021681713 \t -3.0269049669752817\n",
      "3      \t [-0.1358696   0.18795309]. \t  -4.162971967302535 \t -3.0269049669752817\n",
      "4      \t [-0.73962199  1.72672982]. \t  -142.19292905563688 \t -3.0269049669752817\n",
      "5      \t [-0.48972664  0.8965283 ]. \t  -45.344264691173215 \t -3.0269049669752817\n",
      "6      \t [1.29775866 1.82984184]. \t  \u001b[92m-2.210468688425228\u001b[0m \t -2.210468688425228\n",
      "7      \t [1.07273915 1.43914582]. \t  -8.321393493939608 \t -2.210468688425228\n",
      "8      \t [0.95103156 2.048     ]. \t  -130.77053447342914 \t -2.210468688425228\n",
      "9      \t [2.048      1.83499254]. \t  -557.7333627486183 \t -2.210468688425228\n",
      "10     \t [-1.45392288  2.048     ]. \t  -6.4559098081601896 \t -2.210468688425228\n",
      "11     \t [-0.47054785  0.36945993]. \t  -4.354233011887729 \t -2.210468688425228\n",
      "12     \t [0.29704304 0.3065591 ]. \t  -5.260708341068166 \t -2.210468688425228\n",
      "13     \t [1.22344968 1.74710762]. \t  -6.313862868826504 \t -2.210468688425228\n",
      "14     \t [1.3549551 2.048    ]. \t  -4.624492881399356 \t -2.210468688425228\n",
      "15     \t [-0.18551199 -0.80435354]. \t  -71.75865469354464 \t -2.210468688425228\n",
      "16     \t [0.90451832 0.8785756 ]. \t  \u001b[92m-0.3742011396050354\u001b[0m \t -0.3742011396050354\n",
      "17     \t [1.14732785 1.27499799]. \t  \u001b[92m-0.19279701164637889\u001b[0m \t -0.19279701164637889\n",
      "18     \t [0.83646552 0.92702119]. \t  -5.1953925767284295 \t -0.19279701164637889\n",
      "19     \t [ 0.02565603 -0.08212448]. \t  -1.6346439070262757 \t -0.19279701164637889\n",
      "20     \t [1.03566385 1.06972572]. \t  \u001b[92m-0.0020978342412005567\u001b[0m \t -0.0020978342412005567\n",
      "21     \t [1.10405136 1.23789601]. \t  -0.04679989032885618 \t -0.0020978342412005567\n",
      "22     \t [1.01619503 1.03137306]. \t  \u001b[92m-0.0004259362126967473\u001b[0m \t -0.0004259362126967473\n",
      "23     \t [1.08556917 1.18779828]. \t  -0.016041654816298462 \t -0.0004259362126967473\n",
      "24     \t [1.08221867 1.17790148]. \t  -0.011254575451247247 \t -0.0004259362126967473\n",
      "25     \t [1.04810337 1.11745633]. \t  -0.03816986869903025 \t -0.0004259362126967473\n",
      "26     \t [1.10105689 1.21941201]. \t  -0.015233272417783742 \t -0.0004259362126967473\n",
      "27     \t [1.1001992  1.21735869]. \t  -0.014829096719720273 \t -0.0004259362126967473\n",
      "28     \t [1.10507471 1.23955882]. \t  -0.044781666691643414 \t -0.0004259362126967473\n",
      "29     \t [1.10366272 1.22967873]. \t  -0.024218986149863644 \t -0.0004259362126967473\n",
      "30     \t [1.04726698 1.10167466]. \t  -0.004641564443024868 \t -0.0004259362126967473\n",
      "31     \t [1.11927375 1.26428077]. \t  -0.02746743097451103 \t -0.0004259362126967473\n",
      "32     \t [1.08153425 1.17834157]. \t  -0.01408730326866036 \t -0.0004259362126967473\n",
      "33     \t [1.0984536  1.21402565]. \t  -0.015206680395219094 \t -0.0004259362126967473\n",
      "34     \t [1.07140035 1.15168259]. \t  -0.006529774122783111 \t -0.0004259362126967473\n",
      "35     \t [1.0514075  1.11021828]. \t  -0.004909006261691518 \t -0.0004259362126967473\n",
      "36     \t [1.07325677 1.15626232]. \t  -0.007286950702304411 \t -0.0004259362126967473\n",
      "37     \t [1.00280107 1.01429497]. \t  -0.007550741322301712 \t -0.0004259362126967473\n",
      "38     \t [1.07769855 1.1657784 ]. \t  -0.00792429642025244 \t -0.0004259362126967473\n",
      "39     \t [1.05236748 1.11864043]. \t  -0.01520385336550203 \t -0.0004259362126967473\n",
      "40     \t [1.08733118 1.18818213]. \t  -0.01109951630700041 \t -0.0004259362126967473\n",
      "41     \t [1.0957612  1.20932296]. \t  -0.01661851738155149 \t -0.0004259362126967473\n",
      "42     \t [1.12247276 1.27638355]. \t  -0.042021882432463124 \t -0.0004259362126967473\n",
      "43     \t [0.99368299 1.00226604]. \t  -0.022122318593532385 \t -0.0004259362126967473\n",
      "44     \t [1.02921316 1.07348996]. \t  -0.021046495280492954 \t -0.0004259362126967473\n",
      "45     \t [1.02576615 1.03626243]. \t  -0.026052382804551952 \t -0.0004259362126967473\n",
      "46     \t [1.07067634 1.15497394]. \t  -0.01243613731601274 \t -0.0004259362126967473\n",
      "47     \t [1.05359918 1.12321043]. \t  -0.02013674068774137 \t -0.0004259362126967473\n",
      "48     \t [0.97616161 0.95723877]. \t  -0.002458152630279792 \t -0.0004259362126967473\n",
      "49     \t [1.01972127 1.04190972]. \t  -0.0008208455991763325 \t -0.0004259362126967473\n",
      "50     \t [1.04325585 1.08710615]. \t  -0.00203404467318272 \t -0.0004259362126967473\n",
      "51     \t [1.08820869 1.18847148]. \t  -0.009606907329475702 \t -0.0004259362126967473\n",
      "52     \t [1.00143073 1.00227279]. \t  \u001b[92m-3.694290549242325e-05\u001b[0m \t -3.694290549242325e-05\n",
      "53     \t [1.13647504 1.31639355]. \t  -0.08021890847784352 \t -3.694290549242325e-05\n",
      "54     \t [1.01983959 1.04920458]. \t  -0.008732562949902963 \t -3.694290549242325e-05\n",
      "55     \t [1.07546512 1.16115554]. \t  -0.007747354969571944 \t -3.694290549242325e-05\n",
      "56     \t [1.13372959 1.28549746]. \t  -0.01788599558869791 \t -3.694290549242325e-05\n",
      "57     \t [1.11331676 1.23997776]. \t  -0.01286604442652408 \t -3.694290549242325e-05\n",
      "58     \t [0.95466328 0.92479683]. \t  -0.020051254429655017 \t -3.694290549242325e-05\n",
      "59     \t [1.06958174 1.14422648]. \t  -0.004846519651524538 \t -3.694290549242325e-05\n",
      "60     \t [1.08973089 1.19707829]. \t  -0.017200319094014344 \t -3.694290549242325e-05\n",
      "61     \t [1.02792979 1.05373085]. \t  -0.001626187772922638 \t -3.694290549242325e-05\n",
      "62     \t [1.06444678 1.14023532]. \t  -0.00932065136545566 \t -3.694290549242325e-05\n",
      "63     \t [1.08497334 1.20167612]. \t  -0.06728939167187731 \t -3.694290549242325e-05\n",
      "64     \t [1.14724956 1.32749081]. \t  -0.03447237435335563 \t -3.694290549242325e-05\n",
      "65     \t [1.10144166 1.2204651 ]. \t  -0.015606816490771409 \t -3.694290549242325e-05\n",
      "66     \t [1.10229642 1.19539275]. \t  -0.049134383428142386 \t -3.694290549242325e-05\n",
      "67     \t [1.10554414 1.20693705]. \t  -0.03452043594906001 \t -3.694290549242325e-05\n",
      "68     \t [0.97166976 0.94888719]. \t  -0.003054164685350234 \t -3.694290549242325e-05\n",
      "69     \t [1.12007002 1.26081683]. \t  -0.01833554664267283 \t -3.694290549242325e-05\n",
      "70     \t [1.05657872 1.1116648 ]. \t  -0.005404320509421615 \t -3.694290549242325e-05\n",
      "71     \t [1.0800134  1.17015183]. \t  -0.007788133218362693 \t -3.694290549242325e-05\n",
      "72     \t [1.00276126 1.00505406]. \t  \u001b[92m-3.0289856262862817e-05\u001b[0m \t -3.0289856262862817e-05\n",
      "73     \t [1.25587005 1.58360477]. \t  -0.06955931739653601 \t -3.0289856262862817e-05\n",
      "74     \t [1.16097872 1.36304802]. \t  -0.04894656250756713 \t -3.0289856262862817e-05\n",
      "75     \t [0.96334702 0.92998497]. \t  -0.0017227124959059911 \t -3.0289856262862817e-05\n",
      "76     \t [1.09949353 1.20755371]. \t  -0.010076472617159875 \t -3.0289856262862817e-05\n",
      "77     \t [1.19784831 1.45469664]. \t  -0.07857029874276758 \t -3.0289856262862817e-05\n",
      "78     \t [1.11994559 1.25885835]. \t  -0.016484788155954624 \t -3.0289856262862817e-05\n",
      "79     \t [1.14396545 1.32068738]. \t  -0.03519914039404569 \t -3.0289856262862817e-05\n",
      "80     \t [1.04749342 1.09695049]. \t  -0.0022641497238915266 \t -3.0289856262862817e-05\n",
      "81     \t [1.16898244 1.39375619]. \t  -0.10273636894339494 \t -3.0289856262862817e-05\n",
      "82     \t [1.11399786 1.25442496]. \t  -0.031042022793995143 \t -3.0289856262862817e-05\n",
      "83     \t [1.16021056 1.35821175]. \t  -0.040364607043878925 \t -3.0289856262862817e-05\n",
      "84     \t [1.19786005 1.45054113]. \t  -0.06371110813813308 \t -3.0289856262862817e-05\n",
      "85     \t [1.12319531 1.2677212 ]. \t  -0.018963626000623977 \t -3.0289856262862817e-05\n",
      "86     \t [1.14531446 1.35495568]. \t  -0.2078306417436586 \t -3.0289856262862817e-05\n",
      "87     \t [1.18492435 1.41990518]. \t  -0.05934929958610832 \t -3.0289856262862817e-05\n",
      "88     \t [1.03044471 1.0593629 ]. \t  -0.0015288015653751141 \t -3.0289856262862817e-05\n",
      "89     \t [1.15453643 1.33609095]. \t  -0.024865320301064368 \t -3.0289856262862817e-05\n",
      "90     \t [1.20301186 1.4587557 ]. \t  -0.05448062249033533 \t -3.0289856262862817e-05\n",
      "91     \t [1.12472747 1.27958478]. \t  -0.03679390489051756 \t -3.0289856262862817e-05\n",
      "92     \t [1.15418762 1.34073736]. \t  -0.03114972340312305 \t -3.0289856262862817e-05\n",
      "93     \t [1.20544636 1.48848115]. \t  -0.16738420468897416 \t -3.0289856262862817e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     \t [1.02581812 1.04937053]. \t  -0.0015264019859499646 \t -3.0289856262862817e-05\n",
      "95     \t [1.24647443 1.57059681]. \t  -0.0893049219500322 \t -3.0289856262862817e-05\n",
      "96     \t [1.01497826 1.02725899]. \t  -0.0010780810390051175 \t -3.0289856262862817e-05\n",
      "97     \t [1.02535387 1.06060063]. \t  -0.009199224622798235 \t -3.0289856262862817e-05\n",
      "98     \t [1.22631828 1.47631664]. \t  -0.12706451723561424 \t -3.0289856262862817e-05\n",
      "99     \t [1.07522598 1.15710055]. \t  -0.005756885390419813 \t -3.0289856262862817e-05\n",
      "100    \t [1.19450239 1.4479519 ]. \t  -0.08241941686317297 \t -3.0289856262862817e-05\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.54785183  1.44840367]. \t  -134.24641665086395 \t -2.0077595729598063\n",
      "2      \t [0.01346669 0.1059477 ]. \t  -2.0919000770182237 \t -2.0077595729598063\n",
      "3      \t [1.89099019 0.86778963]. \t  -734.1496670553332 \t -2.0077595729598063\n",
      "4      \t [0.88745024 2.048     ]. \t  -158.88156787123947 \t -2.0077595729598063\n",
      "5      \t [-0.22338877  0.71101891]. \t  -45.204165211423934 \t -2.0077595729598063\n",
      "6      \t [-1.58370921  1.61847242]. \t  -85.82548204434605 \t -2.0077595729598063\n",
      "7      \t [ 0.22245861 -1.25307467]. \t  -170.27147782197218 \t -2.0077595729598063\n",
      "8      \t [-1.00936753  1.05728186]. \t  -4.185467804045086 \t -2.0077595729598063\n",
      "9      \t [-0.40867521  0.10611441]. \t  -2.3552593258591132 \t -2.0077595729598063\n",
      "10     \t [1.0288842  1.45579563]. \t  -15.777057480580718 \t -2.0077595729598063\n",
      "11     \t [1.78723372 2.048     ]. \t  -131.9981844309693 \t -2.0077595729598063\n",
      "12     \t [0.65890386 0.32666935]. \t  \u001b[92m-1.2716479999668646\u001b[0m \t -1.2716479999668646\n",
      "13     \t [-1.2318929  2.048    ]. \t  -33.11799217323623 \t -1.2716479999668646\n",
      "14     \t [-0.6505295   0.58436785]. \t  -5.322121805377122 \t -1.2716479999668646\n",
      "15     \t [0.50347882 0.58404181]. \t  -11.172921733145275 \t -1.2716479999668646\n",
      "16     \t [-0.14242244 -0.44852909]. \t  -23.283714792648453 \t -1.2716479999668646\n",
      "17     \t [-1.10189465  0.92949783]. \t  -12.521888819262845 \t -1.2716479999668646\n",
      "18     \t [ 1.10622982 -0.83845991]. \t  -425.27995568827123 \t -1.2716479999668646\n",
      "19     \t [-0.34363476 -2.048     ]. \t  -470.99770992356446 \t -1.2716479999668646\n",
      "20     \t [1.37634716 2.048     ]. \t  -2.503037529419442 \t -1.2716479999668646\n",
      "21     \t [ 0.13396699 -0.07713324]. \t  -1.654041290199357 \t -1.2716479999668646\n",
      "22     \t [1.30634873 1.79925939]. \t  \u001b[92m-0.9534081152565006\u001b[0m \t -0.9534081152565006\n",
      "23     \t [1.19576349 1.42049857]. \t  \u001b[92m-0.04706887330743509\u001b[0m \t -0.04706887330743509\n",
      "24     \t [-0.13940283 -0.05054173]. \t  -1.7878871205631743 \t -0.04706887330743509\n",
      "25     \t [0.44472963 0.17156569]. \t  -0.37706749100006565 \t -0.04706887330743509\n",
      "26     \t [1.25507919 1.59300203]. \t  -0.09667205947362262 \t -0.04706887330743509\n",
      "27     \t [1.30960563 1.71802115]. \t  -0.09672840122924038 \t -0.04706887330743509\n",
      "28     \t [1.29413029 1.69942129]. \t  -0.1472653993435197 \t -0.04706887330743509\n",
      "29     \t [1.26900255 1.61950106]. \t  -0.08070458812009676 \t -0.04706887330743509\n",
      "30     \t [1.25775442 1.60157841]. \t  -0.10497975750441962 \t -0.04706887330743509\n",
      "31     \t [1.26141791 1.60409326]. \t  -0.08502707026699889 \t -0.04706887330743509\n",
      "32     \t [0.51424488 0.25481382]. \t  -0.24523939184490326 \t -0.04706887330743509\n",
      "33     \t [1.25090794 1.57480487]. \t  -0.07302330645974443 \t -0.04706887330743509\n",
      "34     \t [1.27927914 1.64261569]. \t  -0.08166989501692587 \t -0.04706887330743509\n",
      "35     \t [1.19575441 1.4538587 ]. \t  -0.0960643176021869 \t -0.04706887330743509\n",
      "36     \t [0.51296319 0.25590741]. \t  -0.24242321420298205 \t -0.04706887330743509\n",
      "37     \t [1.28342835 1.65228529]. \t  -0.08292952755694849 \t -0.04706887330743509\n",
      "38     \t [0.49845578 0.24168213]. \t  -0.2561380662103856 \t -0.04706887330743509\n",
      "39     \t [1.25755075 1.5942352 ]. \t  -0.08271972969034315 \t -0.04706887330743509\n",
      "40     \t [1.31524994 1.73488709]. \t  -0.10188721760153459 \t -0.04706887330743509\n",
      "41     \t [0.5059136  0.23993149]. \t  -0.269776062152982 \t -0.04706887330743509\n",
      "42     \t [1.26048129 1.62994898]. \t  -0.2370665840849997 \t -0.04706887330743509\n",
      "43     \t [1.26312939 1.61078825]. \t  -0.09262284469288273 \t -0.04706887330743509\n",
      "44     \t [1.25332272 1.6104663 ]. \t  -0.22137255107819376 \t -0.04706887330743509\n",
      "45     \t [1.22923262 1.53992491]. \t  -0.13613848543237933 \t -0.04706887330743509\n",
      "46     \t [1.25780475 1.59425587]. \t  -0.08130606290742344 \t -0.04706887330743509\n",
      "47     \t [1.30799554 1.7267899 ]. \t  -0.12026185979801086 \t -0.04706887330743509\n",
      "48     \t [1.31674091 1.75667576]. \t  -0.1526245320412839 \t -0.04706887330743509\n",
      "49     \t [1.25954026 1.59375972]. \t  -0.07271653329234412 \t -0.04706887330743509\n",
      "50     \t [1.22521191 1.51897317]. \t  -0.08250754618710819 \t -0.04706887330743509\n",
      "51     \t [1.28331717 1.68692481]. \t  -0.24044347925124276 \t -0.04706887330743509\n",
      "52     \t [1.26128011 1.60249638]. \t  -0.08188350240224299 \t -0.04706887330743509\n",
      "53     \t [1.22934245 1.55438799]. \t  -0.2384031689848144 \t -0.04706887330743509\n",
      "54     \t [1.25692146 1.58577148]. \t  -0.06951318142571845 \t -0.04706887330743509\n",
      "55     \t [0.52108756 0.25978764]. \t  -0.24315069911790677 \t -0.04706887330743509\n",
      "56     \t [1.25938025 1.59814407]. \t  -0.08193230459342096 \t -0.04706887330743509\n",
      "57     \t [1.27717794 1.61095449]. \t  -0.117748912369805 \t -0.04706887330743509\n",
      "58     \t [1.28657164 1.66390577]. \t  -0.0895868625282238 \t -0.04706887330743509\n",
      "59     \t [0.54477198 0.25490732]. \t  -0.38253538204804555 \t -0.04706887330743509\n",
      "60     \t [0.47280867 0.23048218]. \t  -0.28273892075438545 \t -0.04706887330743509\n",
      "61     \t [1.29267149 1.67593581]. \t  -0.08809323550771668 \t -0.04706887330743509\n",
      "62     \t [0.48084151 0.22122441]. \t  -0.27949386930577785 \t -0.04706887330743509\n",
      "63     \t [0.56155831 0.28962955]. \t  -0.25837362454829593 \t -0.04706887330743509\n",
      "64     \t [0.54098527 0.28407362]. \t  -0.2180758044803256 \t -0.04706887330743509\n",
      "65     \t [0.53870173 0.26897111]. \t  -0.2578607489824015 \t -0.04706887330743509\n",
      "66     \t [1.27527884 1.61031599]. \t  -0.10144292168026 \t -0.04706887330743509\n",
      "67     \t [0.4764107  0.18914954]. \t  -0.4171629633767655 \t -0.04706887330743509\n",
      "68     \t [1.30386448 1.68680362]. \t  -0.10991360454666065 \t -0.04706887330743509\n",
      "69     \t [0.49374771 0.267011  ]. \t  -0.3102277025573806 \t -0.04706887330743509\n",
      "70     \t [1.2011715  1.45851506]. \t  -0.06512553471673749 \t -0.04706887330743509\n",
      "71     \t [1.20595793 1.50120644]. \t  -0.2621162371303397 \t -0.04706887330743509\n",
      "72     \t [0.46236383 0.2220486 ]. \t  -0.2958891091415233 \t -0.04706887330743509\n",
      "73     \t [1.29894075 1.65749446]. \t  -0.1778873090199538 \t -0.04706887330743509\n",
      "74     \t [1.3086786  1.71912179]. \t  -0.09948425559164059 \t -0.04706887330743509\n",
      "75     \t [1.32270048 1.75698258]. \t  -0.10967991867169986 \t -0.04706887330743509\n",
      "76     \t [1.21499222 1.53552265]. \t  -0.39806697643646366 \t -0.04706887330743509\n",
      "77     \t [0.51580614 0.26551288]. \t  -0.23447319126693061 \t -0.04706887330743509\n",
      "78     \t [0.47282822 0.22391317]. \t  -0.27792210360037234 \t -0.04706887330743509\n",
      "79     \t [0.48467398 0.24878888]. \t  -0.2848263745310201 \t -0.04706887330743509\n",
      "80     \t [0.48093687 0.22557641]. \t  -0.2727027939643819 \t -0.04706887330743509\n",
      "81     \t [1.31531228 1.73245991]. \t  -0.10000433954461285 \t -0.04706887330743509\n",
      "82     \t [1.18510087 1.43783167]. \t  -0.14560194608760527 \t -0.04706887330743509\n",
      "83     \t [1.19754686 1.44244971]. \t  \u001b[92m-0.04596571148243915\u001b[0m \t -0.04596571148243915\n",
      "84     \t [1.25698731 1.59457238]. \t  -0.08722812341771635 \t -0.04596571148243915\n",
      "85     \t [1.16426938 1.40996905]. \t  -0.3234197247267459 \t -0.04596571148243915\n",
      "86     \t [1.20818183 1.49586325]. \t  -0.17409362390699973 \t -0.04596571148243915\n",
      "87     \t [1.28467466 1.62801081]. \t  -0.13111791726880598 \t -0.04596571148243915\n",
      "88     \t [0.54227528 0.28806317]. \t  -0.2131110910492515 \t -0.04596571148243915\n",
      "89     \t [1.3452344  1.82146363]. \t  -0.13312977412669105 \t -0.04596571148243915\n",
      "90     \t [1.27391374 1.62878919]. \t  -0.07854875000037628 \t -0.04596571148243915\n",
      "91     \t [1.22717347 1.52255828]. \t  -0.07917561792584113 \t -0.04596571148243915\n",
      "92     \t [1.33664347 1.80027253]. \t  -0.131979553532514 \t -0.04596571148243915\n",
      "93     \t [1.27061074 1.62661581]. \t  -0.08802687368648399 \t -0.04596571148243915\n",
      "94     \t [1.24114    1.54781461]. \t  -0.06360397343646683 \t -0.04596571148243915\n",
      "95     \t [1.29851805 1.68953996]. \t  -0.09026280273123477 \t -0.04596571148243915\n",
      "96     \t [1.29091486 1.67369849]. \t  -0.08986934078446465 \t -0.04596571148243915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.54919259 0.26905349]. \t  -0.3092362251037024 \t -0.04596571148243915\n",
      "98     \t [0.59568152 0.32995198]. \t  -0.2253972528327863 \t -0.04596571148243915\n",
      "99     \t [0.52987475 0.26355229]. \t  -0.2506532172447424 \t -0.04596571148243915\n",
      "100    \t [1.28842485 1.60540868]. \t  -0.3816314963906323 \t -0.04596571148243915\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.65776384 0.51003997]. \t  \u001b[92m-0.7159958105596282\u001b[0m \t -0.7159958105596282\n",
      "2      \t [ 0.40369021 -0.12235126]. \t  -8.496167030329525 \t -0.7159958105596282\n",
      "3      \t [-1.47129253  1.02291479]. \t  -136.47502717178082 \t -0.7159958105596282\n",
      "4      \t [1.56388923 0.92474068]. \t  -231.6647640262991 \t -0.7159958105596282\n",
      "5      \t [-0.60917833  0.82731669]. \t  -23.402982944599103 \t -0.7159958105596282\n",
      "6      \t [0.84483617 0.15571338]. \t  -31.164357689755132 \t -0.7159958105596282\n",
      "7      \t [0.52609347 1.78188028]. \t  -226.75897545119733 \t -0.7159958105596282\n",
      "8      \t [0.10954594 0.68049644]. \t  -45.48161541878578 \t -0.7159958105596282\n",
      "9      \t [-1.01669665  0.49611848]. \t  -32.96345182143055 \t -0.7159958105596282\n",
      "10     \t [0.78316531 0.87689056]. \t  -6.992490315085866 \t -0.7159958105596282\n",
      "11     \t [-0.65910732  0.4809492 ]. \t  -2.969110898957423 \t -0.7159958105596282\n",
      "12     \t [-1.26345188  2.048     ]. \t  -25.5255408542817 \t -0.7159958105596282\n",
      "13     \t [0.68933569 0.64642289]. \t  -3.0287985499766052 \t -0.7159958105596282\n",
      "14     \t [ 0.58410394 -0.03895404]. \t  -14.622962011879666 \t -0.7159958105596282\n",
      "15     \t [-0.19852889  0.1418071 ]. \t  -2.4849118488883866 \t -0.7159958105596282\n",
      "16     \t [1.35160558 1.92338733]. \t  -1.0558105507241904 \t -0.7159958105596282\n",
      "17     \t [1.25130621 1.72564272]. \t  -2.6191720668428404 \t -0.7159958105596282\n",
      "18     \t [-1.26345181  2.04799991]. \t  -25.5255484506229 \t -0.7159958105596282\n",
      "19     \t [-1.33882242  1.08880033]. \t  -54.98173970220263 \t -0.7159958105596282\n",
      "20     \t [0.6977919  1.75509723]. \t  -160.92031714723723 \t -0.7159958105596282\n",
      "21     \t [-0.21954606  0.36855217]. \t  -11.749813474532832 \t -0.7159958105596282\n",
      "22     \t [-0.25896759  1.05540738]. \t  -99.26722111254946 \t -0.7159958105596282\n",
      "23     \t [-0.26091269  1.86462128]. \t  -324.34760033415404 \t -0.7159958105596282\n",
      "24     \t [0.98548171 1.90631552]. \t  -87.44914130666564 \t -0.7159958105596282\n",
      "25     \t [0.47475314 1.75949646]. \t  -235.62398067075662 \t -0.7159958105596282\n",
      "26     \t [-1.13761452  0.56876086]. \t  -57.19077210105198 \t -0.7159958105596282\n",
      "27     \t [-0.48949062  0.73595592]. \t  -26.85539619748126 \t -0.7159958105596282\n",
      "28     \t [-1.66812921  0.77422216]. \t  -410.4991830811671 \t -0.7159958105596282\n",
      "29     \t [0.86545139 1.96158368]. \t  -147.0525403112487 \t -0.7159958105596282\n",
      "30     \t [0.37115901 0.43982115]. \t  -9.519594816710446 \t -0.7159958105596282\n",
      "31     \t [-1.24749056  0.71895971]. \t  -75.15382124883207 \t -0.7159958105596282\n",
      "32     \t [-1.34348396  0.95998139]. \t  -76.88896958911748 \t -0.7159958105596282\n",
      "33     \t [0.29524706 0.54519326]. \t  -21.47513177642129 \t -0.7159958105596282\n",
      "34     \t [-0.40251636  1.66430413]. \t  -227.65298856707702 \t -0.7159958105596282\n",
      "35     \t [0.45651748 0.18643547]. \t  \u001b[92m-0.3436533642863658\u001b[0m \t -0.3436533642863658\n",
      "36     \t [ 1.11680149 -0.46944093]. \t  -294.7148970067409 \t -0.3436533642863658\n",
      "37     \t [-2.04007732  1.73210541]. \t  -599.6397734779823 \t -0.3436533642863658\n",
      "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.3436533642863658\n",
      "39     \t [-0.11215858  1.13031619]. \t  -126.17041680603957 \t -0.3436533642863658\n",
      "40     \t [1.25653309 0.69302244]. \t  -78.53935865097625 \t -0.3436533642863658\n",
      "41     \t [0.58046656 0.21875041]. \t  -1.5729201370608095 \t -0.3436533642863658\n",
      "42     \t [-1.40810143  1.03539045]. \t  -95.54789758651106 \t -0.3436533642863658\n",
      "43     \t [-0.74171133  1.62040369]. \t  -117.58091609460371 \t -0.3436533642863658\n",
      "44     \t [-0.66904423  1.19340861]. \t  -58.40574725562008 \t -0.3436533642863658\n",
      "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.3436533642863658\n",
      "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.3436533642863658\n",
      "47     \t [0.97148335 1.15966157]. \t  -4.661302411937311 \t -0.3436533642863658\n",
      "48     \t [-1.80410344  1.01640338]. \t  -508.9001103697548 \t -0.3436533642863658\n",
      "49     \t [0.85892397 0.34161258]. \t  -15.7124183297375 \t -0.3436533642863658\n",
      "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.3436533642863658\n",
      "51     \t [-0.48642513  1.40715418]. \t  -139.22696638962924 \t -0.3436533642863658\n",
      "52     \t [0.2832129  1.32113205]. \t  -154.50264834274344 \t -0.3436533642863658\n",
      "53     \t [-1.92850496  0.52318824]. \t  -1029.9814077744916 \t -0.3436533642863658\n",
      "54     \t [-1.47463102  0.70227909]. \t  -222.87803284572018 \t -0.3436533642863658\n",
      "55     \t [1.07041467 0.54422904]. \t  -36.19222534510453 \t -0.3436533642863658\n",
      "56     \t [-0.48733242  0.93400645]. \t  -50.725271825127635 \t -0.3436533642863658\n",
      "57     \t [-0.65653234 -0.28597787]. \t  -54.154802970295506 \t -0.3436533642863658\n",
      "58     \t [-0.68810805  1.08056737]. \t  -39.703675359299346 \t -0.3436533642863658\n",
      "59     \t [0.89389391 0.81594664]. \t  \u001b[92m-0.039820613055831275\u001b[0m \t -0.039820613055831275\n",
      "60     \t [-0.31118971  0.7063599 ]. \t  -38.87078741801655 \t -0.039820613055831275\n",
      "61     \t [-0.10098878  1.23025377]. \t  -150.06560641581916 \t -0.039820613055831275\n",
      "62     \t [-0.23210322  1.7244254 ]. \t  -280.5929754733092 \t -0.039820613055831275\n",
      "63     \t [1.4253935  1.88040468]. \t  -2.471397975869653 \t -0.039820613055831275\n",
      "64     \t [-0.92951036  1.74748492]. \t  -81.77942543905903 \t -0.039820613055831275\n",
      "65     \t [1.32181967 1.08348102]. \t  -44.156819713175494 \t -0.039820613055831275\n",
      "66     \t [-2.03941581  1.19299086]. \t  -889.0877031988127 \t -0.039820613055831275\n",
      "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.039820613055831275\n",
      "68     \t [-0.50074329  1.82558668]. \t  -250.26522547400722 \t -0.039820613055831275\n",
      "69     \t [-0.83278     1.95142874]. \t  -161.59188393298533 \t -0.039820613055831275\n",
      "70     \t [0.0298615  0.22010932]. \t  -5.746804652743761 \t -0.039820613055831275\n",
      "71     \t [-0.00551753  0.64296036]. \t  -42.34695289953527 \t -0.039820613055831275\n",
      "72     \t [-0.45455404  0.97947863]. \t  -61.84687062939467 \t -0.039820613055831275\n",
      "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.039820613055831275\n",
      "74     \t [0.71477743 1.71083142]. \t  -144.06326466720935 \t -0.039820613055831275\n",
      "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.039820613055831275\n",
      "76     \t [1.72328771 0.32733523]. \t  -698.7431578285583 \t -0.039820613055831275\n",
      "77     \t [1.94318017 1.60408364]. \t  -472.58958535236195 \t -0.039820613055831275\n",
      "78     \t [1.48420354 0.95128231]. \t  -156.87915809862778 \t -0.039820613055831275\n",
      "79     \t [1.90543093 1.74194433]. \t  -357.5471490506435 \t -0.039820613055831275\n",
      "80     \t [0.35795649 0.45117842]. \t  -10.848063624852834 \t -0.039820613055831275\n",
      "81     \t [-1.29233838 -1.04312521]. \t  -741.4348115165175 \t -0.039820613055831275\n",
      "82     \t [-1.08263569  1.50197853]. \t  -15.219353636984508 \t -0.039820613055831275\n",
      "83     \t [0.52359919 1.39203173]. \t  -125.19154897335613 \t -0.039820613055831275\n",
      "84     \t [0.40328726 0.71726092]. \t  -31.116434818539897 \t -0.039820613055831275\n",
      "85     \t [-1.40863496  1.83851691]. \t  -7.925407505157292 \t -0.039820613055831275\n",
      "86     \t [-1.99787614  1.28104003]. \t  -743.6515028883296 \t -0.039820613055831275\n",
      "87     \t [-0.89921436  0.8996675 ]. \t  -4.436590599339098 \t -0.039820613055831275\n",
      "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.039820613055831275\n",
      "89     \t [ 0.31133267 -0.3654709 ]. \t  -21.855539664990918 \t -0.039820613055831275\n",
      "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.039820613055831275\n",
      "91     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.039820613055831275\n",
      "92     \t [-0.08108255  1.84567656]. \t  -339.3984214099406 \t -0.039820613055831275\n",
      "93     \t [-0.97822663 -0.28198707]. \t  -157.40427083772664 \t -0.039820613055831275\n",
      "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.039820613055831275\n",
      "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.039820613055831275\n",
      "96     \t [-0.37198484  0.42945312]. \t  -10.355122021105933 \t -0.039820613055831275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.64696938  1.83863795]. \t  -83.37135463490554 \t -0.039820613055831275\n",
      "98     \t [1.44967043 0.08369504]. \t  -407.37378725319707 \t -0.039820613055831275\n",
      "99     \t [-0.43274793  0.60373022]. \t  -19.396613918289827 \t -0.039820613055831275\n",
      "100    \t [-2.02018778  0.35071223]. \t  -1400.7446034771988 \t -0.039820613055831275\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [ 0.40971704 -1.59139552]. \t  -309.8492636194474 \t -29.9831488845538\n",
      "2      \t [-0.1529143   0.77817238]. \t  -58.29994457848973 \t -29.9831488845538\n",
      "3      \t [ 0.70562182 -0.04881437]. \t  \u001b[92m-29.976554169168498\u001b[0m \t -29.976554169168498\n",
      "4      \t [ 0.7043472  -0.04930191]. \t  \u001b[92m-29.834278487460708\u001b[0m \t -29.834278487460708\n",
      "5      \t [-0.15283772  0.7779944 ]. \t  -58.276438110376 \t -29.834278487460708\n",
      "6      \t [ 0.69991437 -0.04870937]. \t  \u001b[92m-29.09791642945634\u001b[0m \t -29.09791642945634\n",
      "7      \t [0.38021058 0.29374019]. \t  \u001b[92m-2.6096093131526454\u001b[0m \t -2.6096093131526454\n",
      "8      \t [0.37918726 0.21601747]. \t  \u001b[92m-0.9071905702219136\u001b[0m \t -0.9071905702219136\n",
      "9      \t [0.37921884 0.21039402]. \t  \u001b[92m-0.8287532682638552\u001b[0m \t -0.8287532682638552\n",
      "10     \t [0.3786742  0.20975132]. \t  \u001b[92m-0.826373115237881\u001b[0m \t -0.826373115237881\n",
      "11     \t [0.37768469 0.20952425]. \t  -0.8345500728197206 \t -0.826373115237881\n",
      "12     \t [0.37828473 0.21425758]. \t  -0.8928794685508692 \t -0.826373115237881\n",
      "13     \t [0.37755825 0.21397708]. \t  -0.8976131397483946 \t -0.826373115237881\n",
      "14     \t [0.37967203 0.2112837 ]. \t  -0.835488629476463 \t -0.826373115237881\n",
      "15     \t [0.37880259 0.21316925]. \t  -0.8713864741867003 \t -0.826373115237881\n",
      "16     \t [0.37846766 0.22599141]. \t  -1.0711188496044284 \t -0.826373115237881\n",
      "17     \t [0.37843453 0.20570095]. \t  \u001b[92m-0.7768218371447237\u001b[0m \t -0.7768218371447237\n",
      "18     \t [0.37770094 0.21178321]. \t  -0.8650855728290681 \t -0.7768218371447237\n",
      "19     \t [0.38461383 0.18258813]. \t  \u001b[92m-0.49883402739874105\u001b[0m \t -0.49883402739874105\n",
      "20     \t [0.56546479 2.048     ]. \t  -298.8734781201346 \t -0.49883402739874105\n",
      "21     \t [-2.048      1.6551892]. \t  -654.0007024171091 \t -0.49883402739874105\n",
      "22     \t [0.3835563  0.18285565]. \t  -0.5077391422038044 \t -0.49883402739874105\n",
      "23     \t [0.38430533 0.18105397]. \t  \u001b[92m-0.4903914539507161\u001b[0m \t -0.4903914539507161\n",
      "24     \t [0.38615404 0.17552264]. \t  \u001b[92m-0.4465435607343164\u001b[0m \t -0.4465435607343164\n",
      "25     \t [0.38581809 0.18025688]. \t  -0.47582345586995867 \t -0.4465435607343164\n",
      "26     \t [0.38923223 0.16840821]. \t  \u001b[92m-0.4016202045110782\u001b[0m \t -0.4016202045110782\n",
      "27     \t [0.38779221 0.17621055]. \t  -0.4415056298540555 \t -0.4016202045110782\n",
      "28     \t [0.39241752 0.16183386]. \t  \u001b[92m-0.3753067127851261\u001b[0m \t -0.3753067127851261\n",
      "29     \t [0.39833697 0.15506087]. \t  \u001b[92m-0.3633026759386155\u001b[0m \t -0.3633026759386155\n",
      "30     \t [0.3989954  0.15754454]. \t  \u001b[92m-0.3614796962752485\u001b[0m \t -0.3614796962752485\n",
      "31     \t [0.40068828 0.15301362]. \t  -0.3648558996508552 \t -0.3614796962752485\n",
      "32     \t [0.40111315 0.16159077]. \t  \u001b[92m-0.3587143232313986\u001b[0m \t -0.3587143232313986\n",
      "33     \t [0.39661445 0.16342458]. \t  -0.3678214591626611 \t -0.3587143232313986\n",
      "34     \t [0.39627944 0.17540724]. \t  -0.3982236493542763 \t -0.3587143232313986\n",
      "35     \t [0.4052154  0.16294179]. \t  \u001b[92m-0.35392691160553785\u001b[0m \t -0.35392691160553785\n",
      "36     \t [0.4075226 0.1511103]. \t  -0.3734226916026099 \t -0.35392691160553785\n",
      "37     \t [0.4044305  0.14810916]. \t  -0.37858831795748715 \t -0.35392691160553785\n",
      "38     \t [0.41353361 0.15128975]. \t  -0.3828318185117582 \t -0.35392691160553785\n",
      "39     \t [0.39024535 0.16829936]. \t  -0.39742608067763374 \t -0.35392691160553785\n",
      "40     \t [0.40922844 0.15964159]. \t  -0.3551361816874169 \t -0.35392691160553785\n",
      "41     \t [0.41463081 0.14930877]. \t  -0.3937780357492043 \t -0.35392691160553785\n",
      "42     \t [0.39772785 0.16517598]. \t  -0.3676157130868477 \t -0.35392691160553785\n",
      "43     \t [0.39475897 0.15663848]. \t  -0.3663813213176981 \t -0.35392691160553785\n",
      "44     \t [0.39660497 0.13948966]. \t  -0.3957903449326253 \t -0.35392691160553785\n",
      "45     \t [0.40009728 0.16773204]. \t  -0.36574196742749154 \t -0.35392691160553785\n",
      "46     \t [0.40541175 0.15326555]. \t  -0.3658409577868659 \t -0.35392691160553785\n",
      "47     \t [0.39558421 0.19070508]. \t  -0.48240704785456023 \t -0.35392691160553785\n",
      "48     \t [0.40576147 0.16165222]. \t  -0.3540135302805838 \t -0.35392691160553785\n",
      "49     \t [0.39393186 0.16676486]. \t  -0.3807341365756005 \t -0.35392691160553785\n",
      "50     \t [0.40448513 0.14307495]. \t  -0.3967994891592624 \t -0.35392691160553785\n",
      "51     \t [0.40445533 0.15705879]. \t  -0.35893144030666124 \t -0.35392691160553785\n",
      "52     \t [0.39491666 0.14489785]. \t  -0.37836112783473064 \t -0.35392691160553785\n",
      "53     \t [0.41062691 0.15595439]. \t  -0.3633883802033709 \t -0.35392691160553785\n",
      "54     \t [-2.048      1.6551891]. \t  -654.0007531294857 \t -0.35392691160553785\n",
      "55     \t [0.060601   0.17843966]. \t  -3.9368273566223424 \t -0.35392691160553785\n",
      "56     \t [-1.99105648  1.56657107]. \t  -583.8596632289197 \t -0.35392691160553785\n",
      "57     \t [-1.58475928  1.78431713]. \t  -59.554943721368396 \t -0.35392691160553785\n",
      "58     \t [-1.80259419  2.03696439]. \t  -154.8414079670564 \t -0.35392691160553785\n",
      "59     \t [-1.19827514  1.16319662]. \t  -12.267126894038888 \t -0.35392691160553785\n",
      "60     \t [0.30621915 0.78901936]. \t  -48.818475294684006 \t -0.35392691160553785\n",
      "61     \t [ 0.7145709  -0.14399233]. \t  -42.93209714563802 \t -0.35392691160553785\n",
      "62     \t [0.24358821 0.22400033]. \t  -3.2836187755662185 \t -0.35392691160553785\n",
      "63     \t [0.46873837 1.96743036]. \t  -305.7329054062828 \t -0.35392691160553785\n",
      "64     \t [-1.14218003  0.42603594]. \t  -81.77206091477343 \t -0.35392691160553785\n",
      "65     \t [0.20794621 1.13015541]. \t  -118.76550660160018 \t -0.35392691160553785\n",
      "66     \t [0.00140331 1.36362061]. \t  -186.94277558703737 \t -0.35392691160553785\n",
      "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.35392691160553785\n",
      "68     \t [-0.48444591  0.78185585]. \t  -32.14286348293593 \t -0.35392691160553785\n",
      "69     \t [-0.98530195  0.58584656]. \t  -18.761873466916697 \t -0.35392691160553785\n",
      "70     \t [ 0.82562593 -0.0764867 ]. \t  -57.508771876499125 \t -0.35392691160553785\n",
      "71     \t [-0.69966795  0.58970535]. \t  -3.892276231415377 \t -0.35392691160553785\n",
      "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.35392691160553785\n",
      "73     \t [-0.79939892  0.39102862]. \t  -9.388733320723247 \t -0.35392691160553785\n",
      "74     \t [-1.61799966  1.97817755]. \t  -47.78133274699538 \t -0.35392691160553785\n",
      "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.35392691160553785\n",
      "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.35392691160553785\n",
      "77     \t [-1.84889648  0.60502484]. \t  -799.6344366354048 \t -0.35392691160553785\n",
      "78     \t [-1.33130355  0.17315875]. \t  -261.1823631696615 \t -0.35392691160553785\n",
      "79     \t [-1.68165728  1.02400967]. \t  -332.6190097074854 \t -0.35392691160553785\n",
      "80     \t [-0.15874084  0.96917522]. \t  -90.45185647108262 \t -0.35392691160553785\n",
      "81     \t [-0.24042955 -0.00345236]. \t  -1.9139286892239176 \t -0.35392691160553785\n",
      "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.35392691160553785\n",
      "83     \t [-1.59424609  1.66355524]. \t  -83.82998988084474 \t -0.35392691160553785\n",
      "84     \t [-1.55117519  1.64824354]. \t  -63.94987468167431 \t -0.35392691160553785\n",
      "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.35392691160553785\n",
      "86     \t [ 0.21163978 -0.04288026]. \t  -1.390143766668992 \t -0.35392691160553785\n",
      "87     \t [-0.07920339  1.00895488]. \t  -101.70174027007197 \t -0.35392691160553785\n",
      "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.35392691160553785\n",
      "89     \t [-1.26185781 -0.30445998]. \t  -364.8802056357584 \t -0.35392691160553785\n",
      "90     \t [-1.71017628  0.20698374]. \t  -745.944797422678 \t -0.35392691160553785\n",
      "91     \t [-1.72247233  0.22194458]. \t  -760.8958697327137 \t -0.35392691160553785\n",
      "92     \t [ 0.00244132 -0.26977652]. \t  -8.273382184230009 \t -0.35392691160553785\n",
      "93     \t [-0.82308262  1.00524497]. \t  -14.067601499049726 \t -0.35392691160553785\n",
      "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.35392691160553785\n",
      "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.35392691160553785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.60126767 0.17820721]. \t  -3.5194485155097466 \t -0.35392691160553785\n",
      "97     \t [-1.03840028 -0.95345358]. \t  -416.9472377976473 \t -0.35392691160553785\n",
      "98     \t [-0.65541128  0.80886722]. \t  -17.12748433920213 \t -0.35392691160553785\n",
      "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.35392691160553785\n",
      "100    \t [-1.06396649 -0.09110957]. \t  -153.86570010327534 \t -0.35392691160553785\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.18624574 -1.34403967]. \t  -190.75104997517573 \t -190.75104997517573\n",
      "init   \t [ 1.44560869 -1.30980491]. \t  -1155.919375128387 \t -190.75104997517573\n",
      "init   \t [ 0.36331541 -1.55330601]. \t  -284.4303550043376 \t -190.75104997517573\n",
      "init   \t [ 0.22404608 -1.71202077]. \t  -311.14312856964693 \t -190.75104997517573\n",
      "init   \t [ 2.0181204  -1.01286191]. \t  -2587.442372593159 \t -190.75104997517573\n",
      "1      \t [-0.85302419 -0.42819194]. \t  \u001b[92m-137.03081910254275\u001b[0m \t -137.03081910254275\n",
      "2      \t [-1.45054227 -2.048     ]. \t  -1729.9760694574027 \t -137.03081910254275\n",
      "3      \t [-0.07620257 -0.19988049]. \t  \u001b[92m-5.388939436258231\u001b[0m \t -5.388939436258231\n",
      "4      \t [-0.20253109 -0.58272728]. \t  -40.352003826676466 \t -5.388939436258231\n",
      "5      \t [-0.5515767  1.1171376]. \t  -68.48815174910499 \t -5.388939436258231\n",
      "6      \t [-2.03684394  0.56598346]. \t  -1292.8320263615365 \t -5.388939436258231\n",
      "7      \t [0.30846261 1.09147117]. \t  -99.74397385087858 \t -5.388939436258231\n",
      "8      \t [-0.26390584  0.45498759]. \t  -16.44624968598213 \t -5.388939436258231\n",
      "9      \t [-0.46771688  2.048     ]. \t  -336.76642832416275 \t -5.388939436258231\n",
      "10     \t [0.19455969 0.01379236]. \t  \u001b[92m-0.7066278234147788\u001b[0m \t -0.7066278234147788\n",
      "11     \t [ 0.12399279 -0.12846096]. \t  -2.8362443179320156 \t -0.7066278234147788\n",
      "12     \t [0.05778949 0.15633232]. \t  -3.228437060322587 \t -0.7066278234147788\n",
      "13     \t [0.15603125 0.0271023 ]. \t  -0.7130430984564483 \t -0.7066278234147788\n",
      "14     \t [0.16263511 0.02967201]. \t  \u001b[92m-0.7022179705573176\u001b[0m \t -0.7022179705573176\n",
      "15     \t [0.16264843 0.02626321]. \t  \u001b[92m-0.701161314708637\u001b[0m \t -0.701161314708637\n",
      "16     \t [0.16303088 0.02852009]. \t  \u001b[92m-0.7008940632744043\u001b[0m \t -0.7008940632744043\n",
      "17     \t [0.15866406 0.02582111]. \t  -0.7078880049088943 \t -0.7008940632744043\n",
      "18     \t [0.16100407 0.03126843]. \t  -0.7067722603274136 \t -0.7008940632744043\n",
      "19     \t [0.16551191 0.03509827]. \t  -0.7023056597077424 \t -0.7008940632744043\n",
      "20     \t [0.17301114 0.03226912]. \t  \u001b[92m-0.6844563955508911\u001b[0m \t -0.6844563955508911\n",
      "21     \t [0.16081754 0.02635782]. \t  -0.7042517547070346 \t -0.6844563955508911\n",
      "22     \t [0.16543633 0.02817245]. \t  -0.6965610516445947 \t -0.6844563955508911\n",
      "23     \t [0.15853264 0.02718448]. \t  -0.7084883327492583 \t -0.6844563955508911\n",
      "24     \t [0.15007439 0.02466043]. \t  -0.7228306897489751 \t -0.6844563955508911\n",
      "25     \t [0.16247781 0.02772492]. \t  -0.7016192209918503 \t -0.6844563955508911\n",
      "26     \t [0.15753328 0.03059949]. \t  -0.7130941883769342 \t -0.6844563955508911\n",
      "27     \t [0.16089779 0.03297958]. \t  -0.7091214353572809 \t -0.6844563955508911\n",
      "28     \t [0.17109539 0.03023177]. \t  -0.6871746495065681 \t -0.6844563955508911\n",
      "29     \t [0.14974724 0.03143986]. \t  -0.7310578890777089 \t -0.6844563955508911\n",
      "30     \t [0.1567623  0.03310907]. \t  -0.7183338401557144 \t -0.6844563955508911\n",
      "31     \t [0.14506293 0.01676001]. \t  -0.7327520074057852 \t -0.6844563955508911\n",
      "32     \t [0.15330707 0.02816526]. \t  -0.7190625344652346 \t -0.6844563955508911\n",
      "33     \t [0.14863395 0.02310698]. \t  -0.7249271670216545 \t -0.6844563955508911\n",
      "34     \t [0.16924595 0.03163259]. \t  -0.6910453516736166 \t -0.6844563955508911\n",
      "35     \t [1.58318786e-01 5.77916693e-05]. \t  -0.7709625026463949 \t -0.6844563955508911\n",
      "36     \t [0.17199174 0.025725  ]. \t  -0.6870846740586299 \t -0.6844563955508911\n",
      "37     \t [0.16005502 0.03337416]. \t  -0.7115239744075249 \t -0.6844563955508911\n",
      "38     \t [0.17713256 0.03964187]. \t  \u001b[92m-0.6839433678961927\u001b[0m \t -0.6839433678961927\n",
      "39     \t [0.18883808 0.04522203]. \t  \u001b[92m-0.6671272568518504\u001b[0m \t -0.6671272568518504\n",
      "40     \t [0.16837456 0.01979661]. \t  -0.6989169036655942 \t -0.6671272568518504\n",
      "41     \t [ 0.14877122 -0.00257682]. \t  -0.7856473578777824 \t -0.6671272568518504\n",
      "42     \t [0.17788091 0.03351673]. \t  -0.6762313972289011 \t -0.6671272568518504\n",
      "43     \t [0.1789033  0.00885945]. \t  -0.7277778888848603 \t -0.6671272568518504\n",
      "44     \t [0.14560794 0.02063215]. \t  -0.7300182236477437 \t -0.6671272568518504\n",
      "45     \t [0.19029147 0.02791697]. \t  \u001b[92m-0.6625067327800044\u001b[0m \t -0.6625067327800044\n",
      "46     \t [0.18861961 0.03827577]. \t  \u001b[92m-0.6590662884263129\u001b[0m \t -0.6590662884263129\n",
      "47     \t [0.16465523 0.02735379]. \t  -0.697806767618123 \t -0.6590662884263129\n",
      "48     \t [0.16676414 0.04939562]. \t  -0.7408746762806235 \t -0.6590662884263129\n",
      "49     \t [0.16752018 0.04258536]. \t  -0.7141125350765241 \t -0.6590662884263129\n",
      "50     \t [0.21441401 0.04813887]. \t  \u001b[92m-0.6176142855454835\u001b[0m \t -0.6176142855454835\n",
      "51     \t [0.19015861 0.05365708]. \t  -0.6864568168969475 \t -0.6176142855454835\n",
      "52     \t [0.19860136 0.05365128]. \t  -0.6624287221382907 \t -0.6176142855454835\n",
      "53     \t [0.19090041 0.04921908]. \t  -0.6709650397447556 \t -0.6176142855454835\n",
      "54     \t [0.21954488 0.04606828]. \t  \u001b[92m-0.6095645963233671\u001b[0m \t -0.6095645963233671\n",
      "55     \t [0.18958086 0.04456869]. \t  -0.6642230589759023 \t -0.6095645963233671\n",
      "56     \t [0.14142784 0.02314965]. \t  -0.7381370346031604 \t -0.6095645963233671\n",
      "57     \t [0.1786496  0.04192353]. \t  -0.6846321882462184 \t -0.6095645963233671\n",
      "58     \t [0.19309348 0.07782917]. \t  -0.815480368993404 \t -0.6095645963233671\n",
      "59     \t [0.1793813  0.04058538]. \t  -0.6804840476510912 \t -0.6095645963233671\n",
      "60     \t [0.1595678  0.02432729]. \t  -0.706455014498395 \t -0.6095645963233671\n",
      "61     \t [0.1940843  0.06140802]. \t  -0.7058555774018589 \t -0.6095645963233671\n",
      "62     \t [0.19825783 0.05190603]. \t  -0.6586661653190907 \t -0.6095645963233671\n",
      "63     \t [0.17951486 0.02201253]. \t  -0.6836265081923033 \t -0.6095645963233671\n",
      "64     \t [0.17738971 0.03803868]. \t  -0.6810062416569972 \t -0.6095645963233671\n",
      "65     \t [0.19825739 0.04473678]. \t  -0.6457405555604656 \t -0.6095645963233671\n",
      "66     \t [0.18975493 0.03443915]. \t  -0.6567428728987359 \t -0.6095645963233671\n",
      "67     \t [0.13760786 0.04740647]. \t  -0.8247774295906296 \t -0.6095645963233671\n",
      "68     \t [0.16903002 0.00771983]. \t  -0.7339888470882396 \t -0.6095645963233671\n",
      "69     \t [0.17068285 0.03514262]. \t  -0.6913789206969683 \t -0.6095645963233671\n",
      "70     \t [0.17505642 0.04247877]. \t  -0.6945363210828965 \t -0.6095645963233671\n",
      "71     \t [0.1808993  0.04095118]. \t  -0.6776937018176268 \t -0.6095645963233671\n",
      "72     \t [0.18884938 0.03436977]. \t  -0.6581328566830935 \t -0.6095645963233671\n",
      "73     \t [0.18781902 0.01709426]. \t  -0.6926954411399321 \t -0.6095645963233671\n",
      "74     \t [0.18255398 0.0370843 ]. \t  -0.669630511375914 \t -0.6095645963233671\n",
      "75     \t [0.1870004  0.04681464]. \t  -0.6749998967285301 \t -0.6095645963233671\n",
      "76     \t [0.18367724 0.03889071]. \t  -0.6690385766819734 \t -0.6095645963233671\n",
      "77     \t [0.16384516 0.03930175]. \t  -0.7146713851989742 \t -0.6095645963233671\n",
      "78     \t [0.15151468 0.00073184]. \t  -0.7693217611321754 \t -0.6095645963233671\n",
      "79     \t [0.18983025 0.02850643]. \t  -0.6620437526718962 \t -0.6095645963233671\n",
      "80     \t [0.17155442 0.02305378]. \t  -0.6903888658036769 \t -0.6095645963233671\n",
      "81     \t [0.16599574 0.04197444]. \t  -0.7163563100109093 \t -0.6095645963233671\n",
      "82     \t [0.21741048 0.07002674]. \t  -0.6642454638753553 \t -0.6095645963233671\n",
      "83     \t [ 0.17423589 -0.01545468]. \t  -0.8917679013774886 \t -0.6095645963233671\n",
      "84     \t [0.19171183 0.043909  ]. \t  -0.6584499848077847 \t -0.6095645963233671\n",
      "85     \t [0.19662236 0.02728895]. \t  -0.6583465143490735 \t -0.6095645963233671\n",
      "86     \t [0.19880048 0.0378329 ]. \t  -0.6422058525517217 \t -0.6095645963233671\n",
      "87     \t [0.14068822 0.03565177]. \t  -0.7635662330432788 \t -0.6095645963233671\n",
      "88     \t [0.20076166 0.08333202]. \t  -0.8239122112438932 \t -0.6095645963233671\n",
      "89     \t [0.17834443 0.01575781]. \t  -0.7008746613178759 \t -0.6095645963233671\n",
      "90     \t [0.22019487 0.06600824]. \t  -0.6387997016756103 \t -0.6095645963233671\n",
      "91     \t [0.19292274 0.04647812]. \t  -0.6599464925426465 \t -0.6095645963233671\n",
      "92     \t [0.19784461 0.04131129]. \t  -0.6439236444306426 \t -0.6095645963233671\n",
      "93     \t [0.19018185 0.04816079]. \t  -0.6701853951270489 \t -0.6095645963233671\n",
      "94     \t [0.14389853 0.01263388]. \t  -0.7394269065597538 \t -0.6095645963233671\n",
      "95     \t [0.10365324 0.02572978]. \t  -0.8258948821967114 \t -0.6095645963233671\n",
      "96     \t [0.19230122 0.02474586]. \t  -0.6673441543002091 \t -0.6095645963233671\n",
      "97     \t [0.23899141 0.08288118]. \t  -0.6455138969441528 \t -0.6095645963233671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.1998082  0.04020833]. \t  -0.6403150442278757 \t -0.6095645963233671\n",
      "99     \t [0.17667992 0.02141443]. \t  -0.6874626347480124 \t -0.6095645963233671\n",
      "100    \t [0.23529585 0.07217108]. \t  -0.6130197722793481 \t -0.6095645963233671\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.59615628  0.88803019]. \t  \u001b[92m-30.91696088480326\u001b[0m \t -30.91696088480326\n",
      "2      \t [-0.34049827  0.06411122]. \t  \u001b[92m-2.065548211240625\u001b[0m \t -2.065548211240625\n",
      "3      \t [-1.24180343  0.84652626]. \t  -53.40459440596763 \t -2.065548211240625\n",
      "4      \t [ 1.42616103 -1.15669469]. \t  -1018.1935794700095 \t -2.065548211240625\n",
      "5      \t [-0.81490664  0.44812685]. \t  -7.957152699171788 \t -2.065548211240625\n",
      "6      \t [-0.38370973  2.048     ]. \t  -363.20611146047975 \t -2.065548211240625\n",
      "7      \t [0.49208344 0.11666061]. \t  \u001b[92m-1.8326402914924764\u001b[0m \t -1.8326402914924764\n",
      "8      \t [-0.0158903  -0.34151708]. \t  -12.712677509663703 \t -1.8326402914924764\n",
      "9      \t [ 0.32208904 -0.08257181]. \t  -3.930822441411724 \t -1.8326402914924764\n",
      "10     \t [-0.49046172 -0.06526841]. \t  -11.574130676511992 \t -1.8326402914924764\n",
      "11     \t [0.15329789 0.84010931]. \t  -67.40194185651852 \t -1.8326402914924764\n",
      "12     \t [-0.5541121   0.22013112]. \t  -3.1705836645006342 \t -1.8326402914924764\n",
      "13     \t [0.84007037 0.11729708]. \t  -34.64952200400355 \t -1.8326402914924764\n",
      "14     \t [0.39270323 0.08418228]. \t  \u001b[92m-0.8592791206896702\u001b[0m \t -0.8592791206896702\n",
      "15     \t [0.4003817  0.08521192]. \t  -0.9234468269471409 \t -0.8592791206896702\n",
      "16     \t [0.39812432 0.08187641]. \t  -0.9494173544883754 \t -0.8592791206896702\n",
      "17     \t [0.40600688 0.09083729]. \t  -0.9004914694105428 \t -0.8592791206896702\n",
      "18     \t [0.40412165 0.08383953]. \t  -0.9866950508241448 \t -0.8592791206896702\n",
      "19     \t [0.41128205 0.08559108]. \t  -1.044847111566911 \t -0.8592791206896702\n",
      "20     \t [0.38756889 0.0810209 ]. \t  \u001b[92m-0.853780113247814\u001b[0m \t -0.853780113247814\n",
      "21     \t [0.40209495 0.0851643 ]. \t  -0.942960993555954 \t -0.853780113247814\n",
      "22     \t [0.40054882 0.09108898]. \t  \u001b[92m-0.8402892310546679\u001b[0m \t -0.8402892310546679\n",
      "23     \t [0.36038969 0.04493317]. \t  -1.130710182153447 \t -0.8402892310546679\n",
      "24     \t [0.37418245 0.08520569]. \t  \u001b[92m-0.692026271769562\u001b[0m \t -0.692026271769562\n",
      "25     \t [0.39190467 0.07284929]. \t  -1.021674263266652 \t -0.692026271769562\n",
      "26     \t [0.37594258 0.07354161]. \t  -0.8490126224857032 \t -0.692026271769562\n",
      "27     \t [0.40241528 0.0944928 ]. \t  -0.8119936958567682 \t -0.692026271769562\n",
      "28     \t [0.37735705 0.07843854]. \t  -0.7967698527969955 \t -0.692026271769562\n",
      "29     \t [0.41302972 0.10377963]. \t  -0.7909440391547942 \t -0.692026271769562\n",
      "30     \t [0.40073616 0.0972286 ]. \t  -0.7605771727369806 \t -0.692026271769562\n",
      "31     \t [0.38380136 0.09864982]. \t  \u001b[92m-0.6164186836720389\u001b[0m \t -0.6164186836720389\n",
      "32     \t [0.36893624 0.09135511]. \t  \u001b[92m-0.5985768383367811\u001b[0m \t -0.5985768383367811\n",
      "33     \t [0.35783405 0.11092992]. \t  \u001b[92m-0.44167042473685975\u001b[0m \t -0.44167042473685975\n",
      "34     \t [0.38479556 0.10839752]. \t  -0.5358482554395809 \t -0.44167042473685975\n",
      "35     \t [0.40975621 0.10344736]. \t  -0.7638040279027409 \t -0.44167042473685975\n",
      "36     \t [0.36665239 0.13225873]. \t  \u001b[92m-0.40160236039440456\u001b[0m \t -0.40160236039440456\n",
      "37     \t [0.36052969 0.13225371]. \t  -0.40943850177285174 \t -0.40160236039440456\n",
      "38     \t [0.38791851 0.14362482]. \t  \u001b[92m-0.3793441581293602\u001b[0m \t -0.3793441581293602\n",
      "39     \t [0.31370263 0.11243883]. \t  -0.490686732281418 \t -0.3793441581293602\n",
      "40     \t [0.35471148 0.14811469]. \t  -0.4661015716468442 \t -0.3793441581293602\n",
      "41     \t [0.39276784 0.14420554]. \t  \u001b[92m-0.37885333532635085\u001b[0m \t -0.37885333532635085\n",
      "42     \t [0.36005631 0.112111  ]. \t  -0.4402564309836596 \t -0.37885333532635085\n",
      "43     \t [0.3582498  0.17561988]. \t  -0.6353544040793857 \t -0.37885333532635085\n",
      "44     \t [0.38727024 0.12581041]. \t  -0.43384613725055665 \t -0.37885333532635085\n",
      "45     \t [0.37128495 0.12392984]. \t  -0.4146666900166568 \t -0.37885333532635085\n",
      "46     \t [0.37479516 0.13394074]. \t  -0.3951460595487889 \t -0.37885333532635085\n",
      "47     \t [0.35776873 0.12257288]. \t  -0.4154047028476034 \t -0.37885333532635085\n",
      "48     \t [0.40571285 0.12851928]. \t  -0.48338009822327094 \t -0.37885333532635085\n",
      "49     \t [0.36816043 0.16490038]. \t  -0.48541206298138434 \t -0.37885333532635085\n",
      "50     \t [0.42517788 0.09205472]. \t  -1.1175710870725846 \t -0.37885333532635085\n",
      "51     \t [0.32535658 0.12696692]. \t  -0.49970700862788664 \t -0.37885333532635085\n",
      "52     \t [0.40376441 0.15903094]. \t  \u001b[92m-0.357092692628262\u001b[0m \t -0.357092692628262\n",
      "53     \t [0.33716329 0.11051689]. \t  -0.4403524513883036 \t -0.357092692628262\n",
      "54     \t [0.39794935 0.11488561]. \t  -0.5514992773428804 \t -0.357092692628262\n",
      "55     \t [0.4046656  0.12870701]. \t  -0.4772539132685153 \t -0.357092692628262\n",
      "56     \t [0.35910891 0.14743101]. \t  -0.4448621508313909 \t -0.357092692628262\n",
      "57     \t [0.39453172 0.15329879]. \t  -0.36714713831715906 \t -0.357092692628262\n",
      "58     \t [0.35162503 0.12146473]. \t  -0.4208633505163588 \t -0.357092692628262\n",
      "59     \t [0.41029382 0.17843885]. \t  -0.35794998208906237 \t -0.357092692628262\n",
      "60     \t [0.39171057 0.12085316]. \t  -0.4761877895753665 \t -0.357092692628262\n",
      "61     \t [0.37464953 0.14052041]. \t  -0.3910657133473877 \t -0.357092692628262\n",
      "62     \t [0.34948048 0.11881158]. \t  -0.4242812223432969 \t -0.357092692628262\n",
      "63     \t [0.39757789 0.13882468]. \t  -0.39994363740115185 \t -0.357092692628262\n",
      "64     \t [0.3579195  0.10562497]. \t  -0.4628086846333195 \t -0.357092692628262\n",
      "65     \t [0.38648362 0.17462305]. \t  -0.44017611061414097 \t -0.357092692628262\n",
      "66     \t [0.40346563 0.12194641]. \t  -0.5226283235801436 \t -0.357092692628262\n",
      "67     \t [0.39339247 0.14078945]. \t  -0.3874837152394776 \t -0.357092692628262\n",
      "68     \t [0.37638303 0.13314832]. \t  -0.3961501293872082 \t -0.357092692628262\n",
      "69     \t [0.38472513 0.15704309]. \t  -0.3867166448126355 \t -0.357092692628262\n",
      "70     \t [0.31990537 0.13107595]. \t  -0.5451074008621621 \t -0.357092692628262\n",
      "71     \t [0.34797236 0.13030614]. \t  -0.433643418970516 \t -0.357092692628262\n",
      "72     \t [0.38807619 0.14431101]. \t  -0.37840982596407097 \t -0.357092692628262\n",
      "73     \t [0.43573291 0.15528916]. \t  -0.4379335602547434 \t -0.357092692628262\n",
      "74     \t [0.39229245 0.1272698 ]. \t  -0.4401899318450036 \t -0.357092692628262\n",
      "75     \t [0.39948614 0.16592765]. \t  -0.36463452745410496 \t -0.357092692628262\n",
      "76     \t [0.39763093 0.16358204]. \t  -0.36584242563794744 \t -0.357092692628262\n",
      "77     \t [0.43025263 0.14803588]. \t  -0.4621154695307349 \t -0.357092692628262\n",
      "78     \t [0.39133636 0.11192662]. \t  -0.5403598393702633 \t -0.357092692628262\n",
      "79     \t [0.34749057 0.13209558]. \t  -0.43864145658362336 \t -0.357092692628262\n",
      "80     \t [0.38226575 0.15046639]. \t  -0.3834785421247947 \t -0.357092692628262\n",
      "81     \t [0.37661511 0.16484496]. \t  -0.4415363915292031 \t -0.357092692628262\n",
      "82     \t [0.41874403 0.15640564]. \t  -0.37373434899288477 \t -0.357092692628262\n",
      "83     \t [0.40401273 0.16482428]. \t  \u001b[92m-0.3554561897427472\u001b[0m \t -0.3554561897427472\n",
      "84     \t [0.39597838 0.18324088]. \t  -0.43476006244162674 \t -0.3554561897427472\n",
      "85     \t [0.41384048 0.14629258]. \t  -0.40593989182168255 \t -0.3554561897427472\n",
      "86     \t [0.33135702 0.11449155]. \t  -0.449286878484633 \t -0.3554561897427472\n",
      "87     \t [0.41107282 0.1632243 ]. \t  \u001b[92m-0.3501490223191421\u001b[0m \t -0.3501490223191421\n",
      "88     \t [0.36706445 0.16911656]. \t  -0.5188075839995072 \t -0.3501490223191421\n",
      "89     \t [0.35740719 0.10981315]. \t  -0.4450623616369367 \t -0.3501490223191421\n",
      "90     \t [0.36991889 0.13280203]. \t  -0.39863271613554546 \t -0.3501490223191421\n",
      "91     \t [0.4127398  0.13516175]. \t  -0.4687250037129624 \t -0.3501490223191421\n",
      "92     \t [0.36770953 0.13414657]. \t  -0.3999043950235449 \t -0.3501490223191421\n",
      "93     \t [0.35036602 0.13887075]. \t  -0.44799169560285645 \t -0.3501490223191421\n",
      "94     \t [0.3866069  0.15212756]. \t  -0.37696007766770007 \t -0.3501490223191421\n",
      "95     \t [0.32724352 0.07967873]. \t  -0.5277298474597478 \t -0.3501490223191421\n",
      "96     \t [0.41552233 0.19865008]. \t  -0.40916876932076973 \t -0.3501490223191421\n",
      "97     \t [0.39114067 0.14885912]. \t  -0.3724169490271525 \t -0.3501490223191421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.34097637 0.09595979]. \t  -0.47554185526598775 \t -0.3501490223191421\n",
      "99     \t [-0.38370973  2.048     ]. \t  -363.20611091096526 \t -0.3501490223191421\n",
      "100    \t [0.1115959  0.37609472]. \t  -14.012745334125741 \t -0.3501490223191421\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-1.50615817  2.048     ]. \t  \u001b[92m-11.143401927177141\u001b[0m \t -11.143401927177141\n",
      "2      \t [-1.02358075  1.79472381]. \t  -59.89671504885757 \t -11.143401927177141\n",
      "3      \t [-2.048     1.683886]. \t  -639.5101594545214 \t -11.143401927177141\n",
      "4      \t [-0.61899189  0.53631097]. \t  \u001b[92m-4.966933877958154\u001b[0m \t -4.966933877958154\n",
      "5      \t [-0.75359381  0.83039025]. \t  -9.965013195116992 \t -4.966933877958154\n",
      "6      \t [-0.91450853 -0.97176154]. \t  -330.5833425912473 \t -4.966933877958154\n",
      "7      \t [-0.01774724  0.23751625]. \t  -6.662254606816513 \t -4.966933877958154\n",
      "8      \t [-1.34951982  2.048     ]. \t  -10.663897375859031 \t -4.966933877958154\n",
      "9      \t [-0.29821508  0.25230376]. \t  \u001b[92m-4.354387954098375\u001b[0m \t -4.354387954098375\n",
      "10     \t [-0.30604929  0.25596826]. \t  \u001b[92m-4.33996180839436\u001b[0m \t -4.33996180839436\n",
      "11     \t [-0.30200939  0.25442576]. \t  -4.359177461903259 \t -4.33996180839436\n",
      "12     \t [-0.36915118  0.28854114]. \t  \u001b[92m-4.193145905841729\u001b[0m \t -4.193145905841729\n",
      "13     \t [-0.32033098  0.26216065]. \t  -4.288853146555454 \t -4.193145905841729\n",
      "14     \t [-0.32092187  0.26283029]. \t  -4.299699411717583 \t -4.193145905841729\n",
      "15     \t [-0.31811902  0.26041171]. \t  -4.272283751547427 \t -4.193145905841729\n",
      "16     \t [-0.31779336  0.26110088]. \t  -4.300044653007908 \t -4.193145905841729\n",
      "17     \t [-0.3626456   0.28622191]. \t  -4.250323918713096 \t -4.193145905841729\n",
      "18     \t [-0.4445986   0.34102974]. \t  \u001b[92m-4.1421263886937325\u001b[0m \t -4.1421263886937325\n",
      "19     \t [-0.30039654  0.25498633]. \t  -4.405229797135213 \t -4.1421263886937325\n",
      "20     \t [-0.32220176  0.26343861]. \t  -4.296219777231553 \t -4.1421263886937325\n",
      "21     \t [-0.35920028  0.28273272]. \t  -4.21003676636301 \t -4.1421263886937325\n",
      "22     \t [-0.34396699  0.27461312]. \t  -4.24921086774301 \t -4.1421263886937325\n",
      "23     \t [-0.39659486  0.3065    ]. \t  -4.176914620542231 \t -4.1421263886937325\n",
      "24     \t [-0.46596032  0.36004587]. \t  -4.1918480447151865 \t -4.1421263886937325\n",
      "25     \t [-0.37123822  0.29006618]. \t  -4.1982506164047635 \t -4.1421263886937325\n",
      "26     \t [-0.37440706  0.2919195 ]. \t  -4.1914625869776465 \t -4.1421263886937325\n",
      "27     \t [-0.37775815  0.29426783]. \t  -4.195461164244901 \t -4.1421263886937325\n",
      "28     \t [-0.32734477  0.26621967]. \t  -4.292013881371733 \t -4.1421263886937325\n",
      "29     \t [-0.43372249  0.33084468]. \t  \u001b[92m-4.092730672636636\u001b[0m \t -4.092730672636636\n",
      "30     \t [-0.41645568  0.32082542]. \t  -4.178730312585097 \t -4.092730672636636\n",
      "31     \t [-0.37986278  0.29487257]. \t  -4.171359612518511 \t -4.092730672636636\n",
      "32     \t [-0.51601668  0.4072958 ]. \t  -4.287043584420086 \t -4.092730672636636\n",
      "33     \t [-0.49373585  0.38288931]. \t  -4.166523391195318 \t -4.092730672636636\n",
      "34     \t [-0.41488453  0.31910316]. \t  -4.16203365283179 \t -4.092730672636636\n",
      "35     \t [-0.41666249  0.32047207]. \t  -4.163848910547387 \t -4.092730672636636\n",
      "36     \t [-0.42865253  0.32866503]. \t  -4.141287814712447 \t -4.092730672636636\n",
      "37     \t [-0.47379282  0.36604681]. \t  -4.176191699738805 \t -4.092730672636636\n",
      "38     \t [-0.46262954  0.35639996]. \t  -4.166317127972787 \t -4.092730672636636\n",
      "39     \t [-0.35219665  0.27966135]. \t  -4.250158991100335 \t -4.092730672636636\n",
      "40     \t [-0.39315352  0.30191537]. \t  -4.111951717051916 \t -4.092730672636636\n",
      "41     \t [-0.47580645  0.3666232 ]. \t  -4.144489678056127 \t -4.092730672636636\n",
      "42     \t [-0.52656936  0.41592323]. \t  -4.252739348743312 \t -4.092730672636636\n",
      "43     \t [-0.43193439  0.33255014]. \t  -4.18153448653239 \t -4.092730672636636\n",
      "44     \t [-0.42073191  0.32177353]. \t  -4.113972385429916 \t -4.092730672636636\n",
      "45     \t [-0.44579462  0.34507277]. \t  -4.231859467679641 \t -4.092730672636636\n",
      "46     \t [-0.39647424  0.30475919]. \t  -4.127753070736054 \t -4.092730672636636\n",
      "47     \t [-0.41722422  0.32095926]. \t  -4.165992157662709 \t -4.092730672636636\n",
      "48     \t [-0.40487054  0.31100554]. \t  -4.137072201287845 \t -4.092730672636636\n",
      "49     \t [-0.43657709  0.33605027]. \t  -4.179344736406314 \t -4.092730672636636\n",
      "50     \t [-0.3929202   0.30449549]. \t  -4.19350396195656 \t -4.092730672636636\n",
      "51     \t [-0.49255492  0.38198862]. \t  -4.170350372392922 \t -4.092730672636636\n",
      "52     \t [-0.48115535  0.36975233]. \t  -4.104902294699487 \t -4.092730672636636\n",
      "53     \t [-0.39274714  0.30376472]. \t  -4.175200290160761 \t -4.092730672636636\n",
      "54     \t [-0.50926929  0.40172459]. \t  -4.30479773906662 \t -4.092730672636636\n",
      "55     \t [-0.50662421  0.3963734 ]. \t  -4.2216736778789326 \t -4.092730672636636\n",
      "56     \t [-0.44718786  0.34437781]. \t  -4.1795126201215975 \t -4.092730672636636\n",
      "57     \t [-0.31603993  0.26095996]. \t  -4.326596594585359 \t -4.092730672636636\n",
      "58     \t [-0.46492525  0.35901098]. \t  -4.186775184257739 \t -4.092730672636636\n",
      "59     \t [-0.46244379  0.35516671]. \t  -4.135662876282518 \t -4.092730672636636\n",
      "60     \t [-0.31969081  0.25783414]. \t  -4.16371337114302 \t -4.092730672636636\n",
      "61     \t [-0.36509649  0.28567961]. \t  -4.1855818713207995 \t -4.092730672636636\n",
      "62     \t [-0.31802321  0.25955609]. \t  -4.246790014462304 \t -4.092730672636636\n",
      "63     \t [-0.3305364   0.26556327]. \t  -4.21357615378974 \t -4.092730672636636\n",
      "64     \t [-0.4983939   0.39222153]. \t  -4.313748829017927 \t -4.092730672636636\n",
      "65     \t [-0.46992541  0.3588473 ]. \t  \u001b[92m-4.0655611887808405\u001b[0m \t -4.0655611887808405\n",
      "66     \t [-0.45507851  0.3469969 ]. \t  -4.074466899606254 \t -4.0655611887808405\n",
      "67     \t [-0.50728571  0.39543744]. \t  -4.179033913338699 \t -4.0655611887808405\n",
      "68     \t [-0.38094601  0.29285733]. \t  -4.089647644363693 \t -4.0655611887808405\n",
      "69     \t [-0.32655149  0.26840121]. \t  -4.376540986663067 \t -4.0655611887808405\n",
      "70     \t [-0.36499134  0.28849741]. \t  -4.2743499103132105 \t -4.0655611887808405\n",
      "71     \t [-0.3868176   0.29640792]. \t  -4.077701794629345 \t -4.0655611887808405\n",
      "72     \t [-0.44719473  0.34096349]. \t  -4.08191897315022 \t -4.0655611887808405\n",
      "73     \t [-0.3485683   0.27529845]. \t  -4.184037000414131 \t -4.0655611887808405\n",
      "74     \t [-0.49691828  0.37742458]. \t  \u001b[92m-3.9437060765830947\u001b[0m \t -3.9437060765830947\n",
      "75     \t [-0.44636011  0.34157478]. \t  -4.117951984036541 \t -3.9437060765830947\n",
      "76     \t [-0.42189582  0.32228312]. \t  -4.103662752589592 \t -3.9437060765830947\n",
      "77     \t [-0.47197754  0.36039875]. \t  -4.061083459750137 \t -3.9437060765830947\n",
      "78     \t [-0.53366255  0.41406036]. \t  -4.023055382185529 \t -3.9437060765830947\n",
      "79     \t [-0.49892676  0.38878115]. \t  -4.202674140848686 \t -3.9437060765830947\n",
      "80     \t [-0.55402702  0.44652956]. \t  -4.3633587236789255 \t -3.9437060765830947\n",
      "81     \t [-0.40275182  0.30882191]. \t  -4.117246347495476 \t -3.9437060765830947\n",
      "82     \t [-0.42768814  0.31618525]. \t  \u001b[92m-3.8143322187056095\u001b[0m \t -3.8143322187056095\n",
      "83     \t [-0.42111805  0.31420596]. \t  -3.892794248138774 \t -3.8143322187056095\n",
      "84     \t [-0.42670928  0.32080387]. \t  -3.959908141821102 \t -3.8143322187056095\n",
      "85     \t [-0.52546935  0.39402976]. \t  \u001b[92m-3.7173741423516726\u001b[0m \t -3.7173741423516726\n",
      "86     \t [-0.46920236  0.3543647 ]. \t  -3.9598911859738237 \t -3.7173741423516726\n",
      "87     \t [-0.48194086  0.35251315]. \t  \u001b[92m-3.6420625758602823\u001b[0m \t -3.6420625758602823\n",
      "88     \t [-0.39282711  0.30249063]. \t  -4.135624360248103 \t -3.6420625758602823\n",
      "89     \t [-0.52774926  0.385839  ]. \t  \u001b[92m-3.4857698737179703\u001b[0m \t -3.4857698737179703\n",
      "90     \t [-0.57680697  0.37608473]. \t  \u001b[92m-2.6744891589722157\u001b[0m \t -2.6744891589722157\n",
      "91     \t [-0.5667109   0.36686082]. \t  \u001b[92m-2.663428209181725\u001b[0m \t -2.663428209181725\n",
      "92     \t [-0.57301591  0.34799029]. \t  \u001b[92m-2.512964013335883\u001b[0m \t -2.512964013335883\n",
      "93     \t [-0.49495138  0.35902887]. \t  -3.5356656397173483 \t -2.512964013335883\n",
      "94     \t [-0.51627836  0.36383574]. \t  -3.245681021933389 \t -2.512964013335883\n",
      "95     \t [-0.56662967  0.36381079]. \t  -2.6370130221220767 \t -2.512964013335883\n",
      "96     \t [-0.57550929  0.35864325]. \t  -2.5574826768504617 \t -2.512964013335883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-0.578454   0.3522122]. \t  -2.522504205974143 \t -2.512964013335883\n",
      "98     \t [-0.53251036  0.34741014]. \t  -2.7561790289976327 \t -2.512964013335883\n",
      "99     \t [-0.61627387  0.36346048]. \t  -2.6390179069331308 \t -2.512964013335883\n",
      "100    \t [-0.61116765  0.39976829]. \t  -2.6647275337291942 \t -2.512964013335883\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.27224749 0.9449606 ]. \t  -76.36618478514346 \t -62.0309701572776\n",
      "2      \t [1.59098532 1.72063151]. \t  -66.05694829400647 \t -62.0309701572776\n",
      "3      \t [0.35539299 0.16801849]. \t  \u001b[92m-0.5895266029438668\u001b[0m \t -0.5895266029438668\n",
      "4      \t [0.93710427 1.09376317]. \t  -4.652238154929302 \t -0.5895266029438668\n",
      "5      \t [0.69722709 0.47979951]. \t  \u001b[92m-0.09567340132962632\u001b[0m \t -0.09567340132962632\n",
      "6      \t [-1.03562772  2.048     ]. \t  -99.29897276256447 \t -0.09567340132962632\n",
      "7      \t [ 0.3599488  -0.23150691]. \t  -13.446823398853294 \t -0.09567340132962632\n",
      "8      \t [1.4060061  1.17919555]. \t  -63.790604286639926 \t -0.09567340132962632\n",
      "9      \t [0.63081121 2.048     ]. \t  -272.4117812311064 \t -0.09567340132962632\n",
      "10     \t [-1.17905291  0.99081938]. \t  -20.696025008908627 \t -0.09567340132962632\n",
      "11     \t [0.93375464 0.83465555]. \t  -0.14308637299401325 \t -0.09567340132962632\n",
      "12     \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.09567340132962632\n",
      "13     \t [-0.79229965  1.09781462]. \t  -25.309471313838873 \t -0.09567340132962632\n",
      "14     \t [0.51104777 0.06822821]. \t  -3.961720833442294 \t -0.09567340132962632\n",
      "15     \t [-0.73801758  0.54421406]. \t  -3.0207259022492705 \t -0.09567340132962632\n",
      "16     \t [-0.74587133  0.72417246]. \t  -5.865375939947288 \t -0.09567340132962632\n",
      "17     \t [-0.47981434  0.40184161]. \t  -5.135186321031092 \t -0.09567340132962632\n",
      "18     \t [0.83018747 0.73442968]. \t  -0.23330713435684372 \t -0.09567340132962632\n",
      "19     \t [0.8523569  0.72200812]. \t  \u001b[92m-0.023827230184689165\u001b[0m \t -0.023827230184689165\n",
      "20     \t [0.84371075 0.70648001]. \t  -0.027307675793847953 \t -0.023827230184689165\n",
      "21     \t [0.83877428 0.70846646]. \t  -0.028418486323968033 \t -0.023827230184689165\n",
      "22     \t [0.47803933 0.23738911]. \t  -0.28030619730632556 \t -0.023827230184689165\n",
      "23     \t [0.84935136 0.71359167]. \t  -0.028788489476901043 \t -0.023827230184689165\n",
      "24     \t [0.84827724 0.71238226]. \t  -0.02819230362882308 \t -0.023827230184689165\n",
      "25     \t [0.85522372 0.72428959]. \t  -0.026026791884741034 \t -0.023827230184689165\n",
      "26     \t [0.84630018 0.71699412]. \t  \u001b[92m-0.02368294282408886\u001b[0m \t -0.02368294282408886\n",
      "27     \t [0.86119075 0.73419803]. \t  -0.024820464454077023 \t -0.02368294282408886\n",
      "28     \t [0.88156324 0.78029733]. \t  \u001b[92m-0.015015482362307726\u001b[0m \t -0.015015482362307726\n",
      "29     \t [0.84727999 0.72042358]. \t  -0.02396865938794952 \t -0.015015482362307726\n",
      "30     \t [0.83496033 0.69421325]. \t  -0.028105686055404207 \t -0.015015482362307726\n",
      "31     \t [0.89864007 0.80916649]. \t  \u001b[92m-0.01053385877842467\u001b[0m \t -0.01053385877842467\n",
      "32     \t [0.85629648 0.72607642]. \t  -0.025787626479685927 \t -0.01053385877842467\n",
      "33     \t [0.86251802 0.74034048]. \t  -0.02019503652205566 \t -0.01053385877842467\n",
      "34     \t [0.85876952 0.73151437]. \t  -0.023510992673105052 \t -0.01053385877842467\n",
      "35     \t [0.84978272 0.71981686]. \t  -0.02310060253770629 \t -0.01053385877842467\n",
      "36     \t [0.87590994 0.76109218]. \t  -0.01915118696286661 \t -0.01053385877842467\n",
      "37     \t [0.85402172 0.72300609]. \t  -0.025338114646194357 \t -0.01053385877842467\n",
      "38     \t [0.9025391  0.80836192]. \t  -0.013361138118550305 \t -0.01053385877842467\n",
      "39     \t [0.83513868 0.69654718]. \t  -0.027261962504056016 \t -0.01053385877842467\n",
      "40     \t [0.91037402 0.85504872]. \t  -0.07703285834642035 \t -0.01053385877842467\n",
      "41     \t [0.84263603 0.69436398]. \t  -0.04932301575499231 \t -0.01053385877842467\n",
      "42     \t [0.84587077 0.70860123]. \t  -0.028511475855297168 \t -0.01053385877842467\n",
      "43     \t [0.86297831 0.74194344]. \t  -0.019552308644728896 \t -0.01053385877842467\n",
      "44     \t [0.87386344 0.75894551]. \t  -0.01811173511851153 \t -0.01053385877842467\n",
      "45     \t [0.85455856 0.72536789]. \t  -0.02355659809756414 \t -0.01053385877842467\n",
      "46     \t [0.89997949 0.80665996]. \t  -0.011095167856823417 \t -0.01053385877842467\n",
      "47     \t [0.84794552 0.70417169]. \t  -0.0451428938823851 \t -0.01053385877842467\n",
      "48     \t [0.83258758 0.6859646 ]. \t  -0.033265031587290016 \t -0.01053385877842467\n",
      "49     \t [0.88972048 0.79316165]. \t  -0.012404654637840816 \t -0.01053385877842467\n",
      "50     \t [0.82571727 0.66160862]. \t  -0.07118005595015199 \t -0.01053385877842467\n",
      "51     \t [0.88366561 0.78009571]. \t  -0.013592856457997538 \t -0.01053385877842467\n",
      "52     \t [0.83553526 0.69445859]. \t  -0.028388632839152213 \t -0.01053385877842467\n",
      "53     \t [0.85768702 0.73016302]. \t  -0.02323852682562268 \t -0.01053385877842467\n",
      "54     \t [0.87230923 0.77828189]. \t  -0.04643670546694445 \t -0.01053385877842467\n",
      "55     \t [0.60650124 0.36440002]. \t  -0.15602720732032654 \t -0.01053385877842467\n",
      "56     \t [0.87353674 0.76038689]. \t  -0.016710952982322575 \t -0.01053385877842467\n",
      "57     \t [0.87806245 0.78638904]. \t  -0.03857048422069931 \t -0.01053385877842467\n",
      "58     \t [0.82599551 0.67530781]. \t  -0.03512279194623001 \t -0.01053385877842467\n",
      "59     \t [0.83421309 0.69445407]. \t  -0.027697702461040263 \t -0.01053385877842467\n",
      "60     \t [0.895485   0.80021214]. \t  -0.011206042106455656 \t -0.01053385877842467\n",
      "61     \t [0.84849651 0.73446888]. \t  -0.04404374931174991 \t -0.01053385877842467\n",
      "62     \t [0.81522776 0.66686079]. \t  -0.034653574967026445 \t -0.01053385877842467\n",
      "63     \t [0.85319564 0.72498623]. \t  -0.022425654727912633 \t -0.01053385877842467\n",
      "64     \t [0.89328879 0.7829891 ]. \t  -0.03381461268961289 \t -0.01053385877842467\n",
      "65     \t [0.86653506 0.75086819]. \t  -0.01781291328989779 \t -0.01053385877842467\n",
      "66     \t [0.83485446 0.6978938 ]. \t  -0.02735619054042314 \t -0.01053385877842467\n",
      "67     \t [0.89518529 0.79741642]. \t  -0.012538704900471336 \t -0.01053385877842467\n",
      "68     \t [0.8860332  0.77879235]. \t  -0.016910306044220204 \t -0.01053385877842467\n",
      "69     \t [0.86018459 0.74126545]. \t  -0.019730040846113326 \t -0.01053385877842467\n",
      "70     \t [0.90142949 0.80472676]. \t  -0.015875826867629964 \t -0.01053385877842467\n",
      "71     \t [0.51661351 0.26134054]. \t  -0.23674161571455757 \t -0.01053385877842467\n",
      "72     \t [0.48282504 0.2420724 ]. \t  -0.27548446016711353 \t -0.01053385877842467\n",
      "73     \t [0.86675324 0.74372893]. \t  -0.023428183405811404 \t -0.01053385877842467\n",
      "74     \t [0.85538352 0.70585603]. \t  -0.08760663962350308 \t -0.01053385877842467\n",
      "75     \t [0.80679791 0.64662484]. \t  -0.03917435693961157 \t -0.01053385877842467\n",
      "76     \t [0.45165907 0.23010323]. \t  -0.3688369230405224 \t -0.01053385877842467\n",
      "77     \t [0.87444004 0.73583558]. \t  -0.09876577144517382 \t -0.01053385877842467\n",
      "78     \t [0.84976054 0.72261463]. \t  -0.02259910638760933 \t -0.01053385877842467\n",
      "79     \t [0.80118169 0.65355185]. \t  -0.05312371241388559 \t -0.01053385877842467\n",
      "80     \t [0.84762347 0.68201109]. \t  -0.15611131082189486 \t -0.01053385877842467\n",
      "81     \t [0.46588582 0.2323801 ]. \t  -0.3087803753153262 \t -0.01053385877842467\n",
      "82     \t [0.86658603 0.79363137]. \t  -0.19978700488278026 \t -0.01053385877842467\n",
      "83     \t [0.85667114 0.72877418]. \t  -0.023155662379528635 \t -0.01053385877842467\n",
      "84     \t [0.86028637 0.739916  ]. \t  -0.01952301758947917 \t -0.01053385877842467\n",
      "85     \t [0.90504401 0.80223395]. \t  -0.037478722487380274 \t -0.01053385877842467\n",
      "86     \t [0.87742871 0.76612611]. \t  -0.016433750290663685 \t -0.01053385877842467\n",
      "87     \t [0.88483044 0.77766561]. \t  -0.01603004509435437 \t -0.01053385877842467\n",
      "88     \t [0.79100341 0.61563438]. \t  -0.05378388573604451 \t -0.01053385877842467\n",
      "89     \t [0.84973212 0.69705691]. \t  -0.0850193135999078 \t -0.01053385877842467\n",
      "90     \t [0.8201106  0.66314987]. \t  -0.04125557546580573 \t -0.01053385877842467\n",
      "91     \t [0.86180203 0.72938259]. \t  -0.036841296917261415 \t -0.01053385877842467\n",
      "92     \t [0.90106309 0.8063088 ]. \t  -0.012931116597702635 \t -0.01053385877842467\n",
      "93     \t [0.79662111 0.63422699]. \t  -0.041377276555356876 \t -0.01053385877842467\n",
      "94     \t [0.83118326 0.69810597]. \t  -0.03374137453421749 \t -0.01053385877842467\n",
      "95     \t [0.86167643 0.74003435]. \t  -0.01973460083845551 \t -0.01053385877842467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.48037447 0.23193822]. \t  -0.2701496012602758 \t -0.01053385877842467\n",
      "97     \t [0.5186508  0.27916702]. \t  -0.24203661057936085 \t -0.01053385877842467\n",
      "98     \t [0.88685539 0.78409065]. \t  -0.013388234731843633 \t -0.01053385877842467\n",
      "99     \t [0.81206292 0.66920546]. \t  -0.044844699614077214 \t -0.01053385877842467\n",
      "100    \t [0.89728222 0.75776117]. \t  -0.23479300574052495 \t -0.01053385877842467\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [1.51129299 0.5361465 ]. \t  -305.76288177593995 \t -4.306489127802793\n",
      "2      \t [0.29162357 0.47434297]. \t  -15.657142539475686 \t -4.306489127802793\n",
      "3      \t [0.61452792 0.48085136]. \t  \u001b[92m-1.2137529760213668\u001b[0m \t -1.2137529760213668\n",
      "4      \t [1.41223309 2.02081873]. \t  \u001b[92m-0.23971885159555667\u001b[0m \t -0.23971885159555667\n",
      "5      \t [1.09719267 1.57511295]. \t  -13.794418087037226 \t -0.23971885159555667\n",
      "6      \t [ 0.19403036 -0.22774406]. \t  -7.69286984700596 \t -0.23971885159555667\n",
      "7      \t [2.01205162 2.048     ]. \t  -401.16494772306635 \t -0.23971885159555667\n",
      "8      \t [1.019557 2.048   ]. \t  -101.70831859359649 \t -0.23971885159555667\n",
      "9      \t [-1.54569263  2.048     ]. \t  -18.11995430190136 \t -0.23971885159555667\n",
      "10     \t [0.39806705 0.05041728]. \t  -1.5295894779215709 \t -0.23971885159555667\n",
      "11     \t [1.36353118 2.048     ]. \t  -3.6960462413386246 \t -0.23971885159555667\n",
      "12     \t [1.35581387 1.61466113]. \t  -5.124963012503213 \t -0.23971885159555667\n",
      "13     \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.23971885159555667\n",
      "14     \t [-0.9878871  2.048    ]. \t  -118.88704931984896 \t -0.23971885159555667\n",
      "15     \t [-1.32371847  1.61102915]. \t  -7.3934523561356364 \t -0.23971885159555667\n",
      "16     \t [-1.35941923  1.91484658]. \t  -6.01342947261534 \t -0.23971885159555667\n",
      "17     \t [-1.27578563  1.19631405]. \t  -23.782455591361156 \t -0.23971885159555667\n",
      "18     \t [0.57281981 0.78751051]. \t  -21.286213984006995 \t -0.23971885159555667\n",
      "19     \t [-2.048       2.04799998]. \t  -469.95239673313205 \t -0.23971885159555667\n",
      "20     \t [0.67488825 0.77580661]. \t  -10.36698611687549 \t -0.23971885159555667\n",
      "21     \t [1.28496257 1.04363211]. \t  -36.98642785984151 \t -0.23971885159555667\n",
      "22     \t [ 0.16506614 -0.30106949]. \t  -11.476275562119199 \t -0.23971885159555667\n",
      "23     \t [ 0.06799926 -0.18649937]. \t  -4.521435691221365 \t -0.23971885159555667\n",
      "24     \t [-1.64700289  1.34109852]. \t  -195.1133310497174 \t -0.23971885159555667\n",
      "25     \t [0.25140687 0.63172346]. \t  -32.881668596983594 \t -0.23971885159555667\n",
      "26     \t [-1.13100717  1.68532403]. \t  -21.036715114683613 \t -0.23971885159555667\n",
      "27     \t [0.81163417 0.77740557]. \t  -1.4433955101968565 \t -0.23971885159555667\n",
      "28     \t [0.08710804 1.04892057]. \t  -109.27076301525734 \t -0.23971885159555667\n",
      "29     \t [1.0663463  1.93585246]. \t  -63.8058421388457 \t -0.23971885159555667\n",
      "30     \t [-0.5638154   1.13074125]. \t  -68.51859129863618 \t -0.23971885159555667\n",
      "31     \t [-1.49879001 -0.56464325]. \t  -796.4243320829295 \t -0.23971885159555667\n",
      "32     \t [0.43695338 0.68523927]. \t  -24.75135904281406 \t -0.23971885159555667\n",
      "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.23971885159555667\n",
      "34     \t [1.40614072 0.34507446]. \t  -266.55868472324113 \t -0.23971885159555667\n",
      "35     \t [0.40043731 0.78788668]. \t  -39.73969814204101 \t -0.23971885159555667\n",
      "36     \t [-0.19035366  1.41301706]. \t  -190.96996020774495 \t -0.23971885159555667\n",
      "37     \t [0.65145693 0.8343735 ]. \t  -16.92962599141516 \t -0.23971885159555667\n",
      "38     \t [-0.11871895 -0.2983413 ]. \t  -11.013125868869146 \t -0.23971885159555667\n",
      "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.23971885159555667\n",
      "40     \t [ 0.10463596 -0.106465  ]. \t  -2.180274051730791 \t -0.23971885159555667\n",
      "41     \t [0.45014774 1.91013468]. \t  -291.8585425470606 \t -0.23971885159555667\n",
      "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.23971885159555667\n",
      "43     \t [-0.01326109 -0.49478701]. \t  -25.525522378701645 \t -0.23971885159555667\n",
      "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.23971885159555667\n",
      "45     \t [-0.90572846 -0.05370564]. \t  -80.02808514972179 \t -0.23971885159555667\n",
      "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.23971885159555667\n",
      "47     \t [ 0.43653176 -0.3040504 ]. \t  -24.781439316937753 \t -0.23971885159555667\n",
      "48     \t [0.18682749 0.66196596]. \t  -39.9818550325717 \t -0.23971885159555667\n",
      "49     \t [0.82781372 1.58810966]. \t  -81.5405908561071 \t -0.23971885159555667\n",
      "50     \t [1.25603621 1.63077538]. \t  -0.34802990111876136 \t -0.23971885159555667\n",
      "51     \t [-0.88946588  0.72718265]. \t  -3.9792578357610764 \t -0.23971885159555667\n",
      "52     \t [ 0.01097993 -0.32453214]. \t  -11.518098461469041 \t -0.23971885159555667\n",
      "53     \t [0.97665313 1.68528034]. \t  -53.49938424655507 \t -0.23971885159555667\n",
      "54     \t [-0.32417485  1.87771659]. \t  -315.9741788024435 \t -0.23971885159555667\n",
      "55     \t [-0.02383424  0.49117906]. \t  -25.11815089190025 \t -0.23971885159555667\n",
      "56     \t [0.99468116 0.01617597]. \t  -94.71470333508002 \t -0.23971885159555667\n",
      "57     \t [-1.2143308  1.0796106]. \t  -20.504867626281474 \t -0.23971885159555667\n",
      "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.23971885159555667\n",
      "59     \t [-1.61254213 -0.85417974]. \t  -1200.1629602713915 \t -0.23971885159555667\n",
      "60     \t [-1.32088165  0.18350019]. \t  -249.12981988210257 \t -0.23971885159555667\n",
      "61     \t [-0.01788301  1.90709868]. \t  -364.6166540076725 \t -0.23971885159555667\n",
      "62     \t [0.16590183 0.1288118 ]. \t  -1.7216534870286875 \t -0.23971885159555667\n",
      "63     \t [-1.66722611  0.57154307]. \t  -494.6845860274701 \t -0.23971885159555667\n",
      "64     \t [-1.47493362  1.96319206]. \t  -10.629756162307363 \t -0.23971885159555667\n",
      "65     \t [ 0.49730935 -0.35084715]. \t  -36.03268277207172 \t -0.23971885159555667\n",
      "66     \t [1.26610023 1.43870638]. \t  -2.7703706633725393 \t -0.23971885159555667\n",
      "67     \t [0.75194154 0.60615149]. \t  \u001b[92m-0.22747038512025927\u001b[0m \t -0.22747038512025927\n",
      "68     \t [-1.39873869 -0.84992713]. \t  -793.3403815546698 \t -0.22747038512025927\n",
      "69     \t [-1.70860006 -0.65519372]. \t  -1285.0471706975995 \t -0.22747038512025927\n",
      "70     \t [0.20571481 0.83706811]. \t  -63.793570430597526 \t -0.22747038512025927\n",
      "71     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.22747038512025927\n",
      "72     \t [0.964233   0.36784277]. \t  -31.574722517174585 \t -0.22747038512025927\n",
      "73     \t [1.41555776 0.41417151]. \t  -252.86576381150346 \t -0.22747038512025927\n",
      "74     \t [0.84384095 0.0379231 ]. \t  -45.471459668446435 \t -0.22747038512025927\n",
      "75     \t [1.33175515 1.65638105]. \t  -1.4834281338044168 \t -0.22747038512025927\n",
      "76     \t [-1.42594287  0.86447026]. \t  -142.50454748351717 \t -0.22747038512025927\n",
      "77     \t [0.72221627 0.95023055]. \t  -18.449892393169733 \t -0.22747038512025927\n",
      "78     \t [-1.89103271  1.6425476 ]. \t  -382.1837115519547 \t -0.22747038512025927\n",
      "79     \t [-1.75967524 -0.55885592]. \t  -1343.747036702027 \t -0.22747038512025927\n",
      "80     \t [-1.18472947  0.67165453]. \t  -58.34510526662471 \t -0.22747038512025927\n",
      "81     \t [0.18644642 1.95962289]. \t  -371.1707108307474 \t -0.22747038512025927\n",
      "82     \t [0.73817965 1.161579  ]. \t  -38.09671422254698 \t -0.22747038512025927\n",
      "83     \t [-0.42112012  1.37668659]. \t  -145.86228869002719 \t -0.22747038512025927\n",
      "84     \t [-1.48593948 -0.37848861]. \t  -675.1805717013438 \t -0.22747038512025927\n",
      "85     \t [-0.56163036  1.02144014]. \t  -52.28390925390679 \t -0.22747038512025927\n",
      "86     \t [ 0.31385582 -0.29620545]. \t  -16.05046502921451 \t -0.22747038512025927\n",
      "87     \t [-0.64746324 -0.68384168]. \t  -124.38613855044598 \t -0.22747038512025927\n",
      "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.22747038512025927\n",
      "89     \t [0.91576258 0.60932277]. \t  -5.2648683228106234 \t -0.22747038512025927\n",
      "90     \t [1.40177852 1.54680814]. \t  -17.64844784898751 \t -0.22747038512025927\n",
      "91     \t [-1.2511616   1.55097647]. \t  -5.0885478413268865 \t -0.22747038512025927\n",
      "92     \t [1.60491789 0.40059911]. \t  -473.4990365626116 \t -0.22747038512025927\n",
      "93     \t [1.36304737 0.11126409]. \t  -305.2048472287886 \t -0.22747038512025927\n",
      "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.22747038512025927\n",
      "95     \t [-0.80388873  1.05320344]. \t  -19.816176015265253 \t -0.22747038512025927\n",
      "96     \t [0.30670097 0.40717588]. \t  -10.284475405427806 \t -0.22747038512025927\n",
      "97     \t [-1.73494379  0.9563097 ]. \t  -429.2566066568558 \t -0.22747038512025927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.85587425  0.9558534 ]. \t  -627.3773533603351 \t -0.22747038512025927\n",
      "99     \t [-1.14063526  0.83743759]. \t  -26.075855590565894 \t -0.22747038512025927\n",
      "100    \t [-1.07376708 -0.71204382]. \t  -352.13031147852405 \t -0.22747038512025927\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-1.98800237  0.7291254 ]. \t  -1047.7191179151364 \t -6.867717811955245\n",
      "2      \t [-0.3595548   0.27951463]. \t  \u001b[92m-4.105444069840028\u001b[0m \t -4.105444069840028\n",
      "3      \t [-0.58607972  0.7818671 ]. \t  -21.733146311497595 \t -4.105444069840028\n",
      "4      \t [1.40303048 2.048     ]. \t  \u001b[92m-0.7945455498636537\u001b[0m \t -0.7945455498636537\n",
      "5      \t [-0.81148089  0.00468857]. \t  -46.028563418891935 \t -0.7945455498636537\n",
      "6      \t [1.49324318 0.39986143]. \t  -335.10172282064167 \t -0.7945455498636537\n",
      "7      \t [0.96196662 2.048     ]. \t  -126.02906381274227 \t -0.7945455498636537\n",
      "8      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.7945455498636537\n",
      "9      \t [1.19342949 1.50163537]. \t  \u001b[92m-0.6358939015103258\u001b[0m \t -0.6358939015103258\n",
      "10     \t [-0.03415298 -0.26528149]. \t  -8.16892180704414 \t -0.6358939015103258\n",
      "11     \t [1.2947756  1.77378547]. \t  -1.034431746838214 \t -0.6358939015103258\n",
      "12     \t [-0.80016296  0.62045667]. \t  -3.2798068860127265 \t -0.6358939015103258\n",
      "13     \t [-0.33169952 -0.04217847]. \t  -4.090000227323671 \t -0.6358939015103258\n",
      "14     \t [0.50344196 0.51049414]. \t  -6.8535435635430275 \t -0.6358939015103258\n",
      "15     \t [0.1504202  0.08539595]. \t  -1.115789540139584 \t -0.6358939015103258\n",
      "16     \t [0.73842944 1.06455364]. \t  -27.03313399356457 \t -0.6358939015103258\n",
      "17     \t [1.27818915 1.48969276]. \t  -2.153142603899591 \t -0.6358939015103258\n",
      "18     \t [-0.96516998  1.01759378]. \t  -4.602193103213363 \t -0.6358939015103258\n",
      "19     \t [0.07652079 0.27430248]. \t  -8.059195616798078 \t -0.6358939015103258\n",
      "20     \t [ 0.32303398 -0.03048373]. \t  -2.2763220207477834 \t -0.6358939015103258\n",
      "21     \t [-1.88182475 -2.048     ]. \t  -3132.292555520234 \t -0.6358939015103258\n",
      "22     \t [1.36586696 2.048     ]. \t  -3.461105978078594 \t -0.6358939015103258\n",
      "23     \t [1.2488529  1.55130055]. \t  \u001b[92m-0.06887168861584271\u001b[0m \t -0.06887168861584271\n",
      "24     \t [1.09412591 1.22937185]. \t  -0.11293258579708944 \t -0.06887168861584271\n",
      "25     \t [1.18519884 1.40757766]. \t  \u001b[92m-0.03512883632926792\u001b[0m \t -0.03512883632926792\n",
      "26     \t [1.20027913 1.43923105]. \t  -0.040318780266831694 \t -0.03512883632926792\n",
      "27     \t [1.29076884 1.65391034]. \t  -0.09936680494825433 \t -0.03512883632926792\n",
      "28     \t [1.1174574  1.27156296]. \t  -0.06601727981321531 \t -0.03512883632926792\n",
      "29     \t [1.08393274 1.1959041 ]. \t  -0.05111919993822479 \t -0.03512883632926792\n",
      "30     \t [1.26422686 1.58889804]. \t  -0.07859835175778197 \t -0.03512883632926792\n",
      "31     \t [1.20486234 1.45189008]. \t  -0.041972452233838636 \t -0.03512883632926792\n",
      "32     \t [1.32369376 1.74983737]. \t  -0.10531951476196078 \t -0.03512883632926792\n",
      "33     \t [1.1326233  1.30142948]. \t  -0.052162416217947136 \t -0.03512883632926792\n",
      "34     \t [1.24164322 1.53424968]. \t  -0.06390928489717716 \t -0.03512883632926792\n",
      "35     \t [1.08199879 1.21344693]. \t  -0.18927098679214582 \t -0.03512883632926792\n",
      "36     \t [1.28312176 1.63832328]. \t  -0.08668363642622069 \t -0.03512883632926792\n",
      "37     \t [0.27076981 0.07097755]. \t  -0.53232363841818 \t -0.03512883632926792\n",
      "38     \t [1.14505818 1.32703556]. \t  -0.046250800067546435 \t -0.03512883632926792\n",
      "39     \t [1.29972824 1.68327781]. \t  -0.09345588023591224 \t -0.03512883632926792\n",
      "40     \t [1.27697576 1.62872377]. \t  -0.07709321369324793 \t -0.03512883632926792\n",
      "41     \t [1.21803668 1.4914883 ]. \t  -0.053741464477804916 \t -0.03512883632926792\n",
      "42     \t [1.22446569 1.50124871]. \t  -0.05075829857772975 \t -0.03512883632926792\n",
      "43     \t [1.19786257 1.44270036]. \t  -0.04527363782184143 \t -0.03512883632926792\n",
      "44     \t [1.1594951  1.34898731]. \t  \u001b[92m-0.02751661301809361\u001b[0m \t -0.02751661301809361\n",
      "45     \t [1.24154394 1.53447715]. \t  -0.06317958233353325 \t -0.02751661301809361\n",
      "46     \t [1.31344497 1.72034788]. \t  -0.10054197751720698 \t -0.02751661301809361\n",
      "47     \t [1.27519653 1.57151396]. \t  -0.37398275622323146 \t -0.02751661301809361\n",
      "48     \t [1.3141051  1.72132693]. \t  -0.10173701869073233 \t -0.02751661301809361\n",
      "49     \t [1.13022838 1.28191421]. \t  \u001b[92m-0.018982643809331994\u001b[0m \t -0.018982643809331994\n",
      "50     \t [0.18751202 0.03194175]. \t  -0.6611729204253174 \t -0.018982643809331994\n",
      "51     \t [1.14472956 1.3438507 ]. \t  -0.1328029820697795 \t -0.018982643809331994\n",
      "52     \t [0.23448114 0.03458382]. \t  -0.6276252815759508 \t -0.018982643809331994\n",
      "53     \t [2.048      2.04799989]. \t  -461.76043639357346 \t -0.018982643809331994\n",
      "54     \t [0.96021954 0.95337528]. \t  -0.09988798143770664 \t -0.018982643809331994\n",
      "55     \t [0.85204685 0.41798736]. \t  -9.50807366407585 \t -0.018982643809331994\n",
      "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.018982643809331994\n",
      "57     \t [0.6965491  0.38085714]. \t  -1.1804217257349525 \t -0.018982643809331994\n",
      "58     \t [-0.42475128  1.02612634]. \t  -73.55291025568293 \t -0.018982643809331994\n",
      "59     \t [-1.00820291  0.88383206]. \t  -5.792243630465565 \t -0.018982643809331994\n",
      "60     \t [1.97446429 0.08177345]. \t  -1457.6967749248383 \t -0.018982643809331994\n",
      "61     \t [0.88156119 1.34933248]. \t  -32.75329275091675 \t -0.018982643809331994\n",
      "62     \t [1.52244102 1.20037607]. \t  -125.14252790837422 \t -0.018982643809331994\n",
      "63     \t [0.70728379 0.1569508 ]. \t  -11.871141423257278 \t -0.018982643809331994\n",
      "64     \t [0.13572092 0.95688946]. \t  -88.81944004137992 \t -0.018982643809331994\n",
      "65     \t [-0.06869879  1.41939878]. \t  -201.27385673306617 \t -0.018982643809331994\n",
      "66     \t [1.39273604 0.72569187]. \t  -147.53913752994876 \t -0.018982643809331994\n",
      "67     \t [1.2068672  1.09539853]. \t  -13.084276110670455 \t -0.018982643809331994\n",
      "68     \t [0.89570069 0.51791815]. \t  -8.097028886318645 \t -0.018982643809331994\n",
      "69     \t [0.49899078 1.33353291]. \t  -117.87395456416272 \t -0.018982643809331994\n",
      "70     \t [1.97260882 0.96365149]. \t  -857.9915325219232 \t -0.018982643809331994\n",
      "71     \t [ 1.47708114 -0.17078569]. \t  -553.6788252292914 \t -0.018982643809331994\n",
      "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.018982643809331994\n",
      "73     \t [1.06783825 1.26299561]. \t  -1.5105499843251575 \t -0.018982643809331994\n",
      "74     \t [0.26012729 1.04160925]. \t  -95.40391804125632 \t -0.018982643809331994\n",
      "75     \t [-0.42228324 -0.13668873]. \t  -11.946137183602922 \t -0.018982643809331994\n",
      "76     \t [ 1.98125232 -0.3129985 ]. \t  -1797.331771015109 \t -0.018982643809331994\n",
      "77     \t [-0.67907127  0.40375682]. \t  -3.1485378781893436 \t -0.018982643809331994\n",
      "78     \t [ 1.5437683  -0.42129769]. \t  -786.8279497574187 \t -0.018982643809331994\n",
      "79     \t [1.83946825 1.95766627]. \t  -204.04579470920967 \t -0.018982643809331994\n",
      "80     \t [0.279467   1.24420333]. \t  -136.49844539274142 \t -0.018982643809331994\n",
      "81     \t [ 1.7695687  -0.27145466]. \t  -1158.516096615285 \t -0.018982643809331994\n",
      "82     \t [0.10639442 1.08194119]. \t  -115.42155236530361 \t -0.018982643809331994\n",
      "83     \t [-0.62595504  1.55514522]. \t  -137.9763517609367 \t -0.018982643809331994\n",
      "84     \t [-0.78968097  0.20149125]. \t  -21.020202428783165 \t -0.018982643809331994\n",
      "85     \t [ 0.9912325 -0.3307436]. \t  -172.47195152637886 \t -0.018982643809331994\n",
      "86     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.018982643809331994\n",
      "87     \t [ 1.6818926  -0.28641929]. \t  -970.900871202338 \t -0.018982643809331994\n",
      "88     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.018982643809331994\n",
      "89     \t [-0.0834995   0.30859613]. \t  -10.27167249149177 \t -0.018982643809331994\n",
      "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.018982643809331994\n",
      "91     \t [-1.05163473  1.38739377]. \t  -12.131075604635068 \t -0.018982643809331994\n",
      "92     \t [0.38808841 1.19649116]. \t  -109.76062945259233 \t -0.018982643809331994\n",
      "93     \t [ 0.3294741  -1.04696114]. \t  -133.9709406264278 \t -0.018982643809331994\n",
      "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.018982643809331994\n",
      "95     \t [0.49983319 0.85564711]. \t  -36.951213878346806 \t -0.018982643809331994\n",
      "96     \t [-0.78017051  0.70284956]. \t  -4.0560608872733335 \t -0.018982643809331994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.14713637 0.96434494]. \t  -89.5949191742454 \t -0.018982643809331994\n",
      "98     \t [0.95567539 0.63855313]. \t  -7.55139792247555 \t -0.018982643809331994\n",
      "99     \t [ 0.19572035 -0.63308863]. \t  -45.72400177786659 \t -0.018982643809331994\n",
      "100    \t [ 1.92576674 -0.3909806 ]. \t  -1681.4947326634206 \t -0.018982643809331994\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-0.82846093  1.34052693]. \t  -46.13833945095503 \t -43.94591987332933\n",
      "init   \t [ 1.08702204 -1.93893911]. \t  -973.794564983344 \t -43.94591987332933\n",
      "init   \t [1.16845539 0.39302383]. \t  -94.55813856061427 \t -43.94591987332933\n",
      "init   \t [0.40748384 0.826307  ]. \t  -43.94591987332933 \t -43.94591987332933\n",
      "init   \t [2.04295072 1.46997105]. \t  -732.0744672178031 \t -43.94591987332933\n",
      "1      \t [0.26112892 0.29438735]. \t  \u001b[92m-5.662531073463428\u001b[0m \t -5.662531073463428\n",
      "2      \t [0.26123539 0.29460262]. \t  -5.669598959558931 \t -5.662531073463428\n",
      "3      \t [0.2609617  0.29412764]. \t  \u001b[92m-5.654981330098763\u001b[0m \t -5.654981330098763\n",
      "4      \t [0.26076915 0.29543534]. \t  -5.719120501128131 \t -5.654981330098763\n",
      "5      \t [0.26038223 0.29336366]. \t  \u001b[92m-5.634980235389532\u001b[0m \t -5.634980235389532\n",
      "6      \t [0.26113099 0.29330722]. \t  \u001b[92m-5.6137311033100765\u001b[0m \t -5.6137311033100765\n",
      "7      \t [0.26068472 0.29234908]. \t  \u001b[92m-5.581789154438946\u001b[0m \t -5.581789154438946\n",
      "8      \t [0.26104273 0.2929373 ]. \t  -5.599291812201934 \t -5.581789154438946\n",
      "9      \t [0.26058066 0.29611597]. \t  -5.754889914843859 \t -5.581789154438946\n",
      "10     \t [0.47137243 0.23210179]. \t  \u001b[92m-0.28926758070242875\u001b[0m \t -0.28926758070242875\n",
      "11     \t [0.46743596 0.23304294]. \t  -0.3047847021667359 \t -0.28926758070242875\n",
      "12     \t [0.46996039 0.23277173]. \t  -0.29512431886574503 \t -0.28926758070242875\n",
      "13     \t [0.47898583 0.23060659]. \t  \u001b[92m-0.2715948069716746\u001b[0m \t -0.2715948069716746\n",
      "14     \t [0.4625198 0.2314638]. \t  -0.3196474654740753 \t -0.2715948069716746\n",
      "15     \t [0.46658999 0.23334216]. \t  -0.30897451747654375 \t -0.2715948069716746\n",
      "16     \t [0.48502311 0.2303645 ]. \t  \u001b[92m-0.2675854862285817\u001b[0m \t -0.2675854862285817\n",
      "17     \t [0.47849773 0.23130526]. \t  -0.272514609651971 \t -0.2675854862285817\n",
      "18     \t [0.46719122 0.23286761]. \t  -0.3052011401418572 \t -0.2675854862285817\n",
      "19     \t [0.47272465 0.23114238]. \t  -0.2839079918064935 \t -0.2675854862285817\n",
      "20     \t [0.48904403 0.22985763]. \t  -0.26973696972889005 \t -0.2675854862285817\n",
      "21     \t [0.49707834 0.22791808]. \t  -0.289674478878674 \t -0.2675854862285817\n",
      "22     \t [0.48667879 0.23044338]. \t  -0.26761114611022385 \t -0.2675854862285817\n",
      "23     \t [0.45982895 0.23155233]. \t  -0.33222466868806366 \t -0.2675854862285817\n",
      "24     \t [0.46528195 0.23166512]. \t  -0.30896002672137046 \t -0.2675854862285817\n",
      "25     \t [0.47820361 0.23207104]. \t  -0.273422279214877 \t -0.2675854862285817\n",
      "26     \t [0.4855141  0.23178194]. \t  \u001b[92m-0.2662496803652126\u001b[0m \t -0.2662496803652126\n",
      "27     \t [0.48238965 0.22321185]. \t  -0.2769225500138134 \t -0.2662496803652126\n",
      "28     \t [0.4852601  0.23309478]. \t  \u001b[92m-0.26552483273758526\u001b[0m \t -0.26552483273758526\n",
      "29     \t [0.47563927 0.23498884]. \t  -0.2826211639192385 \t -0.26552483273758526\n",
      "30     \t [0.44897648 0.24778862]. \t  -0.517151754485706 \t -0.26552483273758526\n",
      "31     \t [0.47407833 0.22656382]. \t  -0.27692250495015014 \t -0.26552483273758526\n",
      "32     \t [0.44766683 0.23367779]. \t  -0.41577581802983077 \t -0.26552483273758526\n",
      "33     \t [0.51826434 0.22882619]. \t  -0.39024830986685644 \t -0.26552483273758526\n",
      "34     \t [0.47874513 0.23447809]. \t  -0.2744957335695857 \t -0.26552483273758526\n",
      "35     \t [0.45379526 0.2490129 ]. \t  -0.4839520446537337 \t -0.26552483273758526\n",
      "36     \t [0.52032357 0.22267217]. \t  -0.4611085566619565 \t -0.26552483273758526\n",
      "37     \t [0.50767986 0.21561002]. \t  -0.41986283555043663 \t -0.26552483273758526\n",
      "38     \t [-2.048      -0.76383995]. \t  -2467.6094464509365 \t -0.26552483273758526\n",
      "39     \t [-0.47754258  2.01254572]. \t  -320.62673124905155 \t -0.26552483273758526\n",
      "40     \t [0.49223342 0.21996873]. \t  -0.3076675370087797 \t -0.26552483273758526\n",
      "41     \t [0.4818645  0.22219155]. \t  -0.27846809072321277 \t -0.26552483273758526\n",
      "42     \t [0.47765887 0.2257379 ]. \t  -0.27342594044037166 \t -0.26552483273758526\n",
      "43     \t [0.48554438 0.23194914]. \t  -0.2661117853111982 \t -0.26552483273758526\n",
      "44     \t [0.48024661 0.2219339 ]. \t  -0.27771764346226663 \t -0.26552483273758526\n",
      "45     \t [0.503644   0.21871321]. \t  -0.36847807488200063 \t -0.26552483273758526\n",
      "46     \t [0.47992935 0.22803308]. \t  -0.27100206743820326 \t -0.26552483273758526\n",
      "47     \t [0.50387461 0.23222873]. \t  -0.2930598294663575 \t -0.26552483273758526\n",
      "48     \t [0.49145288 0.22329801]. \t  -0.2918459031918512 \t -0.26552483273758526\n",
      "49     \t [0.48872748 0.22249655]. \t  -0.28815799401725156 \t -0.26552483273758526\n",
      "50     \t [0.47879409 0.22907851]. \t  -0.2716583331768836 \t -0.26552483273758526\n",
      "51     \t [0.48967667 0.22774822]. \t  -0.27491407331803314 \t -0.26552483273758526\n",
      "52     \t [0.47828241 0.23933025]. \t  -0.2833748240025476 \t -0.26552483273758526\n",
      "53     \t [0.48126867 0.23930168]. \t  -0.27498373865507636 \t -0.26552483273758526\n",
      "54     \t [0.46285658 0.24294828]. \t  -0.37096133829282135 \t -0.26552483273758526\n",
      "55     \t [0.49903034 0.23054811]. \t  -0.2851333569057605 \t -0.26552483273758526\n",
      "56     \t [0.49527861 0.22288778]. \t  -0.30497847763906716 \t -0.26552483273758526\n",
      "57     \t [0.49207885 0.22285509]. \t  -0.29518080680970626 \t -0.26552483273758526\n",
      "58     \t [0.48052022 0.23171388]. \t  -0.26992553830624405 \t -0.26552483273758526\n",
      "59     \t [0.48341402 0.23925943]. \t  -0.2699639097373035 \t -0.26552483273758526\n",
      "60     \t [0.4881173  0.23375669]. \t  \u001b[92m-0.26405053052010863\u001b[0m \t -0.26405053052010863\n",
      "61     \t [0.46923826 0.22816641]. \t  -0.28807904053615807 \t -0.26405053052010863\n",
      "62     \t [0.49368848 0.21701278]. \t  -0.3277233266116921 \t -0.26405053052010863\n",
      "63     \t [0.48403387 0.22857552]. \t  -0.26948519510757324 \t -0.26405053052010863\n",
      "64     \t [0.4768155  0.23525662]. \t  -0.2799687090604067 \t -0.26405053052010863\n",
      "65     \t [0.47661416 0.23568634]. \t  -0.28120079799401915 \t -0.26405053052010863\n",
      "66     \t [0.48360666 0.23822502]. \t  -0.26855399466414964 \t -0.26405053052010863\n",
      "67     \t [0.46329575 0.24064923]. \t  -0.35568413190744663 \t -0.26405053052010863\n",
      "68     \t [0.46645404 0.23330245]. \t  -0.3093928311049428 \t -0.26405053052010863\n",
      "69     \t [0.46753865 0.2557262 ]. \t  -0.42140703581203 \t -0.26405053052010863\n",
      "70     \t [0.50403657 0.20664109]. \t  -0.47076736383614204 \t -0.26405053052010863\n",
      "71     \t [0.47479323 0.24903835]. \t  -0.33158415879592457 \t -0.26405053052010863\n",
      "72     \t [0.48825266 0.23199811]. \t  -0.2659718130527752 \t -0.26405053052010863\n",
      "73     \t [0.469761   0.23317489]. \t  -0.2967771233244992 \t -0.26405053052010863\n",
      "74     \t [0.49447762 0.23483532]. \t  -0.2649091778244187 \t -0.26405053052010863\n",
      "75     \t [0.49401494 0.23573354]. \t  \u001b[92m-0.2629384952663224\u001b[0m \t -0.2629384952663224\n",
      "76     \t [0.48332846 0.23024674]. \t  -0.2680782132343016 \t -0.2629384952663224\n",
      "77     \t [0.48874551 0.24177772]. \t  \u001b[92m-0.2622253733869029\u001b[0m \t -0.2622253733869029\n",
      "78     \t [0.50219359 0.23121426]. \t  -0.29184465488024436 \t -0.2622253733869029\n",
      "79     \t [0.50513716 0.22704713]. \t  -0.32394252862361966 \t -0.2622253733869029\n",
      "80     \t [0.50358543 0.21219434]. \t  -0.4178561139619597 \t -0.2622253733869029\n",
      "81     \t [0.50538155 0.23977277]. \t  -0.2691013035923714 \t -0.2622253733869029\n",
      "82     \t [0.47951252 0.22369519]. \t  -0.2747973118817869 \t -0.2622253733869029\n",
      "83     \t [0.47692175 0.206403  ]. \t  -0.31792682338056283 \t -0.2622253733869029\n",
      "84     \t [0.47913199 0.21927898]. \t  -0.28188877546893726 \t -0.2622253733869029\n",
      "85     \t [0.48047286 0.23288117]. \t  -0.270319320202878 \t -0.2622253733869029\n",
      "86     \t [0.48337267 0.22452985]. \t  -0.27521994186733617 \t -0.2622253733869029\n",
      "87     \t [0.48463536 0.20114788]. \t  -0.3793285169031497 \t -0.2622253733869029\n",
      "88     \t [0.51345092 0.2220777 ]. \t  -0.40940471116125804 \t -0.2622253733869029\n",
      "89     \t [0.47502192 0.22370723]. \t  -0.27597779698718156 \t -0.2622253733869029\n",
      "90     \t [0.49271842 0.23245768]. \t  -0.267971973317232 \t -0.2622253733869029\n",
      "91     \t [0.46194058 0.23790151]. \t  -0.3495937733045691 \t -0.2622253733869029\n",
      "92     \t [0.45841358 0.24281713]. \t  -0.4000756929350473 \t -0.2622253733869029\n",
      "93     \t [0.48286814 0.22550056]. \t  -0.2732945783621351 \t -0.2622253733869029\n",
      "94     \t [0.50230705 0.22753057]. \t  -0.3091120489907853 \t -0.2622253733869029\n",
      "95     \t [0.46733421 0.24116977]. \t  -0.3355733288679032 \t -0.2622253733869029\n",
      "96     \t [0.49772904 0.19061114]. \t  -0.5785804539634696 \t -0.2622253733869029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.48997714 0.24123026]. \t  \u001b[92m-0.260256176609428\u001b[0m \t -0.260256176609428\n",
      "98     \t [0.48462728 0.25820682]. \t  -0.32009965289724623 \t -0.260256176609428\n",
      "99     \t [0.47017182 0.23477398]. \t  -0.2995210140021277 \t -0.260256176609428\n",
      "100    \t [0.49598124 0.22865593]. \t  -0.2841075334601308 \t -0.260256176609428\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.6609756  0.67072423]. \t  -436.4596189933219 \t -31.22188590191926\n",
      "2      \t [0.62182254 0.94414078]. \t  \u001b[92m-31.221136256320666\u001b[0m \t -31.221136256320666\n",
      "3      \t [0.6208361 0.9407511]. \t  \u001b[92m-30.9810884080407\u001b[0m \t -30.9810884080407\n",
      "4      \t [0.61471848 0.93041794]. \t  \u001b[92m-30.67839007268829\u001b[0m \t -30.67839007268829\n",
      "5      \t [0.62147962 0.94675102]. \t  -31.560883987434984 \t -30.67839007268829\n",
      "6      \t [0.61924054 0.9400827 ]. \t  -31.12798938185915 \t -30.67839007268829\n",
      "7      \t [0.62049502 0.94101553]. \t  -31.057785952764576 \t -30.67839007268829\n",
      "8      \t [0.636109   0.98601056]. \t  -33.93221067408746 \t -30.67839007268829\n",
      "9      \t [0.5888326  0.85838227]. \t  \u001b[92m-26.348495291636908\u001b[0m \t -26.348495291636908\n",
      "10     \t [0.61056466 0.91815478]. \t  -29.8940200186725 \t -26.348495291636908\n",
      "11     \t [0.57566892 0.82076138]. \t  \u001b[92m-24.128031533908562\u001b[0m \t -24.128031533908562\n",
      "12     \t [0.61864493 0.93906951]. \t  -31.097737187066375 \t -24.128031533908562\n",
      "13     \t [0.59986259 0.89078991]. \t  -28.35140798474753 \t -24.128031533908562\n",
      "14     \t [0.45866558 0.13145721]. \t  \u001b[92m-0.9158308023507857\u001b[0m \t -0.9158308023507857\n",
      "15     \t [0.45824888 0.13712013]. \t  \u001b[92m-0.8245257545317286\u001b[0m \t -0.8245257545317286\n",
      "16     \t [0.45826814 0.13481   ]. \t  -0.8589727235682372 \t -0.8245257545317286\n",
      "17     \t [0.45740713 0.12893844]. \t  -0.9389405048438393 \t -0.8245257545317286\n",
      "18     \t [0.45873903 0.14384932]. \t  \u001b[92m-0.7364153061570686\u001b[0m \t -0.7364153061570686\n",
      "19     \t [0.46051079 0.16842421]. \t  \u001b[92m-0.48154578113345503\u001b[0m \t -0.48154578113345503\n",
      "20     \t [0.46344131 0.21002801]. \t  \u001b[92m-0.29015132535797483\u001b[0m \t -0.29015132535797483\n",
      "21     \t [0.46473093 0.23545679]. \t  -0.32446762460592865 \t -0.29015132535797483\n",
      "22     \t [0.46474561 0.23177432]. \t  -0.3114165404536911 \t -0.29015132535797483\n",
      "23     \t [0.46388221 0.22156313]. \t  -0.29148817487881534 \t -0.29015132535797483\n",
      "24     \t [0.46263716 0.201786  ]. \t  -0.3037580678049711 \t -0.29015132535797483\n",
      "25     \t [0.46076607 0.18697148]. \t  -0.35495385384962413 \t -0.29015132535797483\n",
      "26     \t [0.46571428 0.24476875]. \t  -0.3631848662468111 \t -0.29015132535797483\n",
      "27     \t [0.46183299 0.20148988]. \t  -0.30354732018703046 \t -0.29015132535797483\n",
      "28     \t [0.46428641 0.22374023]. \t  -0.29367759863357695 \t -0.29015132535797483\n",
      "29     \t [0.461947   0.20863183]. \t  -0.2917698359193487 \t -0.29015132535797483\n",
      "30     \t [0.46271779 0.20210964]. \t  -0.3030676505213665 \t -0.29015132535797483\n",
      "31     \t [0.4651677  0.23682207]. \t  -0.3278293886503735 \t -0.29015132535797483\n",
      "32     \t [0.46165249 0.1936936 ]. \t  -0.327568292017248 \t -0.29015132535797483\n",
      "33     \t [0.46140112 0.18421646]. \t  -0.3723116295611659 \t -0.29015132535797483\n",
      "34     \t [0.45661886 0.15808366]. \t  -0.5494517300730657 \t -0.29015132535797483\n",
      "35     \t [0.46230486 0.20951677]. \t  -0.2908876401822668 \t -0.29015132535797483\n",
      "36     \t [0.46484702 0.23168017]. \t  -0.3107166419095466 \t -0.29015132535797483\n",
      "37     \t [0.45392013 0.17684972]. \t  -0.3834308263304359 \t -0.29015132535797483\n",
      "38     \t [0.45888325 0.20325005]. \t  -0.29817111593711004 \t -0.29015132535797483\n",
      "39     \t [0.4621107  0.19669095]. \t  -0.31773516090821796 \t -0.29015132535797483\n",
      "40     \t [0.47036373 0.21367471]. \t  \u001b[92m-0.28624102933017265\u001b[0m \t -0.28624102933017265\n",
      "41     \t [0.46710415 0.2411702 ]. \t  -0.33680402253024333 \t -0.28624102933017265\n",
      "42     \t [0.45606734 0.25690155]. \t  -0.5350241319500817 \t -0.28624102933017265\n",
      "43     \t [0.45783987 0.17150463]. \t  -0.4391955432120416 \t -0.28624102933017265\n",
      "44     \t [0.4659143  0.23078151]. \t  -0.30403126138101544 \t -0.28624102933017265\n",
      "45     \t [0.45081517 0.2018679 ]. \t  -0.30179068655966024 \t -0.28624102933017265\n",
      "46     \t [0.4556725 0.2001997]. \t  -0.3018244108420925 \t -0.28624102933017265\n",
      "47     \t [0.47211133 0.25242678]. \t  -0.36591385289048406 \t -0.28624102933017265\n",
      "48     \t [0.4584835  0.20341222]. \t  -0.2978571812859197 \t -0.28624102933017265\n",
      "49     \t [0.45854437 0.19815358]. \t  -0.3078378563213828 \t -0.28624102933017265\n",
      "50     \t [0.46178216 0.22214607]. \t  -0.2976053148282256 \t -0.28624102933017265\n",
      "51     \t [0.47155374 0.21740543]. \t  \u001b[92m-0.2817131257944093\u001b[0m \t -0.2817131257944093\n",
      "52     \t [0.41155758 0.21090364]. \t  -0.518688742430247 \t -0.2817131257944093\n",
      "53     \t [0.44759944 0.22082527]. \t  -0.3470894585670882 \t -0.2817131257944093\n",
      "54     \t [0.42512626 0.16789416]. \t  -0.3469617068356995 \t -0.2817131257944093\n",
      "55     \t [0.46773765 0.24531351]. \t  -0.35371385762684043 \t -0.2817131257944093\n",
      "56     \t [0.45680887 0.24947921]. \t  -0.46156033479170916 \t -0.2817131257944093\n",
      "57     \t [0.4324581  0.22984247]. \t  -0.5054801086122752 \t -0.2817131257944093\n",
      "58     \t [0.46365522 0.1834829 ]. \t  -0.38684826739565564 \t -0.2817131257944093\n",
      "59     \t [0.46357348 0.1679204 ]. \t  -0.5084651972161397 \t -0.2817131257944093\n",
      "60     \t [0.45448156 0.14520123]. \t  -0.6740003302264399 \t -0.2817131257944093\n",
      "61     \t [0.42514748 0.11603535]. \t  -0.7492589330653123 \t -0.2817131257944093\n",
      "62     \t [0.42295916 0.16877006]. \t  -0.3432264561111248 \t -0.2817131257944093\n",
      "63     \t [0.48570267 0.24849752]. \t  \u001b[92m-0.28035366906058135\u001b[0m \t -0.28035366906058135\n",
      "64     \t [0.43368339 0.20110785]. \t  -0.3376836544832734 \t -0.28035366906058135\n",
      "65     \t [0.45687412 0.21394063]. \t  -0.2976966533038536 \t -0.28035366906058135\n",
      "66     \t [0.45020225 0.19953267]. \t  -0.3032694374395307 \t -0.28035366906058135\n",
      "67     \t [0.47843868 0.25422621]. \t  -0.33614979055982397 \t -0.28035366906058135\n",
      "68     \t [0.47062508 0.24343581]. \t  -0.3284085853898281 \t -0.28035366906058135\n",
      "69     \t [0.47686938 0.29408738]. \t  -0.7183274834979787 \t -0.28035366906058135\n",
      "70     \t [0.47440063 0.27378361]. \t  -0.5136931444278179 \t -0.28035366906058135\n",
      "71     \t [0.49494448 0.31832174]. \t  -0.7931282319928675 \t -0.28035366906058135\n",
      "72     \t [0.46666013 0.20206864]. \t  -0.30910994511973344 \t -0.28035366906058135\n",
      "73     \t [0.41240999 0.20875165]. \t  -0.4947962095820395 \t -0.28035366906058135\n",
      "74     \t [0.45066124 0.18790523]. \t  -0.32484765551192823 \t -0.28035366906058135\n",
      "75     \t [0.4540207  0.22037887]. \t  -0.31838276392208686 \t -0.28035366906058135\n",
      "76     \t [0.44731915 0.22294906]. \t  -0.35768957621777686 \t -0.28035366906058135\n",
      "77     \t [0.45775719 0.17708628]. \t  -0.39936232515624015 \t -0.28035366906058135\n",
      "78     \t [0.46851972 0.20677327]. \t  -0.2986955810746423 \t -0.28035366906058135\n",
      "79     \t [0.42860624 0.15480422]. \t  -0.41000655172560474 \t -0.28035366906058135\n",
      "80     \t [0.43265459 0.17622592]. \t  -0.3339019098959873 \t -0.28035366906058135\n",
      "81     \t [0.46608042 0.21502368]. \t  -0.2855573271973277 \t -0.28035366906058135\n",
      "82     \t [0.48963424 0.23160155]. \t  \u001b[92m-0.2670993951581723\u001b[0m \t -0.2670993951581723\n",
      "83     \t [0.44755229 0.2232426 ]. \t  -0.3578207663338576 \t -0.2670993951581723\n",
      "84     \t [0.48539302 0.26446991]. \t  -0.34813064911226776 \t -0.2670993951581723\n",
      "85     \t [0.43094777 0.12418911]. \t  -0.7023760443737397 \t -0.2670993951581723\n",
      "86     \t [0.47165641 0.20762698]. \t  -0.30114810842283096 \t -0.2670993951581723\n",
      "87     \t [0.45475715 0.21897401]. \t  -0.3121005256904608 \t -0.2670993951581723\n",
      "88     \t [0.49346358 0.24407131]. \t  \u001b[92m-0.25661106984365384\u001b[0m \t -0.25661106984365384\n",
      "89     \t [0.44061633 0.12245082]. \t  -0.8268834478055591 \t -0.25661106984365384\n",
      "90     \t [0.44706172 0.21638883]. \t  -0.3330471609457337 \t -0.25661106984365384\n",
      "91     \t [0.443977  0.1929378]. \t  -0.31090695757108877 \t -0.25661106984365384\n",
      "92     \t [0.46998633 0.1746509 ]. \t  -0.4946936175080102 \t -0.25661106984365384\n",
      "93     \t [0.4665693  0.17256821]. \t  -0.4881180486343213 \t -0.25661106984365384\n",
      "94     \t [0.45188953 0.1983771 ]. \t  -0.3038205324270637 \t -0.25661106984365384\n",
      "95     \t [0.49922285 0.18075758]. \t  -0.7195352854378878 \t -0.25661106984365384\n",
      "96     \t [0.39060176 0.1385328 ]. \t  -0.3910697713163294 \t -0.25661106984365384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.53961703 0.30490314]. \t  \u001b[92m-0.23076699486248803\u001b[0m \t -0.23076699486248803\n",
      "98     \t [ 0.0660208  -0.07679973]. \t  -1.5309869397678626 \t -0.23076699486248803\n",
      "99     \t [0.4185444  0.18063336]. \t  -0.3410651732935411 \t -0.23076699486248803\n",
      "100    \t [0.49087624 0.24439128]. \t  -0.26038472987604894 \t -0.23076699486248803\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-0.68507009  0.53674844]. \t  -3.294106792674183 \t -1.7663579664225912\n",
      "2      \t [-0.32487     0.11334371]. \t  \u001b[92m-1.7613694926391932\u001b[0m \t -1.7613694926391932\n",
      "3      \t [-1.50422288  1.58707967]. \t  -51.91558639219928 \t -1.7613694926391932\n",
      "4      \t [0.52686806 2.048     ]. \t  -313.6590269660194 \t -1.7613694926391932\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7613694926391932\n",
      "6      \t [-1.76971986  0.65263262]. \t  -622.3521831460057 \t -1.7613694926391932\n",
      "7      \t [1.23025145 1.49138144]. \t  \u001b[92m-0.10202127869550333\u001b[0m \t -0.10202127869550333\n",
      "8      \t [ 0.4497484  -0.92777515]. \t  -128.00380043502153 \t -0.10202127869550333\n",
      "9      \t [-0.8486682   1.31623844]. \t  -38.93925982338481 \t -0.10202127869550333\n",
      "10     \t [-1.77336481  2.048     ]. \t  -127.99356320894606 \t -0.10202127869550333\n",
      "11     \t [1.23192925 2.048     ]. \t  -28.180936530272934 \t -0.10202127869550333\n",
      "12     \t [1.47542419 1.57553673]. \t  -36.38698289868191 \t -0.10202127869550333\n",
      "13     \t [1.24225655 1.75047787]. \t  -4.355044163300844 \t -0.10202127869550333\n",
      "14     \t [0.59669578 0.70496984]. \t  -12.337449075310628 \t -0.10202127869550333\n",
      "15     \t [-0.53862     0.59535828]. \t  -11.684911001860398 \t -0.10202127869550333\n",
      "16     \t [1.36552209 1.83651051]. \t  -0.21279277985414702 \t -0.10202127869550333\n",
      "17     \t [0.78644441 1.11113327]. \t  -24.314870315393417 \t -0.10202127869550333\n",
      "18     \t [0.37323248 0.18599304]. \t  -0.6108383193900649 \t -0.10202127869550333\n",
      "19     \t [1.44118942 2.048     ]. \t  -0.27890441247716136 \t -0.10202127869550333\n",
      "20     \t [-1.00193258  1.12720383]. \t  -5.528884529106879 \t -0.10202127869550333\n",
      "21     \t [1.30962853 1.70843473]. \t  \u001b[92m-0.10034832551345023\u001b[0m \t -0.10034832551345023\n",
      "22     \t [2.048 2.048]. \t  -461.76039081963273 \t -0.10034832551345023\n",
      "23     \t [0.07096503 1.88057777]. \t  -352.6287875598659 \t -0.10034832551345023\n",
      "24     \t [0.03814268 2.048     ]. \t  -419.7598689066029 \t -0.10034832551345023\n",
      "25     \t [1.58898668 1.23518348]. \t  -166.6782747254324 \t -0.10034832551345023\n",
      "26     \t [1.73046367 0.76664241]. \t  -496.8705260474975 \t -0.10034832551345023\n",
      "27     \t [1.25007914 1.99328278]. \t  -18.60287771607783 \t -0.10034832551345023\n",
      "28     \t [0.28344574 0.70966049]. \t  -40.11769091013365 \t -0.10034832551345023\n",
      "29     \t [-0.88654931  0.51114353]. \t  -11.11200978146468 \t -0.10034832551345023\n",
      "30     \t [0.8748146  0.80973906]. \t  -0.21314923786996376 \t -0.10034832551345023\n",
      "31     \t [-0.28819793  0.75287424]. \t  -46.52482765172438 \t -0.10034832551345023\n",
      "32     \t [-0.62512404  0.87233731]. \t  -25.830765884930774 \t -0.10034832551345023\n",
      "33     \t [1.67341067 1.49030849]. \t  -172.06211788383254 \t -0.10034832551345023\n",
      "34     \t [-0.58912281  0.81024973]. \t  -23.97925692695195 \t -0.10034832551345023\n",
      "35     \t [0.85599637 0.12994185]. \t  -36.35606647247851 \t -0.10034832551345023\n",
      "36     \t [1.55514311 1.75363742]. \t  -44.50843206551259 \t -0.10034832551345023\n",
      "37     \t [0.72451391 1.63299532]. \t  -122.85889167840585 \t -0.10034832551345023\n",
      "38     \t [1.85504876 0.74694451]. \t  -726.6355496890563 \t -0.10034832551345023\n",
      "39     \t [0.94989893 1.37967904]. \t  -22.790823357393055 \t -0.10034832551345023\n",
      "40     \t [0.30296968 0.41658109]. \t  -11.0347355776942 \t -0.10034832551345023\n",
      "41     \t [1.24087071 0.58475771]. \t  -91.2609761846341 \t -0.10034832551345023\n",
      "42     \t [-0.29874947  0.69036876]. \t  -37.820977350770235 \t -0.10034832551345023\n",
      "43     \t [0.48267924 0.79209229]. \t  -31.52836030638179 \t -0.10034832551345023\n",
      "44     \t [-0.12643139  1.80870462]. \t  -322.65324881507644 \t -0.10034832551345023\n",
      "45     \t [-0.55257216  1.73394148]. \t  -206.50184425897686 \t -0.10034832551345023\n",
      "46     \t [1.27800313 1.54580841]. \t  -0.8426235873130634 \t -0.10034832551345023\n",
      "47     \t [1.7438028  1.09851564]. \t  -377.8188197843441 \t -0.10034832551345023\n",
      "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.10034832551345023\n",
      "49     \t [1.10720033 0.46092785]. \t  -58.52859414381701 \t -0.10034832551345023\n",
      "50     \t [-1.76050261  0.63574113]. \t  -614.566821615485 \t -0.10034832551345023\n",
      "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.10034832551345023\n",
      "52     \t [ 1.00983537 -0.1149782 ]. \t  -128.76487064300255 \t -0.10034832551345023\n",
      "53     \t [1.80291454 0.4301237 ]. \t  -796.09739433069 \t -0.10034832551345023\n",
      "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.10034832551345023\n",
      "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.10034832551345023\n",
      "56     \t [1.59519964 1.44374607]. \t  -121.55582937519486 \t -0.10034832551345023\n",
      "57     \t [ 0.57800761 -0.36243324]. \t  -48.69292971116773 \t -0.10034832551345023\n",
      "58     \t [2.03687249 1.09083436]. \t  -936.2207917902165 \t -0.10034832551345023\n",
      "59     \t [-1.2275476   0.94476691]. \t  -36.5583072059832 \t -0.10034832551345023\n",
      "60     \t [1.5119274  1.17699972]. \t  -123.23348088474717 \t -0.10034832551345023\n",
      "61     \t [1.47412293 0.57606711]. \t  -255.256525756207 \t -0.10034832551345023\n",
      "62     \t [0.49069423 0.50307469]. \t  -7.139199772584298 \t -0.10034832551345023\n",
      "63     \t [-0.62637896  1.86472917]. \t  -219.43497216942504 \t -0.10034832551345023\n",
      "64     \t [1.08652979 1.2204029 ]. \t  -0.16633681853579804 \t -0.10034832551345023\n",
      "65     \t [0.3998233  0.67235516]. \t  -26.62547677227166 \t -0.10034832551345023\n",
      "66     \t [-1.49493119  0.75287912]. \t  -225.83933912161478 \t -0.10034832551345023\n",
      "67     \t [-1.15605018  0.84617082]. \t  -28.6861166624198 \t -0.10034832551345023\n",
      "68     \t [1.13363521 1.36889008]. \t  -0.7194538736182172 \t -0.10034832551345023\n",
      "69     \t [-1.5544416   1.56570807]. \t  -78.87391191224928 \t -0.10034832551345023\n",
      "70     \t [ 0.3423718  -0.20981811]. \t  -11.127766104455452 \t -0.10034832551345023\n",
      "71     \t [0.7364389 1.9217544]. \t  -190.34725102672067 \t -0.10034832551345023\n",
      "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.10034832551345023\n",
      "73     \t [ 0.4637287  -0.56851341]. \t  -61.68385668874213 \t -0.10034832551345023\n",
      "74     \t [0.52238958 1.91569957]. \t  -270.1101548982881 \t -0.10034832551345023\n",
      "75     \t [1.50455508 0.62819022]. \t  -267.73921131784306 \t -0.10034832551345023\n",
      "76     \t [1.69261704 0.68893208]. \t  -473.9861771843764 \t -0.10034832551345023\n",
      "77     \t [-0.58102871  0.86870013]. \t  -30.706985985311956 \t -0.10034832551345023\n",
      "78     \t [0.38790795 1.12193343]. \t  -94.74827524395822 \t -0.10034832551345023\n",
      "79     \t [0.29648751 1.01582881]. \t  -86.59921943023568 \t -0.10034832551345023\n",
      "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.10034832551345023\n",
      "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.10034832551345023\n",
      "82     \t [1.72298561 0.8527494 ]. \t  -448.2386917070054 \t -0.10034832551345023\n",
      "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.10034832551345023\n",
      "84     \t [-0.07505235 -0.19377049]. \t  -5.1319069544771185 \t -0.10034832551345023\n",
      "85     \t [1.12129575 1.19077846]. \t  -0.45727944325963854 \t -0.10034832551345023\n",
      "86     \t [ 0.61125294 -0.527113  ]. \t  -81.28494797694184 \t -0.10034832551345023\n",
      "87     \t [0.32763512 0.5533097 ]. \t  -20.340546278129345 \t -0.10034832551345023\n",
      "88     \t [0.22546891 2.02322488]. \t  -389.6315961774345 \t -0.10034832551345023\n",
      "89     \t [1.77666376 1.92780995]. \t  -151.57951465253925 \t -0.10034832551345023\n",
      "90     \t [-0.33067542  1.40742126]. \t  -170.2705748820005 \t -0.10034832551345023\n",
      "91     \t [ 1.89558425 -0.20182531]. \t  -1441.0538640585346 \t -0.10034832551345023\n",
      "92     \t [-1.28097262  0.59107644]. \t  -115.41386774971387 \t -0.10034832551345023\n",
      "93     \t [-0.07741153 -0.39982591]. \t  -17.62967727127099 \t -0.10034832551345023\n",
      "94     \t [-0.1525521  -0.03062486]. \t  -1.6188650158620734 \t -0.10034832551345023\n",
      "95     \t [-0.8550498   1.94987953]. \t  -151.98108682867976 \t -0.10034832551345023\n",
      "96     \t [0.46364211 0.64067337]. \t  -18.41052529003493 \t -0.10034832551345023\n",
      "97     \t [1.26220639 1.04148267]. \t  -30.50408912804051 \t -0.10034832551345023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.10034832551345023\n",
      "99     \t [1.85965894 0.19823019]. \t  -1063.5649873906711 \t -0.10034832551345023\n",
      "100    \t [ 1.12788173 -0.4790417 ]. \t  -306.6721046079674 \t -0.10034832551345023\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.32923463 1.9850312 ]. \t  -4.868057505654614 \t -4.868057505654614\n",
      "init   \t [ 1.61031994 -0.73872624]. \t  -1110.4992966214563 \t -4.868057505654614\n",
      "init   \t [ 0.38197946 -1.24726347]. \t  -194.47470912529033 \t -4.868057505654614\n",
      "init   \t [1.47544785 1.49217491]. \t  -47.11724466380912 \t -4.868057505654614\n",
      "init   \t [-1.7259813   0.88847186]. \t  -444.4665457038575 \t -4.868057505654614\n",
      "1      \t [-0.53443808 -1.83227427]. \t  -450.9038339855385 \t -4.868057505654614\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -4.868057505654614\n",
      "3      \t [0.43565402 1.71132895]. \t  -231.8252169001694 \t -4.868057505654614\n",
      "4      \t [1.04458315 2.048     ]. \t  -91.55742450663668 \t -4.868057505654614\n",
      "5      \t [1.15025429 1.56952558]. \t  -6.0958754750420665 \t -4.868057505654614\n",
      "6      \t [1.30944431 1.75444491]. \t  \u001b[92m-0.2541638330711082\u001b[0m \t -0.2541638330711082\n",
      "7      \t [-0.0503826   0.56419216]. \t  -32.648797446270294 \t -0.2541638330711082\n",
      "8      \t [1.2736836  1.65277423]. \t  \u001b[92m-0.16795396783882366\u001b[0m \t -0.16795396783882366\n",
      "9      \t [1.28400035 1.69401051]. \t  -0.28635133007889874 \t -0.16795396783882366\n",
      "10     \t [1.25436196 1.56839818]. \t  \u001b[92m-0.06722583211564255\u001b[0m \t -0.06722583211564255\n",
      "11     \t [0.39560109 0.7909307 ]. \t  -40.61550106992308 \t -0.06722583211564255\n",
      "12     \t [1.30239819 1.70716914]. \t  -0.10338697935921039 \t -0.06722583211564255\n",
      "13     \t [1.27231899 1.65072753]. \t  -0.1761223772498946 \t -0.06722583211564255\n",
      "14     \t [1.27570372 1.65295172]. \t  -0.1411995243399476 \t -0.06722583211564255\n",
      "15     \t [1.28190385 1.67983771]. \t  -0.21313478874118763 \t -0.06722583211564255\n",
      "16     \t [1.25958077 1.63536746]. \t  -0.3057579611181295 \t -0.06722583211564255\n",
      "17     \t [1.26018135 1.76655154]. \t  -3.25372329750565 \t -0.06722583211564255\n",
      "18     \t [1.23448813 1.51922933]. \t  \u001b[92m-0.057223506872296215\u001b[0m \t -0.057223506872296215\n",
      "19     \t [1.30511401 1.48972158]. \t  -4.655633071437271 \t -0.057223506872296215\n",
      "20     \t [1.23158555 1.52260929]. \t  \u001b[92m-0.057003202485430046\u001b[0m \t -0.057003202485430046\n",
      "21     \t [1.23895843 1.52799346]. \t  -0.06203554895736395 \t -0.057003202485430046\n",
      "22     \t [1.12596777 1.40238635]. \t  -1.827124652903406 \t -0.057003202485430046\n",
      "23     \t [1.24801498 1.56559791]. \t  -0.06800219778219743 \t -0.057003202485430046\n",
      "24     \t [1.33429859 1.86768044]. \t  -0.8743682304932551 \t -0.057003202485430046\n",
      "25     \t [1.24557459 1.54415731]. \t  -0.065634068290518 \t -0.057003202485430046\n",
      "26     \t [1.28992169 1.67755653]. \t  -0.10271021925415112 \t -0.057003202485430046\n",
      "27     \t [1.29730467 1.73739355]. \t  -0.38426232359007717 \t -0.057003202485430046\n",
      "28     \t [1.24864539 1.60361144]. \t  -0.2598150024063092 \t -0.057003202485430046\n",
      "29     \t [1.22616333 1.50957362]. \t  \u001b[92m-0.0548673111966347\u001b[0m \t -0.0548673111966347\n",
      "30     \t [1.005883   1.19054631]. \t  -3.1950371940041222 \t -0.0548673111966347\n",
      "31     \t [2.04799998 2.048     ]. \t  -461.7603629674715 \t -0.0548673111966347\n",
      "32     \t [0.53122994 1.58658125]. \t  -170.35941962819885 \t -0.0548673111966347\n",
      "33     \t [1.42812072 0.25798982]. \t  -317.57139355070905 \t -0.0548673111966347\n",
      "34     \t [0.45170495 1.15707616]. \t  -91.12892354527776 \t -0.0548673111966347\n",
      "35     \t [0.78370757 0.76507867]. \t  -2.3232931757658126 \t -0.0548673111966347\n",
      "36     \t [0.30237987 1.47946494]. \t  -193.1497771713646 \t -0.0548673111966347\n",
      "37     \t [-0.06001577  1.20143747]. \t  -144.6046409142586 \t -0.0548673111966347\n",
      "38     \t [1.24940008 1.41321054]. \t  -2.246389385208256 \t -0.0548673111966347\n",
      "39     \t [ 1.59915978 -0.74396064]. \t  -1090.1991033005265 \t -0.0548673111966347\n",
      "40     \t [0.01871844 0.46722463]. \t  -22.760070255000347 \t -0.0548673111966347\n",
      "41     \t [0.08507716 1.64568961]. \t  -269.2894109963981 \t -0.0548673111966347\n",
      "42     \t [1.8696076  0.40727646]. \t  -954.4270475785644 \t -0.0548673111966347\n",
      "43     \t [1.3398442  0.23094435]. \t  -244.79958576080475 \t -0.0548673111966347\n",
      "44     \t [0.4864249 1.5088892]. \t  -162.1334023003635 \t -0.0548673111966347\n",
      "45     \t [1.09848229 1.08017825]. \t  -1.6095462449417448 \t -0.0548673111966347\n",
      "46     \t [1.14428931 1.75305326]. \t  -19.703815277403727 \t -0.0548673111966347\n",
      "47     \t [1.86675092 1.17833976]. \t  -532.7082306774877 \t -0.0548673111966347\n",
      "48     \t [0.61899573 0.3014779 ]. \t  -0.8122907512089677 \t -0.0548673111966347\n",
      "49     \t [0.32614122 0.35409698]. \t  -6.59104589708126 \t -0.0548673111966347\n",
      "50     \t [1.15936433 1.08171592]. \t  -6.911284353091073 \t -0.0548673111966347\n",
      "51     \t [0.67320861 0.00853758]. \t  -19.88013347251789 \t -0.0548673111966347\n",
      "52     \t [1.72710301 0.92062509]. \t  -425.8201870499101 \t -0.0548673111966347\n",
      "53     \t [1.33966942 1.68613905]. \t  -1.2942309374171341 \t -0.0548673111966347\n",
      "54     \t [0.55419272 1.44719906]. \t  -130.17458831786033 \t -0.0548673111966347\n",
      "55     \t [0.71428754 0.35716389]. \t  -2.423841626833947 \t -0.0548673111966347\n",
      "56     \t [-0.29220319  0.8367686 ]. \t  -58.12786555327704 \t -0.0548673111966347\n",
      "57     \t [0.42159315 0.60091775]. \t  -18.242428523561376 \t -0.0548673111966347\n",
      "58     \t [ 0.57920252 -0.42213703]. \t  -57.57475450969327 \t -0.0548673111966347\n",
      "59     \t [1.12453236 1.91860053]. \t  -42.790706073729474 \t -0.0548673111966347\n",
      "60     \t [0.40792322 1.45919249]. \t  -167.48144704227366 \t -0.0548673111966347\n",
      "61     \t [ 1.14273613 -0.08051936]. \t  -192.22122779342888 \t -0.0548673111966347\n",
      "62     \t [1.63490988 0.59942487]. \t  -430.3455989980764 \t -0.0548673111966347\n",
      "63     \t [0.76941414 1.49349123]. \t  -81.32215467639988 \t -0.0548673111966347\n",
      "64     \t [1.62462243 0.67856554]. \t  -384.87656493450123 \t -0.0548673111966347\n",
      "65     \t [ 1.44812319 -0.42584777]. \t  -636.7075642408931 \t -0.0548673111966347\n",
      "66     \t [0.97759761 1.09783196]. \t  -2.0207342726328945 \t -0.0548673111966347\n",
      "67     \t [0.62414979 1.20631751]. \t  -66.85006201233524 \t -0.0548673111966347\n",
      "68     \t [1.97161585 0.01905404]. \t  -1497.2527754453217 \t -0.0548673111966347\n",
      "69     \t [0.9883756  0.33907829]. \t  -40.68004288264089 \t -0.0548673111966347\n",
      "70     \t [-0.38285158  0.82565883]. \t  -48.027718808438074 \t -0.0548673111966347\n",
      "71     \t [ 1.74610647 -0.65934242]. \t  -1375.6538028086516 \t -0.0548673111966347\n",
      "72     \t [-0.4548899   0.40944105]. \t  -6.217987031708978 \t -0.0548673111966347\n",
      "73     \t [0.06617496 0.88734982]. \t  -78.8357535955873 \t -0.0548673111966347\n",
      "74     \t [0.26259509 1.48845303]. \t  -202.04089770475292 \t -0.0548673111966347\n",
      "75     \t [1.08179054 0.83719209]. \t  -11.100830076702366 \t -0.0548673111966347\n",
      "76     \t [0.97585864 1.1267272 ]. \t  -3.043064676883468 \t -0.0548673111966347\n",
      "77     \t [-0.08781068  0.58801714]. \t  -34.85888624547329 \t -0.0548673111966347\n",
      "78     \t [1.93400063 0.06121481]. \t  -1354.4821503920234 \t -0.0548673111966347\n",
      "79     \t [1.27559926 1.16962711]. \t  -21.008991899108807 \t -0.0548673111966347\n",
      "80     \t [1.47610801 1.92590384]. \t  -6.6271238052696395 \t -0.0548673111966347\n",
      "81     \t [-0.06608506  0.80914616]. \t  -65.90344905197144 \t -0.0548673111966347\n",
      "82     \t [-0.58936881  1.56002078]. \t  -149.58177851457168 \t -0.0548673111966347\n",
      "83     \t [1.18925804 0.42132062]. \t  -98.64351452511646 \t -0.0548673111966347\n",
      "84     \t [0.63554189 0.73031865]. \t  -10.786862586103801 \t -0.0548673111966347\n",
      "85     \t [0.00228158 1.5898143 ]. \t  -253.74473838629163 \t -0.0548673111966347\n",
      "86     \t [1.51835932 1.13934622]. \t  -136.24033904722302 \t -0.0548673111966347\n",
      "87     \t [1.56538101 1.32422323]. \t  -127.15105424064248 \t -0.0548673111966347\n",
      "88     \t [-0.17009038  1.37045824]. \t  -181.33871666301542 \t -0.0548673111966347\n",
      "89     \t [-0.69919572  1.87985453]. \t  -196.3697685312902 \t -0.0548673111966347\n",
      "90     \t [1.32505852 0.14556577]. \t  -259.3846755527956 \t -0.0548673111966347\n",
      "91     \t [1.24422745 0.42949902]. \t  -125.1868968060451 \t -0.0548673111966347\n",
      "92     \t [0.4066115  0.27604639]. \t  -1.577857253045722 \t -0.0548673111966347\n",
      "93     \t [ 1.37897653 -0.76578697]. \t  -711.6262862569836 \t -0.0548673111966347\n",
      "94     \t [ 1.31212478 -0.56061688]. \t  -520.9814222872618 \t -0.0548673111966347\n",
      "95     \t [1.4453656  1.38114854]. \t  -50.31528722352641 \t -0.0548673111966347\n",
      "96     \t [1.54080265 0.33354737]. \t  -416.66687400525046 \t -0.0548673111966347\n",
      "97     \t [0.48700967 0.6075774 ]. \t  -13.982699066887886 \t -0.0548673111966347\n",
      "98     \t [0.94626634 1.94421001]. \t  -109.99893651529146 \t -0.0548673111966347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 1.35734514 -0.67896209]. \t  -635.8472293359067 \t -0.0548673111966347\n",
      "100    \t [0.39741165 1.650069  ]. \t  -223.0091967958785 \t -0.0548673111966347\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [-0.18505974  0.44301828]. \t  -18.113753752570958 \t -3.777577453542735\n",
      "2      \t [-0.04752846 -0.09999051]. \t  \u001b[92m-2.142811230820951\u001b[0m \t -2.142811230820951\n",
      "3      \t [1.6648603  0.62011749]. \t  -463.39850949963716 \t -2.142811230820951\n",
      "4      \t [ 1.01835357 -1.62506838]. \t  -708.684559888092 \t -2.142811230820951\n",
      "5      \t [0.49056322 0.91818769]. \t  -46.16495032288053 \t -2.142811230820951\n",
      "6      \t [-0.36621238 -0.14417168]. \t  -9.610689047898282 \t -2.142811230820951\n",
      "7      \t [-0.32914214  0.10556755]. \t  \u001b[92m-1.7673844668530303\u001b[0m \t -1.7673844668530303\n",
      "8      \t [ 0.48532343 -0.04913854]. \t  -8.369012466507339 \t -1.7673844668530303\n",
      "9      \t [0.29144517 0.03050104]. \t  \u001b[92m-0.7984130393440437\u001b[0m \t -0.7984130393440437\n",
      "10     \t [0.29898991 0.03568581]. \t  \u001b[92m-0.7798824719868717\u001b[0m \t -0.7798824719868717\n",
      "11     \t [0.29964822 0.03768756]. \t  \u001b[92m-0.7619492012178057\u001b[0m \t -0.7619492012178057\n",
      "12     \t [0.30190197 0.03801064]. \t  -0.7696647856323986 \t -0.7619492012178057\n",
      "13     \t [0.29548354 0.03609757]. \t  \u001b[92m-0.7586200742490192\u001b[0m \t -0.7586200742490192\n",
      "14     \t [0.29788677 0.04046217]. \t  \u001b[92m-0.7260042993581475\u001b[0m \t -0.7260042993581475\n",
      "15     \t [0.29462986 0.04041288]. \t  \u001b[92m-0.7127861969494367\u001b[0m \t -0.7127861969494367\n",
      "16     \t [0.30040769 0.03668209]. \t  -0.7763255709218433 \t -0.7127861969494367\n",
      "17     \t [0.29507661 0.04006537]. \t  -0.7178624981921361 \t -0.7127861969494367\n",
      "18     \t [0.29951823 0.04074065]. \t  -0.7304858371088944 \t -0.7127861969494367\n",
      "19     \t [0.30176881 0.04568682]. \t  \u001b[92m-0.6934394394656995\u001b[0m \t -0.6934394394656995\n",
      "20     \t [0.30397857 0.04462158]. \t  -0.7127519476128256 \t -0.6934394394656995\n",
      "21     \t [0.30121922 0.04168108]. \t  -0.7289038664492374 \t -0.6934394394656995\n",
      "22     \t [0.30270152 0.04381161]. \t  -0.7148679147853458 \t -0.6934394394656995\n",
      "23     \t [0.30554879 0.04523842]. \t  -0.7138317106720689 \t -0.6934394394656995\n",
      "24     \t [0.3135033 0.0528142]. \t  \u001b[92m-0.6780309095813353\u001b[0m \t -0.6780309095813353\n",
      "25     \t [0.30409352 0.04400953]. \t  -0.7191553379416601 \t -0.6780309095813353\n",
      "26     \t [0.31917747 0.05588157]. \t  \u001b[92m-0.6750520058771899\u001b[0m \t -0.6750520058771899\n",
      "27     \t [0.32666428 0.06981748]. \t  \u001b[92m-0.589483453697003\u001b[0m \t -0.589483453697003\n",
      "28     \t [0.32508036 0.06281401]. \t  -0.639242169027282 \t -0.589483453697003\n",
      "29     \t [0.33539435 0.09052704]. \t  \u001b[92m-0.4899350765530395\u001b[0m \t -0.4899350765530395\n",
      "30     \t [0.3545076  0.11691583]. \t  \u001b[92m-0.424333864402557\u001b[0m \t -0.424333864402557\n",
      "31     \t [0.36525145 0.12107602]. \t  \u001b[92m-0.41811503741462536\u001b[0m \t -0.41811503741462536\n",
      "32     \t [0.37546336 0.12252252]. \t  -0.4240870624267168 \t -0.41811503741462536\n",
      "33     \t [0.37238603 0.12736392]. \t  \u001b[92m-0.40668511677230024\u001b[0m \t -0.40668511677230024\n",
      "34     \t [0.38449339 0.11276138]. \t  -0.5018654276947416 \t -0.40668511677230024\n",
      "35     \t [0.36479307 0.12434832]. \t  -0.4111015721841772 \t -0.40668511677230024\n",
      "36     \t [0.3741998  0.14810012]. \t  \u001b[92m-0.3981458520892103\u001b[0m \t -0.3981458520892103\n",
      "37     \t [0.36257961 0.11608451]. \t  -0.42995753756225297 \t -0.3981458520892103\n",
      "38     \t [0.38876939 0.16671185]. \t  \u001b[92m-0.3978460083428073\u001b[0m \t -0.3978460083428073\n",
      "39     \t [0.33721984 0.10370507]. \t  -0.449301854099434 \t -0.3978460083428073\n",
      "40     \t [0.3646835  0.10262284]. \t  -0.4958681515768151 \t -0.3978460083428073\n",
      "41     \t [0.38530151 0.15447589]. \t  \u001b[92m-0.38147663016622674\u001b[0m \t -0.38147663016622674\n",
      "42     \t [0.38333529 0.14602656]. \t  \u001b[92m-0.3803598860333887\u001b[0m \t -0.3803598860333887\n",
      "43     \t [0.38341842 0.16825563]. \t  -0.42531186476105926 \t -0.3803598860333887\n",
      "44     \t [0.40146462 0.15626531]. \t  \u001b[92m-0.36065397259436655\u001b[0m \t -0.36065397259436655\n",
      "45     \t [0.38902967 0.13668231]. \t  -0.39478150941859985 \t -0.36065397259436655\n",
      "46     \t [0.3722061  0.11926607]. \t  -0.431263528612176 \t -0.36065397259436655\n",
      "47     \t [0.38956332 0.12522298]. \t  -0.4430520511314235 \t -0.36065397259436655\n",
      "48     \t [0.38644001 0.13973216]. \t  -0.38567901122323467 \t -0.36065397259436655\n",
      "49     \t [0.40008726 0.19709578]. \t  -0.49698751549575876 \t -0.36065397259436655\n",
      "50     \t [0.40791488 0.1810178 ]. \t  -0.37194873058204125 \t -0.36065397259436655\n",
      "51     \t [0.41009675 0.16006704]. \t  \u001b[92m-0.3545667870919942\u001b[0m \t -0.3545667870919942\n",
      "52     \t [0.40910341 0.19663785]. \t  -0.43484522940712056 \t -0.3545667870919942\n",
      "53     \t [0.40465943 0.17775246]. \t  -0.37403935020467094 \t -0.3545667870919942\n",
      "54     \t [0.3883541  0.12207409]. \t  -0.45673717137116154 \t -0.3545667870919942\n",
      "55     \t [0.40366058 0.17066649]. \t  -0.36158768205270614 \t -0.3545667870919942\n",
      "56     \t [0.39204015 0.13179946]. \t  -0.4175587673151435 \t -0.3545667870919942\n",
      "57     \t [0.39248175 0.14747123]. \t  -0.37339582851536113 \t -0.3545667870919942\n",
      "58     \t [0.39550663 0.17338099]. \t  -0.3941611026110936 \t -0.3545667870919942\n",
      "59     \t [0.39196553 0.13267781]. \t  -0.4136345688957558 \t -0.3545667870919942\n",
      "60     \t [0.40584022 0.17396537]. \t  -0.36159890955009766 \t -0.3545667870919942\n",
      "61     \t [0.32386829 0.06313469]. \t  -0.6315102837178508 \t -0.3545667870919942\n",
      "62     \t [0.3723329  0.13352707]. \t  -0.3965718004927514 \t -0.3545667870919942\n",
      "63     \t [0.3774935  0.13415406]. \t  -0.39448205391895846 \t -0.3545667870919942\n",
      "64     \t [0.39943325 0.17012914]. \t  -0.3718787604775948 \t -0.3545667870919942\n",
      "65     \t [0.40920133 0.17715896]. \t  -0.3584777468454415 \t -0.3545667870919942\n",
      "66     \t [0.38498543 0.17771742]. \t  -0.46528937915824925 \t -0.3545667870919942\n",
      "67     \t [0.36980209 0.14043629]. \t  -0.3985056443064948 \t -0.3545667870919942\n",
      "68     \t [0.39582063 0.1531452 ]. \t  -0.36627793814326004 \t -0.3545667870919942\n",
      "69     \t [0.40457264 0.16885602]. \t  -0.3572138693495767 \t -0.3545667870919942\n",
      "70     \t [0.38940823 0.15179651]. \t  -0.3728248031801333 \t -0.3545667870919942\n",
      "71     \t [0.39785436 0.13850186]. \t  -0.40172889166103376 \t -0.3545667870919942\n",
      "72     \t [0.403601   0.18136333]. \t  -0.3898042310975919 \t -0.3545667870919942\n",
      "73     \t [0.3794449  0.11043808]. \t  -0.4975841578998626 \t -0.3545667870919942\n",
      "74     \t [0.41167857 0.21360397]. \t  -0.5408212740674048 \t -0.3545667870919942\n",
      "75     \t [0.37550243 0.15039248]. \t  -0.3988151910414962 \t -0.3545667870919942\n",
      "76     \t [0.39817882 0.16709061]. \t  -0.3694891276301503 \t -0.3545667870919942\n",
      "77     \t [0.39149296 0.13437778]. \t  -0.4059600951016373 \t -0.3545667870919942\n",
      "78     \t [0.36099301 0.08463102]. \t  -0.6170412465983078 \t -0.3545667870919942\n",
      "79     \t [0.37264256 0.12500366]. \t  -0.4127840319894843 \t -0.3545667870919942\n",
      "80     \t [0.42846281 0.19005631]. \t  \u001b[92m-0.3308485365872879\u001b[0m \t -0.3308485365872879\n",
      "81     \t [0.3842864  0.16014125]. \t  -0.39464139727342307 \t -0.3308485365872879\n",
      "82     \t [0.37679058 0.16284865]. \t  -0.4319770142562716 \t -0.3308485365872879\n",
      "83     \t [0.37264932 0.11661226]. \t  -0.4430985099774053 \t -0.3308485365872879\n",
      "84     \t [0.37002343 0.13172302]. \t  -0.3995685748043778 \t -0.3308485365872879\n",
      "85     \t [0.40209021 0.16997864]. \t  -0.36438861245426335 \t -0.3308485365872879\n",
      "86     \t [0.3858581  0.14440278]. \t  -0.3791806189427447 \t -0.3308485365872879\n",
      "87     \t [0.41560777 0.1571191 ]. \t  -0.365883740464403 \t -0.3308485365872879\n",
      "88     \t [0.40469497 0.20335153]. \t  -0.5109943557704655 \t -0.3308485365872879\n",
      "89     \t [0.33405172 0.12140382]. \t  -0.4531171460940413 \t -0.3308485365872879\n",
      "90     \t [0.37898209 0.15310685]. \t  -0.3946492029856444 \t -0.3308485365872879\n",
      "91     \t [0.38188998 0.19794978]. \t  -0.6536034392551897 \t -0.3308485365872879\n",
      "92     \t [0.39577719 0.12924644]. \t  -0.4401236226036704 \t -0.3308485365872879\n",
      "93     \t [0.40819981 0.17442546]. \t  -0.35630893056236435 \t -0.3308485365872879\n",
      "94     \t [0.38524654 0.14894898]. \t  -0.37795034456017235 \t -0.3308485365872879\n",
      "95     \t [0.39951467 0.14500121]. \t  -0.3819300626434397 \t -0.3308485365872879\n",
      "96     \t [0.41552946 0.19776614]. \t  -0.40461388473312687 \t -0.3308485365872879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.38609679 0.15481518]. \t  -0.3801770175665723 \t -0.3308485365872879\n",
      "98     \t [0.42413366 0.21100162]. \t  -0.4284192671246131 \t -0.3308485365872879\n",
      "99     \t [0.4016354  0.14161807]. \t  -0.3968213129409544 \t -0.3308485365872879\n",
      "100    \t [0.39355048 0.15179568]. \t  -0.3687335462534513 \t -0.3308485365872879\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1407835185949384, 0.05817333792061004)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.8576891666921584, -5.489707448582341)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1818430666042277, -3.434349944101301)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.41628102359093866, -0.5225329745527926)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7679450152949523, -1.0289860307662604)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.774045731197184, -10.404697678302423)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0418117846610935, -3.0798595629993297)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7904121473059839, -3.223370584788274)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.399286253484284, -1.0386648515562051)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3790251444945975, -0.4950103531935925)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16555419266970178, -1.0493964370617408)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4563835868119455, 0.9214629382583164)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.7920593137215763, -4.553160564273888)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.604666802079234, -1.4807352243602347)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.374360679716665, -3.9642302010155785)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7338710816157707, -1.3460888383991307)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.4405469321055913, -1.4663467577695837)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.2825871302528498, -2.299107890341888)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7716876058448898, -2.9028375322715116)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.963770557402326, -1.1060946017247595)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1407835185949384, -0.97870984460124)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Yellow')\n",
    "plt.plot(median_winner, color = 'Green')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Yellow', alpha=0.4, label='GP CBM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Green', alpha=0.4, label='STP CBM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
