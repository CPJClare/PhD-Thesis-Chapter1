{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Classification - 'real-world' example: UCI Skin Segmentation dataset\n",
    "\n",
    "GP ERM versus STP nu = 3 ERM (winner)\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some default Python modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential, matern32, matern52\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    #Data in format [B G R Label] from\n",
    "    data = np.genfromtxt('/home/ulsterconorc/Downloads/Skin_NonSkin.txt', dtype=np.int32)\n",
    "\n",
    "    labels = data[:,3]\n",
    "    data = data[:,0:3]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = ReadData()\n",
    "\n",
    "X = data\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayesian Optimization - inputs:\n",
    "\n",
    "obj_func = 'XGBoost'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "test_perc = 0.15\n",
    "train_perc = 1 - test_perc\n",
    "n_est = 2\n",
    "\n",
    "obj_classifier = 'binary:logistic'\n",
    "cov_func = squaredExponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'XGBoost':\n",
    "    \n",
    "    # Constraints:\n",
    "    param_lb_alpha = 0\n",
    "    param_ub_alpha = 10\n",
    "    \n",
    "    param_lb_gamma = 0\n",
    "    param_ub_gamma = 10\n",
    "    \n",
    "    param_lb_max_depth = 5\n",
    "    param_ub_max_depth = 15\n",
    "    \n",
    "    param_lb_min_child_weight = 1\n",
    "    param_ub_min_child_weight = 20\n",
    "    \n",
    "    param_lb_subsample = .5\n",
    "    param_ub_subsample = 1\n",
    "    \n",
    "    param_lb_colsample = .1\n",
    "    param_ub_colsample = 1\n",
    "    \n",
    "    # 6-D inputs' parameter bounds:\n",
    "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
    "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
    "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
    "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
    "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
    "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
    "        }\n",
    "       \n",
    "    # True y bounds:\n",
    "    y_global_orig = 1\n",
    "    dim = 6\n",
    "    \n",
    "    max_iter = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 44444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 7777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1557\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a. Add new acquisition functions: add CBM & ERM (Nyugen and Osborne, 2019) method .\n",
    "\n",
    "### Inherits from class Acquisition()\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "            \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.26128727 7.00737964 6.53282638 0.87482164 5.4671733  0.74932303]. \t  0.9779738706123698 \t 0.9846806025668978\n",
      "2      \t [3.78128046 7.08001843 7.28349467 0.81606759 4.93438906 0.7105907 ]. \t  0.9823714094297742 \t 0.9846806025668978\n",
      "3      \t [4.43495488 6.67503667 7.45921469 0.91537911 5.27821457 0.94593868]. \t  0.9822225858715052 \t 0.9846806025668978\n",
      "4      \t [3.68016409 6.77604232 7.27169507 1.         5.69453543 1.        ]. \t  \u001b[92m0.9929283980981055\u001b[0m \t 0.9929283980981055\n",
      "5      \t [3.94377561 6.50688609 7.22839034 0.59466167 5.45018393 0.36502336]. \t  0.8875505296477145 \t 0.9929283980981055\n",
      "6      \t [4.41877541 7.76880846 7.06693971 0.82186923 5.8623705  0.62412751]. \t  0.8873728978160903 \t 0.9929283980981055\n",
      "7      \t [3.99951539 6.87954847 7.09827907 0.5        5.33772404 1.        ]. \t  0.9916033654914598 \t 0.9929283980981055\n",
      "8      \t [ 4.64029006  2.26793414 11.00000003  0.8568146  14.3343997   0.75219898]. \t  0.9844789695451731 \t 0.9929283980981055\n",
      "9      \t [ 4.77044407  2.79327663 10.29424364  0.86393244 14.21150674  0.77815176]. \t  0.9844933720569718 \t 0.9929283980981055\n",
      "10     \t [ 4.8460223   2.30338293 10.62960286  0.86832639 13.54957104  0.7557789 ]. \t  0.9845701846465542 \t 0.9929283980981055\n",
      "11     \t [ 5.34333438  2.1512391  10.48286015  0.87866378 14.24960578  0.66335567]. \t  0.8890099911832218 \t 0.9929283980981055\n",
      "12     \t [ 4.80527328  2.6015079  10.73877464  0.63627975 14.00199581  0.10030124]. \t  0.8886691301009156 \t 0.9929283980981055\n",
      "13     \t [ 4.88210432  2.50797511 10.70850992  0.5        14.03199066  1.        ]. \t  \u001b[92m0.9935525052371701\u001b[0m \t 0.9935525052371701\n",
      "14     \t [4.84119943 7.20068917 6.56654891 1.         6.48662855 0.7880537 ]. \t  0.9833267743402093 \t 0.9935525052371701\n",
      "15     \t [4.09272251 7.48345997 6.338479   1.         6.518583   0.41514376]. \t  0.8978770869776707 \t 0.9935525052371701\n",
      "16     \t [4.82211471 7.27326904 6.60600987 1.         6.11870979 0.1       ]. \t  0.8978434814394779 \t 0.9935525052371701\n",
      "17     \t [3.56631901 6.34991001 7.94205345 0.82996947 4.91444527 1.        ]. \t  0.9930196122314735 \t 0.9935525052371701\n",
      "18     \t [4.54225837 7.28613986 6.467682   0.5        6.25421075 0.61954201]. \t  0.88573100441837 \t 0.9935525052371701\n",
      "19     \t [ 4.37068563  3.01221855 11.03573364  1.         13.77771211  0.83735485]. \t  0.9904175735295638 \t 0.9935525052371701\n",
      "20     \t [ 4.80463126  2.63381063 10.79472848  1.         13.98774045  0.76314075]. \t  0.9900287090299001 \t 0.9935525052371701\n",
      "21     \t [ 4.80156716  3.43523697 10.54005175  0.52231805 13.27074067  0.72643107]. \t  0.9830147174517755 \t 0.9935525052371701\n",
      "22     \t [ 3.90493279  2.89823188 10.34960286  0.5171559  13.47710925  0.72281431]. \t  0.9838020526660776 \t 0.9935525052371701\n",
      "23     \t [ 4.3424758   2.81983964 11.19441984  0.5        13.01505795  0.66798185]. \t  0.9831347395127801 \t 0.9935525052371701\n",
      "24     \t [ 3.73078354  2.23902392 11.1937452   0.5        13.79003576  0.69222816]. \t  0.9835044045123827 \t 0.9935525052371701\n",
      "25     \t [ 4.31075141  2.92465313 10.93945422  0.5        13.76172864  0.66474749]. \t  0.8879057941406884 \t 0.9935525052371701\n",
      "26     \t [ 3.74102736  2.50928284 10.8796482   1.         13.0622096   1.        ]. \t  \u001b[92m0.994963945585367\u001b[0m \t 0.994963945585367\n",
      "27     \t [ 4.37639463  3.19336415 10.50511824  1.         12.85007647  1.        ]. \t  0.9948871325117782 \t 0.994963945585367\n",
      "28     \t [ 3.88510483  1.80872888 10.31609602  0.66360591 14.08921059  0.70681909]. \t  0.984320542606825 \t 0.994963945585367\n",
      "29     \t [ 3.88608939  1.95735921 10.64170936  0.59846577 13.15789777  0.27749738]. \t  0.8883138664376672 \t 0.994963945585367\n",
      "30     \t [ 4.22160398  1.38186915 11.17102336  0.5        13.82928026  0.88245964]. \t  0.9832019501051588 \t 0.994963945585367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05920696212231716"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, \n",
    "                       min_child_weight=min_child_weight,colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_1 = loser_1.getResult()[0]\n",
    "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
    "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
    "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
    "\n",
    "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
    "rmse_loser_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 3.43112436  8.55676877  6.00000044  0.80756459 11.59622208  0.77577814]. \t  0.9901679379187686 \t 0.9907392220424587\n",
      "2      \t [ 3.40451653  8.03172613  5.80879517  0.89095486 12.28462328  0.78873698]. \t  0.9864761193439872 \t 0.9907392220424587\n",
      "3      \t [ 3.29659784  8.42987735  6.5650638   0.99791771 12.20245508  0.61092166]. \t  0.8960528104491554 \t 0.9907392220424587\n",
      "4      \t [ 3.19144269  8.86220105  5.80241021  0.70717263 12.34562072  0.92554256]. \t  0.9849062529560961 \t 0.9907392220424587\n",
      "5      \t [ 3.20001864  8.47957282  5.83505107  0.50100561 12.10197879  0.1909809 ]. \t  0.894156569914102 \t 0.9907392220424587\n",
      "6      \t [ 3.10519607  8.27835422  6.15316376  0.5        12.03755606  1.        ]. \t  0.9885452595782814 \t 0.9907392220424587\n",
      "7      \t [ 6.37234395  8.83416908  9.99940826  0.72238621 15.03227565  0.39525125]. \t  0.8906902862621925 \t 0.9907392220424587\n",
      "8      \t [ 6.34996857  9.22675414  9.27947116  0.73091815 15.36173232  0.8799284 ]. \t  0.9904799827763435 \t 0.9907392220424587\n",
      "9      \t [ 6.16362128  9.25503281  9.41040873  0.7681615  14.50884419  0.79250868]. \t  \u001b[92m0.9913009246352145\u001b[0m \t 0.9913009246352145\n",
      "10     \t [ 5.64661118  9.31919634  9.74695769  0.74355482 15.13639524  0.79042692]. \t  0.9904847839132289 \t 0.9913009246352145\n",
      "11     \t [ 6.15322937  9.61813634  9.46431181  0.64460486 15.04693252  0.21439809]. \t  0.8955007346067171 \t 0.9913009246352145\n",
      "12     \t [ 6.16899876  9.1985769   9.6787108   0.5        14.97542339  0.91605708]. \t  0.9896158388443537 \t 0.9913009246352145\n",
      "13     \t [ 6.17940943  9.29440451  9.64346854  1.         15.02880934  0.73905619]. \t  \u001b[92m0.9916561880218918\u001b[0m \t 0.9916561880218918\n",
      "14     \t [ 5.84154668 10.          9.07170249  0.66457176 14.95440203  1.        ]. \t  \u001b[92m0.9937541371517818\u001b[0m \t 0.9937541371517818\n",
      "15     \t [ 5.48521651  9.19731339  8.59042285  0.65605412 14.99812391  0.81049509]. \t  0.9903503609307167 \t 0.9937541371517818\n",
      "16     \t [ 6.33751778  9.49416558  8.2904607   0.60962678 14.75951561  0.97770477]. \t  0.9902063356744341 \t 0.9937541371517818\n",
      "17     \t [ 5.96614507  9.63029545  8.32474808  0.5        15.64719442  1.        ]. \t  0.9917378175820747 \t 0.9937541371517818\n",
      "18     \t [ 5.9784213   9.52016013  8.52325086  1.         15.20663864  1.        ]. \t  \u001b[92m0.9937829410690965\u001b[0m \t 0.9937829410690965\n",
      "19     \t [ 6.23987814  9.00663027  8.27134693  0.5        15.32626137  0.40173873]. \t  0.8926345909911889 \t 0.9937829410690965\n",
      "20     \t [ 6.01382303  9.46946829  8.76523903  0.5        15.10761358  0.83023603]. \t  0.9894862129884453 \t 0.9937829410690965\n",
      "21     \t [ 5.79522831  9.13572389  7.4557735   0.59844503 15.22274543  1.        ]. \t  0.9915793752248642 \t 0.9937829410690965\n",
      "22     \t [ 6.12325574  8.48861278  8.07355009  0.78629008 14.90145107  1.        ]. \t  0.9937205323741528 \t 0.9937829410690965\n",
      "23     \t [ 5.74636038  8.6689599   8.11046968  0.67983995 15.8475481   1.        ]. \t  0.9936821249384971 \t 0.9937829410690965\n",
      "24     \t [ 6.61301332  8.98290891  7.789585    0.71329324 15.61669756  1.        ]. \t  0.991420954163632 \t 0.9937829410690965\n",
      "25     \t [ 6.04444154  9.00822173  8.05973497  0.62667491 15.32452165  1.        ]. \t  0.9925539396355078 \t 0.9937829410690965\n",
      "26     \t [ 5.89889938  9.31124754  7.50851675  0.93259734 16.24955018  0.6772544 ]. \t  0.9898174677866988 \t 0.9937829410690965\n",
      "27     \t [ 5.99477608  8.38950858  7.37165493  1.         15.55415939  0.52870289]. \t  0.8870752445301892 \t 0.9937829410690965\n",
      "28     \t [ 5.92339137  8.9890329   7.92528886  1.         14.0826683   0.62452383]. \t  0.8870752445301892 \t 0.9937829410690965\n",
      "29     \t [ 5.24680495  9.44534511  8.67673196  0.84503287 16.20581659  0.64752832]. \t  0.8967009239641422 \t 0.9937829410690965\n",
      "30     \t [ 6.78758004  8.94846481  7.39855582  1.         14.6337042   0.79700005]. \t  0.9910416808056753 \t 0.9937829410690965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.064326562111266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_2 = loser_2.getResult()[0]\n",
    "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
    "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
    "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
    "\n",
    "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
    "rmse_loser_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [6.32090591 3.59919158 9.99999852 0.65981308 9.00000002 0.90744883]. \t  0.9845317837795499 \t 0.9850406749037081\n",
      "2      \t [6.69946221 4.09064519 9.42869281 0.7203414  8.71440443 0.80642204]. \t  0.9849254499001097 \t 0.9850406749037081\n",
      "3      \t [6.00755335 3.9957583  9.33568101 0.63043484 9.24569543 0.71071081]. \t  \u001b[92m0.9853287261076954\u001b[0m \t 0.9853287261076954\n",
      "4      \t [5.90920883 3.9844455  9.67028866 0.64609421 8.42900603 0.71312467]. \t  \u001b[92m0.985712790092764\u001b[0m \t 0.985712790092764\n",
      "5      \t [6.29727657 3.94982515 9.7813652  0.99999567 8.91610843 0.20566511]. \t  0.8861102862119461 \t 0.985712790092764\n",
      "6      \t [6.12898119 4.06686336 9.64303954 1.         8.88378491 1.        ]. \t  \u001b[92m0.9944886641480286\u001b[0m \t 0.9944886641480286\n",
      "7      \t [6.26497574 4.03331836 9.69716516 0.5        8.87562682 0.74685704]. \t  0.9846662080757781 \t 0.9944886641480286\n",
      "8      \t [6.25708839 3.08455862 9.06503165 0.890501   8.64101107 0.73604824]. \t  0.9856983884106908 \t 0.9944886641480286\n",
      "9      \t [6.64997505 3.24487842 9.76585727 1.         8.00282815 0.85661569]. \t  0.9656645765723401 \t 0.9944886641480286\n",
      "10     \t [5.74458699 2.86420751 9.89739534 0.96326888 8.37340525 0.74250783]. \t  0.9867497593261164 \t 0.9944886641480286\n",
      "11     \t [6.59423018 2.55896282 9.87892524 1.         8.78657673 0.87938698]. \t  0.9655445548570545 \t 0.9944886641480286\n",
      "12     \t [6.09590285 5.130412   9.19441776 0.82303052 8.9728823  0.48943618]. \t  0.884996493671725 \t 0.9944886641480286\n",
      "13     \t [6.66848778 4.8466534  9.49953518 0.90204648 9.79101967 0.72115811]. \t  0.9861688678814144 \t 0.9944886641480286\n",
      "14     \t [6.25401901 2.84684488 9.68630863 0.5        8.35069701 1.        ]. \t  0.9939941835563482 \t 0.9944886641480286\n",
      "15     \t [5.75125002 5.00782021 9.76103823 0.82787459 9.87762162 0.56768941]. \t  0.8849388835553863 \t 0.9944886641480286\n",
      "16     \t [6.27450549 3.19638114 9.69521254 0.91732425 8.56783968 0.78058685]. \t  0.98578960219834 \t 0.9944886641480286\n",
      "17     \t [6.25248412 2.16516116 9.2703076  1.         7.91689879 0.77319777]. \t  0.9657077836928737 \t 0.9944886641480286\n",
      "18     \t [5.77980831 3.1931188  9.07344269 0.86229048 7.58023316 0.82222929]. \t  0.9861016492683566 \t 0.9944886641480286\n",
      "19     \t [ 6.45471355  4.01190929 10.07996367  0.96161134 10.04081604  0.80375967]. \t  0.9862072698547192 \t 0.9944886641480286\n",
      "20     \t [6.28752167 3.99876768 8.66557372 0.98567459 7.97217243 0.50651456]. \t  0.8881506295811694 \t 0.9944886641480286\n",
      "21     \t [6.82139663 3.00777291 8.75728343 0.77016589 7.78750933 1.        ]. \t  0.9939413736088859 \t 0.9944886641480286\n",
      "22     \t [6.41670335 2.96405501 9.12819197 0.57476899 7.74218505 0.26545868]. \t  0.8849724752648224 \t 0.9944886641480286\n",
      "23     \t [6.52851414 3.88461239 9.27382889 0.54416096 7.38464055 1.        ]. \t  0.9939461746766313 \t 0.9944886641480286\n",
      "24     \t [5.73474869 2.05232422 9.55370788 0.85731857 9.03499025 1.        ]. \t  0.9944166523496166 \t 0.9944886641480286\n",
      "25     \t [ 5.82140843  2.74646025 10.26292808  0.78426435  9.680394    0.91545036]. \t  0.9859432210854204 \t 0.9944886641480286\n",
      "26     \t [ 5.94808489  2.16313354 10.0438057   0.54918429  9.02262557  0.35387222]. \t  0.8868592070070624 \t 0.9944886641480286\n",
      "27     \t [6.45816288 2.69395853 9.57406871 0.56279938 9.82626134 0.96045008]. \t  0.9834659981830197 \t 0.9944886641480286\n",
      "28     \t [6.57284517 2.95858122 9.37594437 1.         7.06813173 1.        ]. \t  \u001b[92m0.9946662939053393\u001b[0m \t 0.9946662939053393\n",
      "29     \t [6.41897089 3.34477457 9.19004037 0.94403194 7.66697527 0.96873718]. \t  0.986663347781657 \t 0.9946662939053393\n",
      "30     \t [7.37434914 2.94649168 9.55895514 0.5        7.35975894 1.        ]. \t  0.9941094015072807 \t 0.9946662939053393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0625316655752366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_3 = loser_3.getResult()[0]\n",
    "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
    "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
    "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
    "\n",
    "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
    "rmse_loser_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [ 2.54475597  3.9179364  10.87889719  0.88293341  1.          1.        ]. \t  \u001b[92m0.9959049123393965\u001b[0m \t 0.9959049123393965\n",
      "2      \t [ 2.38764344  4.57153636 10.32997261  0.88759426  1.00000007  0.82653912]. \t  0.9680217732607336 \t 0.9959049123393965\n",
      "3      \t [ 3.11678541  4.07118715 10.23789093  1.          1.          0.99680721]. \t  0.9679497663023008 \t 0.9959049123393965\n",
      "4      \t [ 2.7243506   3.97471786 10.42554934  0.5         1.00000001  0.38145872]. \t  0.7980297915340859 \t 0.9959049123393965\n",
      "5      \t [ 2.54828654  4.06014801 10.3798909   1.          1.69527379  1.        ]. \t  \u001b[92m0.9963897885831384\u001b[0m \t 0.9963897885831384\n",
      "6      \t [2.2896253  8.31159312 6.00014595 0.77113171 7.25251108 0.85623655]. \t  0.954829144913834 \t 0.9963897885831384\n",
      "7      \t [1.71581168 8.6695876  6.63134258 0.84496474 7.34222281 0.92415518]. \t  0.9563030102748539 \t 0.9963897885831384\n",
      "8      \t [1.89640048 7.84711509 6.53516331 0.8193611  7.70416467 0.81750725]. \t  0.9553380261505744 \t 0.9963897885831384\n",
      "9      \t [2.43239639 8.55425694 6.56807198 0.99719106 7.8893294  0.87528374]. \t  0.9555972720543973 \t 0.9963897885831384\n",
      "10     \t [2.07795261 8.4647232  6.45986151 0.52917411 7.61172063 0.2335379 ]. \t  0.7938723238036528 \t 0.9963897885831384\n",
      "11     \t [2.1049327  8.3846611  6.42658633 0.5        7.70242068 1.        ]. \t  0.9888189017019103 \t 0.9963897885831384\n",
      "12     \t [ 2.61393181  4.16894457 10.35111685  0.5         1.24903199  1.        ]. \t  0.9955736569880006 \t 0.9963897885831384\n",
      "13     \t [ 2.53429176  4.05784093 10.37681759  1.          1.13179058  1.        ]. \t  \u001b[92m0.9964281950507949\u001b[0m \t 0.9964281950507949\n",
      "14     \t [ 3.10004123  4.66473661 11.15626413  0.92955833  1.51312758  0.89792695]. \t  0.9687851130929691 \t 0.9964281950507949\n",
      "15     \t [ 3.3316612   3.64215351 11.08684505  0.85698841  1.72819368  1.        ]. \t  \u001b[92m0.9965386145614495\u001b[0m \t 0.9965386145614495\n",
      "16     \t [ 2.52387884  3.99941097 11.48596989  0.70837346  1.9592864   0.95423578]. \t  0.9691931770928255 \t 0.9965386145614495\n",
      "17     \t [ 2.48599807  3.02192324 10.71441911  0.60366588  1.73303028  1.        ]. \t  0.9960681338460861 \t 0.9965386145614495\n",
      "18     \t [3.09735065 3.02305155 9.93829377 0.76328827 1.44331708 1.        ]. \t  0.9951559792366709 \t 0.9965386145614495\n",
      "19     \t [ 3.01016872  3.47665542 10.42022085  0.5         2.32180944  1.        ]. \t  0.9954248307331182 \t 0.9965386145614495\n",
      "20     \t [ 2.8817079   3.57929123 10.60465224  0.68129168  1.67534409  1.        ]. \t  0.9960441285765161 \t 0.9965386145614495\n",
      "21     \t [2.35175129 2.93397324 9.67747801 0.64977223 2.10837159 0.9685506 ]. \t  0.9668359723791223 \t 0.9965386145614495\n",
      "22     \t [ 2.92984692  2.71955528 10.43968433  1.          2.42908263  0.95924703]. \t  0.9679497651960163 \t 0.9965386145614495\n",
      "23     \t [2.90451863 3.85585304 9.17653358 0.75039543 1.6306428  0.8863832 ]. \t  0.9665911332048095 \t 0.9965386145614495\n",
      "24     \t [3.10430227 3.31218927 9.73126678 1.         2.21052269 0.44765078]. \t  0.7933633230881755 \t 0.9965386145614495\n",
      "25     \t [ 3.05314432  3.9510391  11.97288149  1.          1.18821912  0.98881127]. \t  0.9693612078952493 \t 0.9965386145614495\n",
      "26     \t [ 2.86099981  3.00825258 11.73847965  1.          2.09860872  1.        ]. \t  \u001b[92m0.9966058250847066\u001b[0m \t 0.9966058250847066\n",
      "27     \t [ 2.34871659  3.24314765 10.98067354  0.92404792  2.73547279  1.        ]. \t  0.9962745698715962 \t 0.9966058250847066\n",
      "28     \t [ 2.91772302  3.41623438 11.30634467  1.          2.34465029  0.40782753]. \t  0.7930560590395063 \t 0.9966058250847066\n",
      "29     \t [ 2.82875695  2.77008088 11.23161855  0.5         2.63151189  1.        ]. \t  0.9960057237682864 \t 0.9966058250847066\n",
      "30     \t [2.68614284 3.05251769 9.12414145 0.5        1.07229479 0.70276104]. \t  0.9657990045283867 \t 0.9966058250847066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05498246135836864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_4 = loser_4.getResult()[0]\n",
    "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
    "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
    "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
    "\n",
    "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
    "rmse_loser_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 8.04875096  7.60250855 11.99995594  0.72320561 16.47912318  0.65900801]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "2      \t [ 8.28903908  7.4554031  11.46697547  0.7748613  17.2901625   0.64952432]. \t  0.8875025212520042 \t 0.9875226863588296\n",
      "3      \t [ 7.85136279  8.22426624 11.89960844  0.6635413  17.22136924  0.82816725]. \t  \u001b[92m0.9878395388526501\u001b[0m \t 0.9878395388526501\n",
      "4      \t [ 8.09536546  7.93327327 12.10255576  0.93982568 17.20358738  0.1       ]. \t  0.8875793339107304 \t 0.9878395388526501\n",
      "5      \t [ 8.1626096   7.57956794 12.3981503   0.55933326 17.34721432  0.88939182]. \t  0.9874458736309594 \t 0.9878395388526501\n",
      "6      \t [ 8.10070413  7.77681836 12.07091328  1.         17.13911401  1.        ]. \t  \u001b[92m0.9947959174795408\u001b[0m \t 0.9947959174795408\n",
      "7      \t [5.61712585 9.72734704 5.69140993 0.86093722 6.34570498 0.58828895]. \t  0.8875793339107304 \t 0.9947959174795408\n",
      "8      \t [5.17644416 9.73318178 5.         0.84883466 6.90840278 0.77729859]. \t  0.9790444501548358 \t 0.9947959174795408\n",
      "9      \t [ 4.67560522 10.          5.53455279  0.89449216  6.46508203  0.68748566]. \t  0.9790444501548358 \t 0.9947959174795408\n",
      "10     \t [4.89137041 9.14960679 5.36315533 0.93811449 6.41483077 0.74222106]. \t  0.9790492500471372 \t 0.9947959174795408\n",
      "11     \t [5.05493202 9.64602872 5.41596927 0.5        6.46009971 1.        ]. \t  0.9839844857037349 \t 0.9947959174795408\n",
      "12     \t [4.98737472 9.6268541  5.16752044 0.52075037 6.43480779 0.14944945]. \t  0.8874977192853896 \t 0.9947959174795408\n",
      "13     \t [ 8.20598801  7.84286812 11.97756688  0.5        17.10947064  0.67225417]. \t  0.9864041013938357 \t 0.9947959174795408\n",
      "14     \t [ 8.41221233  8.18596459 12.11118655  0.87063441 18.08961539  0.79993122]. \t  0.9878683429082346 \t 0.9947959174795408\n",
      "15     \t [ 8.23830617  8.53574013 12.8013474   0.81788945 17.50497668  0.83512324]. \t  0.9876139011836353 \t 0.9947959174795408\n",
      "16     \t [ 7.54649533  8.17208754 12.61263269  0.78342934 18.06355613  0.7765744 ]. \t  0.9877627253641985 \t 0.9947959174795408\n",
      "17     \t [ 8.3141782   7.80160135 12.99471925  1.         18.04885069  0.58944789]. \t  0.8945836932758181 \t 0.9947959174795408\n",
      "18     \t [ 8.08986078  8.09584604 12.39204231  0.8568551  17.64201577  0.72651336]. \t  0.9878539403964358 \t 0.9947959174795408\n",
      "19     \t [ 8.17415111  8.47032834 12.97863292  0.5        18.5147513   1.        ]. \t  0.9929860126396463 \t 0.9947959174795408\n",
      "20     \t [ 7.64330281  8.14321227 13.56221548  0.5        17.8280882   1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "21     \t [ 7.59311188  8.86711085 13.37231385  1.         18.21350073  1.        ]. \t  0.9947575111501776 \t 0.9947959174795408\n",
      "22     \t [ 7.35492066  8.94489978 12.93613165  0.5        17.7571301   1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "23     \t [ 7.90529317  9.19102763 12.18816419  0.83799048 18.30767667  1.        ]. \t  0.9944838660534657 \t 0.9947959174795408\n",
      "24     \t [ 7.78060497  8.86001329 13.08082579  0.56844864 18.22499937  0.29569584]. \t  0.8874977192853896 \t 0.9947959174795408\n",
      "25     \t [ 8.64365621  8.95260877 11.41777471  0.83481013 17.66664022  1.        ]. \t  0.9939845796922625 \t 0.9947959174795408\n",
      "26     \t [ 8.04298868  8.64782922 11.27040074  0.5        18.25729163  1.        ]. \t  0.9930052158043279 \t 0.9947959174795408\n",
      "27     \t [ 8.27816643  8.83138665 11.97293579  0.5        17.91272037  1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "28     \t [ 8.18671409  8.41278712 11.1035492   1.         17.61834781  1.        ]. \t  \u001b[92m0.9948103205444897\u001b[0m \t 0.9948103205444897\n",
      "29     \t [ 8.24735508  8.86561885 11.3002479   0.92872161 18.0206077   0.24051501]. \t  0.8874977192853896 \t 0.9948103205444897\n",
      "30     \t [ 8.67274964  8.83399663 11.39391238  1.         18.69526342  1.        ]. \t  0.9943926457663008 \t 0.9948103205444897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06481725208023831"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_5 = loser_5.getResult()[0]\n",
    "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
    "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
    "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
    "\n",
    "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
    "rmse_loser_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.37727055  6.24418657 13.99999715  0.78872166 12.90500021  0.66278343]. \t  0.8568442162918638 \t 0.989121369524546\n",
      "2      \t [ 5.10602925  5.87911652 14.00000117  0.71578879 11.93143582  0.71227668]. \t  \u001b[92m0.9892269854782749\u001b[0m \t 0.9892269854782749\n",
      "3      \t [ 5.19776217  6.47317782 14.62821187  0.72116838 12.09222515  0.61050561]. \t  0.8615009913329534 \t 0.9892269854782749\n",
      "4      \t [ 5.8241017   5.69739345 14.40262522  0.69528095 12.21497675  0.69720949]. \t  \u001b[92m0.9892845963551952\u001b[0m \t 0.9892845963551952\n",
      "5      \t [ 5.62944616  6.06332352 14.02046351  0.99610243 12.07909324  0.1       ]. \t  0.8634213022696047 \t 0.9892845963551952\n",
      "6      \t [ 5.50521301  6.02021277 14.20263036  1.         12.17436658  1.        ]. \t  \u001b[92m0.9950743657874556\u001b[0m \t 0.9950743657874556\n",
      "7      \t [ 5.80357559  5.68199107 13.48719101  0.5        12.28008587  0.82225347]. \t  0.9884972596197299 \t 0.9950743657874556\n",
      "8      \t [ 5.54593366  6.0110179  14.02594079  0.5        12.23389356  0.72071637]. \t  0.9885452675314338 \t 0.9950743657874556\n",
      "9      \t [ 5.97508583  5.56207131 13.77619675  0.81681984 11.4111564   0.84032612]. \t  0.9893950145521141 \t 0.9950743657874556\n",
      "10     \t [ 6.60945944  5.75863514 13.70152317  1.         12.14594896  0.84374082]. \t  0.9911665015847775 \t 0.9950743657874556\n",
      "11     \t [ 5.98796585  5.00959098 13.71802995  1.         12.13771837  0.69315203]. \t  0.9912625167167474 \t 0.9950743657874556\n",
      "12     \t [ 5.89006459  5.72009371 13.69733226  1.         12.03367355  0.75993223]. \t  0.9912625167167474 \t 0.9950743657874556\n",
      "13     \t [ 6.57499771  5.09185858 13.85089965  0.5        11.96695526  1.        ]. \t  0.9934420905665841 \t 0.9950743657874556\n",
      "14     \t [ 6.53623801  5.27718011 13.59175375  0.5        11.93098895  0.21647234]. \t  0.8564985544875313 \t 0.9950743657874556\n",
      "15     \t [ 6.52244665  5.19438362 13.705371    0.65827657 13.04275181  0.78968468]. \t  0.9892173869382606 \t 0.9950743657874556\n",
      "16     \t [ 5.13291994  5.29038509 13.84240154  0.89587679 14.00641698  0.48919595]. \t  0.8636757437867724 \t 0.9950743657874556\n",
      "17     \t [ 5.42980256  6.23273396 14.48358106  0.8506491  11.04489029  0.83458604]. \t  0.9892077809998615 \t 0.9950743657874556\n",
      "18     \t [ 6.64880242  4.99206016 12.97125185  0.71330361 12.44800051  1.        ]. \t  0.9942006173000996 \t 0.9950743657874556\n",
      "19     \t [ 5.8346774   4.65880798 13.3477482   0.5        12.92551897  1.        ]. \t  0.9935429055217115 \t 0.9950743657874556\n",
      "20     \t [ 5.90403339  5.30557441 13.07596497  1.         13.30994781  1.        ]. \t  0.9949783494800414 \t 0.9950743657874556\n",
      "21     \t [ 5.95101308  5.03348371 13.08548371  0.70407175 13.07564093  0.28668367]. \t  0.8570026503520216 \t 0.9950743657874556\n",
      "22     \t [ 6.19731628  5.16229163 13.4670111   0.75119943 12.66198402  1.        ]. \t  0.9945366717140135 \t 0.9950743657874556\n",
      "23     \t [ 5.66630413  5.66567539 13.20952606  0.5        14.03233755  0.94808028]. \t  0.9885788727239077 \t 0.9950743657874556\n",
      "24     \t [ 4.9511722   5.38086547 13.0401164   0.5        13.4197869   1.        ]. \t  0.993466092655554 \t 0.9950743657874556\n",
      "25     \t [ 4.79182342  6.01141869 13.26507956  1.         14.20834328  0.82773999]. \t  0.9911617002404568 \t 0.9950743657874556\n",
      "26     \t [ 5.18838882  5.06107909 12.8969862   0.98352186 14.23071385  1.        ]. \t  0.9949351419446452 \t 0.9950743657874556\n",
      "27     \t [ 6.2044297   4.76287163 12.69542971  0.5        13.72274931  1.        ]. \t  0.9932404556779773 \t 0.9950743657874556\n",
      "28     \t [ 4.51703264  5.46584649 13.53214649  0.5        14.59490782  1.        ]. \t  0.9934948969877135 \t 0.9950743657874556\n",
      "29     \t [ 4.66293429  5.56138973 13.0881664   0.54015236 14.54028312  0.2820511 ]. \t  0.8563161235933313 \t 0.9950743657874556\n",
      "30     \t [ 6.35422647  6.0431156  14.65703883  1.         11.40862624  0.82674043]. \t  0.9911809034742821 \t 0.9950743657874556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0601842959615071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_6 = loser_6.getResult()[0]\n",
    "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
    "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
    "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
    "\n",
    "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
    "rmse_loser_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 3.11101452  9.28260134 13.36659845  0.68217491  1.21135528  0.82304321]. \t  \u001b[92m0.9893085666380245\u001b[0m \t 0.9893085666380245\n",
      "2      \t [ 3.38436655  9.99954672 13.74124648  0.53563981  1.08616909  0.99570233]. \t  0.9882907964898706 \t 0.9893085666380245\n",
      "3      \t [ 2.67569874  9.97902812 13.20884853  0.5034907   1.05365897  0.99601565]. \t  0.9881707758808855 \t 0.9893085666380245\n",
      "4      \t [ 2.84932295  9.80353054 13.68015054  0.5314374   1.75891761  0.96223228]. \t  0.9881947805281754 \t 0.9893085666380245\n",
      "5      \t [ 2.96636065  9.98000797 13.51806424  0.99815195  1.2845649   0.44806549]. \t  0.8283183479283774 \t 0.9893085666380245\n",
      "6      \t [ 2.89003732  9.67536377 13.63848342  1.          1.20369776  1.        ]. \t  \u001b[92m0.9967066425294901\u001b[0m \t 0.9967066425294901\n",
      "7      \t [ 1.83630705  2.01820556 11.54187639  0.75214075  5.57916544  0.82258573]. \t  0.988996516663969 \t 0.9967066425294901\n",
      "8      \t [ 1.4879311   1.23986131 11.71351612  0.7669766   5.67065675  1.        ]. \t  0.99610653976114 \t 0.9967066425294901\n",
      "9      \t [ 1.01098539  1.92307129 11.78186543  0.63663746  5.72776664  0.77115151]. \t  0.9883916179445135 \t 0.9967066425294901\n",
      "10     \t [ 1.23161615  1.64636208 11.16532745  0.72116779  5.21449501  0.84453371]. \t  0.9886892666513584 \t 0.9967066425294901\n",
      "11     \t [ 1.49749641  1.52420269 11.65842362  0.50276129  5.44253668  0.25429818]. \t  0.827454201092506 \t 0.9967066425294901\n",
      "12     \t [ 1.33821897  1.75305931 11.66846998  1.          5.44270697  0.87376329]. \t  0.9934420855190881 \t 0.9967066425294901\n",
      "13     \t [ 1.43028531  1.74548795 11.62208843  0.5         5.43479995  1.        ]. \t  0.9954632306328383 \t 0.9967066425294901\n",
      "14     \t [ 0.49133395  1.03341781 11.85632184  0.61302333  5.15010685  0.87048011]. \t  0.9884828302801433 \t 0.9967066425294901\n",
      "15     \t [ 2.92772217  9.66793661 13.62499129  0.5         1.19442308  1.        ]. \t  0.9950311547257263 \t 0.9967066425294901\n",
      "16     \t [ 4.76454133  8.7376055   6.6488849   0.7555268  14.56707217  0.88665274]. \t  0.9816128657560318 \t 0.9967066425294901\n",
      "17     \t [ 5.47228689  8.80715208  6.23128228  0.78380235 14.22141815  1.        ]. \t  0.9887564943913957 \t 0.9967066425294901\n",
      "18     \t [ 4.80110493  9.36858132  6.09749927  0.74181728 14.27370191  0.99995317]. \t  0.9814544396474091 \t 0.9967066425294901\n",
      "19     \t [ 5.29489441  9.32684333  6.55577793  0.74179363 14.55462043  0.48740833]. \t  0.8280591088697972 \t 0.9967066425294901\n",
      "20     \t [ 4.85658628  8.67686277  5.88952484  0.93703033 14.35951327  0.5246929 ]. \t  0.8257931347459376 \t 0.9967066425294901\n",
      "21     \t [ 1.98350086  9.24092494 13.28396862  0.79450126  1.51462947  0.65050106]. \t  0.8289088531938699 \t 0.9967066425294901\n",
      "22     \t [ 3.61939887  9.95311725 12.75889868  0.59702365  1.6789081   1.        ]. \t  0.9955400405949568 \t 0.9967066425294901\n",
      "23     \t [ 2.80034782  9.48438307 12.53023209  0.7685322   1.89178553  1.        ]. \t  0.9963225766775393 \t 0.9967066425294901\n",
      "24     \t [ 3.24752382  9.54467393 12.29607139  0.88588405  1.00715847  1.        ]. \t  0.9964521992145841 \t 0.9967066425294901\n",
      "25     \t [ 3.66021094  9.04256209 12.57977751  1.          1.64972237  1.        ]. \t  0.9964810044456128 \t 0.9967066425294901\n",
      "26     \t [ 3.33385535  9.23273849 12.35514864  0.5         1.51177805  0.46090643]. \t  0.8263644026205758 \t 0.9967066425294901\n",
      "27     \t [ 3.23812387  9.55354904 12.81537227  0.91890231  1.46957176  1.        ]. \t  0.9965290129796106 \t 0.9967066425294901\n",
      "28     \t [ 4.25012939  9.35269357 12.68724589  0.51808934  1.          1.        ]. \t  0.9948151181550461 \t 0.9967066425294901\n",
      "29     \t [ 3.54556836  8.57534722 12.38393324  0.58542583  1.          1.        ]. \t  0.9955976501581453 \t 0.9967066425294901\n",
      "30     \t [ 3.94732444  9.21086704 11.82461945  0.5         1.35353805  1.        ]. \t  0.9946902970314658 \t 0.9967066425294901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06331398538451588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_7 = loser_7.getResult()[0]\n",
    "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
    "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
    "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
    "\n",
    "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
    "rmse_loser_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [6.07385922e-01 1.77561699e-04 7.70123324e+00 5.30464199e-01\n",
      " 1.66493835e+01 5.76554988e-01]. \t  \u001b[92m0.8897972824072298\u001b[0m \t 0.8897972824072298\n",
      "2      \t [ 0.99314801  0.80566768  7.92019244  0.66762468 17.04262647  0.66074261]. \t  0.889475634031314 \t 0.8897972824072298\n",
      "3      \t [ 1.59221857  0.1126534   7.73119865  0.68373097 16.66426633  0.68053068]. \t  \u001b[92m0.983283548617921\u001b[0m \t 0.983283548617921\n",
      "4      \t [ 1.11327686  0.59178104  7.45267191  0.61730255 16.26258513  1.        ]. \t  \u001b[92m0.9921362769573058\u001b[0m \t 0.9921362769573058\n",
      "5      \t [ 1.21779383  0.57468836  7.60059072  0.5        16.33591533  0.14826368]. \t  0.888851540167597 \t 0.9921362769573058\n",
      "6      \t [ 1.06911769  0.36540255  7.55009648  1.         16.56228521  0.63046063]. \t  0.8956879064836031 \t 0.9921362769573058\n",
      "7      \t [ 1.19310028  0.3291751   7.56371177  0.5        16.83179701  1.        ]. \t  0.9917522113127405 \t 0.9921362769573058\n",
      "8      \t [ 1.23411236  0.38915981  8.44878108  0.5        16.14046449  0.91172016]. \t  0.983624418827075 \t 0.9921362769573058\n",
      "9      \t [ 1.89774287  0.96488757  8.05135654  0.5        16.32805035  0.94797657]. \t  0.9834227846298779 \t 0.9921362769573058\n",
      "10     \t [ 1.90561699  0.28671195  7.91267148  0.5        15.65382966  0.99513956]. \t  0.9829762935578142 \t 0.9921362769573058\n",
      "11     \t [ 1.49492301  0.47263532  7.88652126  0.5        16.25323975  0.8303152 ]. \t  0.9831923297828115 \t 0.9921362769573058\n",
      "12     \t [ 2.24257758  0.48645872  8.7964161   0.84128562 15.82648231  1.        ]. \t  \u001b[92m0.9938357585531158\u001b[0m \t 0.9938357585531158\n",
      "13     \t [ 1.70265413  1.05982465  8.48182676  0.7015384  15.33416696  1.        ]. \t  0.9937013428997391 \t 0.9938357585531158\n",
      "14     \t [ 2.50818034  0.94818337  7.93068881  1.         15.57268729  1.        ]. \t  0.9918962338033116 \t 0.9938357585531158\n",
      "15     \t [ 1.81969458  0.67597681  8.25733191  1.         15.79919657  1.        ]. \t  \u001b[92m0.9938933708128838\u001b[0m \t 0.9938933708128838\n",
      "16     \t [ 2.43714085  0.91888894  8.39632362  0.5        15.53837501  1.        ]. \t  0.9928467987523796 \t 0.9938933708128838\n",
      "17     \t [ 1.86445069  1.36350233  7.22911854  0.53302948 15.46424761  1.        ]. \t  0.991709008409908 \t 0.9938933708128838\n",
      "18     \t [ 2.19241953  0.81403473  6.86882369  0.7117335  16.22899196  1.        ]. \t  0.98862207735359 \t 0.9938933708128838\n",
      "19     \t [ 1.52908502  0.08265746  9.11666705  0.5        15.25110471  1.        ]. \t  0.9935333094698894 \t 0.9938933708128838\n",
      "20     \t [ 1.66303128  1.58301783  7.10383049  0.74710599 16.45781512  0.88375429]. \t  0.9835187828910374 \t 0.9938933708128838\n",
      "21     \t [ 2.31540576  1.26705811  7.36598751  0.69245213 16.00625379  0.3248611 ]. \t  0.8894228260889996 \t 0.9938933708128838\n",
      "22     \t [ 1.74636422  0.36551102  8.90785787  0.53757812 15.56763438  0.25656655]. \t  0.8893460123239728 \t 0.9938933708128838\n",
      "23     \t [ 2.53955575  1.66672727  7.40836404  0.56082018 16.09990808  1.        ]. \t  0.9915937807098038 \t 0.9938933708128838\n",
      "24     \t [ 1.44010773  0.73189808  9.50830775  0.5        15.82300269  1.        ]. \t  0.9933460752262091 \t 0.9938933708128838\n",
      "25     \t [ 1.34026745  0.          9.3893377   0.93084914 16.11942891  0.82302353]. \t  0.9859335980667959 \t 0.9938933708128838\n",
      "26     \t [ 2.06416324  1.17830047  7.44450596  0.70347499 15.98664987  1.        ]. \t  0.9916657937528018 \t 0.9938933708128838\n",
      "27     \t [ 1.31939991  1.10991149  6.35046764  0.5        15.99133334  0.77775808]. \t  0.9788283799777714 \t 0.9938933708128838\n",
      "28     \t [ 2.32409496  1.15602962  7.31999372  0.5        17.23293274  0.73993341]. \t  0.9828658769511565 \t 0.9938933708128838\n",
      "29     \t [ 1.03018407  0.48894325  9.18432386  1.         15.4805768   0.82930308]. \t  0.9929140031910713 \t 0.9938933708128838\n",
      "30     \t [ 2.19373381  0.41874891  8.14611235  0.93978675 14.69186342  0.94977098]. \t  0.9848390102123 \t 0.9938933708128838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0646401566773533"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_8 = loser_8.getResult()[0]\n",
    "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
    "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
    "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
    "\n",
    "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
    "rmse_loser_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 7.60863201  5.02369233 14.          0.86260025  4.76578023  0.79156586]. \t  0.9682042036061591 \t 0.968425042143454\n",
      "2      \t [ 7.01867964  5.61674133 14.30475738  0.94188818  4.70375305  0.99335795]. \t  \u001b[92m0.9686170722691062\u001b[0m \t 0.9686170722691062\n",
      "3      \t [ 6.94707024  5.15822773 13.77895865  0.85858015  4.19716459  0.71477874]. \t  0.9683578287853237 \t 0.9686170722691062\n",
      "4      \t [ 7.05664528  4.83312407 14.58751021  0.82670471  4.39436483  0.84261108]. \t  0.9681801987514379 \t 0.9686170722691062\n",
      "5      \t [ 7.05006332  5.22547049 14.26975394  0.99999212  4.7068809   0.17813064]. \t  0.7911454074482495 \t 0.9686170722691062\n",
      "6      \t [ 7.07089656  5.14575715 14.09874337  0.5         4.62795132  0.98076935]. \t  0.9642435257913982 \t 0.9686170722691062\n",
      "7      \t [ 7.08730422  4.99311405 14.07067438  1.          4.56062392  1.        ]. \t  \u001b[92m0.9961209418580762\u001b[0m \t 0.9961209418580762\n",
      "8      \t [ 5.97121737  5.28031426 14.41860967  0.84478762  4.30612081  0.84878932]. \t  0.9685162576596978 \t 0.9961209418580762\n",
      "9      \t [ 6.01321665  5.75236883 13.59193839  0.92303988  4.70439538  0.81645187]. \t  0.968837909630957 \t 0.9961209418580762\n",
      "10     \t [ 5.76573988  4.92507603 13.54967529  0.65775359  4.40803189  0.52013798]. \t  0.7917118958280024 \t 0.9961209418580762\n",
      "11     \t [ 6.14256639  5.96582493 13.91720365  0.53302315  3.93714356  0.59771276]. \t  0.7893307039606419 \t 0.9961209418580762\n",
      "12     \t [ 5.91852392  5.62650688 14.2540838   0.5         5.06704737  0.61341902]. \t  0.791956736177692 \t 0.9961209418580762\n",
      "13     \t [7.04763741 7.02413675 7.00030053 0.7202028  9.51741427 0.94726596]. \t  0.9568022707134878 \t 0.9961209418580762\n",
      "14     \t [7.2822724  7.68609197 6.38924343 0.71666196 9.52134286 1.        ]. \t  0.9885452594416112 \t 0.9961209418580762\n",
      "15     \t [7.23223866 7.88907959 7.24600091 0.69813547 9.63169467 0.96818666]. \t  0.9564854155230597 \t 0.9961209418580762\n",
      "16     \t [6.83413727 7.67502132 6.86038236 0.55554614 8.94007543 1.        ]. \t  0.988449242788478 \t 0.9961209418580762\n",
      "17     \t [7.12476556 7.60145877 6.83429492 0.90950607 9.27507859 0.32684107]. \t  0.7876840247068085 \t 0.9961209418580762\n",
      "18     \t [7.28830892 7.50923933 6.86751489 0.5        9.35381708 1.        ]. \t  0.9884684463680222 \t 0.9961209418580762\n",
      "19     \t [7.10675764 7.58379693 6.8920361  1.         9.23497782 1.        ]. \t  0.9883964336015972 \t 0.9961209418580762\n",
      "20     \t [6.24191802 7.73472495 6.61741674 0.59498053 9.91073175 1.        ]. \t  0.9885068527665292 \t 0.9961209418580762\n",
      "21     \t [6.59288259 8.60259075 6.45051916 0.55381321 9.52207719 1.        ]. \t  0.988410836459115 \t 0.9961209418580762\n",
      "22     \t [6.2805638  7.92502177 5.86092957 0.54259368 9.26211939 1.        ]. \t  0.9835908194448876 \t 0.9961209418580762\n",
      "23     \t [ 6.60648134  8.20026605  5.81629667  0.84802601 10.10425996  1.        ]. \t  0.98369643747293 \t 0.9961209418580762\n",
      "24     \t [6.47909555 8.08039825 6.26225433 1.         9.56603059 1.        ]. \t  0.9884300392089339 \t 0.9961209418580762\n",
      "25     \t [6.56975745 8.09908571 6.07736353 0.5        9.80060026 0.51435088]. \t  0.786968706545846 \t 0.9961209418580762\n",
      "26     \t [6.45519724 7.03490854 5.74873127 0.77705281 9.95815254 1.        ]. \t  0.9837348436640055 \t 0.9961209418580762\n",
      "27     \t [6.19674723 6.77585584 6.23614729 0.53231444 9.16797326 1.        ]. \t  0.9884588447165377 \t 0.9961209418580762\n",
      "28     \t [7.04145043 6.79632054 5.94986268 0.80290775 8.8987493  1.        ]. \t  0.9837588482421517 \t 0.9961209418580762\n",
      "29     \t [6.62484352 7.26305117 6.13759431 0.63283633 9.41632085 1.        ]. \t  0.9884924501164428 \t 0.9961209418580762\n",
      "30     \t [6.67509736 5.964078   6.1385742  1.         9.41854069 1.        ]. \t  0.9884300392089339 \t 0.9961209418580762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06109824360218985"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_9 = loser_9.getResult()[0]\n",
    "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
    "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
    "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
    "\n",
    "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
    "rmse_loser_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.99921704  3.04883105  8.50435002  0.87840248 19.64332185  0.82478253]. \t  \u001b[92m0.9856695950032489\u001b[0m \t 0.9856695950032489\n",
      "2      \t [ 9.23926256  3.00793564  8.47774568  0.86206135 19.17157042  0.78704598]. \t  \u001b[92m0.9858280233936165\u001b[0m \t 0.9858280233936165\n",
      "3      \t [ 9.81193127  3.58930446  8.14938051  0.94791713 19.06354926  0.81955086]. \t  0.9858088201597912 \t 0.9858280233936165\n",
      "4      \t [ 9.994002    2.85865856  8.3654222   0.59931602 18.83430448  0.80929993]. \t  0.9844453849575467 \t 0.9858280233936165\n",
      "5      \t [ 9.83332198  3.15855023  8.57283148  0.99831584 19.09233391  0.17516287]. \t  0.8899557709675366 \t 0.9858280233936165\n",
      "6      \t [ 9.67882081  3.36956462  8.46809066  0.5        19.31373852  0.67777536]. \t  0.9840613215256284 \t 0.9858280233936165\n",
      "7      \t [ 6.74858966  1.04266026  6.70937147  0.94571366 13.29062732  0.13414017]. \t  0.8925865933967584 \t 0.9858280233936165\n",
      "8      \t [ 6.22620065  1.23246257  6.95214656  0.9649824  14.10001513  0.15452516]. \t  0.8927978313197255 \t 0.9858280233936165\n",
      "9      \t [ 6.48926092  0.34281502  6.5940952   0.99516472 13.95599695  0.14673069]. \t  0.8928890457296689 \t 0.9858280233936165\n",
      "10     \t [ 5.8557686   0.87887135  6.38975097  0.93251579 13.47895957  0.35498523]. \t  0.8927738238375406 \t 0.9858280233936165\n",
      "11     \t [ 6.56200607  0.95583591  6.56125133  0.9954017  13.84937813  0.92667167]. \t  0.9828131237728633 \t 0.9858280233936165\n",
      "12     \t [ 6.45699477  0.90722154  6.56526173  0.5        13.82098306  0.44892558]. \t  0.888390716019174 \t 0.9858280233936165\n",
      "13     \t [ 7.14180678  0.75916542  8.31617604  1.         11.78610783  0.21337261]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "14     \t [ 7.81357606  1.11163773  8.16014779  0.96050286 11.11675339  0.1       ]. \t  0.8917032519008908 \t 0.9858280233936165\n",
      "15     \t [ 6.84693381  1.14036218  8.31720722  0.92771523 10.88769272  0.1       ]. \t  0.8920489103863217 \t 0.9858280233936165\n",
      "16     \t [ 7.29653909  0.27468277  8.09571317  1.         10.93037716  0.1       ]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "17     \t [ 7.32951866  0.81530133  8.37064997  1.         11.04752763  0.89540526]. \t  0.9832355567496521 \t 0.9858280233936165\n",
      "18     \t [ 7.33249277  0.75633699  8.3865702   0.5        11.14509249  0.40417605]. \t  0.8901190087229036 \t 0.9858280233936165\n",
      "19     \t [ 7.07931098  0.98654361  7.34989274  1.         11.38856241  0.65241097]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "20     \t [ 9.79611416  3.10954991  8.51303953  1.         19.09334607  1.        ]. \t  \u001b[92m0.9938453588917785\u001b[0m \t 0.9938453588917785\n",
      "21     \t [ 9.3109297   3.47418297  8.49243598  0.62574072 18.1532274   0.65290782]. \t  0.8918856839013913 \t 0.9938453588917785\n",
      "22     \t [ 6.31853166  0.62908772  7.37912811  1.         13.05933007  0.86102402]. \t  0.9819489393227738 \t 0.9938453588917785\n",
      "23     \t [ 6.67446926  0.32504749  6.62360805  1.         12.73986322  0.95306315]. \t  0.9777770211650133 \t 0.9938453588917785\n",
      "24     \t [ 7.29549281  0.60455057  7.39169872  1.         12.56646067  1.        ]. \t  0.9911808964907601 \t 0.9938453588917785\n",
      "25     \t [ 6.7177025   0.19063457  7.57486693  1.         11.94998833  1.        ]. \t  0.9920066439807904 \t 0.9938453588917785\n",
      "26     \t [ 6.595531    0.98761017  7.1511251   1.         12.39451089  1.        ]. \t  0.9919346319749472 \t 0.9938453588917785\n",
      "27     \t [ 6.77968524  0.39930482  7.22941571  1.         12.36838335  0.43269132]. \t  0.8873680894882475 \t 0.9938453588917785\n",
      "28     \t [ 9.27606876  3.02019827  7.49634273  0.70667719 18.57655027  0.46739614]. \t  0.889274054126996 \t 0.9938453588917785\n",
      "29     \t [ 7.74793554  0.25124411  7.73976221  1.         11.58598616  0.99347626]. \t  0.9819009309270635 \t 0.9938453588917785\n",
      "30     \t [ 8.67297315  3.81182241  7.90088619  1.         18.71468693  0.44675353]. \t  0.8873392847412253 \t 0.9938453588917785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06738310386109525"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_10 = loser_10.getResult()[0]\n",
    "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
    "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
    "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
    "\n",
    "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
    "rmse_loser_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [8.02776449 5.47540798 5.27568131 0.55701687 1.00000002 0.68903624]. \t  \u001b[92m0.9810991916797995\u001b[0m \t 0.9810991916797995\n",
      "2      \t [8.67133264 5.02712129 5.49265605 0.55613132 1.3803911  0.77636521]. \t  \u001b[92m0.9810991916797995\u001b[0m \t 0.9810991916797995\n",
      "3      \t [8.58941355 5.13466314 5.00072391 0.500018   1.0000999  0.1466084 ]. \t  0.8133783707838496 \t 0.9810991916797995\n",
      "4      \t [8.67387658 5.72743749 5.82100424 0.53256176 1.         0.52979637]. \t  0.8069404171025498 \t 0.9810991916797995\n",
      "5      \t [8.41967062 5.67757242 5.08955131 0.50001164 1.75574822 0.67224153]. \t  0.980033410646759 \t 0.9810991916797995\n",
      "6      \t [8.38081825 5.26498487 5.         0.5        1.32805045 1.        ]. \t  \u001b[92m0.9838548567363183\u001b[0m \t 0.9838548567363183\n",
      "7      \t [8.52557286 5.51111179 5.20467464 1.         1.3105852  0.69829372]. \t  0.9812720215448092 \t 0.9838548567363183\n",
      "8      \t [7.7402235  5.05308436 5.69711818 0.63134108 1.87957187 0.35753008]. \t  0.8133207458015974 \t 0.9838548567363183\n",
      "9      \t [8.05007272 5.64269555 5.85898751 0.51794583 1.71880048 1.        ]. \t  0.9837684395912124 \t 0.9838548567363183\n",
      "10     \t [ 4.24607657  3.15825169  8.24876415  0.80095874 17.50138334  0.22439416]. \t  0.8267773655264996 \t 0.9838548567363183\n",
      "11     \t [ 4.46019619  2.88430144  9.2760866   0.7931472  17.39278585  0.27417854]. \t  0.8261772584712351 \t 0.9838548567363183\n",
      "12     \t [ 3.45107353  2.93892675  8.91675997  0.85860425 17.39567533  0.4218199 ]. \t  0.8289905408350459 \t 0.9838548567363183\n",
      "13     \t [ 4.18712237  3.04132678  8.71132361  0.78022707 16.91576228  0.99942588]. \t  \u001b[92m0.9853815255453027\u001b[0m \t 0.9853815255453027\n",
      "14     \t [ 4.15405658  3.416848    8.901019    0.97163227 17.68232549  0.99643371]. \t  0.98504547133882 \t 0.9853815255453027\n",
      "15     \t [ 4.0709942   3.29857218  8.86055872  0.5        17.39642505  0.7592885 ]. \t  0.9848246339078256 \t 0.9853815255453027\n",
      "16     \t [ 4.22026193  2.45931881  8.60751051  1.         17.69047689  1.        ]. \t  \u001b[92m0.9933748738895524\u001b[0m \t 0.9933748738895524\n",
      "17     \t [ 4.15254256  2.99505761  8.76643474  1.         17.36660297  0.74642947]. \t  0.9845989876649819 \t 0.9933748738895524\n",
      "18     \t [ 5.02285689  2.83547726  8.50014665  0.5735889  17.68070851  1.        ]. \t  0.9931444338390604 \t 0.9933748738895524\n",
      "19     \t [ 4.66437322  2.17312781  7.58215871  0.60092738 18.23946257  0.78689454]. \t  0.984411761580258 \t 0.9933748738895524\n",
      "20     \t [ 4.43305837  2.77385619  8.28663923  0.59933414 18.46077833  1.        ]. \t  0.9931924415433327 \t 0.9933748738895524\n",
      "21     \t [ 4.33324307  2.62712471  7.93506907  0.5        17.65220066  1.        ]. \t  0.9905904027722793 \t 0.9933748738895524\n",
      "22     \t [ 4.63170766  2.1276489   8.42669581  0.5        18.07248025  0.50924676]. \t  0.8236711984625216 \t 0.9933748738895524\n",
      "23     \t [ 4.81479246  2.71124281  7.93272333  1.         18.08542613  1.        ]. \t  0.9912049052866768 \t 0.9933748738895524\n",
      "24     \t [ 4.58902162  2.28069622  6.70925221  0.85809453 18.80011925  0.14418478]. \t  0.8240457068458653 \t 0.9933748738895524\n",
      "25     \t [ 4.85322951  1.56969196  6.74906953  1.         18.99686662  0.89049157]. \t  0.9838404603091725 \t 0.9933748738895524\n",
      "26     \t [ 5.41406922  1.95868731  6.61891958  0.52883063 18.75199207  0.66698502]. \t  0.9820593748056385 \t 0.9933748738895524\n",
      "27     \t [ 5.00796095  1.63735054  6.49023332  1.         18.21453177  0.573226  ]. \t  0.7870598907399559 \t 0.9933748738895524\n",
      "28     \t [ 5.18136766  1.49994282  7.08004649  0.70432498 19.0058663   0.20578982]. \t  0.8252267078350081 \t 0.9933748738895524\n",
      "29     \t [ 4.64019426  1.81057749  6.54208564  0.5        18.86828755  0.79662719]. \t  0.983874076426364 \t 0.9933748738895524\n",
      "30     \t [ 4.94843632  3.87647215  8.64219148  0.72645105 17.01511945  1.        ]. \t  \u001b[92m0.9934852898738701\u001b[0m \t 0.9934852898738701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06350779003544968"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_11 = loser_11.getResult()[0]\n",
    "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
    "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
    "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
    "\n",
    "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
    "rmse_loser_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.20850053  6.60048665  8.57964171  0.86300391 15.36962087  0.71585139]. \t  0.9902207578919282 \t 0.9904271937791528\n",
      "2      \t [ 1.45617388  6.55019346  9.05143081  0.82970662 15.32482527  0.76226071]. \t  \u001b[92m0.9905136049779549\u001b[0m \t 0.9905136049779549\n",
      "3      \t [ 2.21722526  6.25514507  9.38183001  0.75613866 15.28000918  0.78017764]. \t  0.9903599839473237 \t 0.9905136049779549\n",
      "4      \t [ 1.82903751  5.82368155  8.72299959  0.83636053 15.2772349   0.77249248]. \t  0.9902015541741003 \t 0.9905136049779549\n",
      "5      \t [ 1.91662896  6.32723391  8.86663821  0.5        15.46596048  1.        ]. \t  \u001b[92m0.9933604722757284\u001b[0m \t 0.9933604722757284\n",
      "6      \t [ 1.90480431  6.25797608  8.96530636  0.63230995 15.57813388  0.15195825]. \t  0.8899173595058852 \t 0.9933604722757284\n",
      "7      \t [ 1.93576127  6.29430916  8.97306839  1.         15.42803472  0.78384771]. \t  0.9883196130589503 \t 0.9933604722757284\n",
      "8      \t [ 1.9748437   6.47343758  8.7752699   0.5        14.37973619  0.47155165]. \t  0.8906614928545813 \t 0.9933604722757284\n",
      "9      \t [ 1.3574762   5.66131954  9.66514247  0.5440455  15.76437399  0.82961716]. \t  0.9900239244167598 \t 0.9933604722757284\n",
      "10     \t [ 1.36574245  5.80085466  9.55839257  0.5        14.73208926  0.61558931]. \t  0.8901333982200228 \t 0.9933604722757284\n",
      "11     \t [ 2.83281059  7.40745109  7.68036306  0.77106742 14.02765493  0.36454908]. \t  0.8898357448805444 \t 0.9933604722757284\n",
      "12     \t [ 3.02020244  6.44941865  8.18746338  0.55329374 14.70192964  0.46959879]. \t  0.8903254326325895 \t 0.9933604722757284\n",
      "13     \t [ 2.14562021  6.63795377  7.61325748  0.63665844 14.75430338  0.41539621]. \t  0.8895909010045218 \t 0.9933604722757284\n",
      "14     \t [ 2.51935631  7.30108451  8.27631854  0.5        14.88935316  0.16602242]. \t  0.8897157233726901 \t 0.9933604722757284\n",
      "15     \t [ 2.51865364  7.1052863   8.09313268  0.5        14.60021308  1.        ]. \t  0.9933268654238159 \t 0.9933604722757284\n",
      "16     \t [ 1.54616138  6.55504856  9.99251779  0.53963151 15.9095262   0.74103612]. \t  0.9901967516543593 \t 0.9933604722757284\n",
      "17     \t [ 2.95161298  7.19195612  7.6092232   0.96292332 15.1194367   0.74317687]. \t  0.9897646769917956 \t 0.9933604722757284\n",
      "18     \t [ 2.59439506  6.95238592  8.08246698  1.         14.65841868  0.63994611]. \t  0.8979635018268172 \t 0.9933604722757284\n",
      "19     \t [ 3.37155149  7.77131022  6.90661374  0.5        14.11249519  0.65013402]. \t  0.8878385869898504 \t 0.9933604722757284\n",
      "20     \t [ 2.71435808  6.88720263  7.76900165  0.5        15.255063    0.72977448]. \t  0.9885740670910214 \t 0.9933604722757284\n",
      "21     \t [ 3.52318005  7.66450479  7.93135806  0.5        14.66980311  0.80119903]. \t  0.9885308582419091 \t 0.9933604722757284\n",
      "22     \t [ 2.78797078  8.07465983  7.53444617  0.5256517  14.77058978  0.81304094]. \t  0.988564463710966 \t 0.9933604722757284\n",
      "23     \t [ 3.3005073   8.39736106  7.62509535  0.5        13.64865606  0.602905  ]. \t  0.8873585009584336 \t 0.9933604722757284\n",
      "24     \t [ 3.45683117  8.20501051  7.34024398  1.         14.29287695  0.1       ]. \t  0.8964176405702106 \t 0.9933604722757284\n",
      "25     \t [ 2.68132353  8.35388709  6.9503852   0.5        13.73991772  0.1       ]. \t  0.8878337861295362 \t 0.9933604722757284\n",
      "26     \t [ 2.91162397  8.22530849  7.0410631   1.         13.68626889  0.78252065]. \t  0.9862360442460253 \t 0.9933604722757284\n",
      "27     \t [ 3.03421826  7.62818119  8.47415559  0.67602186 15.48639646  1.        ]. \t  0.9931300425966367 \t 0.9933604722757284\n",
      "28     \t [ 2.16682108  5.92029922 10.00857308  0.64343816 16.37746569  0.78967663]. \t  0.9902783693219828 \t 0.9933604722757284\n",
      "29     \t [ 1.7451752   5.91056577 10.1627631   1.         15.74630727  0.49501159]. \t  0.89778106941145 \t 0.9933604722757284\n",
      "30     \t [ 1.39861962  6.14490228  9.78164469  0.9522941  16.70344605  0.77017336]. \t  0.9906816350889508 \t 0.9933604722757284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07131099845430462"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_12 = loser_12.getResult()[0]\n",
    "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
    "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
    "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
    "\n",
    "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
    "rmse_loser_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [8.75818261 1.82535264 9.73293248 0.96041632 6.24428973 0.73785956]. \t  0.9879355470702414 \t 0.9879595510260936\n",
      "2      \t [8.77662081 2.71131097 9.72080185 0.86534868 6.18008817 0.71654634]. \t  0.9877243135033327 \t 0.9879595510260936\n",
      "3      \t [9.49297492 2.25702224 9.53230378 0.95481676 6.19220421 0.87870772]. \t  0.9879211437287174 \t 0.9879595510260936\n",
      "4      \t [8.91513666 2.30534322 9.30625279 0.90492142 6.81868073 0.85152339]. \t  0.9874986743822985 \t 0.9879595510260936\n",
      "5      \t [9.04012806 2.2526465  9.30393708 0.95287273 6.28548835 0.16240568]. \t  0.8217076914683421 \t 0.9879595510260936\n",
      "6      \t [8.94372265 2.20926373 9.47662914 0.5        6.25009266 1.        ]. \t  \u001b[92m0.9938165506184232\u001b[0m \t 0.9938165506184232\n",
      "7      \t [8.90201187 2.30112375 9.52487054 1.         6.28882595 1.        ]. \t  \u001b[92m0.9944550614447311\u001b[0m \t 0.9944550614447311\n",
      "8      \t [9.31451304 1.35979826 8.90205507 0.85046776 6.48845926 0.92456239]. \t  0.9882091963847243 \t 0.9944550614447311\n",
      "9      \t [9.34723654 1.35433936 9.20567661 0.7903233  5.51916693 0.81782643]. \t  0.9869273803713104 \t 0.9944550614447311\n",
      "10     \t [9.66350276 1.14770844 9.73033441 0.61415147 6.24395377 0.69922864]. \t  0.9849542535408314 \t 0.9944550614447311\n",
      "11     \t [8.85033544 0.9115925  9.30908754 0.5        6.0892324  0.60486375]. \t  0.8205170599947572 \t 0.9944550614447311\n",
      "12     \t [9.50002077 1.63931128 9.18072501 0.5        6.10213434 0.67978912]. \t  0.9859240206173464 \t 0.9944550614447311\n",
      "13     \t [ 6.05240039  8.69308814  8.00064318  0.77169566 10.55730831  0.76051579]. \t  0.9870762058656052 \t 0.9944550614447311\n",
      "14     \t [ 6.91654097  8.8860984   8.11839324  0.77415879 10.57751065  0.81561968]. \t  0.9869513850877439 \t 0.9944550614447311\n",
      "15     \t [ 6.55131233  8.92006186  7.33024037  0.79014539 10.40942316  0.89940399]. \t  0.9870906039522018 \t 0.9944550614447311\n",
      "16     \t [6.58554241 8.4687358  7.89607332 0.60540021 9.93461474 0.91668569]. \t  0.986058439935222 \t 0.9944550614447311\n",
      "17     \t [ 6.54230977  8.89613996  7.79759337  0.58264369 10.21956127  0.20458139]. \t  0.815024949710053 \t 0.9944550614447311\n",
      "18     \t [ 6.4554968   8.94470063  7.88995692  0.5        10.42565666  1.        ]. \t  0.99179060485179 \t 0.9944550614447311\n",
      "19     \t [ 6.50432253  8.77955895  7.89829151  1.         10.24009991  0.95612361]. \t  0.9580216765799742 \t 0.9944550614447311\n",
      "20     \t [7.42852851 9.21693599 7.59076799 0.55537493 9.76700558 0.98786424]. \t  0.9856935757959334 \t 0.9944550614447311\n",
      "21     \t [6.62247874 9.16502527 7.14485628 0.5        9.28573958 1.        ]. \t  0.9918530154135804 \t 0.9944550614447311\n",
      "22     \t [6.88874753 9.92161794 7.10155611 0.7486901  9.81868854 0.95198805]. \t  0.9868313581866747 \t 0.9944550614447311\n",
      "23     \t [6.89779438 9.86313978 7.7808318  0.5        9.15862487 1.        ]. \t  0.9921602715108438 \t 0.9944550614447311\n",
      "24     \t [ 6.50714065  7.88885287  7.92570396  0.61451581 11.08609508  0.67579536]. \t  0.9864473040891671 \t 0.9944550614447311\n",
      "25     \t [6.8182911  9.44744255 7.54425923 0.5886058  9.66421462 0.95601685]. \t  0.9859096129889077 \t 0.9944550614447311\n",
      "26     \t [7.44730707 9.73967609 6.93537579 0.78584233 8.91725994 1.        ]. \t  0.9888909159910161 \t 0.9944550614447311\n",
      "27     \t [ 7.35987619  8.11411294  7.40200127  0.5        10.56106778  0.85646776]. \t  0.9858279977412728 \t 0.9944550614447311\n",
      "28     \t [7.40105221 9.00668037 6.46137388 0.64372513 9.76666005 1.        ]. \t  0.9887276888837917 \t 0.9944550614447311\n",
      "29     \t [7.48248385 8.39747636 7.05962598 0.52709241 9.17306991 1.        ]. \t  0.9913921387697856 \t 0.9944550614447311\n",
      "30     \t [6.70783771 8.0102005  6.78742041 0.5        9.90775417 0.88594122]. \t  0.9857559909212132 \t 0.9944550614447311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06176950917580233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_13 = loser_13.getResult()[0]\n",
    "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
    "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
    "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
    "\n",
    "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
    "rmse_loser_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [6.47551049 5.07149688 9.         0.9481426  2.         0.30826454]. \t  0.8841083588522155 \t 0.985981635228031\n",
      "init   \t [1.30128292 8.51503709 6.         0.58757726 8.         0.65878816]. \t  0.8847900532901706 \t 0.985981635228031\n",
      "init   \t [ 1.87389374  2.97833637 11.          0.92510013  4.          0.27446484]. \t  0.8861919064379334 \t 0.985981635228031\n",
      "init   \t [ 9.80899716  0.11774133 11.          0.53577929  6.          0.38130944]. \t  0.8891587951046561 \t 0.985981635228031\n",
      "init   \t [ 6.4601046   0.32698917 10.          0.67912907 15.          0.96967672]. \t  0.985981635228031 \t 0.985981635228031\n",
      "1      \t [ 6.04704707  0.         10.73515922  0.73996947 14.99999999  0.99943441]. \t  0.9855639564395027 \t 0.985981635228031\n",
      "2      \t [ 6.6277366   0.47581753 10.72604035  0.68437373 15.44088114  0.79160505]. \t  \u001b[92m0.9863032877524404\u001b[0m \t 0.9863032877524404\n",
      "3      \t [ 5.90276298  0.42258861 10.29514601  0.58639411 15.61451785  0.98908834]. \t  0.9855591651901744 \t 0.9863032877524404\n",
      "4      \t [ 6.06995672  0.87092503 10.57573309  0.73793331 14.93801457  0.99475319]. \t  0.9856743765033037 \t 0.9863032877524404\n",
      "5      \t [ 6.0693892   0.39411067 10.39546015  0.69220058 15.11343426  0.29380745]. \t  0.8886499012147467 \t 0.9863032877524404\n",
      "6      \t [ 6.2252745   0.41108148 10.41748693  1.         15.29648043  1.        ]. \t  \u001b[92m0.9948727297234042\u001b[0m \t 0.9948727297234042\n",
      "7      \t [ 6.253539    0.41412098 10.50647997  0.5        15.14774933  1.        ]. \t  0.9931732428038534 \t 0.9948727297234042\n",
      "8      \t [ 5.69299975  0.59731708 11.40418592  0.7941576  15.59623767  0.73297979]. \t  0.9858376038179596 \t 0.9948727297234042\n",
      "9      \t [ 4.97871401  0.4756941  10.86267748  0.82328015 15.12510208  0.97297419]. \t  0.9861208531936198 \t 0.9948727297234042\n",
      "10     \t [ 5.51122871  0.53786176 11.49045316  1.         14.6161267   0.77688299]. \t  0.9845989817186166 \t 0.9948727297234042\n",
      "11     \t [ 6.86797451  1.00905541  9.66837495  0.58271672 15.61658204  0.68425494]. \t  0.9853191198927211 \t 0.9948727297234042\n",
      "12     \t [ 5.33749608  0.         11.44996086  1.         15.18171846  1.        ]. \t  0.9948487254218334 \t 0.9948727297234042\n",
      "13     \t [ 5.59677534  0.40410316 11.05520964  0.85776361 15.10654779  0.83119654]. \t  0.9865145148198335 \t 0.9948727297234042\n",
      "14     \t [ 4.83739389  0.82748358 11.90819104  0.7745282  15.16598756  1.        ]. \t  0.9946951003809562 \t 0.9948727297234042\n",
      "15     \t [ 6.2689105   1.40916644 10.38344321  0.67035462 15.86075927  0.5487108 ]. \t  0.8875409226030149 \t 0.9948727297234042\n",
      "16     \t [ 4.78464237  0.15085382 11.57798353  0.5        14.48639852  1.        ]. \t  0.993984576649936 \t 0.9948727297234042\n",
      "17     \t [ 5.62614943  0.31648477 12.17356838  0.5        14.95378944  1.        ]. \t  0.9935477046534311 \t 0.9948727297234042\n",
      "18     \t [ 5.99098877  1.18753294  9.49830176  0.76627525 15.25621107  0.72811304]. \t  0.9847526173384926 \t 0.9948727297234042\n",
      "19     \t [ 6.74642449  1.28454952  9.95200057  0.99934377 14.9643768   0.46048519]. \t  0.8849821042972451 \t 0.9948727297234042\n",
      "20     \t [ 4.78461584  0.19679828 11.58595549  0.5        15.60975486  1.        ]. \t  0.9939125646440926 \t 0.9948727297234042\n",
      "21     \t [ 6.38856357  0.64841252  9.59722862  0.98824685 15.78771812  0.30794089]. \t  0.886657580553997 \t 0.9948727297234042\n",
      "22     \t [ 5.01440718  0.27283258 12.02445091  0.65527685 15.07016799  0.32875492]. \t  0.8883474569716591 \t 0.9948727297234042\n",
      "23     \t [ 5.18410418  0.47728125 11.5877712   0.5        15.02585859  1.        ]. \t  0.9936245171738696 \t 0.9948727297234042\n",
      "24     \t [ 4.03886861  0.22719434 11.4806231   1.         15.0140139   0.99789232]. \t  0.9849782512045989 \t 0.9948727297234042\n",
      "25     \t [ 6.54510122  0.         11.84313491  0.94007796 14.70302495  0.81428975]. \t  0.9858616126138763 \t 0.9948727297234042\n",
      "26     \t [ 6.40670635  0.87362957 12.13898914  1.         15.16169663  0.74848309]. \t  0.9852326929291982 \t 0.9948727297234042\n",
      "27     \t [ 4.63961893  0.06920731 12.28554724  1.         14.75033502  1.        ]. \t  \u001b[92m0.9949495428661369\u001b[0m \t 0.9949495428661369\n",
      "28     \t [ 6.41531923  0.06615884 12.14624897  0.82102899 15.69091191  0.81749668]. \t  0.9857463912057547 \t 0.9949495428661369\n",
      "29     \t [ 5.51995059  0.42773153 12.51247968  1.         15.65842752  0.89967119]. \t  0.9851510792718702 \t 0.9949495428661369\n",
      "30     \t [ 6.04408304  0.23789931 12.24148398  1.         15.13171123  0.47338286]. \t  0.8857310297249951 \t 0.9949495428661369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.053341897983467085"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_14 = loser_14.getResult()[0]\n",
    "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
    "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
    "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
    "\n",
    "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
    "rmse_loser_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.86771762  3.93133782 13.40798203  0.50594722 15.40798203  0.82812472]. \t  \u001b[92m0.9871530246071581\u001b[0m \t 0.9871530246071581\n",
      "2      \t [ 7.48953172  3.50387631 13.8742387   0.5        15.44248454  0.94154453]. \t  0.9870234034529592 \t 0.9871530246071581\n",
      "3      \t [ 6.68745217  3.32291849 13.98016362  0.55847271 15.17302886  0.77467989]. \t  0.9870954217508091 \t 0.9871530246071581\n",
      "4      \t [ 6.98492492  3.17376629 13.40872774  0.85131981 15.68449637  0.79069191]. \t  \u001b[92m0.9895918344044974\u001b[0m \t 0.9895918344044974\n",
      "5      \t [ 7.07978305  3.58157915 13.8058408   0.58536986 15.6007595   0.18666014]. \t  0.7995612137023583 \t 0.9895918344044974\n",
      "6      \t [ 6.9592496   3.72044786 13.89418701  1.         15.46783035  1.        ]. \t  \u001b[92m0.9945558762608239\u001b[0m \t 0.9945558762608239\n",
      "7      \t [ 6.91800804  3.43580192 13.66774814  0.5        15.53542237  1.        ]. \t  0.9925155304021396 \t 0.9945558762608239\n",
      "8      \t [ 7.18314422  2.75771494 14.45144395  0.98604925 15.78700249  0.8286495 ]. \t  0.9897070527011825 \t 0.9945558762608239\n",
      "9      \t [ 7.67100482  3.18779462 13.98384035  1.         16.42276928  0.95761399]. \t  0.991377752090513 \t 0.9945558762608239\n",
      "10     \t [ 7.53231433  4.06397415 13.23396765  0.84980462 16.24077641  0.95152856]. \t  0.9886460759854522 \t 0.9945558762608239\n",
      "11     \t [ 8.1207078   3.38367733 13.17905632  1.         15.85685433  0.88221508]. \t  0.9914161566221718 \t 0.9945558762608239\n",
      "12     \t [ 7.5288928   3.48015359 13.65667295  1.         15.90891722  0.83759592]. \t  0.9912961374651722 \t 0.9945558762608239\n",
      "13     \t [ 7.90426128  3.22453803 13.02895366  0.5        16.52521117  1.        ]. \t  0.9923715072201404 \t 0.9945558762608239\n",
      "14     \t [ 7.79550776  3.8237931  12.497398    0.5        15.70908683  0.94099587]. \t  0.9871050212588682 \t 0.9945558762608239\n",
      "15     \t [ 8.41729823  3.92677982 13.19823234  0.5        16.13791246  1.        ]. \t  0.992385909801083 \t 0.9945558762608239\n",
      "16     \t [ 8.12279532  2.58594627 13.91959266  0.5        16.08355031  1.        ]. \t  0.9924099141026538 \t 0.9945558762608239\n",
      "17     \t [ 7.23675337  2.46420741 14.04368754  0.5        16.53697919  0.80875432]. \t  0.9867305542926488 \t 0.9945558762608239\n",
      "18     \t [ 7.58057274  2.91891452 14.73503536  0.5        16.36757922  1.        ]. \t  0.9923955115217113 \t 0.9945558762608239\n",
      "19     \t [ 7.97072348  2.99632591 13.96933633  0.5        16.49590523  0.45775839]. \t  0.799753248114925 \t 0.9945558762608239\n",
      "20     \t [ 7.86328538  4.51786596 13.04680045  0.77809328 15.28705769  1.        ]. \t  0.9939749855765103 \t 0.9945558762608239\n",
      "21     \t [ 7.89593461  4.33921192 12.88665313  0.5        15.73912818  0.38405006]. \t  0.8000893010077813 \t 0.9945558762608239\n",
      "22     \t [ 7.3266348   3.82792713 12.94926445  1.         14.69905421  0.79017114]. \t  0.9914305592031143 \t 0.9945558762608239\n",
      "23     \t [ 7.42067769  2.90493333 14.07858893  0.51395345 16.16901269  1.        ]. \t  0.9924723226593121 \t 0.9945558762608239\n",
      "24     \t [ 7.86281559  2.12416949 14.52595813  1.         16.61986948  1.        ]. \t  \u001b[92m0.9946134864463083\u001b[0m \t 0.9946134864463083\n",
      "25     \t [ 6.78223958  2.80775217 14.75979335  1.         16.75583058  0.8621299 ]. \t  0.9914737648716581 \t 0.9946134864463083\n",
      "26     \t [ 7.78117568  3.81825413 13.11083548  0.62365454 15.34843205  1.        ]. \t  0.9935909137090942 \t 0.9946134864463083\n",
      "27     \t [ 8.28455152  3.97991476 12.40902409  0.96861192 16.48721096  1.        ]. \t  0.9945414743713092 \t 0.9946134864463083\n",
      "28     \t [ 7.25985831  4.34764336 12.43285407  1.         15.42088048  0.98240523]. \t  0.9914161548244592 \t 0.9946134864463083\n",
      "29     \t [ 8.08491095  2.12812386 13.34643202  1.         16.61348369  0.98358027]. \t  0.9913249426270573 \t 0.9946134864463083\n",
      "30     \t [ 8.85057707  2.9154273  13.22688354  0.8574567  16.71953883  1.        ]. \t  0.9939557833798238 \t 0.9946134864463083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05849860225050029"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_15 = loser_15.getResult()[0]\n",
    "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
    "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
    "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
    "\n",
    "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
    "rmse_loser_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 8.48805787  2.49755915  8.31841837  0.67868543 14.00001893  0.36396769]. \t  0.8844684259178325 \t 0.9857703856869544\n",
      "2      \t [ 8.56157492  1.79574967  8.75341858  0.7138917  14.58784722  0.29422982]. \t  0.8850061096890321 \t 0.9857703856869544\n",
      "3      \t [ 7.88179626  1.76206626  8.08445235  0.69262908 14.24431966  0.27670481]. \t  0.8850109099962039 \t 0.9857703856869544\n",
      "4      \t [ 8.32811056  1.71246855  8.71851384  0.5        13.65064661  0.1       ]. \t  0.8876465830001101 \t 0.9857703856869544\n",
      "5      \t [ 8.34548221  1.7566088   8.52497564  0.93275604 13.91885967  0.94617298]. \t  0.9833363977693369 \t 0.9857703856869544\n",
      "6      \t [ 8.45909157  1.8191101   8.31056726  0.5        14.16777757  0.77350482]. \t  0.9806047515339601 \t 0.9857703856869544\n",
      "7      \t [ 9.22700761  1.97850489  8.936257    0.86861877 13.73937322  0.6666441 ]. \t  0.8870896252620106 \t 0.9857703856869544\n",
      "8      \t [ 4.11252287  9.52166156 13.82333521  0.91488212 16.64481753  0.69793394]. \t  \u001b[92m0.9862600680458621\u001b[0m \t 0.9862600680458621\n",
      "9      \t [ 3.64329013  9.38923152 13.33790416  0.95934252 17.17953337  0.49372859]. \t  0.8860046869749522 \t 0.9862600680458621\n",
      "10     \t [ 3.47752616  8.97199276 13.90018664  0.87685185 16.68706984  0.99176256]. \t  0.985876009868718 \t 0.9862600680458621\n",
      "11     \t [ 3.91574979  9.22165947 14.18599799  1.         17.34332374  0.88878905]. \t  0.9684202629205055 \t 0.9862600680458621\n",
      "12     \t [ 3.5849946   9.21224385 14.21235971  0.99999377 16.82106457  0.22462946]. \t  0.8869072041860605 \t 0.9862600680458621\n",
      "13     \t [ 8.53887108  1.88531434  8.53994174  1.         13.96189582  0.38278713]. \t  0.7898155662770684 \t 0.9862600680458621\n",
      "14     \t [ 8.3623585   2.13004095  9.3643752   0.5        13.82199236  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "15     \t [ 8.72975426  1.19935588  9.17119412  0.5        13.75384599  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "16     \t [ 8.65689004  1.84132373  8.74535759  0.5        13.12059615  1.        ]. \t  0.9933700774534961 \t 0.9935237048452641\n",
      "17     \t [ 7.68035554  1.43574811  8.92050482  0.5        13.58221249  1.        ]. \t  0.9935189039849499 \t 0.9935237048452641\n",
      "18     \t [ 3.71514261  9.37366726 13.94916291  0.5        17.00269197  0.76189732]. \t  0.9837444617691932 \t 0.9935237048452641\n",
      "19     \t [ 7.60575157  2.39453578  8.4812389   0.5        13.48069854  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "20     \t [ 7.44772726  2.24793077  8.90578399  0.5        14.34133103  0.67046836]. \t  0.9813824909048554 \t 0.9935237048452641\n",
      "21     \t [ 8.05260005  1.92531319  8.80362354  0.5        13.77476542  0.9440125 ]. \t  0.9810032230783218 \t 0.9935237048452641\n",
      "22     \t [ 6.79489621  1.9564714   8.49695115  0.94817811 13.69188244  0.88153516]. \t  0.9851318915931495 \t 0.9935237048452641\n",
      "23     \t [ 7.67695312  1.3863587   7.94116524  0.82338248 13.13005915  0.99533557]. \t  0.982304202568658 \t 0.9935237048452641\n",
      "24     \t [ 8.52874402  0.78955144  8.21144346  0.68748612 13.4432987   0.95696227]. \t  0.9822513958709137 \t 0.9935237048452641\n",
      "25     \t [ 7.03031     2.29692868  8.54087778  0.5        13.45673882  0.19882899]. \t  0.8887459513869406 \t 0.9935237048452641\n",
      "26     \t [ 7.42493209  1.81644609  8.89631229  1.         12.84839638  1.        ]. \t  \u001b[92m0.9936437255234049\u001b[0m \t 0.9936437255234049\n",
      "27     \t [ 7.16050817  2.61160042  9.36583883  0.91777669 13.6170323   0.88785922]. \t  0.9847526270854696 \t 0.9936437255234049\n",
      "28     \t [ 8.23978028  1.0375371   8.94031464  1.         12.97941985  1.        ]. \t  \u001b[92m0.9937013321134643\u001b[0m \t 0.9937013321134643\n",
      "29     \t [ 7.19620089  3.01285401  8.44504743  0.97057276 14.07060279  0.69859045]. \t  0.9849494577949266 \t 0.9937013321134643\n",
      "30     \t [ 7.90250757  3.22013602  9.21184851  0.5        14.06731531  0.50346184]. \t  0.8869840700847491 \t 0.9937013321134643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06523883728737642"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_16 = loser_16.getResult()[0]\n",
    "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
    "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
    "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
    "\n",
    "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
    "rmse_loser_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.61098819  0.          5.99999995  0.99341655 14.75556096  1.        ]. \t  0.9842341302995434 \t 0.991204913651782\n",
      "2      \t [ 3.03974411  0.60245649  5.55543211  0.80741988 14.68963656  0.84693465]. \t  0.9864568961278931 \t 0.991204913651782\n",
      "3      \t [ 2.45176142  0.15874403  5.28604039  0.92880775 14.28810841  0.88207516]. \t  0.9866969228259036 \t 0.991204913651782\n",
      "4      \t [ 2.22906307  0.75200914  5.83332335  0.93678201 14.57879917  0.86893081]. \t  0.9867689357306162 \t 0.991204913651782\n",
      "5      \t [ 9.36280535  5.26377785  9.72590595  0.81320221 14.54825659  0.79597671]. \t  0.9904799919031916 \t 0.991204913651782\n",
      "6      \t [ 2.61067822  0.29197881  5.78111242  1.         14.52607958  0.22482838]. \t  0.8873777037082623 \t 0.991204913651782\n",
      "7      \t [ 9.13647957  6.09242521  9.77046821  0.94454226 14.37774373  0.71140106]. \t  \u001b[92m0.991468955161067\u001b[0m \t 0.991468955161067\n",
      "8      \t [ 9.38387735  5.72020514 10.46602284  0.91535114 14.62972155  0.89391511]. \t  0.9913057242509584 \t 0.991468955161067\n",
      "9      \t [ 9.80038904  5.92063658  9.87247169  0.56180067 14.75383541  0.63117056]. \t  0.8913863378546775 \t 0.991468955161067\n",
      "10     \t [ 9.31420798  5.5831926  10.19019741  0.70617579 14.12950496  0.27609677]. \t  0.8883473719798305 \t 0.991468955161067\n",
      "11     \t [ 2.55265903  0.31093389  5.82194886  0.5        14.4145402   0.89563966]. \t  0.9847093887815301 \t 0.991468955161067\n",
      "12     \t [ 9.1221849   5.72154351 10.02833886  0.5        14.33512769  1.        ]. \t  \u001b[92m0.9930916281084167\u001b[0m \t 0.9930916281084167\n",
      "13     \t [ 6.33438621  7.41799776 12.18258806  0.64912586 17.72091333  0.62036436]. \t  0.8884433908455408 \t 0.9930916281084167\n",
      "14     \t [ 6.79208916  7.19384429 12.8479075   0.5400177  17.16767124  0.65811013]. \t  0.8913191158537178 \t 0.9930916281084167\n",
      "15     \t [ 7.08307783  6.88792699 12.45964948  0.61516953 17.97764039  0.79646419]. \t  0.9904415872332474 \t 0.9930916281084167\n",
      "16     \t [ 7.01105471  7.61015569 12.66588099  1.         17.81147333  1.        ]. \t  \u001b[92m0.9944166526945688\u001b[0m \t 0.9944166526945688\n",
      "17     \t [ 7.12555237  7.58355812 12.61020092  0.50098728 17.90033834  0.29501951]. \t  0.8951453583790104 \t 0.9944166526945688\n",
      "18     \t [ 6.7469555   7.06617267 12.74135771  1.         17.81605074  0.41034413]. \t  0.8873777037082623 \t 0.9944166526945688\n",
      "19     \t [ 6.83725518  7.36734365 12.57560251  0.5        17.77819923  1.        ]. \t  0.9930628232231028 \t 0.9944166526945688\n",
      "20     \t [ 6.82913702  7.45733976 12.38519154  0.89534265 18.80053192  0.85235205]. \t  0.9916273823068645 \t 0.9944166526945688\n",
      "21     \t [ 7.41311348  7.27118321 13.1076469   0.81755857 18.63437251  0.97841872]. \t  0.9906096237053797 \t 0.9944166526945688\n",
      "22     \t [ 6.8699726   8.08432741 13.16808843  0.97745919 18.72664321  0.84707087]. \t  0.9915937775292353 \t 0.9944166526945688\n",
      "23     \t [ 6.95421559  7.52885211 12.83242028  0.82559356 18.40127143  0.81848239]. \t  0.9905856101386754 \t 0.9944166526945688\n",
      "24     \t [ 7.36014337  7.87831606 12.91498794  1.         19.61494178  1.        ]. \t  0.9939173704127451 \t 0.9944166526945688\n",
      "25     \t [ 7.85065589  7.62324511 12.28754672  1.         18.90662706  1.        ]. \t  0.9939701798762011 \t 0.9944166526945688\n",
      "26     \t [ 7.128398    8.504108   12.3597365   1.         19.07543577  1.        ]. \t  0.9942390200332311 \t 0.9944166526945688\n",
      "27     \t [ 6.2508699   8.37502945 13.10653747  0.71742169 17.53390133  0.69114265]. \t  0.9912097093955299 \t 0.9944166526945688\n",
      "28     \t [ 7.8365926   8.35241484 13.16749236  1.         18.87073118  1.        ]. \t  0.9939701798762011 \t 0.9944166526945688\n",
      "29     \t [ 7.47330008  7.9778902  12.74109892  0.5        19.1142437   1.        ]. \t  0.9928035831272743 \t 0.9944166526945688\n",
      "30     \t [ 8.28791286  7.20188633 12.89176478  0.76916514 17.93821155  1.        ]. \t  0.993432489467294 \t 0.9944166526945688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06305140594321365"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_17 = loser_17.getResult()[0]\n",
    "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
    "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
    "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
    "\n",
    "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
    "rmse_loser_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8596968089427542 \t 0.9883196147183771\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.9878107251845023 \t 0.9883196147183771\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8627837722196116 \t 0.9883196147183771\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9856023554376728 \t 0.9883196147183771\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9883196147183771 \t 0.9883196147183771\n",
      "1      \t [ 3.36304015  2.54200031 10.59349774  0.76424639 13.88376601  0.99173224]. \t  0.9882956156716581 \t 0.9883196147183771\n",
      "2      \t [ 3.72872634  1.74869111 10.57828719  0.77525317 13.69728711  0.99980498]. \t  \u001b[92m0.9883628279926278\u001b[0m \t 0.9883628279926278\n",
      "3      \t [ 3.4516628   2.03810652 10.74267026  0.75440905 14.01753987  0.29442374]. \t  0.8639983806139625 \t 0.9883628279926278\n",
      "4      \t [ 3.63164744  2.20995818 11.32533713  0.73058937 13.62104733  1.        ]. \t  \u001b[92m0.9947095060034464\u001b[0m \t 0.9947095060034464\n",
      "5      \t [ 3.13247284  2.08218497 10.77369717  0.63562099 13.21795148  0.83488729]. \t  0.9877099109899007 \t 0.9947095060034464\n",
      "6      \t [ 3.33812785  2.09159152 10.86960361  1.         13.61514576  0.956039  ]. \t  0.986908182805419 \t 0.9947095060034464\n",
      "7      \t [ 3.40855554  2.07722341 10.84778798  0.5        13.76245194  1.        ]. \t  0.9941430096029409 \t 0.9947095060034464\n",
      "8      \t [ 4.16485525  2.61125289 10.58159602  0.6670012  13.09855907  0.68334383]. \t  0.9875322867639832 \t 0.9947095060034464\n",
      "9      \t [ 3.45922496  3.1569097  11.11756165  0.57260863 13.19059045  0.53242551]. \t  0.8633646670524923 \t 0.9947095060034464\n",
      "10     \t [ 4.00071767  1.95642515 11.24242519  0.55000844 12.78946893  0.3985917 ]. \t  0.8626541375134261 \t 0.9947095060034464\n",
      "11     \t [ 2.46155812  2.61795016 11.44364045  0.64473299 13.96502858  0.67795254]. \t  0.9875850847497363 \t 0.9947095060034464\n",
      "12     \t [ 2.1126979   2.32251128 10.55213937  0.66993638 14.07932175  0.67769443]. \t  0.9890349406926191 \t 0.9947095060034464\n",
      "13     \t [ 2.42320668  2.4502324  10.98632029  0.87569266 14.81932064  0.87037673]. \t  0.9887084877226786 \t 0.9947095060034464\n",
      "14     \t [ 2.0083133   1.81749065 11.28831885  0.73667519 14.45107172  0.55814876]. \t  0.8631534343152353 \t 0.9947095060034464\n",
      "15     \t [ 4.76205859  1.86056563 10.92506904  0.80998002 13.41545654  0.92518772]. \t  0.98830041473426 \t 0.9947095060034464\n",
      "16     \t [ 4.33989449  1.63670175 10.43461646  0.67380329 12.81451582  1.        ]. \t  0.9945798816686793 \t 0.9947095060034464\n",
      "17     \t [ 4.48089283  1.66867065 10.30060073  0.70775112 13.31622757  0.3735714 ]. \t  0.8624332993218302 \t 0.9947095060034464\n",
      "18     \t [ 2.55609676  2.5262809  10.90737194  1.         14.20946713  0.44061691]. \t  0.7918366936213684 \t 0.9947095060034464\n",
      "19     \t [ 2.30468733  2.30795093 11.01249687  0.5        14.31051252  1.        ]. \t  0.9942102209559113 \t 0.9947095060034464\n",
      "20     \t [ 2.39995182  1.74009869 10.17839199  0.87390519 14.82218602  0.97644738]. \t  0.9886172711001383 \t 0.9947095060034464\n",
      "21     \t [ 3.07570002  1.74241354 10.87393068  0.87195293 15.2947021   1.        ]. \t  \u001b[92m0.9949015361291602\u001b[0m \t 0.9949015361291602\n",
      "22     \t [ 2.62269703  1.93071315 10.59396151  0.5        15.02611886  0.54967814]. \t  0.8630190051098722 \t 0.9949015361291602\n",
      "23     \t [ 2.99602875  2.18433634 11.91056629  0.82193572 14.9266354   1.        ]. \t  0.9947335082307337 \t 0.9949015361291602\n",
      "24     \t [ 3.7044705   2.451783   11.24861499  0.91167609 14.89788117  1.        ]. \t  \u001b[92m0.9949447438719878\u001b[0m \t 0.9949447438719878\n",
      "25     \t [ 3.25217982  3.11925483 11.81114833  0.5        14.397379    0.9765806 ]. \t  0.9875082743035635 \t 0.9949447438719878\n",
      "26     \t [ 2.10976328  1.48769969 10.00058942  0.70107753 13.73035636  0.84294058]. \t  0.9883340210330304 \t 0.9949447438719878\n",
      "27     \t [ 2.57355382  2.10369785  9.61896457  0.64980729 13.82217701  1.        ]. \t  0.9936581263066347 \t 0.9949447438719878\n",
      "28     \t [ 3.36178321  2.56145079 11.8706288   0.5        14.32246218  0.3746966 ]. \t  0.8634030739350056 \t 0.9949447438719878\n",
      "29     \t [ 3.08834687  2.66270245 11.41107983  0.5        15.40042012  1.        ]. \t  0.9941478161329637 \t 0.9949447438719878\n",
      "30     \t [ 3.0621332   2.4080114  11.3399099   0.66175872 14.74235709  1.        ]. \t  0.9944166547688523 \t 0.9949447438719878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.055682260635581474"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_18 = loser_18.getResult()[0]\n",
    "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
    "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
    "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
    "\n",
    "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
    "rmse_loser_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.48286332  0.44976506 10.81683489  0.82470355  4.27227938  0.75785374]. \t  0.9873018619268663 \t 0.9874362876751138\n",
      "2      \t [ 1.27876751  0.62889541 10.50308127  0.74925252  4.09690348  0.75233918]. \t  0.9872970641780224 \t 0.9874362876751138\n",
      "3      \t [ 0.64610537  0.97350837 10.17345905  0.91184411  4.37191674  0.99887846]. \t  0.9873930794482798 \t 0.9874362876751138\n",
      "4      \t [ 0.93239818  0.28828379 10.27713922  1.          4.75597017  0.74057459]. \t  \u001b[92m0.9905904062986123\u001b[0m \t 0.9905904062986123\n",
      "5      \t [ 0.71888918  0.7023983  10.27234863  0.5         4.44812472  0.28060729]. \t  0.8908535177409543 \t 0.9905904062986123\n",
      "6      \t [ 0.81835872  0.40498455 10.37295151  0.5         4.44031975  1.        ]. \t  \u001b[92m0.9954152241040101\u001b[0m \t 0.9954152241040101\n",
      "7      \t [ 0.80231678  0.51556509 10.36365048  1.          4.23032642  0.76272639]. \t  0.9906912236046282 \t 0.9954152241040101\n",
      "8      \t [1.42686097 0.66780067 9.39634647 0.68648541 4.42660603 0.84688987]. \t  0.986077652226883 \t 0.9954152241040101\n",
      "9      \t [0.49862677 0.4452121  9.20142951 0.81801462 4.8048822  0.89975646]. \t  0.9864761195534758 \t 0.9954152241040101\n",
      "10     \t [0.60787974 0.90994641 9.11647698 0.5        3.98514243 1.        ]. \t  0.994834324085479 \t 0.9954152241040101\n",
      "11     \t [ 1.06404297  0.88955211 11.41683419  0.80640527  4.82635066  0.75553143]. \t  0.9872730566958375 \t 0.9954152241040101\n",
      "12     \t [0.84910787 0.06025809 8.95167559 0.5        4.1085203  0.68403868]. \t  0.9846950108867745 \t 0.9954152241040101\n",
      "13     \t [0.80400154 0.50312979 9.44300706 0.63457863 4.29807283 0.83559347]. \t  0.9862504831981932 \t 0.9954152241040101\n",
      "14     \t [0.95700125 0.68614281 8.33903783 1.         4.33600641 1.        ]. \t  0.9949063359530347 \t 0.9954152241040101\n",
      "15     \t [ 1.48627182  1.16718961 10.4866141   0.74629322  5.09625638  0.89588624]. \t  0.9867977774019564 \t 0.9954152241040101\n",
      "16     \t [0.05858551 0.24664536 8.70718306 0.98703291 3.89817033 1.        ]. \t  0.9951271728308789 \t 0.9954152241040101\n",
      "17     \t [1.00029361 0.41032731 8.81441728 0.90169123 3.33156809 1.        ]. \t  0.9950071546419262 \t 0.9954152241040101\n",
      "18     \t [0.58363828 0.65661452 8.63359696 0.92549362 3.76127783 0.34763552]. \t  0.8907862976758771 \t 0.9954152241040101\n",
      "19     \t [0.57322617 0.47051812 8.1682425  0.5        3.65237901 1.        ]. \t  0.9945990859404241 \t 0.9954152241040101\n",
      "20     \t [0.20536104 0.1228765  9.21687953 0.5        3.09174332 0.82189927]. \t  0.9863032907256231 \t 0.9954152241040101\n",
      "21     \t [ 0.5255674   0.95228859 10.7463946   0.75600143  5.32909215  0.87269521]. \t  0.9871194344207117 \t 0.9954152241040101\n",
      "22     \t [1.04440085 1.26982112 9.28262805 1.         5.28488747 1.        ]. \t  \u001b[92m0.9956216586083432\u001b[0m \t 0.9956216586083432\n",
      "23     \t [1.46109095 0.41589746 9.12856088 1.         5.4611314  1.        ]. \t  0.9955112404114242 \t 0.9956216586083432\n",
      "24     \t [1.31104618 0.75912566 9.04303732 1.         5.26535928 0.27575252]. \t  0.8897829077060798 \t 0.9956216586083432\n",
      "25     \t [1.20933534 0.81837101 8.583406   0.5        5.19946127 1.        ]. \t  0.993802150526657 \t 0.9956216586083432\n",
      "26     \t [1.0876281  0.69972075 9.71101515 0.50933651 5.64476533 0.80657776]. \t  0.9860296439694602 \t 0.9956216586083432\n",
      "27     \t [ 0.80415864  1.36652409 11.00476881  0.5         4.53522553  0.89480667]. \t  0.9869129965974771 \t 0.9956216586083432\n",
      "28     \t [ 0.96552744  0.8387879  10.73390774  0.68845447  4.80598973  0.78751794]. \t  0.9871338370707979 \t 0.9956216586083432\n",
      "29     \t [ 1.58955563  1.64441538 11.50321443  1.          4.45164343  1.        ]. \t  \u001b[92m0.9967978582531654\u001b[0m \t 0.9967978582531654\n",
      "30     \t [ 0.97564956  1.78465176 11.5025357   1.          5.24658185  1.        ]. \t  \u001b[92m0.9969802886633587\u001b[0m \t 0.9969802886633587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05273921155674804"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_19 = loser_19.getResult()[0]\n",
    "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
    "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
    "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
    "\n",
    "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
    "rmse_loser_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.44423671  5.70594793 11.64407075  0.52149642 17.06778273  0.7049659 ]. \t  0.9912625141572762 \t 0.9913825280594238\n",
      "2      \t [ 5.45180339  5.13144524 12.22660653  0.5        16.60706141  0.73874764]. \t  0.9912241031261956 \t 0.9913825280594238\n",
      "3      \t [ 6.01079364  5.74447328 12.0577313   0.5        16.50953391  0.96839561]. \t  0.9908448505110751 \t 0.9913825280594238\n",
      "4      \t [ 5.25977667  5.91628491 12.41554483  0.5        16.73767644  1.        ]. \t  \u001b[92m0.9933268609986774\u001b[0m \t 0.9933268609986774\n",
      "5      \t [ 5.59300638  5.82061959 12.24771225  0.70431615 16.58618292  0.2564824 ]. \t  0.8891443864239102 \t 0.9933268609986774\n",
      "6      \t [ 5.42829202  5.58745507 11.95941969  1.         16.56064108  1.        ]. \t  \u001b[92m0.9949495376797377\u001b[0m \t 0.9949495376797377\n",
      "7      \t [ 5.84950782  5.50056017 11.85651343  0.60889182 18.20042994  0.87363636]. \t  0.9912289068905068 \t 0.9949495376797377\n",
      "8      \t [ 6.06367807  5.15979945 11.07060611  0.5        18.10490765  0.64063878]. \t  0.888083413303605 \t 0.9949495376797377\n",
      "9      \t [ 5.14916918  5.16990369 11.39746218  0.5        18.35194028  0.79809715]. \t  0.991094483493138 \t 0.9949495376797377\n",
      "10     \t [ 5.53005193  5.42870016 11.59641828  0.5        18.34020036  0.1       ]. \t  0.8879537900751223 \t 0.9949495376797377\n",
      "11     \t [ 5.4751131   5.9099154  11.20540982  0.5        18.21498234  0.87837036]. \t  0.9911760964590518 \t 0.9949495376797377\n",
      "12     \t [ 5.53460754  5.45842381 11.2923197   1.         18.30620791  0.70635438]. \t  0.991334532455126 \t 0.9949495376797377\n",
      "13     \t [1.02906448 5.6058813  6.24956611 0.6214823  8.73387925 0.70493476]. \t  0.9910608800983648 \t 0.9949495376797377\n",
      "14     \t [1.28177756 6.05793241 6.59379038 0.66860407 8.10094577 0.75521313]. \t  0.9916273835514345 \t 0.9949495376797377\n",
      "15     \t [0.59535767 6.20175622 6.57065721 0.50168502 8.54884102 1.        ]. \t  0.9889245184162528 \t 0.9949495376797377\n",
      "16     \t [1.19299975 6.31381048 5.93054323 0.50153685 8.51917704 0.99778065]. \t  0.9867353778317972 \t 0.9949495376797377\n",
      "17     \t [0.79861129 6.26604949 6.17341589 0.81516283 8.4292869  0.32118032]. \t  0.8894612408537906 \t 0.9949495376797377\n",
      "18     \t [0.94755813 5.97209365 6.25913847 1.         8.41413552 1.        ]. \t  0.9895486263850987 \t 0.9949495376797377\n",
      "19     \t [ 5.64593757  5.56870065 12.81850904  0.7556384  15.91817392  1.        ]. \t  0.9945318632472618 \t 0.9949495376797377\n",
      "20     \t [0.97297134 5.96623663 6.28591547 0.5        8.3592232  0.85265389]. \t  0.9906768031143823 \t 0.9949495376797377\n",
      "21     \t [0.78697614 6.8519695  6.30673556 0.74360914 7.60741911 1.        ]. \t  0.9891165514459638 \t 0.9949495376797377\n",
      "22     \t [1.31450896 7.08276955 6.82220894 0.78209076 8.41241127 1.        ]. \t  0.9891213527211348 \t 0.9949495376797377\n",
      "23     \t [1.47397377 6.25951    6.76485251 0.7877452  9.31012248 0.73987021]. \t  0.9911857024665331 \t 0.9949495376797377\n",
      "24     \t [0.78679491 5.99084115 6.06002527 0.68702297 9.70159299 0.88784922]. \t  0.991896219421612 \t 0.9949495376797377\n",
      "25     \t [0.85106254 6.95680459 6.33281744 0.69690279 9.29228726 1.        ]. \t  0.9890733441179931 \t 0.9949495376797377\n",
      "26     \t [1.22093193 6.13021911 5.77625663 0.91084253 7.1348939  0.78894543]. \t  0.9868697954211719 \t 0.9949495376797377\n",
      "27     \t [0.57544985 5.94296308 6.44369894 0.94603585 7.03605727 0.76216087]. \t  0.9921746629586972 \t 0.9949495376797377\n",
      "28     \t [0.27871041 6.16670801 5.58941187 0.83584382 7.10486616 1.        ]. \t  0.9844789661550025 \t 0.9949495376797377\n",
      "29     \t [0.60164951 6.21567783 5.90989848 0.5        7.07461521 0.38121233]. \t  0.887924968733825 \t 0.9949495376797377\n",
      "30     \t [0.56294287 6.05297742 6.89253476 0.80479187 9.49265778 0.64798313]. \t  0.8901189490368416 \t 0.9949495376797377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06258804691391658"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_20 = loser_20.getResult()[0]\n",
    "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
    "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
    "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
    "\n",
    "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
    "rmse_loser_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.27108909 7.02401602 6.52266625 0.87317851 5.47733257 0.74417218]. \t  0.9779162613257566 \t 0.9846806025668978\n",
      "2      \t [3.73720652 7.19598031 7.38156205 0.79284175 4.88163343 0.67031263]. \t  0.9822369876918661 \t 0.9846806025668978\n",
      "3      \t [4.45042438 6.70829193 7.57960408 0.87757424 5.3039096  0.86753454]. \t  0.9821745766460696 \t 0.9846806025668978\n",
      "4      \t [3.63951015 6.85586092 7.32939549 1.         5.67993611 0.99727376]. \t  \u001b[92m0.9857319807424209\u001b[0m \t 0.9857319807424209\n",
      "5      \t [3.87774961 6.5917282  7.23797354 0.51060666 5.40056228 0.3828689 ]. \t  0.8879922096263435 \t 0.9857319807424209\n",
      "6      \t [4.01488678 6.9173958  7.14785297 0.53594996 5.19715364 1.        ]. \t  \u001b[92m0.9914785549468784\u001b[0m \t 0.9914785549468784\n",
      "7      \t [3.98886576 6.81776821 7.20302229 1.         5.18920402 0.75595385]. \t  0.9857463831850758 \t 0.9914785549468784\n",
      "8      \t [3.55238939 6.46985766 8.03498773 0.70964656 4.98589807 1.        ]. \t  \u001b[92m0.9944550593704174\u001b[0m \t 0.9944550593704174\n",
      "9      \t [4.437748   7.71708547 7.0755278  0.69358442 5.98838444 0.60519369]. \t  0.8869984307115852 \t 0.9944550593704174\n",
      "10     \t [4.76924344 6.98375229 6.4841593  0.78407988 6.33234702 0.93109624]. \t  0.9771289321269637 \t 0.9944550593704174\n",
      "11     \t [3.99945073 7.39118386 6.1798027  0.71438377 6.29422132 0.8059109 ]. \t  0.9776618181491381 \t 0.9944550593704174\n",
      "12     \t [4.50866238 7.13670001 6.26986689 0.5        6.1538636  0.26006849]. \t  0.88573100441837 \t 0.9944550593704174\n",
      "13     \t [2.92867365 6.62296903 7.30916387 0.69387692 4.80926245 0.9459963 ]. \t  0.982088162889009 \t 0.9944550593704174\n",
      "14     \t [3.55653867 6.58755437 7.49767109 0.73492702 5.05725525 0.9693296 ]. \t  0.9820449550078939 \t 0.9944550593704174\n",
      "15     \t [2.91220852 7.09349272 8.25458641 0.72069961 4.62652263 0.64938494]. \t  0.8876417466851216 \t 0.9944550593704174\n",
      "16     \t [3.9332337  7.14189946 8.52587097 0.65380019 5.28495614 0.76183216]. \t  0.9832691605592502 \t 0.9944550593704174\n",
      "17     \t [4.1930706  6.88027609 6.83604698 0.67807992 6.05423365 0.99706948]. \t  0.9777722375906451 \t 0.9944550593704174\n",
      "18     \t [3.03893466 6.62962656 6.30886869 0.93651328 5.13479163 0.89971786]. \t  0.9780074767728566 \t 0.9944550593704174\n",
      "19     \t [2.92323183 7.45732522 6.88909996 0.67603578 5.32649949 0.70593929]. \t  0.9778490505259464 \t 0.9944550593704174\n",
      "20     \t [4.11367856 6.62215021 8.50475911 0.52555285 4.57705351 0.62989003]. \t  0.8881938457595385 \t 0.9944550593704174\n",
      "21     \t [3.74539151 6.44478189 8.57212836 1.         5.28194714 0.52464392]. \t  0.8976034336528483 \t 0.9944550593704174\n",
      "22     \t [3.10286818 6.93450239 6.57829605 0.5        4.5344411  0.57941737]. \t  0.8864511334654953 \t 0.9944550593704174\n",
      "23     \t [2.89314737 7.18414187 6.88741632 1.         4.78265743 1.        ]. \t  0.9894766141732538 \t 0.9944550593704174\n",
      "24     \t [4.60743799 7.48751453 6.29083541 0.5        6.03249116 1.        ]. \t  0.9886220698877363 \t 0.9944550593704174\n",
      "25     \t [2.7214853  6.84226207 7.0288167  1.         5.02786119 0.43658917]. \t  0.898241948005489 \t 0.9944550593704174\n",
      "26     \t [3.37373368 7.05839282 6.7408682  0.8004375  5.28586252 0.89346298]. \t  0.97806988629749 \t 0.9944550593704174\n",
      "27     \t [3.29951069 7.38006174 7.85710313 0.66186804 5.39013399 0.81785776]. \t  0.9820737608612168 \t 0.9944550593704174\n",
      "28     \t [3.03388806 6.0268144  6.47884102 1.         4.43387403 1.        ]. \t  0.9894766141732538 \t 0.9944550593704174\n",
      "29     \t [3.86110367 6.92780269 8.05472541 0.6636386  5.1698121  0.76058406]. \t  0.9833219701609935 \t 0.9944550593704174\n",
      "30     \t [3.62241339 6.86639023 8.83248116 0.98913088 4.87277538 1.        ]. \t  \u001b[92m0.9948391248766494\u001b[0m \t 0.9948391248766494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06522339577276871"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_1 = winner_1.getResult()[0]\n",
    "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
    "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
    "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
    "\n",
    "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
    "rmse_winner_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 3.54093606  8.41671899  6.00000265  0.95081569 12.69216968  0.79662408]. \t  0.9907200188777733 \t 0.9907392220424587\n",
      "2      \t [ 3.25814479  9.04690956  6.50202839  0.91440652 12.07713796  0.90619792]. \t  \u001b[92m0.9907488244545148\u001b[0m \t 0.9907488244545148\n",
      "3      \t [ 3.18249765  8.12885701  6.77638484  0.87327224 12.27092604  0.69418203]. \t  \u001b[92m0.9909648609560833\u001b[0m \t 0.9909648609560833\n",
      "4      \t [ 2.73412602  8.6839011   6.42157092  0.86547304 12.78199423  0.83649622]. \t  0.9901631313887459 \t 0.9909648609560833\n",
      "5      \t [ 3.13984443  8.67420308  6.29764196  1.         12.35538611  0.15726238]. \t  0.8870752445301892 \t 0.9909648609560833\n",
      "6      \t [ 3.14176576  8.49685518  6.23311117  0.5        12.28688078  0.89611753]. \t  0.9878347423466316 \t 0.9909648609560833\n",
      "7      \t [ 3.08407926  8.47607804  6.31459844  1.         12.3178663   0.96344741]. \t  \u001b[92m0.9910800922516126\u001b[0m \t 0.9910800922516126\n",
      "8      \t [ 3.77700594  8.83825729  6.90226704  0.70012361 12.95206116  0.75597223]. \t  0.9900287133845144 \t 0.9910800922516126\n",
      "9      \t [ 2.2773517   8.7210942   6.85229995  0.63474419 11.79622287  0.5812103 ]. \t  0.8960528313302772 \t 0.9910800922516126\n",
      "10     \t [ 3.07383085  9.00098472  7.37064401  0.55074809 12.42096603  0.6693669 ]. \t  0.9899230815970442 \t 0.9910800922516126\n",
      "11     \t [ 3.42328916  8.03274588  6.5527251   0.52714471 13.33631642  0.45119047]. \t  0.8901141888324865 \t 0.9910800922516126\n",
      "12     \t [ 3.20114899  8.66591281  6.72656886  0.67421571 12.57378185  0.63953613]. \t  0.894569409191441 \t 0.9910800922516126\n",
      "13     \t [ 3.74602301  9.34710502  7.70172179  0.89855361 12.32052981  1.        ]. \t  \u001b[92m0.9924051103383426\u001b[0m \t 0.9924051103383426\n",
      "14     \t [ 3.09588789  8.8235473   7.63215536  0.72427701 11.60844969  1.        ]. \t  0.9916802024183097 \t 0.9924051103383426\n",
      "15     \t [ 2.71345891  9.57849641  7.4520608   0.92530608 12.14657623  1.        ]. \t  0.9924003085100294 \t 0.9924051103383426\n",
      "16     \t [ 2.74599114  7.70974024  5.88929719  0.76377316 12.87860279  0.58861588]. \t  0.896446506162892 \t 0.9924051103383426\n",
      "17     \t [ 4.3033932   8.06000642  6.98200591  0.88455479 13.126277    1.        ]. \t  0.9887660987378766 \t 0.9924051103383426\n",
      "18     \t [ 3.57549621  8.55521245  7.80874498  0.93025848 12.98347518  1.        ]. \t  0.9923859064822292 \t 0.9924051103383426\n",
      "19     \t [ 3.84933289  8.54053612  6.7351474   1.         13.76083519  1.        ]. \t  0.9885932676282808 \t 0.9924051103383426\n",
      "20     \t [ 2.96909803  8.90790755  7.66619293  1.         12.3071441   1.        ]. \t  \u001b[92m0.9925587366238258\u001b[0m \t 0.9925587366238258\n",
      "21     \t [ 4.24392132  8.858016    7.49334521  0.59336642 13.407934    1.        ]. \t  0.9918242149523194 \t 0.9925587366238258\n",
      "22     \t [ 2.6520241   8.89681872  5.45773466  0.76175427 12.767326    0.74873298]. \t  0.9851222898725213 \t 0.9925587366238258\n",
      "23     \t [ 3.09875037  9.31779281  7.39170623  0.9598627  11.70325813  0.43072169]. \t  0.8933787691444112 \t 0.9925587366238258\n",
      "24     \t [ 4.13806393  8.50980062  7.29086086  1.         13.34016623  0.50136173]. \t  0.8870752445301892 \t 0.9925587366238258\n",
      "25     \t [ 3.89251046  8.58325383  7.5816307   0.5        12.45135634  1.        ]. \t  0.9902783539722839 \t 0.9925587366238258\n",
      "26     \t [ 3.12026284  9.31373797  7.40963611  0.5086054  12.03684595  1.        ]. \t  0.9907056171265441 \t 0.9925587366238258\n",
      "27     \t [ 3.3322943   8.2032598   6.7121675   0.73806559 11.29131047  0.69802793]. \t  0.9899999065632027 \t 0.9925587366238258\n",
      "28     \t [ 3.38387972  9.44282421  7.52952036  0.79784019 13.15174862  1.        ]. \t  0.9918290194080585 \t 0.9925587366238258\n",
      "29     \t [ 2.35213471  9.50171511  6.20812115  0.96819246 12.38276903  0.79849991]. \t  0.9895966307013874 \t 0.9925587366238258\n",
      "30     \t [ 3.93794243  8.82683057  7.31350274  0.89112846 13.04377316  1.        ]. \t  0.9920690585517709 \t 0.9925587366238258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06991622358732306"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_2 = winner_2.getResult()[0]\n",
    "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
    "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
    "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
    "\n",
    "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
    "rmse_winner_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [6.2238357  4.86747812 9.09892109 0.7157897  9.45053191 0.49171647]. \t  0.8853181522116434 \t 0.9850406749037081\n",
      "2      \t [6.91036171 4.37931307 9.34945508 0.64079469 8.7090407  0.43288341]. \t  0.8845163992739105 \t 0.9850406749037081\n",
      "3      \t [5.882403   4.04958462 9.2020446  0.53922125 8.82063678 0.59820433]. \t  0.8855677910707587 \t 0.9850406749037081\n",
      "4      \t [6.52151911 3.97287987 9.58085647 0.59966524 9.58934856 0.54050123]. \t  0.8840603145708817 \t 0.9850406749037081\n",
      "5      \t [6.19125701 4.38174201 9.65454952 1.         9.03725746 0.1       ]. \t  0.7999788745413382 \t 0.9850406749037081\n",
      "6      \t [6.42887283 4.35143455 9.33178464 1.         9.08045268 1.        ]. \t  \u001b[92m0.9944742616362299\u001b[0m \t 0.9944742616362299\n",
      "7      \t [6.39252604 4.48695882 9.43962756 0.5        9.09034345 0.80136774]. \t  0.983854874367983 \t 0.9944742616362299\n",
      "8      \t [6.53937452 3.58892895 9.97497165 0.79567654 8.58216572 0.92687372]. \t  0.9865577300301897 \t 0.9944742616362299\n",
      "9      \t [ 7.13374146  4.13260887 10.27806876  1.          9.01181639  0.95048577]. \t  0.9665671279398363 \t 0.9944742616362299\n",
      "10     \t [6.59566819 4.06895227 9.84186534 0.87726533 8.92317366 0.79782478]. \t  0.9859960360803791 \t 0.9944742616362299\n",
      "11     \t [ 6.9244984   3.88027103 10.82305967  0.5         8.37191066  1.        ]. \t  0.9939941835563482 \t 0.9944742616362299\n",
      "12     \t [ 7.50936379  3.51242242 10.19528873  0.5         8.44232151  1.        ]. \t  0.9940325898857112 \t 0.9944742616362299\n",
      "13     \t [ 7.01340998  3.36193417 10.63988777  0.5         9.00910019  1.        ]. \t  0.9939845823197263 \t 0.9944742616362299\n",
      "14     \t [ 7.19929913  3.32468885 10.67712424  1.          8.41234305  1.        ]. \t  \u001b[92m0.9955160393357124\u001b[0m \t 0.9955160393357124\n",
      "15     \t [ 7.12339053  3.57863313 10.49053812  0.54846265  8.56415872  0.40866949]. \t  0.8853229413866582 \t 0.9955160393357124\n",
      "16     \t [ 7.04361473  3.6640386  10.41317892  0.64918488  8.60596325  1.        ]. \t  0.9943302388691313 \t 0.9955160393357124\n",
      "17     \t [ 7.85997991  3.73477598 11.11713002  0.6317967   8.78699101  0.99157801]. \t  0.9850502733054348 \t 0.9955160393357124\n",
      "18     \t [ 6.86473549  4.07766808 11.42557312  0.88011213  9.08320455  0.92583004]. \t  0.9853095232887328 \t 0.9955160393357124\n",
      "19     \t [ 5.91452316  3.54302071 10.93482466  0.79804336  8.71126257  0.9352126 ]. \t  0.9859432286220927 \t 0.9955160393357124\n",
      "20     \t [ 6.84190528  3.32570922 11.54195704  0.54343255  8.5808834   0.93646298]. \t  0.9854199337415479 \t 0.9955160393357124\n",
      "21     \t [ 8.1078877   3.41722949 10.08865721  0.94564941  9.03269166  0.8601647 ]. \t  0.985679185937447 \t 0.9955160393357124\n",
      "22     \t [ 8.22523216  3.99043673 10.19186347  0.96428053  8.36424576  0.83142168]. \t  0.9870282078414624 \t 0.9955160393357124\n",
      "23     \t [ 8.13179175  4.16590473 10.03796438  0.5         9.03768628  0.79475478]. \t  0.9830435383092994 \t 0.9955160393357124\n",
      "24     \t [ 6.68042078  3.65358027 11.11031916  0.77700721  8.74202289  0.94672252]. \t  0.985391137499211 \t 0.9955160393357124\n",
      "25     \t [ 5.6572479   3.40719939 10.17210947  0.50284424  8.73850576  1.        ]. \t  0.9940709964225056 \t 0.9955160393357124\n",
      "26     \t [ 6.32383103  2.88294005 10.68884646  0.5         8.17701882  1.        ]. \t  0.9940085861372907 \t 0.9955160393357124\n",
      "27     \t [ 5.74903756  3.73058381 10.41418351  0.61979828  8.02345565  0.91711546]. \t  0.9864473119024146 \t 0.9955160393357124\n",
      "28     \t [ 7.33614506  4.52337279 11.01699516  0.5         9.39541234  0.7687001 ]. \t  0.9834564089082724 \t 0.9955160393357124\n",
      "29     \t [ 7.85399857  3.9131541  10.41857372  0.82881978  8.8806308   0.7941003 ]. \t  0.9855063517855229 \t 0.9955160393357124\n",
      "30     \t [7.9866006  4.01639319 9.41293945 1.         8.83111907 1.        ]. \t  0.9944406565820437 \t 0.9955160393357124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06342555917713914"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_3 = winner_3.getResult()[0]\n",
    "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
    "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
    "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
    "\n",
    "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
    "rmse_winner_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [ 2.42723676  3.53561946 10.77165053  0.83416106  1.77238319  0.88842918]. \t  \u001b[92m0.9680409811271288\u001b[0m \t 0.9680409811271288\n",
      "2      \t [ 2.46072253  4.47380761 10.63998971  0.78694177  1.3382534   0.99309283]. \t  0.9680169801444117 \t 0.9680409811271288\n",
      "3      \t [2.27603925 4.06718187 9.97692488 0.68539129 1.92363053 0.9886658 ]. \t  0.9667351559719517 \t 0.9680409811271288\n",
      "4      \t [ 3.03325943  3.92462647 10.22453129  0.84461234  1.51997485  0.73352493]. \t  0.9675464990141914 \t 0.9680409811271288\n",
      "5      \t [ 2.23246791  4.00689243 10.35134749  0.98255908  1.48912867  0.34575216]. \t  0.7991147465536992 \t 0.9680409811271288\n",
      "6      \t [ 2.52769247  3.86554949 10.39261399  0.5         1.43265666  1.        ]. \t  \u001b[92m0.9959529140282596\u001b[0m \t 0.9959529140282596\n",
      "7      \t [ 2.52025443  3.93092221 10.31022517  1.          1.54021522  1.        ]. \t  \u001b[92m0.9963897885831384\u001b[0m \t 0.9963897885831384\n",
      "8      \t [ 2.80384758  4.33209008 10.89402121  0.60929052  2.39061507  0.79472873]. \t  0.9677001215659642 \t 0.9963897885831384\n",
      "9      \t [2.61395052 4.60935927 9.48239373 0.61030854 1.         0.92188128]. \t  0.9665095171966129 \t 0.9963897885831384\n",
      "10     \t [ 2.91251781  4.94770456 10.08101705  0.5         1.86742727  0.80181495]. \t  0.9672488449832851 \t 0.9963897885831384\n",
      "11     \t [ 2.64408906  4.41034275 10.07991854  0.61315867  1.52109916  0.82062079]. \t  0.9677865362219057 \t 0.9963897885831384\n",
      "12     \t [ 3.48328019  4.91545345 10.90453142  0.78913992  1.80034355  1.        ]. \t  0.9961065405903139 \t 0.9963897885831384\n",
      "13     \t [ 2.76701735  5.32172631 10.81290037  0.93494257  2.14388059  1.        ]. \t  0.9961353454756278 \t 0.9963897885831384\n",
      "14     \t [ 3.22773963  4.80889853 10.37449517  1.          2.31844286  1.        ]. \t  0.9962121575812275 \t 0.9963897885831384\n",
      "15     \t [ 2.94056239  4.74449344 10.71154198  0.76214544  1.96430622  1.        ]. \t  0.9960393290990579 \t 0.9963897885831384\n",
      "16     \t [ 3.40844671  5.34801789 10.64851318  0.83780598  2.22435865  0.43921753]. \t  0.7981594005191543 \t 0.9963897885831384\n",
      "17     \t [ 3.20087607  5.48842488 10.1691157   1.          1.33305011  1.        ]. \t  0.9962841699327978 \t 0.9963897885831384\n",
      "18     \t [2.38623788 5.4856759  9.69104697 1.         1.78305335 1.        ]. \t  0.9953912205623516 \t 0.9963897885831384\n",
      "19     \t [3.12851873 5.06007058 9.28631038 0.99265327 1.74957055 1.        ]. \t  0.995338410407468 \t 0.9963897885831384\n",
      "20     \t [3.07308292 5.60316134 9.95603789 0.89162269 2.1009225  1.        ]. \t  0.9951223740441852 \t 0.9963897885831384\n",
      "21     \t [ 2.96365839  3.44233135 10.33698588  0.54524161  2.58121836  0.77843757]. \t  0.9670952169000895 \t 0.9963897885831384\n",
      "22     \t [ 3.63424397  3.72235978 10.96108211  0.73716705  2.23549929  0.77144463]. \t  0.9675849043755634 \t 0.9963897885831384\n",
      "23     \t [2.82218405 5.16558735 9.84594205 0.94749344 1.59769977 1.        ]. \t  0.9952423948606115 \t 0.9963897885831384\n",
      "24     \t [2.48084103 5.05570554 8.9382992  0.5        1.92206268 0.91426367]. \t  0.9626112612134653 \t 0.9963897885831384\n",
      "25     \t [ 3.64520731  4.1098023  10.16225532  0.5         2.37855891  0.9579961 ]. \t  0.9666151325280996 \t 0.9963897885831384\n",
      "26     \t [ 2.75215459  5.87993963 10.79908238  0.71538152  1.38724338  1.        ]. \t  0.9957512868836268 \t 0.9963897885831384\n",
      "27     \t [ 3.15504552  3.87153656 10.53039476  0.83877276  2.3271418   0.87327809]. \t  0.9676089101982757 \t 0.9963897885831384\n",
      "28     \t [2.82948186 4.82333199 9.43225625 0.73131831 2.73337306 0.96711304]. \t  0.966456720040573 \t 0.9963897885831384\n",
      "29     \t [2.95604502 4.11358192 9.03260719 0.58908998 1.7869853  0.98598112]. \t  0.9658662092436497 \t 0.9963897885831384\n",
      "30     \t [2.3266178  4.49000871 8.97745702 1.         1.54625928 1.        ]. \t  0.9947431089833629 \t 0.9963897885831384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05770597827018724"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_4 = winner_4.getResult()[0]\n",
    "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
    "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
    "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
    "\n",
    "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
    "rmse_winner_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 7.90279917  7.50944526 11.99995139  0.70552558 16.35616999  0.64358037]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "2      \t [ 8.36377856  7.11268814 11.56129717  0.79464309 17.23580443  0.62978159]. \t  0.8875025212520042 \t 0.9875226863588296\n",
      "3      \t [ 8.02052546  8.07616604 11.26597791  0.71827193 16.94416522  0.4771788 ]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "4      \t [ 7.80298324  7.80174777 12.0755247   0.84096734 17.33309778  0.94946159]. \t  \u001b[92m0.9876427062072328\u001b[0m \t 0.9876427062072328\n",
      "5      \t [ 8.0850593   7.72930993 12.10300452  0.99272992 17.13760436  0.1020693 ]. \t  0.8874977192853896 \t 0.9876427062072328\n",
      "6      \t [ 8.16221714  7.71235484 11.9214621   0.5        17.0737092   0.66181162]. \t  0.8874977192853896 \t 0.9876427062072328\n",
      "7      \t [ 8.17797613  7.73958169 11.73164495  1.         16.87914095  0.89570761]. \t  0.9876331028962975 \t 0.9876427062072328\n",
      "8      \t [ 7.88721954  8.68577033 12.3108383   1.         16.76639795  0.70879871]. \t  \u001b[92m0.987676310016831\u001b[0m \t 0.987676310016831\n",
      "9      \t [ 8.2534994   8.64986167 11.92114456  1.         17.63735569  0.72752272]. \t  \u001b[92m0.987676310016831\u001b[0m \t 0.987676310016831\n",
      "10     \t [ 8.31053741  8.38094667 12.80094637  1.         17.3797936   0.99348997]. \t  \u001b[92m0.987676310016831\u001b[0m \t 0.987676310016831\n",
      "11     \t [5.82141087 9.7439839  5.         0.80860869 5.30857111 0.76390936]. \t  0.9790204461989841 \t 0.987676310016831\n",
      "12     \t [5.44880669 9.21545245 5.47748896 0.72846258 5.6513916  0.67683952]. \t  0.9789916413136742 \t 0.987676310016831\n",
      "13     \t [ 8.16160633  8.26658184 12.1975741   1.         17.17608583  0.7474288 ]. \t  \u001b[92m0.987676310016831\u001b[0m \t 0.987676310016831\n",
      "14     \t [ 8.62684936  9.25419802 12.47277224  0.69813029 17.16071995  1.        ]. \t  \u001b[92m0.9937973481451796\u001b[0m \t 0.9937973481451796\n",
      "15     \t [4.99425784 9.56361832 5.05715782 0.78340676 5.19654061 0.82836466]. \t  0.978982039800477 \t 0.9937973481451796\n",
      "16     \t [ 7.71498689  9.11947224 12.61829092  0.71995827 17.53047214  1.        ]. \t  \u001b[92m0.9940613913829757\u001b[0m \t 0.9940613913829757\n",
      "17     \t [5.33401253 9.64709331 5.00831773 0.50003729 5.54829199 0.24566682]. \t  0.8874977192853896 \t 0.9940613913829757\n",
      "18     \t [5.34700986 9.97154349 5.56416749 0.75442688 5.49927944 0.84170465]. \t  0.9789868406607912 \t 0.9940613913829757\n",
      "19     \t [5.41576589 9.58620739 5.11418565 0.5        5.59108172 1.        ]. \t  0.9839844857037349 \t 0.9940613913829757\n",
      "20     \t [5.37890172 9.61276526 5.18619757 1.         5.54393892 0.74271265]. \t  0.9788764192141146 \t 0.9940613913829757\n",
      "21     \t [ 8.61211573  8.76331931 12.51807994  0.53564495 17.97087225  1.        ]. \t  0.9930484202282538 \t 0.9940613913829757\n",
      "22     \t [ 7.19912945  7.26059359 11.59635142  1.         17.03585321  0.54042826]. \t  0.8990628860613761 \t 0.9940613913829757\n",
      "23     \t [ 8.25967905  8.85384752 12.39595689  0.71395194 17.40629107  1.        ]. \t  \u001b[92m0.9940949971977435\u001b[0m \t 0.9940949971977435\n",
      "24     \t [ 8.4243387   9.22273817 12.82598199  1.         17.65406785  0.5940369 ]. \t  0.8950925775547415 \t 0.9940949971977435\n",
      "25     \t [ 6.91111451  8.39136384 12.45816977  1.         17.0543874   0.76038515]. \t  0.9878059320698692 \t 0.9940949971977435\n",
      "26     \t [ 9.29918573  8.57822109 12.26155788  0.97811934 17.66553573  0.94061521]. \t  0.9877915306643711 \t 0.9940949971977435\n",
      "27     \t [ 7.44391916  8.9859627  13.11718344  1.         16.81564674  1.        ]. \t  \u001b[92m0.9947575111501776\u001b[0m \t 0.9947575111501776\n",
      "28     \t [ 9.12811991  8.88015905 11.76156307  0.6079639  17.55071778  0.44508178]. \t  0.8875793339107304 \t 0.9947575111501776\n",
      "29     \t [ 8.83087132  7.9650727  11.79265025  0.93645626 17.96961998  0.71066374]. \t  0.987623501867107 \t 0.9947575111501776\n",
      "30     \t [ 7.35089769  7.89469106 12.83622063  1.         16.72922119  0.92798706]. \t  0.9877003140418267 \t 0.9947575111501776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06365349232477487"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_5 = winner_5.getResult()[0]\n",
    "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
    "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
    "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
    "\n",
    "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
    "rmse_winner_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.26304251  6.18604318 14.00000158  0.8115314  13.15633873  0.61869284]. \t  0.8600607327546964 \t 0.989121369524546\n",
      "2      \t [ 4.98619559  5.79431317 13.99999953  0.72633179 12.03060142  0.681622  ]. \t  \u001b[92m0.989202981176704\u001b[0m \t 0.989202981176704\n",
      "3      \t [ 5.41940126  6.19064856 13.3784207   0.6669053  12.24945671  0.25373442]. \t  0.8563785326339582 \t 0.989202981176704\n",
      "4      \t [ 4.90807543  6.7171473  14.02853088  0.766389   12.20997685  0.71070707]. \t  \u001b[92m0.9895102363059546\u001b[0m \t 0.9895102363059546\n",
      "5      \t [ 5.35384765  6.27571324 14.41987803  0.61823424 12.2377248   0.21936966]. \t  0.8553607454072898 \t 0.9895102363059546\n",
      "6      \t [ 5.29907141  6.22882668 13.91575296  1.         12.29182451  0.85136625]. \t  \u001b[92m0.9912529151344066\u001b[0m \t 0.9912529151344066\n",
      "7      \t [ 5.22387851  6.27883699 13.90790104  0.5        12.2881621   0.86308903]. \t  0.9885308651579225 \t 0.9912529151344066\n",
      "8      \t [ 5.19404645  6.41258009 13.87553278  0.75461667 11.34428639  0.59410499]. \t  0.8650727838357716 \t 0.9912529151344066\n",
      "9      \t [ 4.21757563  5.85853299 13.93101274  0.92690046 13.7739502   0.25651555]. \t  0.8635653264887226 \t 0.9912529151344066\n",
      "10     \t [ 4.68123652  6.61108758 13.95907436  0.96693059 14.36551898  0.29585069]. \t  0.8635509232163424 \t 0.9912529151344066\n",
      "11     \t [ 4.99950275  5.77809944 13.51957989  0.90775377 14.30201604  0.1       ]. \t  0.8635749323579781 \t 0.9912529151344066\n",
      "12     \t [ 4.89454288  5.82462213 14.49452063  1.         14.27627674  0.13306047]. \t  0.7963206569418789 \t 0.9912529151344066\n",
      "13     \t [ 4.66212748  5.91963372 13.91820857  0.5        14.36165489  0.79598146]. \t  0.9885548682840491 \t 0.9912529151344066\n",
      "14     \t [ 4.67495153  5.92279006 13.84671327  1.         14.2948128   0.75111688]. \t  0.9911857039888775 \t 0.9912529151344066\n",
      "15     \t [ 4.50221791  6.17831391 13.77126775  0.86616783 12.31118827  0.2646722 ]. \t  0.8623891208975353 \t 0.9912529151344066\n",
      "16     \t [ 5.69810822  7.24151673 13.76033686  0.96575718 11.9907081   0.36594713]. \t  0.863570126381024 \t 0.9912529151344066\n",
      "17     \t [ 5.28866515  6.43928845 13.92532157  0.78765808 12.04888047  0.55707846]. \t  0.8601567545244695 \t 0.9912529151344066\n",
      "18     \t [ 4.44916876  6.33985253 14.56974405  0.78709801 12.48098319  1.        ]. \t  \u001b[92m0.9952327920343658\u001b[0m \t 0.9952327920343658\n",
      "19     \t [ 5.44292112  7.30035583 14.6335509   0.80453238 11.94533552  1.        ]. \t  0.995203988047925 \t 0.9952327920343658\n",
      "20     \t [ 5.78987291  5.51700402 13.25697592  0.7675856  12.19056972  1.        ]. \t  0.9948583293550627 \t 0.9952327920343658\n",
      "21     \t [ 4.72805306  5.77032808 13.19606369  0.87124813 12.62178339  1.        ]. \t  0.9950167567774174 \t 0.9952327920343658\n",
      "22     \t [ 5.55613485  6.06764991 14.92899521  0.61891408 11.93001324  1.        ]. \t  0.9945126725290826 \t 0.9952327920343658\n",
      "23     \t [ 4.52869355  6.80284166 14.44745197  0.68366033 11.51423744  1.        ]. \t  0.9949879505783756 \t 0.9952327920343658\n",
      "24     \t [ 5.11216061  7.53238395 13.7556323   0.88998817 11.56973418  1.        ]. \t  0.9951751835774779 \t 0.9952327920343658\n",
      "25     \t [ 4.2029258   6.84383254 13.76629382  0.95162775 12.31885639  1.        ]. \t  0.9951319750049249 \t 0.9952327920343658\n",
      "26     \t [ 5.85841944  5.55733767 14.23004472  0.70587154 12.29929288  1.        ]. \t  0.9942054181604137 \t 0.9952327920343658\n",
      "27     \t [ 4.67931318  7.36412769 14.28884117  0.89907625 12.2419528   1.        ]. \t  0.9951463785538804 \t 0.9952327920343658\n",
      "28     \t [ 5.8830441   7.06557366 14.11765882  0.83106498 11.12830172  1.        ]. \t  0.9950887704427114 \t 0.9952327920343658\n",
      "29     \t [ 5.28527718  5.3171754  13.26967129  0.52169999 12.6988184   0.30293863]. \t  0.8559992624565379 \t 0.9952327920343658\n",
      "30     \t [ 5.99543736  6.88825388 13.11681818  0.91791076 11.68660768  1.        ]. \t  \u001b[92m0.9952952026652997\u001b[0m \t 0.9952952026652997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05997686677125566"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None, eval_metric = 'rmse')\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_6 = winner_6.getResult()[0]\n",
    "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
    "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
    "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
    "\n",
    "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
    "rmse_winner_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 3.22353665  9.19323525 13.21712508  0.6906878   1.26153529  0.82657378]. \t  \u001b[92m0.9893421726602237\u001b[0m \t 0.9893421726602237\n",
      "2      \t [ 2.59854822  8.64305765 13.8049161   0.71224941  1.06520282  0.67753769]. \t  0.9889101014548891 \t 0.9893421726602237\n",
      "3      \t [ 2.81697168  9.20026743 13.9020785   0.76105367  1.84799105  0.75700902]. \t  \u001b[92m0.9895198023483908\u001b[0m \t 0.9895198023483908\n",
      "4      \t [ 3.36349713  9.09112328 14.13175928  0.73177334  1.11814034  0.73116239]. \t  0.9887708767132096 \t 0.9895198023483908\n",
      "5      \t [ 2.87915484  9.28425464 13.68406492  0.96598376  1.21121537  0.19493277]. \t  0.8285631918044 \t 0.9895198023483908\n",
      "6      \t [ 2.87395614  9.13537115 13.77480128  0.5         1.25258713  1.        ]. \t  \u001b[92m0.9950935649417977\u001b[0m \t 0.9950935649417977\n",
      "7      \t [ 2.89182377  9.17056105 13.80935448  1.          1.19611339  0.9982625 ]. \t  0.9925923434767602 \t 0.9950935649417977\n",
      "8      \t [ 3.40579145  8.2884562  13.49772675  0.81778612  1.7164977   0.63032905]. \t  0.830104267619529 \t 0.9950935649417977\n",
      "9      \t [ 2.40532243  9.12835792 14.83364182  0.71517554  1.25881894  0.59954986]. \t  0.8294129469149029 \t 0.9950935649417977\n",
      "10     \t [ 1.22385999  2.00066862 11.61316141  0.74255154  5.77365999  1.        ]. \t  \u001b[92m0.9961065385856958\u001b[0m \t 0.9961065385856958\n",
      "11     \t [ 1.25629037  2.19358776 11.28571599  0.72507273  5.01310916  0.89067236]. \t  0.9887084705074778 \t 0.9961065385856958\n",
      "12     \t [ 1.25094048  2.47996957 11.99873637  0.67247571  5.29200199  0.76226867]. \t  0.9888476930365563 \t 0.9961065385856958\n",
      "13     \t [ 1.81806209  2.08701289 11.62028213  0.68467242  5.39463204  0.65453811]. \t  0.8286592117072908 \t 0.9961065385856958\n",
      "14     \t [ 0.88026357  1.91751059 11.7033125   0.65927697  5.29009255  0.55994692]. \t  0.8279486910877409 \t 0.9961065385856958\n",
      "15     \t [ 1.27776783  2.14118192 11.79258488  1.          5.2180596   1.        ]. \t  \u001b[92m0.9968122592438008\u001b[0m \t 0.9968122592438008\n",
      "16     \t [ 1.2925152   2.12234192 11.75396076  0.5         5.23512438  1.        ]. \t  0.9955544460799383 \t 0.9968122592438008\n",
      "17     \t [ 1.05027181  2.92557483 11.13593191  0.80323467  5.83702953  0.9718727 ]. \t  0.9886652621423563 \t 0.9968122592438008\n",
      "18     \t [ 2.93803064  9.05485134 13.90038534  0.6927101   1.33007071  0.65050512]. \t  0.8291584981376379 \t 0.9968122592438008\n",
      "19     \t [ 2.45841095  9.61750405 13.55926423  0.82627404  2.37895912  1.        ]. \t  0.9966826387119258 \t 0.9968122592438008\n",
      "20     \t [ 4.0035605   9.58660371 14.20775985  0.76626198  1.          1.        ]. \t  0.9961737512523882 \t 0.9968122592438008\n",
      "21     \t [ 2.1320461   8.3066873  13.22450076  0.6843641   1.          1.        ]. \t  0.9965578174500579 \t 0.9968122592438008\n",
      "22     \t [ 3.29131319  9.63825241 14.1469365   0.86388079  2.33420629  1.        ]. \t  0.9964473985617013 \t 0.9968122592438008\n",
      "23     \t [ 4.02323779  8.57848402 14.00158098  0.7852952   1.          1.        ]. \t  0.996183353457023 \t 0.9968122592438008\n",
      "24     \t [ 2.58380256  8.84108054 14.0386799   0.88334201  2.58548387  1.        ]. \t  0.9965770197158702 \t 0.9968122592438008\n",
      "25     \t [ 1.80183012  8.58453403 14.12326399  0.66656375  1.          1.        ]. \t  0.9965338134250622 \t 0.9968122592438008\n",
      "26     \t [ 3.16168989  9.06537751 13.30092613  0.84545635  2.36136357  1.        ]. \t  0.9963465814631166 \t 0.9968122592438008\n",
      "27     \t [ 3.60232522  9.07970559 14.91021402  0.76984712  1.          1.        ]. \t  0.9962409632967866 \t 0.9968122592438008\n",
      "28     \t [ 2.31362007  9.62113274 14.43342489  0.73303146  2.05056126  1.        ]. \t  0.9965722189246998 \t 0.9968122592438008\n",
      "29     \t [ 2.61681547  7.88437364 13.93881308  0.73553736  1.          1.        ]. \t  0.9963945883376638 \t 0.9968122592438008\n",
      "30     \t [ 1.91164152  8.77537193 13.4943499   1.          1.88485374  0.62107306]. \t  0.7883896926082067 \t 0.9968122592438008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05498489064592459"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_7 = winner_7.getResult()[0]\n",
    "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
    "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
    "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
    "\n",
    "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
    "rmse_winner_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [ 1.03151411  1.4354061   7.46125772  0.68986665 16.53874371  0.41044046]. \t  0.8892451957785167 \t 0.8894852354753713\n",
      "2      \t [ 0.91764962  0.68106745  8.0987548   0.6531703  17.05579396  0.70217133]. \t  \u001b[92m0.9843925430636409\u001b[0m \t 0.9843925430636409\n",
      "3      \t [ 1.81183121  0.83373425  7.70299608  0.68408113 16.88644353  0.66284562]. \t  0.8891779856701163 \t 0.9843925430636409\n",
      "4      \t [ 1.12223513  1.14906702  7.53208337  0.5        17.54142889  0.3985197 ]. \t  0.8886211055102246 \t 0.9843925430636409\n",
      "5      \t [ 1.14213944  0.66323818  7.75041741  0.5        16.78722322  0.1       ]. \t  0.888851540167597 \t 0.9843925430636409\n",
      "6      \t [ 1.05098456  0.88595392  7.54172452  1.         17.00919303  0.6624928 ]. \t  0.895721512644088 \t 0.9843925430636409\n",
      "7      \t [ 1.09033208  0.90263882  7.55718697  0.5        16.81024002  0.95933431]. \t  0.983283543086498 \t 0.9843925430636409\n",
      "8      \t [ 1.28267968  1.57054266  8.46437349  0.5        16.92359184  0.65353197]. \t  0.889547644861743 \t 0.9843925430636409\n",
      "9      \t [ 5.53024049  2.6545633  14.          0.87903478 15.63022658  0.51557283]. \t  0.8880978232136828 \t 0.9843925430636409\n",
      "10     \t [ 5.30355725  3.08430346 13.72528857  0.83900974 14.77137104  0.53186907]. \t  0.8891491813379447 \t 0.9843925430636409\n",
      "11     \t [ 5.67445102  2.61222761 14.4956183   1.         14.79282172  0.47658482]. \t  0.896643291929537 \t 0.9843925430636409\n",
      "12     \t [ 5.91372709  2.37266368 13.60663281  1.         14.89499039  0.52100354]. \t  0.8967969132367394 \t 0.9843925430636409\n",
      "13     \t [ 5.64895527  2.60720735 14.02404885  0.5        14.98343719  1.        ]. \t  \u001b[92m0.9930244289245769\u001b[0m \t 0.9930244289245769\n",
      "14     \t [ 5.62006908  2.57437253 13.98636952  0.5        14.97740647  0.2671473 ]. \t  0.8893364130924848 \t 0.9930244289245769\n",
      "15     \t [ 5.75666131  2.8436431  13.99124659  0.96988722 15.05920575  0.83752837]. \t  0.9866009024390521 \t 0.9930244289245769\n",
      "16     \t [ 1.19208231  1.03809181  7.84642614  0.5        16.98152836  0.59446424]. \t  0.8886211055102246 \t 0.9930244289245769\n",
      "17     \t [ 1.12115306  0.17466033  7.87964354  0.74443539 16.24483023  1.        ]. \t  0.9918770307769117 \t 0.9930244289245769\n",
      "18     \t [ 1.01149458  1.0979112   8.66039255  0.87123826 16.21784737  1.        ]. \t  \u001b[92m0.9937925506720027\u001b[0m \t 0.9937925506720027\n",
      "19     \t [ 1.46706788  1.01983019  7.54235378  0.78317382 15.84945654  1.        ]. \t  0.9919010335573413 \t 0.9937925506720027\n",
      "20     \t [ 1.53803268  1.95034734  8.28799184  0.87854134 16.14979014  1.        ]. \t  \u001b[92m0.9938021525309165\u001b[0m \t 0.9938021525309165\n",
      "21     \t [ 1.84658346  1.10269218  8.47589406  0.88461384 16.27501686  1.        ]. \t  0.9936965334657194 \t 0.9938021525309165\n",
      "22     \t [ 1.46450231  1.5018417   8.43941152  0.88904096 15.97640696  0.31237103]. \t  0.8888947466658544 \t 0.9938021525309165\n",
      "23     \t [ 1.29302616  1.10034964  8.05281287  0.83079106 16.24857901  1.        ]. \t  \u001b[92m0.993960580921284\u001b[0m \t 0.993960580921284\n",
      "24     \t [ 1.48061468  1.76471392  9.13959649  0.90695033 16.50867765  0.91548142]. \t  0.9858759888493106 \t 0.993960580921284\n",
      "25     \t [ 0.24523868  0.34899394  7.71676602  0.51399489 16.59717724  0.79367937]. \t  0.9834851763156959 \t 0.993960580921284\n",
      "26     \t [ 1.5256888   1.62389137  8.81243716  0.5        16.09429155  0.99581431]. \t  0.9835572087186746 \t 0.993960580921284\n",
      "27     \t [ 0.71390446  0.          7.76454318  0.5        17.10066987  1.        ]. \t  0.9917570121730547 \t 0.993960580921284\n",
      "28     \t [5.65108419e-01 1.41401362e-02 8.55348045e+00 6.82645858e-01\n",
      " 1.66538505e+01 5.95724780e-01]. \t  0.8894660303746877 \t 0.993960580921284\n",
      "29     \t [ 0.68165722  2.12232471  8.8014467   0.7229698  16.42262171  0.73087165]. \t  0.9846853871073851 \t 0.993960580921284\n",
      "30     \t [ 2.05080209  0.50027688  7.40524607  0.50096885 16.10642513  1.        ]. \t  0.9917666136171118 \t 0.993960580921284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0635847681229449"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_8 = winner_8.getResult()[0]\n",
    "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
    "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
    "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
    "\n",
    "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
    "rmse_winner_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 6.90894599  5.46585464 13.34394195  0.89953928  5.65583539  0.7849335 ]. \t  \u001b[92m0.9685930714938683\u001b[0m \t 0.9685930714938683\n",
      "2      \t [ 7.11436761  5.83633719 14.00578676  0.94778985  4.99410337  0.98277168]. \t  \u001b[92m0.9686890845515247\u001b[0m \t 0.9686890845515247\n",
      "3      \t [ 7.1619699   5.26841726 13.24712501  0.89269095  4.72397353  0.87317563]. \t  0.9683770340243189 \t 0.9686890845515247\n",
      "4      \t [ 6.34750923  5.58409803 13.51484475  0.86408346  4.95080618  0.99777461]. \t  0.9685306588577647 \t 0.9686890845515247\n",
      "5      \t [ 6.83863591  5.50927934 13.69227472  0.77459985  5.01266731  0.26307857]. \t  0.7914286488723746 \t 0.9686890845515247\n",
      "6      \t [ 6.9162011   5.31423923 13.64837283  1.          5.16206227  1.        ]. \t  \u001b[92m0.9961209416506448\u001b[0m \t 0.9961209416506448\n",
      "7      \t [ 6.92119874  5.48341982 13.6033466   0.5         5.05537354  1.        ]. \t  0.9942918309494612 \t 0.9961209416506448\n",
      "8      \t [ 7.09195436  6.32901788 12.95978729  0.93741364  5.18136593  0.99711127]. \t  0.9685930709407181 \t 0.9961209416506448\n",
      "9      \t [ 6.72859784  5.1833808  14.16785319  0.84115027  4.01962459  1.        ]. \t  0.9958328916912454 \t 0.9961209416506448\n",
      "10     \t [ 6.95871496  6.10179635 13.51416838  0.89452339  4.11462989  1.        ]. \t  0.995957713713695 \t 0.9961209416506448\n",
      "11     \t [ 7.63245894  5.51737267 14.03168537  0.72099642  4.02065503  1.        ]. \t  0.9951127735688381 \t 0.9961209416506448\n",
      "12     \t [ 7.40926119  4.63581806 14.44697005  0.65398673  4.38342888  0.97340748]. \t  0.9661302515181291 \t 0.9961209416506448\n",
      "13     \t [ 7.09858807  5.36152716 13.93878644  0.78711862  4.38299365  1.        ]. \t  0.9957272744929284 \t 0.9961209416506448\n",
      "14     \t [ 7.83607934  6.23985871 13.36859097  0.81840802  4.55040928  0.9415427 ]. \t  0.9676137030424439 \t 0.9961209416506448\n",
      "15     \t [ 6.63992482  4.19395701 14.42159036  0.90191228  4.07739517  0.7254555 ]. \t  0.96895313124651 \t 0.9961209416506448\n",
      "16     \t [ 7.24807083  4.86440166 14.63236124  1.          3.7600673   0.54894079]. \t  0.7970503805878227 \t 0.9961209416506448\n",
      "17     \t [ 6.50383537  4.6141414  14.8486723   0.55013675  4.38400631  1.        ]. \t  0.994603882098961 \t 0.9961209416506448\n",
      "18     \t [ 7.11140389  5.95481264 13.32843991  0.8638591   4.90756437  0.95622355]. \t  0.968314622425372 \t 0.9961209416506448\n",
      "19     \t [ 7.62478034  6.56270012 14.02574832  1.          3.88848034  1.        ]. \t  0.996000922217104 \t 0.9961209416506448\n",
      "20     \t [ 7.73409407  6.10417769 13.33188803  1.          3.54835266  0.94377889]. \t  0.9873402512468586 \t 0.9961209416506448\n",
      "21     \t [ 7.54625396  6.2965172  13.69076551  0.5         3.7330072   0.67133192]. \t  0.9639746789275366 \t 0.9961209416506448\n",
      "22     \t [ 6.72357465  4.49188552 14.09912197  0.5         4.15716164  1.        ]. \t  0.9945126677581616 \t 0.9961209416506448\n",
      "23     \t [ 7.36764799  7.00453809 13.06636537  1.          4.13826462  1.        ]. \t  0.9960441294067813 \t 0.9961209416506448\n",
      "24     \t [ 7.53068605  6.44641309 13.48714811  0.89652305  3.96759906  1.        ]. \t  0.9959049048033894 \t 0.9961209416506448\n",
      "25     \t [ 7.3118644   7.21208111 13.42915645  1.          4.94278274  0.79596175]. \t  0.9870426022634385 \t 0.9961209416506448\n",
      "26     \t [ 7.94836139  6.02632088 13.8261433   1.          3.91672336  0.4463557 ]. \t  0.7969831695114374 \t 0.9961209416506448\n",
      "27     \t [ 7.1460821   6.03673163 13.92490219  0.88714416  3.17271539  0.93901393]. \t  0.9683578296150492 \t 0.9961209416506448\n",
      "28     \t [ 7.70625909  7.10463847 12.61619412  1.          4.87516017  0.84049798]. \t  0.9872970441263251 \t 0.9961209416506448\n",
      "29     \t [ 7.23609879  5.32704359 13.21628317  0.67785961  3.42582887  0.81650972]. \t  0.9666631396146169 \t 0.9961209416506448\n",
      "30     \t [ 7.91975295  6.61702189 13.38884305  1.          5.54747327  1.        ]. \t  0.9958232911460615 \t 0.9961209416506448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06104399410455243"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_9 = winner_9.getResult()[0]\n",
    "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
    "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
    "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
    "\n",
    "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
    "rmse_winner_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.04640152  3.64611669  9.54832523  0.71318582 18.47635127  0.89886932]. \t  0.9848150471222544 \t 0.9854871661142187\n",
      "2      \t [ 8.91670779  2.87331718  9.06455944  0.7765722  18.9714783   0.90503195]. \t  0.9854391575802207 \t 0.9854871661142187\n",
      "3      \t [ 9.27579733  3.29126208  8.68726329  0.80553123 18.23922875  0.86185324]. \t  \u001b[92m0.9855831802090317\u001b[0m \t 0.9855831802090317\n",
      "4      \t [ 9.56641729  2.87662451  9.46265158  0.86555101 18.41757587  0.90680887]. \t  \u001b[92m0.9857176051966977\u001b[0m \t 0.9857176051966977\n",
      "5      \t [ 9.28543809  3.27554495  9.17721255  0.88281923 18.69361724  0.25593758]. \t  0.8905510739818729 \t 0.9857176051966977\n",
      "6      \t [ 9.33414888  3.23994195  9.13749029  0.5        18.65805911  1.        ]. \t  \u001b[92m0.9924195203186975\u001b[0m \t 0.9924195203186975\n",
      "7      \t [ 9.26812938  3.32827392  9.14541663  1.         18.69042805  1.        ]. \t  \u001b[92m0.9938309565182674\u001b[0m \t 0.9938309565182674\n",
      "8      \t [ 8.47553732  2.89866191  9.40148519  0.75806776 17.97796877  0.76785228]. \t  0.9852855294278733 \t 0.9938309565182674\n",
      "9      \t [ 9.16463099  3.48684934  9.53583925  0.75954039 17.41940451  0.67710255]. \t  0.9855879834893783 \t 0.9938309565182674\n",
      "10     \t [ 9.05297066  3.2086739   9.34590221  0.7579375  18.05267754  0.7297278 ]. \t  0.9853143345897584 \t 0.9938309565182674\n",
      "11     \t [ 8.2995698   3.77582031  9.85008751  0.72743804 17.45600369  1.        ]. \t  0.9935525086943592 \t 0.9938309565182674\n",
      "12     \t [ 8.78152056  3.22899514 10.38447308  0.80652486 17.63369098  1.        ]. \t  0.9937253380062185 \t 0.9938309565182674\n",
      "13     \t [ 9.75298752  2.59109852  8.48596702  0.89755023 18.8170698   0.74962879]. \t  0.985770413208134 \t 0.9938309565182674\n",
      "14     \t [ 8.54822257  3.64662165 10.16254703  0.79578015 17.663305    0.33563157]. \t  0.889883762211451 \t 0.9938309565182674\n",
      "15     \t [ 9.19616182  4.07256718 10.14234512  0.77539888 17.54130303  1.        ]. \t  0.9937685437438762 \t 0.9938309565182674\n",
      "16     \t [ 8.03908281  3.14501945 10.06146571  0.71105312 18.39335195  1.        ]. \t  0.9933796784836307 \t 0.9938309565182674\n",
      "17     \t [ 8.66697706  3.50554882  9.97409686  0.5        17.80211733  1.        ]. \t  0.9925491416802981 \t 0.9938309565182674\n",
      "18     \t [ 8.64264889  3.51371155  9.91392622  1.         17.76623956  0.99917316]. \t  0.9833603816069966 \t 0.9938309565182674\n",
      "19     \t [ 8.89413145  3.72009881 10.21091902  0.75266505 16.71310594  0.85861531]. \t  0.9856455922228413 \t 0.9938309565182674\n",
      "20     \t [ 8.93573273  4.39650713  9.50541456  0.65978103 16.97751625  0.71287884]. \t  0.9843013603927098 \t 0.9938309565182674\n",
      "21     \t [ 8.51704173  2.4363002  10.08542085  0.78619707 18.54164509  0.79254374]. \t  0.9858184202901126 \t 0.9938309565182674\n",
      "22     \t [ 7.94144612  2.6150877  10.2426025   0.74328367 17.65400043  0.85027871]. \t  0.9851415058310492 \t 0.9938309565182674\n",
      "23     \t [ 9.76341587  4.24639092  9.19033092  0.70850268 17.9156482   0.76971271]. \t  0.9844693895356927 \t 0.9938309565182674\n",
      "24     \t [10.          3.26433082  8.79257257  0.84369221 18.36186583  0.72026306]. \t  0.985679198106753 \t 0.9938309565182674\n",
      "25     \t [ 9.00302496  3.99124208  9.78121313  0.69945847 17.2606519   0.8172326 ]. \t  0.9848246488428828 \t 0.9938309565182674\n",
      "26     \t [ 8.21729436  3.22619896  9.75081588  0.75866602 16.71018078  0.6257091 ]. \t  0.8884195147506869 \t 0.9938309565182674\n",
      "27     \t [ 9.9664369   3.92532813 10.02644649  0.86364294 18.23435875  0.86512247]. \t  0.9857032026157552 \t 0.9938309565182674\n",
      "28     \t [ 9.76732744  4.20186403  9.39271625  1.         16.75368864  0.56696464]. \t  0.8873920942046811 \t 0.9938309565182674\n",
      "29     \t [ 9.12307991  4.1237225   8.58050407  0.86005893 16.97902553  0.45418657]. \t  0.8892548541429285 \t 0.9938309565182674\n",
      "30     \t [ 8.2353947   2.97872494 10.04137732  0.66902894 17.89216325  0.78206512]. \t  0.9845846067260436 \t 0.9938309565182674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06545807235601239"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_10 = winner_10.getResult()[0]\n",
    "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
    "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
    "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
    "\n",
    "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
    "rmse_winner_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [7.85918876 5.44066252 5.33082483 0.56561602 1.         0.6611742 ]. \t  0.8018467755273141 \t 0.9791740729684557\n",
      "2      \t [8.378443   6.1280493  5.01460687 0.58809474 1.79148353 0.84657416]. \t  \u001b[92m0.980758335575846\u001b[0m \t 0.980758335575846\n",
      "3      \t [8.48920598 6.37950262 5.53285875 0.61165387 1.         0.89579517]. \t  0.9805855057799803 \t 0.980758335575846\n",
      "4      \t [ 4.27166845  3.09035631  8.16210237  0.7941089  17.55880329  0.22005885]. \t  0.8273150487443677 \t 0.980758335575846\n",
      "5      \t [8.62031949 5.96287752 5.30432604 0.9336615  1.32216749 0.26907036]. \t  0.8134166904760519 \t 0.980758335575846\n",
      "6      \t [8.31641952 6.02802826 5.         0.94146451 1.08357268 1.        ]. \t  \u001b[92m0.9840516902806046\u001b[0m \t 0.9840516902806046\n",
      "7      \t [8.56337852 5.73531698 5.52728814 0.59146439 1.43460707 1.        ]. \t  0.983874059486137 \t 0.9840516902806046\n",
      "8      \t [8.42113963 6.02606845 5.10662647 0.5        1.20525836 0.84795254]. \t  0.9800430122290997 \t 0.9840516902806046\n",
      "9      \t [9.17359093 6.37736184 5.14195739 0.91097501 1.50572261 1.        ]. \t  0.9839796770301731 \t 0.9840516902806046\n",
      "10     \t [8.41005659 6.75608558 5.59308511 1.         1.8634598  1.        ]. \t  0.9838596565594756 \t 0.9840516902806046\n",
      "11     \t [8.60977542 6.23736467 5.33031511 0.92983198 1.48507521 1.        ]. \t  \u001b[92m0.984056491071775\u001b[0m \t 0.984056491071775\n",
      "12     \t [8.92713361 6.94461294 5.41164602 0.5        2.20344982 0.75959057]. \t  0.9793709014652459 \t 0.984056491071775\n",
      "13     \t [8.57012365 7.34519067 5.         0.80259652 1.64008176 0.8881856 ]. \t  0.9819153380714957 \t 0.984056491071775\n",
      "14     \t [8.93863477 7.31608575 5.80197061 0.61502691 1.45604359 1.        ]. \t  0.9838884613756417 \t 0.984056491071775\n",
      "15     \t [8.18425567 7.20783687 5.67255708 0.5        1.77189388 0.70456912]. \t  0.980033410646759 \t 0.984056491071775\n",
      "16     \t [8.65960739 6.92843394 5.40036917 0.54053856 1.70783266 1.        ]. \t  0.9839172670215334 \t 0.984056491071775\n",
      "17     \t [8.76978156 7.25917204 5.55404511 0.9818711  1.71054815 0.48698423]. \t  0.813503110456053 \t 0.984056491071775\n",
      "18     \t [8.0912768  7.20143807 5.10761857 0.82225332 2.59038142 0.87906717]. \t  0.9822850043156867 \t 0.984056491071775\n",
      "19     \t [8.47815707 7.71942182 5.64941697 0.77878586 2.21636369 1.        ]. \t  0.984032487738217 \t 0.984056491071775\n",
      "20     \t [8.9183283  6.6320563  5.         1.         2.63760731 1.        ]. \t  0.9838548557683054 \t 0.984056491071775\n",
      "21     \t [8.05980308 7.60933814 5.78986401 0.92291756 1.04522305 1.        ]. \t  \u001b[92m0.9840708934452861\u001b[0m \t 0.9840708934452861\n",
      "22     \t [7.72002889 7.45292479 5.31108265 1.         1.74551276 1.        ]. \t  0.9839556737657591 \t 0.9840708934452861\n",
      "23     \t [9.34888888 5.74573728 5.         0.58404126 2.16142611 0.77198907]. \t  0.9801534358883774 \t 0.9840708934452861\n",
      "24     \t [8.26822597 7.44954278 5.57513794 0.8390587  1.67645454 1.        ]. \t  0.9840228851187196 \t 0.9840708934452861\n",
      "25     \t [7.49055481 6.90574146 5.4376589  0.90730323 1.         0.77466007]. \t  0.9819921521130976 \t 0.9840708934452861\n",
      "26     \t [8.02401235 7.04358952 6.48714121 0.5831001  1.         0.92310702]. \t  0.9829331102939632 \t 0.9840708934452861\n",
      "27     \t [9.21753169 7.55522526 5.         0.87895153 2.38105182 1.        ]. \t  \u001b[92m0.984080494958483\u001b[0m \t 0.984080494958483\n",
      "28     \t [7.37113793 6.74287112 5.99588815 0.87452657 1.77151728 0.93555679]. \t  0.9818241297462054 \t 0.984080494958483\n",
      "29     \t [7.49873782 6.61861765 5.16454598 1.         1.87998309 0.82084062]. \t  0.9812528183801276 \t 0.984080494958483\n",
      "30     \t [7.78754609 7.01676246 5.89543543 0.69684794 1.09499651 1.        ]. \t  0.9839508724214383 \t 0.984080494958483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08442651018167217"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_11 = winner_11.getResult()[0]\n",
    "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
    "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
    "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
    "\n",
    "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
    "rmse_winner_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.27782964  6.69041962  8.48003509  0.85599072 15.2200995   0.67841627]. \t  0.9901871520080147 \t 0.9904271937791528\n",
      "2      \t [ 1.73134026  6.505744    8.02846915  0.85712538 15.89841478  0.97790465]. \t  0.9903839870043244 \t 0.9904271937791528\n",
      "3      \t [ 2.24340459  5.83231836  8.27521584  0.89219064 15.65648063  0.61457265]. \t  0.8889859926049365 \t 0.9904271937791528\n",
      "4      \t [ 1.52583534  6.23065537  8.612303    0.88775358 15.24103365  1.        ]. \t  \u001b[92m0.9940133825024623\u001b[0m \t 0.9940133825024623\n",
      "5      \t [ 2.02121043  6.40034084  8.55457272  0.5        15.62211805  1.        ]. \t  0.9932884611687284 \t 0.9940133825024623\n",
      "6      \t [ 1.66392448  6.51086488  8.55263804  0.88872044 15.67877015  0.34335715]. \t  0.8899653681090269 \t 0.9940133825024623\n",
      "7      \t [ 2.00769952  6.41926878  8.52421059  1.         15.6146333   1.        ]. \t  0.993974972024524 \t 0.9940133825024623\n",
      "8      \t [ 1.86234098  6.53101504  7.62589523  0.64618062 14.86312242  0.71664936]. \t  0.9889965404478134 \t 0.9940133825024623\n",
      "9      \t [ 2.77204718  7.37435785  7.58896506  0.76060018 13.98246965  0.37419232]. \t  0.8893892648713266 \t 0.9940133825024623\n",
      "10     \t [ 2.22883263  6.41897602  8.27547984  0.61676794 14.21446625  0.46066836]. \t  0.8903878438166738 \t 0.9940133825024623\n",
      "11     \t [ 1.31810505  5.73873997  8.55030323  0.7085844  16.19370491  1.        ]. \t  0.993830947459669 \t 0.9940133825024623\n",
      "12     \t [ 2.65688556  6.66973404  7.73837964  0.5        14.85404415  0.23541853]. \t  0.8866959822350783 \t 0.9940133825024623\n",
      "13     \t [ 2.0070382   7.18526674  8.07305635  0.5        14.63413322  0.5554323 ]. \t  0.8905990816704971 \t 0.9940133825024623\n",
      "14     \t [ 1.65061727  6.10853994  8.38007093  0.72216291 15.68860734  0.85514803]. \t  0.9904079903378961 \t 0.9940133825024623\n",
      "15     \t [ 1.68429803  6.10003319  8.57023058  0.75694152 16.92295073  1.        ]. \t  0.9938117467841239 \t 0.9940133825024623\n",
      "16     \t [ 1.09536298  6.39424403  8.90651323  0.80926438 16.46193848  1.        ]. \t  \u001b[92m0.9941382000306355\u001b[0m \t 0.9941382000306355\n",
      "17     \t [ 2.38836954  6.65187314  7.88061237  0.77580528 14.65428636  1.        ]. \t  0.9922898950839399 \t 0.9941382000306355\n",
      "18     \t [ 1.37713802  5.68888177  9.13597611  1.         16.67479268  1.        ]. \t  \u001b[92m0.9942390196183744\u001b[0m \t 0.9942390196183744\n",
      "19     \t [ 1.4935206   6.05839512  8.73625312  0.90368397 16.42305349  1.        ]. \t  0.9941237993856911 \t 0.9942390196183744\n",
      "20     \t [ 1.32095659  5.83347541  9.11976492  0.5        16.68144086  0.85894216]. \t  0.989827087761023 \t 0.9942390196183744\n",
      "21     \t [ 0.89609578  5.7427756   9.49390429  0.80425242 15.80851807  1.        ]. \t  \u001b[92m0.9943830441832292\u001b[0m \t 0.9943830441832292\n",
      "22     \t [ 1.65218664  5.18134997  9.34901415  0.75708614 15.7401563   1.        ]. \t  0.9941382018283479 \t 0.9943830441832292\n",
      "23     \t [ 1.75593138  5.89419133  9.51397063  0.8502384  15.28144428  0.76432516]. \t  0.9903983893086957 \t 0.9943830441832292\n",
      "24     \t [ 0.91994558  6.1662102   8.14187324  0.5        17.16149708  1.        ]. \t  0.9930196136825629 \t 0.9943830441832292\n",
      "25     \t [ 1.46185372  6.8859019   8.08421236  0.5        16.90082663  0.9563431 ]. \t  0.9896350533484565 \t 0.9943830441832292\n",
      "26     \t [ 1.46275484  5.745037    9.30974055  0.79102459 15.84337833  1.        ]. \t  0.9943734423243153 \t 0.9943830441832292\n",
      "27     \t [ 0.79662116  5.15737984  9.15908415  0.8747047  16.27727709  0.72022344]. \t  0.9905856205100934 \t 0.9943830441832292\n",
      "28     \t [ 0.56782756  5.71769133  8.91204884  0.70157664 16.70924882  1.        ]. \t  0.993941367246895 \t 0.9943830441832292\n",
      "29     \t [ 1.21903155  6.32993625  8.30400078  0.5        16.71209976  0.77228563]. \t  0.9894094129136904 \t 0.9943830441832292\n",
      "30     \t [ 1.24982219  5.30262428  9.14394212  0.95552713 15.2320285   0.602711  ]. \t  0.8903734412357313 \t 0.9943830441832292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.062295876036504834"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_12 = winner_12.getResult()[0]\n",
    "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
    "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
    "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
    "\n",
    "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
    "rmse_winner_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [ 5.5727035   5.07144975  7.00504377  0.68328094 12.99997309  0.26052396]. \t  0.8181934899981783 \t 0.9879595510260936\n",
      "2      \t [ 6.00544824  8.56542194  7.98536232  0.76912862 10.66028164  0.73724599]. \t  0.9872202275264034 \t 0.9879595510260936\n",
      "3      \t [ 5.47319529  9.28412593  7.75584913  0.75219539 10.31705941  0.9567128 ]. \t  0.9872442295462293 \t 0.9879595510260936\n",
      "4      \t [ 5.89248183  9.29234869  8.57041676  0.8225913  10.57939194  0.89497629]. \t  \u001b[92m0.9879691532307282\u001b[0m \t 0.9879691532307282\n",
      "5      \t [5.72826988 8.81628037 8.31585795 0.67563859 9.89923014 0.71454048]. \t  0.9861064427994298 \t 0.9879691532307282\n",
      "6      \t [ 5.92007462  9.22725759  8.00855853  0.89548297 10.36899315  0.26400697]. \t  0.8206034991967298 \t 0.9879691532307282\n",
      "7      \t [ 5.97276513  9.02820202  8.08747002  0.5        10.35257472  1.        ]. \t  \u001b[92m0.9933604757338119\u001b[0m \t 0.9933604757338119\n",
      "8      \t [ 5.89431012  8.94857255  8.10933133  1.         10.22851734  1.        ]. \t  \u001b[92m0.9939317731328455\u001b[0m \t 0.9939317731328455\n",
      "9      \t [5.69763989 9.94448134 8.37374462 0.70591228 9.65736757 0.99903045]. \t  0.9861592520554542 \t 0.9939317731328455\n",
      "10     \t [ 5.04203687  8.47335377  8.33213805  0.68865616 10.92606548  0.70626614]. \t  0.9868937732428108 \t 0.9939317731328455\n",
      "11     \t [ 4.86169762  9.37479934  8.5865106   0.6390002  10.25674813  0.86532685]. \t  0.986389696323717 \t 0.9939317731328455\n",
      "12     \t [ 5.36510747  8.99695477  8.31522738  0.69631846 10.46320522  0.77603416]. \t  0.9866153266634905 \t 0.9939317731328455\n",
      "13     \t [ 5.39243872  7.61015164  8.05282546  0.75119552 11.48640958  0.60617736]. \t  0.8216788795995101 \t 0.9939317731328455\n",
      "14     \t [4.85589153 9.50347171 8.01229597 0.5968858  9.41537451 1.        ]. \t  0.9937061346341056 \t 0.9939317731328455\n",
      "15     \t [ 5.7354506   8.09949256  8.91623642  0.76839219 11.13801124  0.74018776]. \t  0.9867929596014154 \t 0.9939317731328455\n",
      "16     \t [ 5.57282211  8.51210301  8.4154025   0.89251085 11.64264441  0.96013678]. \t  0.9878587334435024 \t 0.9939317731328455\n",
      "17     \t [5.43140977 9.49936027 8.1590223  0.679611   9.78816232 0.99847188]. \t  0.9860248293495332 \t 0.9939317731328455\n",
      "18     \t [ 4.4141564  10.          8.05679937  0.68070166  9.92852426  0.84050363]. \t  0.9866153273549284 \t 0.9939317731328455\n",
      "19     \t [4.66918753 9.89001762 8.74783029 0.5866791  9.39856183 0.63095678]. \t  0.8179486703916238 \t 0.9939317731328455\n",
      "20     \t [4.1594762  9.11880103 7.99584695 0.60482324 9.95576195 0.89508168]. \t  0.9863368797384515 \t 0.9939317731328455\n",
      "21     \t [6.45353155 9.43848306 8.96647059 0.70597183 9.70589967 0.74997248]. \t  0.9865145164792843 \t 0.9939317731328455\n",
      "22     \t [ 4.45951946  9.37197769  7.8714178   0.90647411 10.8926757   1.        ]. \t  0.9921746724323355 \t 0.9939317731328455\n",
      "23     \t [ 6.49331941  8.61266138  8.72593168  0.73542    10.18311876  0.78999726]. \t  0.9858760059986956 \t 0.9939317731328455\n",
      "24     \t [ 4.51299702  9.46278572  8.05594743  1.         10.04118356  1.        ]. \t  0.9939221709282107 \t 0.9939317731328455\n",
      "25     \t [ 4.54932973  9.48052217  7.91052698  0.5        10.25692369  1.        ]. \t  0.9919874408161089 \t 0.9939317731328455\n",
      "26     \t [ 5.14833283  8.72533029  7.56227022  1.         11.25635641  1.        ]. \t  0.9922034778016519 \t 0.9939317731328455\n",
      "27     \t [ 5.48672177  8.24457889  8.35971126  1.         11.08869942  1.        ]. \t  \u001b[92m0.9939461759903633\u001b[0m \t 0.9939461759903633\n",
      "28     \t [4.53887316 9.43853878 7.88213488 0.68994356 9.79201622 0.39511097]. \t  0.8182318984709988 \t 0.9939461759903633\n",
      "29     \t [ 5.39185449  9.4731232   7.79076027  0.92510107 11.28704284  1.        ]. \t  0.9920978586673089 \t 0.9939461759903633\n",
      "30     \t [4.21816302 9.54167596 8.37226747 0.53337107 9.73577178 1.        ]. \t  0.9935237054685001 \t 0.9939461759903633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06838742928205427"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_13 = winner_13.getResult()[0]\n",
    "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
    "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
    "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
    "\n",
    "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
    "rmse_winner_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [6.47551049 5.07149688 9.         0.9481426  2.         0.30826454]. \t  0.8841083588522155 \t 0.985981635228031\n",
      "init   \t [1.30128292 8.51503709 6.         0.58757726 8.         0.65878816]. \t  0.8847900532901706 \t 0.985981635228031\n",
      "init   \t [ 1.87389374  2.97833637 11.          0.92510013  4.          0.27446484]. \t  0.8861919064379334 \t 0.985981635228031\n",
      "init   \t [ 9.80899716  0.11774133 11.          0.53577929  6.          0.38130944]. \t  0.8891587951046561 \t 0.985981635228031\n",
      "init   \t [ 6.4601046   0.32698917 10.          0.67912907 15.          0.96967672]. \t  0.985981635228031 \t 0.985981635228031\n",
      "1      \t [ 7.16311394  0.40803841 10.26997981  0.6670552  15.80966208  0.88783009]. \t  0.9859624326856435 \t 0.985981635228031\n",
      "2      \t [ 7.42668317  0.06017749 10.25316899  0.63150124 14.86869497  1.        ]. \t  \u001b[92m0.9932692558615098\u001b[0m \t 0.9932692558615098\n",
      "3      \t [ 7.13927411  0.02522502  9.48221307  0.70698427 15.36070094  0.80966086]. \t  0.9850022648405806 \t 0.9932692558615098\n",
      "4      \t [6.94402957e+00 1.42227333e-03 1.02449434e+01 1.00000000e+00\n",
      " 1.52410135e+01 3.97781590e-01]. \t  0.8848860888195563 \t 0.9932692558615098\n",
      "5      \t [ 6.91495296  0.         10.12389377  0.5        15.37349762  1.        ]. \t  0.9932500554625795 \t 0.9932692558615098\n",
      "6      \t [ 7.16130313  0.66691459  9.92929076  0.55497249 15.11813433  0.73188549]. \t  0.9847622274255188 \t 0.9932692558615098\n",
      "7      \t [ 7.09782378  0.34652817  9.98114606  1.         15.24244441  1.        ]. \t  \u001b[92m0.9939077653049417\u001b[0m \t 0.9939077653049417\n",
      "8      \t [ 6.89927154  0.          9.56860516  0.57571539 14.26324369  0.71752571]. \t  0.9850598759940761 \t 0.9939077653049417\n",
      "9      \t [ 6.99776554  0.12865701  9.8591377   0.58980928 14.88020886  0.75787698]. \t  0.9854199374061685 \t 0.9939077653049417\n",
      "10     \t [ 6.68378678  0.42985602 10.09402105  0.7352889  13.63989363  1.        ]. \t  \u001b[92m0.9941382043874208\u001b[0m \t 0.9941382043874208\n",
      "11     \t [ 6.27958988  0.5072982   9.27771512  0.74820437 13.92115299  1.        ]. \t  0.9936725270907432 \t 0.9941382043874208\n",
      "12     \t [ 7.21360741  0.57842584  9.40131378  0.68547307 13.80208361  1.        ]. \t  0.9932596551780382 \t 0.9941382043874208\n",
      "13     \t [6.78502803e+00 5.60018834e-03 9.47236197e+00 1.00000000e+00\n",
      " 1.36522628e+01 1.00000000e+00]. \t  0.9939269686770547 \t 0.9941382043874208\n",
      "14     \t [ 6.69104222  0.56393424  9.59501179  1.         13.85426716  0.43873213]. \t  0.885361367283712 \t 0.9941382043874208\n",
      "15     \t [ 6.74265063  0.36906025  9.64875867  0.68222166 14.03903078  1.        ]. \t  0.9933700752418391 \t 0.9941382043874208\n",
      "16     \t [ 6.73765629  0.2655676   9.36058678  0.5        13.23225291  0.72280448]. \t  0.9853623290184244 \t 0.9941382043874208\n",
      "17     \t [ 7.62997099  0.01556996 10.03528038  0.63116324 13.5060689   0.73281457]. \t  0.9855591609724037 \t 0.9941382043874208\n",
      "18     \t [ 6.3764527   0.55995512  9.59941212  0.73249541 15.8144722   0.73066163]. \t  0.9847094143665861 \t 0.9941382043874208\n",
      "19     \t [ 6.07438438  0.16579135  8.83152278  0.78191513 14.93168599  0.70025008]. \t  0.9846806133533281 \t 0.9941382043874208\n",
      "20     \t [ 6.67354946  0.18741591  8.49660318  0.70117983 14.06763027  0.68729181]. \t  0.9849686613767016 \t 0.9941382043874208\n",
      "21     \t [ 6.56815033  0.74907496  8.80837508  0.54309801 15.0197506   0.8969239 ]. \t  0.9838932888563124 \t 0.9941382043874208\n",
      "22     \t [ 7.43438601  0.42262976 10.91344576  0.78366592 14.04395359  0.88213265]. \t  0.9858472098255024 \t 0.9941382043874208\n",
      "23     \t [ 7.01661256  0.50489136 10.98394878  0.66196615 15.0462595   0.95788427]. \t  0.9859432297283934 \t 0.9941382043874208\n",
      "24     \t [ 6.55946953  0.36343588  9.28519974  0.70170225 15.27202075  0.79333369]. \t  0.9845989938187786 \t 0.9941382043874208\n",
      "25     \t [ 5.99495722  0.63823551  8.62404335  0.5        14.37788511  0.66048276]. \t  0.8866431647665919 \t 0.9941382043874208\n",
      "26     \t [ 6.41372737  0.69563459 10.44934287  0.71251631 15.61455403  0.86477644]. \t  0.9849638590643678 \t 0.9941382043874208\n",
      "27     \t [ 6.32979025  0.45600353  8.55716014  1.         14.5447803   1.        ]. \t  0.9934804929547516 \t 0.9941382043874208\n",
      "28     \t [ 6.96905606  0.         10.71135936  0.62818359 14.04460672  0.67053666]. \t  0.9853575282963978 \t 0.9941382043874208\n",
      "29     \t [ 7.23692734  0.30531584 10.16503902  0.70291428 13.73570613  0.71067525]. \t  0.9847622205111404 \t 0.9941382043874208\n",
      "30     \t [ 7.4864139   0.          9.09228632  0.61045907 13.67876504  0.72951666]. \t  0.9848774496633658 \t 0.9941382043874208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06117873582821761"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_14 = winner_14.getResult()[0]\n",
    "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
    "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
    "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
    "\n",
    "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
    "rmse_winner_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.87913297  3.99579714 13.29375994  0.50441012 15.29375994  0.81482759]. \t  \u001b[92m0.9871722280484146\u001b[0m \t 0.9871722280484146\n",
      "2      \t [ 6.11575641  3.43474362 13.46998522  0.62715602 15.55952499  0.73081637]. \t  \u001b[92m0.9877771288422933\u001b[0m \t 0.9877771288422933\n",
      "3      \t [ 6.96200487  3.20828424 13.51332663  0.96118746 15.45295665  0.8665903 ]. \t  \u001b[92m0.9894190013589036\u001b[0m \t 0.9894190013589036\n",
      "4      \t [ 6.6780291   3.55504308 14.01467199  0.51087142 15.15702115  0.58879727]. \t  0.799753248114925 \t 0.9894190013589036\n",
      "5      \t [ 6.78461143  3.61258851 13.30882857  0.68163346 15.9226247   0.42700597]. \t  0.7998012565797813 \t 0.9894190013589036\n",
      "6      \t [ 6.53958052  3.8400876  13.64461322  1.         15.59290136  1.        ]. \t  \u001b[92m0.9945702789800519\u001b[0m \t 0.9945702789800519\n",
      "7      \t [ 6.66485283  3.47988441 13.47636272  0.5        15.57317622  1.        ]. \t  0.9925155304021396 \t 0.9945702789800519\n",
      "8      \t [ 6.39250011  3.59811428 12.85235001  1.         14.75432229  0.72936109]. \t  0.9916705993148255 \t 0.9945702789800519\n",
      "9      \t [ 6.33547977  2.8803527  14.26969608  1.         16.17925405  0.98011537]. \t  0.991617790681083 \t 0.9945702789800519\n",
      "10     \t [ 7.26616787  3.92339386 12.88614683  1.         14.57977879  1.        ]. \t  0.9945654778431666 \t 0.9945702789800519\n",
      "11     \t [ 6.82597849  3.7707781  13.13562917  0.98244367 14.97808936  0.84189172]. \t  0.9898702782181538 \t 0.9945702789800519\n",
      "12     \t [ 6.80329107  4.11166219 12.46913647  0.5        14.46079271  0.90345597]. \t  0.9869321898727015 \t 0.9945702789800519\n",
      "13     \t [ 6.49058325  3.23786198 13.92918689  1.         15.85197147  0.90413718]. \t  0.9916129889910555 \t 0.9945702789800519\n",
      "14     \t [ 7.15896811  3.23896432 12.57337376  0.60130803 14.50760838  0.87819675]. \t  0.9870474078928156 \t 0.9945702789800519\n",
      "15     \t [ 6.7998889   3.64348804 12.93029644  0.59740884 14.00499771  1.        ]. \t  0.993341276578464 \t 0.9945702789800519\n",
      "16     \t [ 7.07796023  3.74619599 12.49070009  0.85026455 13.99430525  0.43125958]. \t  0.7998780631539578 \t 0.9945702789800519\n",
      "17     \t [ 7.21789704  2.57137991 14.33260567  0.85707791 16.21918239  1.        ]. \t  0.9944262573191941 \t 0.9945702789800519\n",
      "18     \t [ 5.84853292  4.16286569 13.17233876  0.5        15.02027725  1.        ]. \t  0.9932596633359788 \t 0.9945702789800519\n",
      "19     \t [ 6.61987485  3.66865014 12.8466831   0.57861855 14.62657258  1.        ]. \t  0.9935093036471773 \t 0.9945702789800519\n",
      "20     \t [ 6.02510162  4.40015408 13.52827925  0.5        15.70903572  0.8330894 ]. \t  0.9867353519723947 \t 0.9945702789800519\n",
      "21     \t [ 5.75214794  4.1544834  13.00206705  0.93723012 15.3480148   0.57370975]. \t  0.7996764317224724 \t 0.9945702789800519\n",
      "22     \t [ 6.66274629  2.406765   13.97799983  0.59669006 16.07667558  0.90582429]. \t  0.9869465928685007 \t 0.9945702789800519\n",
      "23     \t [ 6.79439349  2.75765353 13.97624037  0.86153322 16.62716895  1.        ]. \t  0.9942534278690255 \t 0.9945702789800519\n",
      "24     \t [ 6.47145937  4.73354252 13.41887549  0.63428559 14.81008929  1.        ]. \t  0.9937733480604595 \t 0.9945702789800519\n",
      "25     \t [ 6.80529852  2.85162419 14.10437841  0.74793254 16.15683739  1.        ]. \t  0.9940085914604239 \t 0.9945702789800519\n",
      "26     \t [ 6.75429068  2.18610446 14.27174511  1.         16.47548769  0.66094535]. \t  0.8876321368592114 \t 0.9945702789800519\n",
      "27     \t [ 7.41750507  2.19688212 13.56904164  1.         15.92539392  1.        ]. \t  \u001b[92m0.994603885693679\u001b[0m \t 0.994603885693679\n",
      "28     \t [ 6.95027876  2.37610934 13.18055616  1.         14.88508963  0.80763239]. \t  0.9916705993148255 \t 0.994603885693679\n",
      "29     \t [ 6.44509012  2.32433437 13.36014084  1.         15.79967114  1.        ]. \t  0.9945270717903668 \t 0.994603885693679\n",
      "30     \t [ 6.35058107  4.16795482 13.7369635   0.5        15.49443949  1.        ]. \t  0.9926259501893654 \t 0.994603885693679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05852229476565897"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_15 = winner_15.getResult()[0]\n",
    "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
    "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
    "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
    "\n",
    "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
    "rmse_winner_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 6.70665613  8.68114599 13.23709963  0.85719617 15.47341237  0.33989206]. \t  0.887243272705187 \t 0.9857703856869544\n",
      "2      \t [ 6.95786654  8.47511655 12.17864738  0.89959124 15.10452831  0.37511268]. \t  0.8861583153347189 \t 0.9857703856869544\n",
      "3      \t [ 6.75017725  8.06137519 13.02861271  0.9453106  14.58963072  0.12891359]. \t  0.885524598730966 \t 0.9857703856869544\n",
      "4      \t [ 7.06587255  7.73733106 12.85597363  0.88199151 15.50493946  0.4562628 ]. \t  0.886782386796173 \t 0.9857703856869544\n",
      "5      \t [ 7.06829329  8.35415841 12.94668633  0.50195002 14.90245543  0.92024881]. \t  0.9812336490237028 \t 0.9857703856869544\n",
      "6      \t [ 7.11446464  8.37984932 12.92837871  1.         15.03934102  0.65055397]. \t  0.7894987111557604 \t 0.9857703856869544\n",
      "7      \t [ 6.98700204  8.24749409 12.81246055  0.5        15.14710127  0.1       ]. \t  0.8858894385320114 \t 0.9857703856869544\n",
      "8      \t [ 5.989707    8.16829452 12.73302379  0.5        15.17511284  0.88473348]. \t  0.9821409998688068 \t 0.9857703856869544\n",
      "9      \t [ 6.71500762  7.63092944 12.22830034  0.5        14.58902694  0.8826786 ]. \t  0.9815697092456946 \t 0.9857703856869544\n",
      "10     \t [ 7.83066101  7.65564229 12.52512461  0.5        14.47978543  0.50620022]. \t  0.8881026327859883 \t 0.9857703856869544\n",
      "11     \t [ 6.5200549   7.50689667 13.31588573  0.5        14.78387037  0.88836188]. \t  0.9814160819922124 \t 0.9857703856869544\n",
      "12     \t [ 6.16630964  8.23593633 13.82388521  0.5        15.43063066  0.69593644]. \t  0.9821458007291209 \t 0.9857703856869544\n",
      "13     \t [ 6.09548778  8.45632016 13.42326506  0.5        14.58416581  0.73021035]. \t  0.9821986106074337 \t 0.9857703856869544\n",
      "14     \t [ 6.44217957  8.07409031 13.11474795  0.5        14.98156748  0.71137016]. \t  0.9813824759700133 \t 0.9857703856869544\n",
      "15     \t [ 5.63356735  9.04723843 13.42698878  0.60553824 15.50271109  0.73781476]. \t  0.9826642583802295 \t 0.9857703856869544\n",
      "16     \t [ 5.49343283  8.36572202 13.5450998   1.         15.21326013  0.32985606]. \t  0.7894747083753307 \t 0.9857703856869544\n",
      "17     \t [ 7.02108051  7.78983548 13.10161317  0.5        13.92150001  0.9426278 ]. \t  0.981339275003179 \t 0.9857703856869544\n",
      "18     \t [ 7.49351808  7.58660438 13.61585986  0.5        14.62929243  0.55667111]. \t  0.8879970162099314 \t 0.9857703856869544\n",
      "19     \t [ 6.34869389  8.70246695 12.27458634  0.5        14.32028988  1.        ]. \t  \u001b[92m0.993552509315721\u001b[0m \t 0.993552509315721\n",
      "20     \t [ 7.16220682  8.42025725 12.42340871  0.5        14.1413502   0.54678375]. \t  0.8880642282543297 \t 0.993552509315721\n",
      "21     \t [ 6.08597114  9.09569847 12.53636327  0.5        15.15776519  0.9071891 ]. \t  0.9821842076116343 \t 0.993552509315721\n",
      "22     \t [ 5.71652219  8.63129027 12.15143482  0.79216709 14.71604061  0.42129236]. \t  0.8853037601245134 \t 0.993552509315721\n",
      "23     \t [ 7.04040861  7.02838146 12.92406169  0.82206949 14.41580324  0.53822276]. \t  0.8853421329887752 \t 0.993552509315721\n",
      "24     \t [ 6.21145855  9.0833034  14.10221551  0.5        15.29694936  0.33406611]. \t  0.8852365260927085 \t 0.993552509315721\n",
      "25     \t [ 7.14563919  7.63161148 12.86707909  0.5        14.57401915  1.        ]. \t  0.9934756970718358 \t 0.993552509315721\n",
      "26     \t [ 6.31996934  8.42918313 11.79114094  0.5        15.02941914  1.        ]. \t  0.9935285050141504 \t 0.993552509315721\n",
      "27     \t [ 6.22784528  8.89768264 13.60924154  0.5        15.28819928  1.        ]. \t  0.9935285050141504 \t 0.993552509315721\n",
      "28     \t [ 6.32154298  8.39158311 12.3581125   0.5        14.73218169  0.95036481]. \t  0.9812912596240441 \t 0.993552509315721\n",
      "29     \t [ 5.89849347  8.52787583 12.39941258  0.67832826 16.0135404   0.58958236]. \t  0.8838731192388757 \t 0.993552509315721\n",
      "30     \t [ 6.35843246  7.53847413 11.79856735  0.78199188 15.45111557  0.51364375]. \t  0.8862015110467015 \t 0.993552509315721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07156566262259194"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_16 = winner_16.getResult()[0]\n",
    "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
    "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
    "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
    "\n",
    "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
    "rmse_winner_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.98873974  1.08077324  6.00000005  0.94509265 13.13078826  0.63376027]. \t  0.8920248356821929 \t 0.991204913651782\n",
      "2      \t [ 3.17060621  1.22021866  5.56515308  0.54792426 14.08786939  0.83001847]. \t  0.9854967284901979 \t 0.991204913651782\n",
      "3      \t [ 2.73202236  1.19491569  6.47411999  0.51260273 13.97829477  0.7898156 ]. \t  0.990206344801282 \t 0.991204913651782\n",
      "4      \t [ 2.32174884  0.97959702  5.67615329  0.5        13.70709748  0.85199642]. \t  0.9850166349913599 \t 0.991204913651782\n",
      "5      \t [ 8.98955998  5.70226534 10.05998285  0.7054428  14.6586818   0.96031875]. \t  0.991027275873878 \t 0.991204913651782\n",
      "6      \t [ 2.67076351  1.1404331   5.88364649  1.         14.02803052  0.3208577 ]. \t  0.8873777037082623 \t 0.991204913651782\n",
      "7      \t [ 2.98170039  0.7410386   5.97066102  0.5        13.73572365  0.62350931]. \t  0.890978243017138 \t 0.991204913651782\n",
      "8      \t [ 2.76489443  1.15671175  5.95326811  0.96088413 13.84838028  1.        ]. \t  0.9841285120640596 \t 0.991204913651782\n",
      "9      \t [ 2.51060033  0.83873131  5.8726313   0.5        14.75665219  1.        ]. \t  0.9840180962871153 \t 0.991204913651782\n",
      "10     \t [ 9.47167536  5.28043235 10.59825401  0.7468588  14.35445005  1.        ]. \t  \u001b[92m0.9937589434052333\u001b[0m \t 0.9937589434052333\n",
      "11     \t [ 9.15512319  6.00853412 10.54943229  0.77019423 14.05394932  0.96008535]. \t  0.9902879696597554 \t 0.9937589434052333\n",
      "12     \t [ 9.67382936  5.97783674 10.40632028  0.84097108 14.67750856  0.97990117]. \t  0.9907344249158551 \t 0.9937589434052333\n",
      "13     \t [ 9.40132604  5.67509231 10.2611985   0.8127008  14.329285    0.40088461]. \t  0.8880977478480444 \t 0.9937589434052333\n",
      "14     \t [ 9.42495975  5.74071289 10.27383483  0.5        14.33822642  1.        ]. \t  0.9930820266643595 \t 0.9937589434052333\n",
      "15     \t [ 9.34221927  5.6765001  10.24026074  1.         14.31573788  1.        ]. \t  \u001b[92m0.994507874848532\u001b[0m \t 0.994507874848532\n",
      "16     \t [ 8.7568588   5.71155021 10.95672583  0.74640817 14.99893615  0.91830244]. \t  0.9910320847547555 \t 0.994507874848532\n",
      "17     \t [10.          5.83347871 10.81736382  0.89837294 13.72767979  0.94412626]. \t  0.9909840757367571 \t 0.994507874848532\n",
      "18     \t [ 9.02601674  5.25874118 10.39890702  0.71180694 15.56591629  0.93180689]. \t  0.9909264647215591 \t 0.994507874848532\n",
      "19     \t [ 8.73152711  6.04677633 10.30261962  0.75405347 15.54617317  0.84270868]. \t  0.9910704874887016 \t 0.994507874848532\n",
      "20     \t [ 8.99787935  5.68275058 10.47275315  0.74412659 15.11655115  0.90536952]. \t  0.9909456692691005 \t 0.994507874848532\n",
      "21     \t [ 8.14341132  5.4110395  10.32015622  0.61589927 15.55931985  1.        ]. \t  0.9935621125574908 \t 0.994507874848532\n",
      "22     \t [ 8.67398262  5.49732413  9.51767201  0.64928663 15.79466908  1.        ]. \t  0.9936965351251462 \t 0.994507874848532\n",
      "23     \t [ 8.51719644  5.41036945 10.01217692  0.61814645 15.76519015  0.38033226]. \t  0.889672395736269 \t 0.994507874848532\n",
      "24     \t [ 8.54222187  5.48094067 10.16233606  1.         16.01290478  1.        ]. \t  0.9943926554455622 \t 0.994507874848532\n",
      "25     \t [ 2.10021125  0.60441986  6.36355912  0.71328684 14.18601142  0.89729434]. \t  0.9916561903727464 \t 0.994507874848532\n",
      "26     \t [ 9.50116853  5.79043278 11.49963484  0.86222858 14.30231234  0.96466823]. \t  0.991617782245663 \t 0.994507874848532\n",
      "27     \t [ 2.56792248  0.90855451  5.9749742   0.60337815 14.18970812  0.88637301]. \t  0.9858567854080539 \t 0.994507874848532\n",
      "28     \t [ 2.56561367  0.25194711  6.42490696  0.98842211 15.0539966   0.99622948]. \t  0.9913297342222379 \t 0.994507874848532\n",
      "29     \t [ 2.08535069  0.99984202  6.52146422  0.79062353 13.21785575  0.70241952]. \t  0.9910032816671541 \t 0.994507874848532\n",
      "30     \t [ 2.31898687  0.10863816  5.60995211  0.96823228 14.93419221  1.        ]. \t  0.984162117671402 \t 0.994507874848532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06555490361552406"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_17 = winner_17.getResult()[0]\n",
    "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
    "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
    "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
    "\n",
    "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
    "rmse_winner_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8299977850025021 \t 0.9872154265261087\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.984824605557074 \t 0.9872154265261087\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8300890052204674 \t 0.9872154265261087\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9802782361972252 \t 0.9872154265261087\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9872154265261087 \t 0.9872154265261087\n",
      "1      \t [ 3.4495367   2.7113321  10.49805101  0.76584925 13.85743429  0.99304665]. \t  0.9871098052483424 \t 0.9872154265261087\n",
      "2      \t [ 3.71387913  2.23569939 10.91583447  0.84035612 14.6487373   0.94374147]. \t  0.9863560777847243 \t 0.9872154265261087\n",
      "3      \t [ 3.81232174  2.17742841 11.15173601  0.77439132 13.74350802  0.62382368]. \t  0.8295033139524107 \t 0.9872154265261087\n",
      "4      \t [ 3.06171254  2.70305359 11.27139119  0.80101369 14.26753921  0.99332506]. \t  \u001b[92m0.9872298274476243\u001b[0m \t 0.9872298274476243\n",
      "5      \t [ 3.0937197   2.3520814  10.65431942  0.8041952  14.33558557  0.41888056]. \t  0.8297385493318119 \t 0.9872298274476243\n",
      "6      \t [ 3.37822951  2.32443533 10.90397371  0.5        14.17885908  1.        ]. \t  \u001b[92m0.9936773242164684\u001b[0m \t 0.9936773242164684\n",
      "7      \t [ 3.34617889  2.32047689 10.90391877  1.         14.10882264  1.        ]. \t  \u001b[92m0.9950119556398151\u001b[0m \t 0.9950119556398151\n",
      "8      \t [ 3.15152548  1.71825178 11.78068887  0.79270034 14.72636628  0.78608801]. \t  0.986879365612689 \t 0.9950119556398151\n",
      "9      \t [ 3.99082688  3.42261871 10.78658036  0.80282379 14.31818114  0.77612755]. \t  0.9867449444278895 \t 0.9950119556398151\n",
      "10     \t [ 3.82672762  2.77541288 11.70100681  0.79948443 14.8538824   0.53045511]. \t  0.8296425350295257 \t 0.9950119556398151\n",
      "11     \t [ 2.62247173  1.76102953 11.91367609  0.64791846 13.89861662  0.61081291]. \t  0.828245523951206 \t 0.9950119556398151\n",
      "12     \t [ 3.43660557  3.29827323 10.39697951  0.80772576 14.75828028  1.        ]. \t  0.9948727319352743 \t 0.9950119556398151\n",
      "13     \t [ 3.61923203  2.98365477 10.79385814  0.79432647 14.43422485  0.89916389]. \t  0.9869369745536033 \t 0.9950119556398151\n",
      "14     \t [ 3.69599681  3.73452321  9.97459011  0.78903863 14.08096288  0.97678023]. \t  0.9866777410954825 \t 0.9950119556398151\n",
      "15     \t [ 4.22167011  3.23519677  9.91947671  0.83442519 14.58154053  0.87631654]. \t  0.9866345213218097 \t 0.9950119556398151\n",
      "16     \t [ 3.20214931  2.07604043 11.53133109  0.75476361 14.35166665  0.87748807]. \t  0.9870377971836248 \t 0.9950119556398151\n",
      "17     \t [ 4.11428284  3.97881499 10.20076827  0.5        14.77867973  1.        ]. \t  0.9935285011421543 \t 0.9950119556398151\n",
      "18     \t [ 3.93755936  3.90827075 10.12686216  1.         14.71288093  0.47428954]. \t  0.8852845384295608 \t 0.9950119556398151\n",
      "19     \t [ 2.46328839  2.06038617 11.32067643  0.79909387 14.92186369  1.        ]. \t  0.9948823333793316 \t 0.9950119556398151\n",
      "20     \t [ 3.2450958   2.11853586 11.40313432  0.87811771 15.53807677  0.80967814]. \t  0.9874986671897977 \t 0.9950119556398151\n",
      "21     \t [ 2.70572498  2.33373154 11.91498383  1.         15.04804254  0.46244795]. \t  0.8854093472457428 \t 0.9950119556398151\n",
      "22     \t [ 4.0761979   3.67950207 10.2660939   0.93430392 14.48951914  1.        ]. \t  0.9950023539191868 \t 0.9950119556398151\n",
      "23     \t [ 3.91419853  3.52717291 10.17181717  0.5        14.4316251   0.7958675 ]. \t  0.9852902820932696 \t 0.9950119556398151\n",
      "24     \t [ 2.96319995  1.79135101 11.31255716  0.57037235 15.00795827  0.61552178]. \t  0.8286967908538947 \t 0.9950119556398151\n",
      "25     \t [ 3.0252933   2.14686363 11.54203448  1.         15.04787467  1.        ]. \t  0.9949927521985584 \t 0.9950119556398151\n",
      "26     \t [ 4.21563961  2.99457703 10.65323787  0.74050854 15.58580172  0.80484541]. \t  0.9863944789975293 \t 0.9950119556398151\n",
      "27     \t [ 2.30554792  2.43509671 10.95264697  0.72732213 14.04682962  1.        ]. \t  0.9949831514459291 \t 0.9950119556398151\n",
      "28     \t [ 3.67689516  2.4922198  10.18027332  0.98979347 15.54892915  1.        ]. \t  0.9949015359908747 \t 0.9950119556398151\n",
      "29     \t [ 3.92103294  3.41473556  9.79027448  0.70524646 15.4291424   1.        ]. \t  0.9937733458478902 \t 0.9950119556398151\n",
      "30     \t [ 3.33123321  2.94257727 10.74016075  0.70954383 15.78643077  1.        ]. \t  0.9947671142529328 \t 0.9950119556398151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05621073077041232"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_18 = winner_18.getResult()[0]\n",
    "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
    "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
    "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
    "\n",
    "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
    "rmse_winner_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.44186754  0.49904519 11.00953372  0.83989094  4.33651137  0.75973315]. \t  \u001b[92m0.9877675392926877\u001b[0m \t 0.9877675392926877\n",
      "2      \t [ 1.09454829  0.9310788  10.34974528  0.78930951  4.55644921  0.6858882 ]. \t  0.9869946101165175 \t 0.9877675392926877\n",
      "3      \t [ 0.14614131  0.77427797 10.15287799  0.87343308  4.63828612  0.81918252]. \t  0.987426682912159 \t 0.9877675392926877\n",
      "4      \t [ 0.47924995  1.05978835 10.4224166   0.77236032  3.84927824  0.74470246]. \t  0.9872394517107951 \t 0.9877675392926877\n",
      "5      \t [ 0.53249046  0.61434777 10.38602246  0.98798758  4.30105736  0.13230192]. \t  0.8918712916920164 \t 0.9877675392926877\n",
      "6      \t [ 0.57690728  0.62032835 10.41714273  0.5         4.34446896  0.81778572]. \t  0.9868169798752001 \t 0.9877675392926877\n",
      "7      \t [ 0.65998171  0.65112616 10.39112234  1.          4.24242414  0.97818053]. \t  \u001b[92m0.9908016413866845\u001b[0m \t 0.9908016413866845\n",
      "8      \t [ 0.40177769  1.48276302 11.01498016  0.93422957  4.75826093  0.63225222]. \t  0.8920489152263866 \t 0.9908016413866845\n",
      "9      \t [0.74659468 1.00285312 9.29421548 0.76449321 4.15861704 0.60413617]. \t  0.8916504492135257 \t 0.9908016413866845\n",
      "10     \t [0.6165678  1.84297888 9.91941768 0.87594101 4.67740302 0.55582779]. \t  0.8913912042085802 \t 0.9908016413866845\n",
      "11     \t [ 0.55915904  1.07163402 10.21299041  0.85249422  4.39184134  0.61156279]. \t  0.8913191915112989 \t 0.9908016413866845\n",
      "12     \t [ 1.33733254  0.65749618 10.84456317  0.68892176  3.53765594  0.53876624]. \t  0.891861683264441 \t 0.9908016413866845\n",
      "13     \t [ 0.          0.40235707 10.65988124  0.81111944  3.2946154   0.65471496]. \t  0.891199167998275 \t 0.9908016413866845\n",
      "14     \t [1.23904006 1.61373689 9.43495303 0.69310226 5.21125608 1.        ]. \t  \u001b[92m0.994963946829955\u001b[0m \t 0.994963946829955\n",
      "15     \t [ 1.30991128  2.01437853 10.44242501  0.72666121  5.1324428   1.        ]. \t  \u001b[92m0.9961785515595522\u001b[0m \t 0.9961785515595522\n",
      "16     \t [ 0.61078261  1.49254793 10.21858396  0.86536707  5.63779232  1.        ]. \t  \u001b[92m0.9966250271435676\u001b[0m \t 0.9966250271435676\n",
      "17     \t [ 1.29206317  1.74555318 10.05934444  0.94483671  5.46485638  0.35403316]. \t  0.8921689359045154 \t 0.9966250271435676\n",
      "18     \t [0.62515237 0.40562667 9.48139874 0.83058723 5.25778174 0.94555001]. \t  0.9863368960563843 \t 0.9966250271435676\n",
      "19     \t [ 0.35524096  0.22478711 10.45088844  0.97869261  5.37327017  0.77706492]. \t  0.9872154469252177 \t 0.9966250271435676\n",
      "20     \t [ 0.55570337  1.21585461 11.23624224  0.66360743  3.37292172  1.        ]. \t  \u001b[92m0.9967114426292226\u001b[0m \t 0.9967114426292226\n",
      "21     \t [ 1.20917515  0.90048713 10.23481658  0.72314821  5.39762021  1.        ]. \t  0.9962505634271079 \t 0.9967114426292226\n",
      "22     \t [ 0.71851962  0.93243673 10.44417321  0.5292759   3.04103863  1.        ]. \t  0.9957704838952067 \t 0.9967114426292226\n",
      "23     \t [6.04421676e-03 0.00000000e+00 9.95338871e+00 9.70598975e-01\n",
      " 4.75258998e+00 5.81852317e-01]. \t  0.8909015168022538 \t 0.9967114426292226\n",
      "24     \t [1.00577421 1.58279046 9.96421833 1.         5.14142779 1.        ]. \t  0.9955976546524913 \t 0.9967114426292226\n",
      "25     \t [0.91179542 1.74849287 9.99436829 0.5        5.23487236 0.80489593]. \t  0.9862840872843663 \t 0.9967114426292226\n",
      "26     \t [ 0.60737144  0.75617456 10.77568283  0.70986904  3.50406662  0.89071904]. \t  0.9875467059411766 \t 0.9967114426292226\n",
      "27     \t [ 1.13220869  1.84733079 10.73085464  0.55235789  3.53393578  0.92726076]. \t  0.986783376342177 \t 0.9967114426292226\n",
      "28     \t [ 0.54020656  1.49064848 10.74585657  0.65873005  3.00433435  0.52461582]. \t  0.8915448347809599 \t 0.9967114426292226\n",
      "29     \t [ 1.28848004  1.45050209 11.32880067  0.65171749  4.3387749   0.99743721]. \t  0.9870810249798784 \t 0.9967114426292226\n",
      "30     \t [ 0.54872802  0.38553228 10.24239302  0.82425146  4.98772787  1.        ]. \t  0.9966874382585079 \t 0.9967114426292226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05356415478998106"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_19 = winner_19.getResult()[0]\n",
    "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
    "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
    "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
    "\n",
    "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
    "rmse_winner_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.48017827  5.69759111 11.55535747  0.52020839 17.33392338  0.68759221]. \t  0.9911328909287936 \t 0.9913825280594238\n",
      "2      \t [ 5.38475127  6.33149911 12.32789748  0.52408608 16.81450591  0.79072943]. \t  0.9911808995319352 \t 0.9913825280594238\n",
      "3      \t [ 4.78713418  5.60090228 12.12415149  0.5        16.81790962  0.99959599]. \t  0.9912913129580248 \t 0.9913825280594238\n",
      "4      \t [ 5.69530054  5.46029212 12.31264075  0.5        16.75068818  0.95375413]. \t  0.9912577091483947 \t 0.9913825280594238\n",
      "5      \t [ 5.23351612  5.67364248 12.15720202  0.50056505 16.72054058  0.2276499 ]. \t  0.8882466425542868 \t 0.9913825280594238\n",
      "6      \t [ 5.35046234  5.85031128 11.88912068  1.         16.67303394  0.97707684]. \t  \u001b[92m0.9913969415649267\u001b[0m \t 0.9913969415649267\n",
      "7      \t [ 5.35180218  5.86492324 11.8976645   0.5        16.66703557  0.96314614]. \t  \u001b[92m0.9914113344658789\u001b[0m \t 0.9914113344658789\n",
      "8      \t [ 5.41160954  5.75128198 11.99254047  0.74161906 18.14465782  0.80771909]. \t  \u001b[92m0.9915409589389316\u001b[0m \t 0.9915409589389316\n",
      "9      \t [ 5.13733249  5.79794282 12.70205498  0.80324481 16.26826031  0.93301741]. \t  0.9912481086723366 \t 0.9915409589389316\n",
      "10     \t [ 5.4338434   5.08888862 11.32700325  0.72869471 18.18889176  0.7338455 ]. \t  0.9913441265700479 \t 0.9915409589389316\n",
      "11     \t [ 6.06526867  5.59426133 11.44324087  0.80559549 18.16401025  0.52905183]. \t  0.8890627651608618 \t 0.9915409589389316\n",
      "12     \t [ 5.1764399   5.81470354 11.1918945   0.8248986  18.21450125  0.52340594]. \t  0.886652758174553 \t 0.9915409589389316\n",
      "13     \t [ 5.28236587  5.68556029 12.25685626  0.77767373 17.16569203  0.83127524]. \t  \u001b[92m0.9916801808457598\u001b[0m \t 0.9916801808457598\n",
      "14     \t [ 5.55828364  5.60805743 11.41800227  0.5        18.26502015  1.        ]. \t  \u001b[92m0.9926163342253229\u001b[0m \t 0.9926163342253229\n",
      "15     \t [ 5.45938496  5.44918532 11.63885794  0.5        18.30977777  0.40681159]. \t  0.8879537900751223 \t 0.9926163342253229\n",
      "16     \t [ 5.09146293  4.94592462 12.07663665  0.79026423 16.22486686  0.91786479]. \t  0.9917281887574737 \t 0.9926163342253229\n",
      "17     \t [ 5.20995691  5.54057017 12.2266729   0.67306676 16.35670296  0.83752133]. \t  0.9916273745628722 \t 0.9926163342253229\n",
      "18     \t [ 5.11049105  4.70041153 11.51574761  0.7263315  16.98559003  1.        ]. \t  \u001b[92m0.9944214481617455\u001b[0m \t 0.9944214481617455\n",
      "19     \t [ 5.74326158  6.46761926 12.62907974  0.81379961 15.86977714  0.92345096]. \t  0.991473740809968 \t 0.9944214481617455\n",
      "20     \t [ 4.85524638  6.61635158 12.44959315  0.80832222 15.93351784  0.89809471]. \t  0.9913297190108247 \t 0.9944214481617455\n",
      "21     \t [ 5.24291264  6.44623346 12.90685272  0.5        15.98823223  0.44348235]. \t  0.887228880633948 \t 0.9944214481617455\n",
      "22     \t [ 5.91844703  4.7556337  11.55320103  0.71833745 16.30042584  0.82726046]. \t  0.9913489268772198 \t 0.9944214481617455\n",
      "23     \t [ 5.85065923  4.95558586 11.76803502  0.86733397 17.26594402  1.        ]. \t  \u001b[92m0.9945414660741747\u001b[0m \t 0.9945414660741747\n",
      "24     \t [ 5.21990455  6.56492279 12.89925963  1.         16.40101021  1.        ]. \t  \u001b[92m0.9949687411209943\u001b[0m \t 0.9949687411209943\n",
      "25     \t [ 5.57117587  4.33786937 12.05925243  0.5        16.5195759   1.        ]. \t  0.9931252248654824 \t 0.9949687411209943\n",
      "26     \t [ 5.27168487  6.32145115 12.50307503  0.76722643 16.14316949  1.        ]. \t  0.9945318639386898 \t 0.9949687411209943\n",
      "27     \t [ 5.57083065  6.53415505 12.03942093  1.         16.01079407  0.33543218]. \t  0.8978530618498972 \t 0.9949687411209943\n",
      "28     \t [ 4.31752082  6.38074211 12.5129647   0.9274713  16.6845838   0.69050784]. \t  0.9920066399002656 \t 0.9949687411209943\n",
      "29     \t [ 6.35882785  5.81196724 12.1003798   0.66298331 16.09618839  0.63652038]. \t  0.8864847492212501 \t 0.9949687411209943\n",
      "30     \t [ 4.46191114  6.20321044 11.58654016  0.7037307  16.38516641  0.58514446]. \t  0.8889619542851142 \t 0.9949687411209943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06279138019663709"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_20 = winner_20.getResult()[0]\n",
    "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
    "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
    "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
    "\n",
    "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
    "rmse_winner_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.291132357728754, -5.266649116320135)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.080458324746212, -4.90071463592742)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.2337089551039755, -5.4072485462710995)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.685694576876569, -5.623988944124582)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.261083345655753, -5.250958922072222)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.313302238528062, -5.35917258132517)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.715847726815147, -5.748442839993327)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0983803459040615, -5.109447450681447)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.552162901741616, -5.552162848266934)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.090548830026827, -5.088211480354667)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.033692562698344, -4.14021018940619)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.014714443886075, -5.181965431446257)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.194869739346894, -5.1070651389015)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.288276518234649, -5.139299303800919)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.223856939399704, -5.222076157332265)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.067417114524272, -5.044064265067675)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.1879668069170135, -5.204440003377566)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.287326759541631, -5.3007113578228084)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.802594036057416, -5.717306299494743)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.288275491318392, -5.292085052027642)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
