{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hartmann3 synthetic function:\n",
    "\n",
    "GP CBM versus STP nu = 3 CBM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/hart3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Hartmann3'\n",
    "n_test = 50 # test points\n",
    "df = 3 # nu\n",
    "\n",
    "util_loser = 'CBMinimized'\n",
    "util_winner = 'tCBMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Hartmann3':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = -3.86278\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 3\n",
    "\n",
    "# 3-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub]),\n",
    "             'x3_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test) \n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    x3_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training, x3_training):\n",
    "       \n",
    "        value = np.array([x1_training, x2_training, x3_training])\n",
    "      \n",
    "        a = np.array([[3.0, 10, 30],\n",
    "                      [0.1, 10, 35],\n",
    "                      [3.0, 10, 30],\n",
    "                      [0.1, 10, 35]])\n",
    "        \n",
    "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "      \n",
    "        p = np.array([[.3689, .1170, .2673],\n",
    "                      [.4699, .4387, .7470],\n",
    "                      [.1091, .8732, .5547],\n",
    "                      [.3810, .5743, .8828]])\n",
    "  \n",
    "        s = 0\n",
    "        for i in [0,1,2,3]:\n",
    "            sm = a[i,0]*(value[0]-p[i,0])**2\n",
    "            sm += a[i,1]*(value[1]-p[i,1])**2\n",
    "            sm += a[i,2]*(value[2]-p[i,2])**2\n",
    "            s += alpha[i]*np.exp(-sm)\n",
    "        result = -s\n",
    "        \n",
    "        return operator * result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 999\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - CBM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'CBMinimized': self.CBMinimized,\n",
    "            'tCBMinimized': self.tCBMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def CBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    \n",
    "    def tCBMinimized(self, mean, std, v=1, delta=.1):\n",
    "        \n",
    "        Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
    "        return (y_global_orig - mean + self.eps) + np.sqrt(Beta_CBM) * (std + self.eps)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.3951473341797507\n",
      "2      \t [0.        0.2700992 1.       ]. \t  0.8671914125258264 \t 2.3951473341797507\n",
      "3      \t [1.         0.6447178  0.66885759]. \t  1.31832882517784 \t 2.3951473341797507\n",
      "4      \t [0.40665772 0.6349177  0.78320317]. \t  \u001b[92m3.231466256196021\u001b[0m \t 3.231466256196021\n",
      "5      \t [0.60536293 0.51291895 1.        ]. \t  2.017550633519514 \t 3.231466256196021\n",
      "6      \t [0.62135069 0.93625785 0.85707277]. \t  0.9890564017586926 \t 3.231466256196021\n",
      "7      \t [0.5846659  0.62915431 0.43342378]. \t  0.5966358503228525 \t 3.231466256196021\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.231466256196021\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.231466256196021\n",
      "10     \t [0.18973654 0.63669548 1.        ]. \t  1.9861185753692279 \t 3.231466256196021\n",
      "11     \t [0.05252872 0.         0.        ]. \t  0.07572558665136561 \t 3.231466256196021\n",
      "12     \t [1.         0.43589964 0.        ]. \t  0.012842475764119605 \t 3.231466256196021\n",
      "13     \t [1.         0.34147472 0.84958578]. \t  2.45879780467178 \t 3.231466256196021\n",
      "14     \t [0.16837629 1.         0.6613767 ]. \t  1.9295182691408037 \t 3.231466256196021\n",
      "15     \t [1.         0.61210459 1.        ]. \t  1.9695040528827918 \t 3.231466256196021\n",
      "16     \t [0.39485941 0.28698957 0.        ]. \t  0.087652037601302 \t 3.231466256196021\n",
      "17     \t [0.         0.12534606 0.62701404]. \t  0.3311253467021013 \t 3.231466256196021\n",
      "18     \t [0.26887669 0.40462458 0.77757455]. \t  2.8397438898604017 \t 3.231466256196021\n",
      "19     \t [0.         0.51324049 0.81794445]. \t  \u001b[92m3.652633314797172\u001b[0m \t 3.652633314797172\n",
      "20     \t [1.         0.34982629 0.62908547]. \t  0.8769048123684444 \t 3.652633314797172\n",
      "21     \t [0.94017816 1.         0.        ]. \t  4.961925311614676e-05 \t 3.652633314797172\n",
      "22     \t [0.76631075 0.476407   0.79756138]. \t  3.3228458877536786 \t 3.652633314797172\n",
      "23     \t [0.         0.74292948 0.84101514]. \t  2.7828690131699756 \t 3.652633314797172\n",
      "24     \t [0.66530603 0.         0.        ]. \t  0.07855646839384708 \t 3.652633314797172\n",
      "25     \t [0.78109698 0.26554425 0.86318093]. \t  1.7478009516631305 \t 3.652633314797172\n",
      "26     \t [1.         0.38795843 1.        ]. \t  1.4666951076378372 \t 3.652633314797172\n",
      "27     \t [0.        1.        0.4987618]. \t  2.252797426384988 \t 3.652633314797172\n",
      "28     \t [1.         0.87452358 0.85179558]. \t  1.3475879280514702 \t 3.652633314797172\n",
      "29     \t [0.         0.54759928 1.        ]. \t  2.049813438885181 \t 3.652633314797172\n",
      "30     \t [0.5745486 1.        0.       ]. \t  0.00017311445809267245 \t 3.652633314797172\n",
      "31     \t [0.79506817 0.66405481 0.85972703]. \t  3.3355374927851993 \t 3.652633314797172\n",
      "32     \t [0.         0.39566532 0.64330928]. \t  1.3370705341506066 \t 3.652633314797172\n",
      "33     \t [0.         0.63369858 0.        ]. \t  0.0055589674563903445 \t 3.652633314797172\n",
      "34     \t [1.         0.         0.65008958]. \t  0.1430276900666697 \t 3.652633314797172\n",
      "35     \t [0.28072542 0.         0.21460982]. \t  0.7839282977169199 \t 3.652633314797172\n",
      "36     \t [0.30666281 0.66981375 0.07535099]. \t  0.01719656558539456 \t 3.652633314797172\n",
      "37     \t [0.         0.         0.81632603]. \t  0.24486434399729 \t 3.652633314797172\n",
      "38     \t [0.         1.         0.75554969]. \t  1.0771879252553802 \t 3.652633314797172\n",
      "39     \t [0.7780462  0.69536408 0.        ]. \t  0.0025577974877704305 \t 3.652633314797172\n",
      "40     \t [0.         0.89805325 0.2508766 ]. \t  0.18191343712178593 \t 3.652633314797172\n",
      "41     \t [0.77225512 0.73777751 0.98261786]. \t  1.7744993446837725 \t 3.652633314797172\n",
      "42     \t [0.79768395 0.         0.2310823 ]. \t  0.4829970507403 \t 3.652633314797172\n",
      "43     \t [0.56545867 0.5382737  0.81649572]. \t  \u001b[92m3.6828833055883425\u001b[0m \t 3.6828833055883425\n",
      "44     \t [0.15833771 0.57378591 0.82578222]. \t  \u001b[92m3.7725827038771245\u001b[0m \t 3.7725827038771245\n",
      "45     \t [0.16070154 0.5744728  0.82505121]. \t  3.7684501258391823 \t 3.7725827038771245\n",
      "46     \t [0.8298604  0.53495542 0.85237261]. \t  3.735684388796839 \t 3.7725827038771245\n",
      "47     \t [0.15965478 0.57380332 0.82449131]. \t  3.7664517575719785 \t 3.7725827038771245\n",
      "48     \t [0.1622552  0.57452263 0.82362839]. \t  3.761477581235936 \t 3.7725827038771245\n",
      "49     \t [0.79270818 0.53743271 0.84333642]. \t  3.7372233641909536 \t 3.7725827038771245\n",
      "50     \t [0.16140532 0.57425444 0.82367082]. \t  3.76194864270252 \t 3.7725827038771245\n",
      "51     \t [0.66955862 0.54167903 0.82397091]. \t  3.699585800779771 \t 3.7725827038771245\n",
      "52     \t [0.16398111 0.57502955 0.82324036]. \t  3.7590020900019985 \t 3.7725827038771245\n",
      "53     \t [0.80956544 0.53825044 0.84805153]. \t  3.741306594960642 \t 3.7725827038771245\n",
      "54     \t [0.16222574 0.57425016 0.8234845 ]. \t  3.7611075957338453 \t 3.7725827038771245\n",
      "55     \t [0.16338935 0.57445119 0.8232847 ]. \t  3.759955381956115 \t 3.7725827038771245\n",
      "56     \t [1.         0.55607251 0.85719287]. \t  3.6709952786157096 \t 3.7725827038771245\n",
      "57     \t [0.86247245 0.54406447 0.85489251]. \t  3.730905401519582 \t 3.7725827038771245\n",
      "58     \t [0.85364885 0.54495782 0.85301739]. \t  3.7337511281785396 \t 3.7725827038771245\n",
      "59     \t [0.84615283 0.54566434 0.85153383]. \t  3.7356224736636534 \t 3.7725827038771245\n",
      "60     \t [0.27819583 0.58560211 0.80518469]. \t  3.6248276692347705 \t 3.7725827038771245\n",
      "61     \t [0.83829475 0.54639319 0.85008206]. \t  3.7371684030957977 \t 3.7725827038771245\n",
      "62     \t [0.83239386 0.54653257 0.84920505]. \t  3.73826044743728 \t 3.7725827038771245\n",
      "63     \t [0.14863798 0.56859312 0.82772202]. \t  \u001b[92m3.7860906698502057\u001b[0m \t 3.7860906698502057\n",
      "64     \t [0.15003821 0.56902871 0.82714721]. \t  3.7832861950190737 \t 3.7860906698502057\n",
      "65     \t [0.15157319 0.56959032 0.82664921]. \t  3.780656150467399 \t 3.7860906698502057\n",
      "66     \t [0.15291042 0.57022206 0.82627396]. \t  3.7784321408859265 \t 3.7860906698502057\n",
      "67     \t [0.15393302 0.5705732  0.82595254]. \t  3.776680010584766 \t 3.7860906698502057\n",
      "68     \t [0.15471894 0.57097411 0.825618  ]. \t  3.7747480168999985 \t 3.7860906698502057\n",
      "69     \t [0.1556993  0.57122344 0.82543884]. \t  3.7737405217896605 \t 3.7860906698502057\n",
      "70     \t [0.1566987  0.57163498 0.82510698]. \t  3.77179166253323 \t 3.7860906698502057\n",
      "71     \t [0.15714189 0.57174017 0.82491071]. \t  3.770768921545054 \t 3.7860906698502057\n",
      "72     \t [0.15760667 0.57186183 0.82477181]. \t  3.7700052799005643 \t 3.7860906698502057\n",
      "73     \t [0.15815749 0.57200409 0.82463706]. \t  3.769244647652783 \t 3.7860906698502057\n",
      "74     \t [0.15905158 0.57227615 0.8243721 ]. \t  3.7677190978391515 \t 3.7860906698502057\n",
      "75     \t [0.15888986 0.57242069 0.82437288]. \t  3.767523854611246 \t 3.7860906698502057\n",
      "76     \t [0.15943887 0.57236044 0.82425439]. \t  3.767079390303784 \t 3.7860906698502057\n",
      "77     \t [0.15956035 0.5725043  0.82409777]. \t  3.7661319194147893 \t 3.7860906698502057\n",
      "78     \t [0.16010413 0.5724929  0.82415191]. \t  3.7664908258445458 \t 3.7860906698502057\n",
      "79     \t [0.82882853 0.54621159 0.84902642]. \t  3.739317877403006 \t 3.7860906698502057\n",
      "80     \t [0.15967127 0.57244444 0.82411679]. \t  3.766316421915567 \t 3.7860906698502057\n",
      "81     \t [0.16001815 0.57252307 0.82405591]. \t  3.7659588958034296 \t 3.7860906698502057\n",
      "82     \t [0.16066811 0.57266627 0.8238731 ]. \t  3.7649429017114797 \t 3.7860906698502057\n",
      "83     \t [0.16018294 0.57252058 0.8240378 ]. \t  3.765892586395246 \t 3.7860906698502057\n",
      "84     \t [0.16046279 0.57278576 0.82391162]. \t  3.7649614908304816 \t 3.7860906698502057\n",
      "85     \t [0.16039637 0.57264389 0.82402009]. \t  3.7656783926010604 \t 3.7860906698502057\n",
      "86     \t [0.16075544 0.57267018 0.82385388]. \t  3.764852067735774 \t 3.7860906698502057\n",
      "87     \t [0.16081372 0.57282845 0.82383517]. \t  3.7645665332636735 \t 3.7860906698502057\n",
      "88     \t [0.16069396 0.57278007 0.82391921]. \t  3.765037542307472 \t 3.7860906698502057\n",
      "89     \t [0.16107405 0.57288018 0.82376604]. \t  3.764184329815623 \t 3.7860906698502057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.16103532 0.57282716 0.82386626]. \t  3.764754999360448 \t 3.7860906698502057\n",
      "91     \t [0.16106082 0.57287973 0.82385016]. \t  3.764610469270953 \t 3.7860906698502057\n",
      "92     \t [0.16094841 0.57287242 0.82384577]. \t  3.764582669504045 \t 3.7860906698502057\n",
      "93     \t [0.82566546 0.54637222 0.84862864]. \t  3.7399518794384554 \t 3.7860906698502057\n",
      "94     \t [0.16082727 0.57288687 0.82379376]. \t  3.7642844628013394 \t 3.7860906698502057\n",
      "95     \t [0.16078323 0.57280051 0.82381186]. \t  3.7644793099322467 \t 3.7860906698502057\n",
      "96     \t [0.16106946 0.57287068 0.82365775]. \t  3.7636440406436966 \t 3.7860906698502057\n",
      "97     \t [0.1616012  0.57280469 0.82368258]. \t  3.763923138380594 \t 3.7860906698502057\n",
      "98     \t [0.16116573 0.57280445 0.82371929]. \t  3.7640537204237763 \t 3.7860906698502057\n",
      "99     \t [0.16141862 0.57272965 0.82370254]. \t  3.7640952184558323 \t 3.7860906698502057\n",
      "100    \t [0.16088419 0.57282257 0.82389482]. \t  3.764885670045885 \t 3.7860906698502057\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0.         0.95537153 1.        ]. \t  0.472165545314589 \t 2.6229838112516717\n",
      "3      \t [0.44651362 0.50797211 1.        ]. \t  2.015826358826586 \t 2.6229838112516717\n",
      "4      \t [0.29991413 0.74840827 0.57708943]. \t  2.5249722915020967 \t 2.6229838112516717\n",
      "5      \t [0.412604   1.         0.65881493]. \t  1.5291388550370715 \t 2.6229838112516717\n",
      "6      \t [0.         0.63614017 0.22595062]. \t  0.10719777002311501 \t 2.6229838112516717\n",
      "7      \t [0.04977712 0.55710337 0.75798749]. \t  \u001b[92m3.1662893782107226\u001b[0m \t 3.1662893782107226\n",
      "8      \t [0.         0.20727574 0.98446023]. \t  0.6667325914591921 \t 3.1662893782107226\n",
      "9      \t [1.         1.         0.44669529]. \t  0.169243408930865 \t 3.1662893782107226\n",
      "10     \t [0.         0.35462423 0.59656646]. \t  0.8071831341210055 \t 3.1662893782107226\n",
      "11     \t [0.         0.85941966 0.60308409]. \t  2.8807180555684173 \t 3.1662893782107226\n",
      "12     \t [0.         0.59138608 1.        ]. \t  2.046771710231328 \t 3.1662893782107226\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1662893782107226\n",
      "14     \t [0.51319503 0.38622629 0.78275085]. \t  2.7322009947516674 \t 3.1662893782107226\n",
      "15     \t [0.66250681 0.53785324 0.5236503 ]. \t  0.6193505719698339 \t 3.1662893782107226\n",
      "16     \t [1.         0.55290306 0.95372327]. \t  2.8010868822330566 \t 3.1662893782107226\n",
      "17     \t [1.         0.70816377 0.79316408]. \t  2.5056281456875906 \t 3.1662893782107226\n",
      "18     \t [0.22051272 0.74425332 0.82319689]. \t  2.7756082928898667 \t 3.1662893782107226\n",
      "19     \t [0.         0.76252204 0.79605086]. \t  2.5248414368042598 \t 3.1662893782107226\n",
      "20     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.1662893782107226\n",
      "21     \t [0.         1.         0.34008831]. \t  0.6194193686858215 \t 3.1662893782107226\n",
      "22     \t [1.         0.3263637  0.87066902]. \t  2.2597608592950973 \t 3.1662893782107226\n",
      "23     \t [0.         0.70719804 0.50981818]. \t  2.172159407658555 \t 3.1662893782107226\n",
      "24     \t [0.21229083 0.77049922 0.        ]. \t  0.0017782876325896468 \t 3.1662893782107226\n",
      "25     \t [1.         0.         0.67861794]. \t  0.17275008748073623 \t 3.1662893782107226\n",
      "26     \t [0.22793692 0.31157631 0.86630177]. \t  2.2091588185063475 \t 3.1662893782107226\n",
      "27     \t [0.         1.         0.71725599]. \t  1.361690899935518 \t 3.1662893782107226\n",
      "28     \t [0.45819956 0.         0.        ]. \t  0.09982956172903489 \t 3.1662893782107226\n",
      "29     \t [0.         0.73456335 0.        ]. \t  0.001953766516998605 \t 3.1662893782107226\n",
      "30     \t [0.84520891 0.41750454 0.97760028]. \t  1.9716916386571657 \t 3.1662893782107226\n",
      "31     \t [0.86258625 0.73885879 1.        ]. \t  1.5269009114885175 \t 3.1662893782107226\n",
      "32     \t [0.23992173 0.52772694 0.68169987]. \t  2.2422146974879085 \t 3.1662893782107226\n",
      "33     \t [0.         0.43102672 0.86937812]. \t  \u001b[92m3.267998548586566\u001b[0m \t 3.267998548586566\n",
      "34     \t [0.73639552 0.         0.55204317]. \t  0.10004020871384565 \t 3.267998548586566\n",
      "35     \t [0.15471998 0.         0.        ]. \t  0.08910085666535782 \t 3.267998548586566\n",
      "36     \t [0.         0.47139612 0.84940087]. \t  \u001b[92m3.5754799833323805\u001b[0m \t 3.5754799833323805\n",
      "37     \t [0.30898415 1.         0.95027971]. \t  0.4781186904180317 \t 3.5754799833323805\n",
      "38     \t [0.         0.51785863 0.82551099]. \t  \u001b[92m3.702762216936936\u001b[0m \t 3.702762216936936\n",
      "39     \t [0.         0.53136167 0.81768305]. \t  3.686635704590093 \t 3.702762216936936\n",
      "40     \t [0.         0.53375173 0.81633163]. \t  3.681630698814037 \t 3.702762216936936\n",
      "41     \t [0.         0.53295763 0.81677805]. \t  3.6833527871881895 \t 3.702762216936936\n",
      "42     \t [0.         0.53177339 0.81749531]. \t  3.686089553184031 \t 3.702762216936936\n",
      "43     \t [0.         0.53054792 0.81818906]. \t  3.6884524734144115 \t 3.702762216936936\n",
      "44     \t [0.         0.5297413  0.81862914]. \t  3.689814399655777 \t 3.702762216936936\n",
      "45     \t [0.         0.52897057 0.81906582]. \t  3.691136814391397 \t 3.702762216936936\n",
      "46     \t [0.         0.52841506 0.81938691]. \t  3.692081118173636 \t 3.702762216936936\n",
      "47     \t [0.         0.52805199 0.81963576]. \t  3.692895699210408 \t 3.702762216936936\n",
      "48     \t [0.         0.52742066 0.81993371]. \t  3.6935215062312263 \t 3.702762216936936\n",
      "49     \t [1.         0.55037653 0.71511518]. \t  2.183761762896082 \t 3.702762216936936\n",
      "50     \t [0.         0.51683148 0.82552764]. \t  3.7004293697314434 \t 3.702762216936936\n",
      "51     \t [0.         0.51680376 0.82548502]. \t  3.700169304912466 \t 3.702762216936936\n",
      "52     \t [0.         0.51669966 0.82561774]. \t  3.7005246560855465 \t 3.702762216936936\n",
      "53     \t [0.         0.51675261 0.82546852]. \t  3.699972321238704 \t 3.702762216936936\n",
      "54     \t [0.         0.51678127 0.82549728]. \t  3.700171663239209 \t 3.702762216936936\n",
      "55     \t [0.         0.51686752 0.82544148]. \t  3.7001222679492938 \t 3.702762216936936\n",
      "56     \t [0.         0.51686266 0.82547911]. \t  3.7002823980559754 \t 3.702762216936936\n",
      "57     \t [0.         0.51687392 0.82545881]. \t  3.700216563490055 \t 3.702762216936936\n",
      "58     \t [0.         0.51656819 0.82563428]. \t  3.700285228768614 \t 3.702762216936936\n",
      "59     \t [0.         0.51719024 0.82523744]. \t  3.69994967685299 \t 3.702762216936936\n",
      "60     \t [0.         0.51728678 0.82522416]. \t  3.7001153052572993 \t 3.702762216936936\n",
      "61     \t [0.         0.5167673  0.82555064]. \t  3.7003812193155183 \t 3.702762216936936\n",
      "62     \t [0.         0.51743421 0.82512888]. \t  3.7000208229719105 \t 3.702762216936936\n",
      "63     \t [0.         0.51714262 0.82533115]. \t  3.700268094129738 \t 3.702762216936936\n",
      "64     \t [0.         0.51736791 0.82523985]. \t  3.7003776796397583 \t 3.702762216936936\n",
      "65     \t [0.         0.51729955 0.82524043]. \t  3.7002201803101302 \t 3.702762216936936\n",
      "66     \t [0.         0.5174488  0.82515206]. \t  3.700161964018846 \t 3.702762216936936\n",
      "67     \t [0.         0.5177343  0.82501088]. \t  3.7001719477953916 \t 3.702762216936936\n",
      "68     \t [0.         0.51761257 0.82511033]. \t  3.700350563820347 \t 3.702762216936936\n",
      "69     \t [0.         0.51751166 0.82511337]. \t  3.7001297632360792 \t 3.702762216936936\n",
      "70     \t [0.         0.51755662 0.82507647]. \t  3.7000637020111373 \t 3.702762216936936\n",
      "71     \t [0.         0.51736134 0.82507591]. \t  3.6996053949316376 \t 3.702762216936936\n",
      "72     \t [0.         0.51785637 0.82490297]. \t  3.6999518378919314 \t 3.702762216936936\n",
      "73     \t [0.         0.51780238 0.82489871]. \t  3.6998073319611917 \t 3.702762216936936\n",
      "74     \t [0.         0.51782446 0.82496465]. \t  3.7001654900871763 \t 3.702762216936936\n",
      "75     \t [0.         0.51750886 0.82506306]. \t  3.6998903398466414 \t 3.702762216936936\n",
      "76     \t [0.         0.51804636 0.82479715]. \t  3.6998943417173153 \t 3.702762216936936\n",
      "77     \t [0.         0.51809636 0.82486898]. \t  3.70034492281638 \t 3.702762216936936\n",
      "78     \t [0.         0.51803108 0.82482782]. \t  3.7000028471994435 \t 3.702762216936936\n",
      "79     \t [0.         0.51780644 0.82495228]. \t  3.7000663150336726 \t 3.702762216936936\n",
      "80     \t [0.         0.51768918 0.82501025]. \t  3.700064480372419 \t 3.702762216936936\n",
      "81     \t [0.         0.51815084 0.82478335]. \t  3.700068996466787 \t 3.702762216936936\n",
      "82     \t [0.         0.51767621 0.82514563]. \t  3.700661640639141 \t 3.702762216936936\n",
      "83     \t [0.         0.51799666 0.82485065]. \t  3.7000305336450126 \t 3.702762216936936\n",
      "84     \t [0.         0.5180039  0.82484845]. \t  3.7000368656573572 \t 3.702762216936936\n",
      "85     \t [0.         0.51773284 0.82500681]. \t  3.7001496325581327 \t 3.702762216936936\n",
      "86     \t [0.         0.51803933 0.82485754]. \t  3.7001607143791846 \t 3.702762216936936\n",
      "87     \t [0.         0.51795488 0.82481027]. \t  3.699745706490522 \t 3.702762216936936\n",
      "88     \t [0.         0.51787524 0.82491384]. \t  3.7000460449910726 \t 3.702762216936936\n",
      "89     \t [0.         0.5182454  0.82473872]. \t  3.7000756398627144 \t 3.702762216936936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.         0.51832617 0.82468703]. \t  3.7000167425352033 \t 3.702762216936936\n",
      "91     \t [0.         0.51824595 0.82473321]. \t  3.7000510429136666 \t 3.702762216936936\n",
      "92     \t [0.         0.51828708 0.82468087]. \t  3.6998987945616046 \t 3.702762216936936\n",
      "93     \t [0.         0.51805842 0.82487018]. \t  3.7002635422501 \t 3.702762216936936\n",
      "94     \t [0.         0.51768502 0.82509769]. \t  3.700460274372148 \t 3.702762216936936\n",
      "95     \t [0.         0.51834245 0.82467868]. \t  3.700014474601066 \t 3.702762216936936\n",
      "96     \t [0.         0.51797468 0.82483311]. \t  3.699898032210232 \t 3.702762216936936\n",
      "97     \t [0.         0.51791721 0.82491985]. \t  3.70017076468704 \t 3.702762216936936\n",
      "98     \t [0.         0.51789439 0.82489954]. \t  3.700023532678195 \t 3.702762216936936\n",
      "99     \t [0.         0.51839778 0.82459809]. \t  3.6997604863495512 \t 3.702762216936936\n",
      "100    \t [0.         0.5179432  0.82488628]. \t  3.7000740353865833 \t 3.702762216936936\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [0. 0. 1.]. \t  0.09028946765482587 \t 0.5647137279144399\n",
      "2      \t [0.79494551 0.57685671 1.        ]. \t  \u001b[92m2.050074555445646\u001b[0m \t 2.050074555445646\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.050074555445646\n",
      "4      \t [0.18385664 0.72889024 1.        ]. \t  1.6127947830848042 \t 2.050074555445646\n",
      "5      \t [1.         0.24116929 1.        ]. \t  0.7117767376570457 \t 2.050074555445646\n",
      "6      \t [0.51983145 0.86274044 1.        ]. \t  0.8852679345199459 \t 2.050074555445646\n",
      "7      \t [1.         0.59103821 0.68705738]. \t  1.6932298575261338 \t 2.050074555445646\n",
      "8      \t [0.         0.48177888 0.83152518]. \t  \u001b[92m3.6007025836270112\u001b[0m \t 3.6007025836270112\n",
      "9      \t [0.         0.69470899 0.6466478 ]. \t  2.4495997124300475 \t 3.6007025836270112\n",
      "10     \t [0.         0.25468752 0.60192535]. \t  0.5504609663111998 \t 3.6007025836270112\n",
      "11     \t [0.35771509 0.56388995 0.724692  ]. \t  2.7420658981314405 \t 3.6007025836270112\n",
      "12     \t [0.16585916 0.37479247 1.        ]. \t  1.4448972670882778 \t 3.6007025836270112\n",
      "13     \t [0.69541808 0.70184986 0.69315216]. \t  1.7532428194901644 \t 3.6007025836270112\n",
      "14     \t [0.80661462 0.39053972 0.68172643]. \t  1.586612973292601 \t 3.6007025836270112\n",
      "15     \t [1.         0.64089406 1.        ]. \t  1.9045566764971995 \t 3.6007025836270112\n",
      "16     \t [1.         0.47803275 0.        ]. \t  0.0096461618385871 \t 3.6007025836270112\n",
      "17     \t [0.         0.7370695  0.85550699]. \t  2.8360070691170027 \t 3.6007025836270112\n",
      "18     \t [0.         0.55106261 1.        ]. \t  2.052377495690398 \t 3.6007025836270112\n",
      "19     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.6007025836270112\n",
      "20     \t [1.         0.         0.68082273]. \t  0.17501214337546225 \t 3.6007025836270112\n",
      "21     \t [0.43698892 0.59236936 0.        ]. \t  0.012165368867220992 \t 3.6007025836270112\n",
      "22     \t [0.17651599 0.73919575 0.75289698]. \t  2.5878136841315067 \t 3.6007025836270112\n",
      "23     \t [0.13468709 0.41442286 0.74612764]. \t  2.5825706291655983 \t 3.6007025836270112\n",
      "24     \t [0.         0.53611175 0.68480878]. \t  2.280428153010744 \t 3.6007025836270112\n",
      "25     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.6007025836270112\n",
      "26     \t [0.50369893 0.         0.        ]. \t  0.09682209843004604 \t 3.6007025836270112\n",
      "27     \t [0.         1.         0.57373754]. \t  2.473960064259278 \t 3.6007025836270112\n",
      "28     \t [0.         0.         0.79294559]. \t  0.2473507704689732 \t 3.6007025836270112\n",
      "29     \t [1.         1.         0.14896766]. \t  0.0017739437165513574 \t 3.6007025836270112\n",
      "30     \t [0.         0.24629062 0.89917882]. \t  1.427422290317982 \t 3.6007025836270112\n",
      "31     \t [0.4218301 1.        0.       ]. \t  0.00023442558906638285 \t 3.6007025836270112\n",
      "32     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.6007025836270112\n",
      "33     \t [0.         1.         0.34795038]. \t  0.6841356355855058 \t 3.6007025836270112\n",
      "34     \t [0.         1.         0.77849603]. \t  0.9490590298715331 \t 3.6007025836270112\n",
      "35     \t [0.23494714 0.13079103 0.        ]. \t  0.11089177998904268 \t 3.6007025836270112\n",
      "36     \t [0.39443008 0.59534606 0.89716511]. \t  \u001b[92m3.621256465606595\u001b[0m \t 3.621256465606595\n",
      "37     \t [0.28821809 0.         0.20284135]. \t  0.755016274778588 \t 3.621256465606595\n",
      "38     \t [0.1558577  0.58272334 0.86654343]. \t  \u001b[92m3.8071733787554414\u001b[0m \t 3.8071733787554414\n",
      "39     \t [0.37512858 0.59580602 0.89907467]. \t  3.60479608421825 \t 3.8071733787554414\n",
      "40     \t [0.14864247 0.58245355 0.86389406]. \t  \u001b[92m3.8127155450082695\u001b[0m \t 3.8127155450082695\n",
      "41     \t [0.15296864 0.5827533  0.8638566 ]. \t  \u001b[92m3.8129599749532703\u001b[0m \t 3.8129599749532703\n",
      "42     \t [0.15695184 0.58294003 0.86409108]. \t  3.8127590066940367 \t 3.8129599749532703\n",
      "43     \t [1.         0.         0.95236032]. \t  0.13498093580283757 \t 3.8129599749532703\n",
      "44     \t [0.         0.57805985 0.83804576]. \t  3.772094089867282 \t 3.8129599749532703\n",
      "45     \t [0.16027108 0.58195266 0.86502511]. \t  \u001b[92m3.8129862847694533\u001b[0m \t 3.8129862847694533\n",
      "46     \t [0.16230358 0.58212701 0.86507112]. \t  3.812880134813417 \t 3.8129862847694533\n",
      "47     \t [0.16422207 0.58225684 0.86518413]. \t  3.8126729627626688 \t 3.8129862847694533\n",
      "48     \t [0.16564455 0.58233476 0.86524718]. \t  3.8125962464276917 \t 3.8129862847694533\n",
      "49     \t [0.16765795 0.58249263 0.86541323]. \t  3.8122129841083376 \t 3.8129862847694533\n",
      "50     \t [0.1686658  0.58264103 0.86545298]. \t  3.8119947723722998 \t 3.8129862847694533\n",
      "51     \t [0.16969189 0.58262252 0.86553291]. \t  3.8119839540539546 \t 3.8129862847694533\n",
      "52     \t [0.         0.57833741 0.83817783]. \t  3.7720107942100127 \t 3.8129862847694533\n",
      "53     \t [0.14659112 0.58162807 0.86176028]. \t  \u001b[92m3.8178743755134246\u001b[0m \t 3.8178743755134246\n",
      "54     \t [0.14872259 0.58172884 0.86207547]. \t  3.8175147918233034 \t 3.8178743755134246\n",
      "55     \t [0.15110769 0.58171875 0.86240227]. \t  3.817347450057478 \t 3.8178743755134246\n",
      "56     \t [0.15312386 0.58188309 0.86269799]. \t  3.816842113702971 \t 3.8178743755134246\n",
      "57     \t [0.1558494  0.58181431 0.86315715]. \t  3.8165253192004496 \t 3.8178743755134246\n",
      "58     \t [0.24182732 0.58725644 0.87670125]. \t  3.771642696084405 \t 3.8178743755134246\n",
      "59     \t [0.14222636 0.58150156 0.86077569]. \t  \u001b[92m3.818910153309129\u001b[0m \t 3.818910153309129\n",
      "60     \t [0.14426649 0.58149609 0.86104125]. \t  3.818869951556437 \t 3.818910153309129\n",
      "61     \t [0.14561915 0.58165495 0.86132336]. \t  3.818378314782077 \t 3.818910153309129\n",
      "62     \t [0.14738339 0.58169008 0.86159818]. \t  3.818168684658274 \t 3.818910153309129\n",
      "63     \t [0.14892799 0.58176698 0.86179224]. \t  3.817967188924729 \t 3.818910153309129\n",
      "64     \t [0.15024361 0.58188158 0.86206955]. \t  3.81750626995137 \t 3.818910153309129\n",
      "65     \t [0.1517444  0.58178416 0.8623508 ]. \t  3.817428556099935 \t 3.818910153309129\n",
      "66     \t [0.15246131 0.58206173 0.86242431]. \t  3.816918052454833 \t 3.818910153309129\n",
      "67     \t [0.15383688 0.58196827 0.86264104]. \t  3.8169111094157837 \t 3.818910153309129\n",
      "68     \t [0.15417952 0.58205211 0.86258819]. \t  3.8169137807732736 \t 3.818910153309129\n",
      "69     \t [0.1552987  0.5820121  0.86277134]. \t  3.816824015732425 \t 3.818910153309129\n",
      "70     \t [0.15579356 0.58182887 0.86289581]. \t  3.816993991727859 \t 3.818910153309129\n",
      "71     \t [0.15760194 0.58199171 0.86321715]. \t  3.81636876010157 \t 3.818910153309129\n",
      "72     \t [0.15782685 0.58196656 0.86318032]. \t  3.816520932349671 \t 3.818910153309129\n",
      "73     \t [0.15824884 0.58213414 0.86327617]. \t  3.8160988716645172 \t 3.818910153309129\n",
      "74     \t [0.15730122 0.58223883 0.86317985]. \t  3.815951598508211 \t 3.818910153309129\n",
      "75     \t [0.15853471 0.58215433 0.86328562]. \t  3.8160884973797096 \t 3.818910153309129\n",
      "76     \t [0.15806592 0.5820094  0.86326103]. \t  3.8163234473572922 \t 3.818910153309129\n",
      "77     \t [0.21862242 0.58551787 0.87269951]. \t  3.7899997407361337 \t 3.818910153309129\n",
      "78     \t [0.14704663 0.58178138 0.86123048]. \t  3.8185436186370136 \t 3.818910153309129\n",
      "79     \t [0.14786905 0.58179153 0.8614172 ]. \t  3.8183657736461503 \t 3.818910153309129\n",
      "80     \t [0.14877436 0.581806   0.86162722]. \t  3.818148083307584 \t 3.818910153309129\n",
      "81     \t [0.1492743  0.58177124 0.86171469]. \t  3.818147509633712 \t 3.818910153309129\n",
      "82     \t [0.15121074 0.58197782 0.86204581]. \t  3.817534793669607 \t 3.818910153309129\n",
      "83     \t [0.1518095  0.58190387 0.86215327]. \t  3.817577142833996 \t 3.818910153309129\n",
      "84     \t [0.15156282 0.58186803 0.86202999]. \t  3.817815754637425 \t 3.818910153309129\n",
      "85     \t [0.15206324 0.58186752 0.86219421]. \t  3.8176111830982693 \t 3.818910153309129\n",
      "86     \t [0.15240837 0.58188939 0.8621354 ]. \t  3.8177318099173743 \t 3.818910153309129\n",
      "87     \t [0.15394737 0.58204502 0.86239219]. \t  3.8172460446545746 \t 3.818910153309129\n",
      "88     \t [0.15571394 0.58202529 0.86271201]. \t  3.8169773418236232 \t 3.818910153309129\n",
      "89     \t [0.15503502 0.5819016  0.86252128]. \t  3.8174421258294355 \t 3.818910153309129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.15504587 0.58210886 0.86261271]. \t  3.816905416683658 \t 3.818910153309129\n",
      "91     \t [0.15525522 0.58199483 0.86264464]. \t  3.817084000111408 \t 3.818910153309129\n",
      "92     \t [0.15481203 0.58196384 0.86255899]. \t  3.817226502381443 \t 3.818910153309129\n",
      "93     \t [0.15619952 0.58204127 0.86279024]. \t  3.816879304171594 \t 3.818910153309129\n",
      "94     \t [0.15736282 0.58215227 0.86298092]. \t  3.8165017201543416 \t 3.818910153309129\n",
      "95     \t [0.15730302 0.58203093 0.86284509]. \t  3.8169682332216164 \t 3.818910153309129\n",
      "96     \t [0.15516055 0.58242421 0.86290802]. \t  3.815801675061272 \t 3.818910153309129\n",
      "97     \t [0.15952188 0.58217055 0.8633899 ]. \t  3.8160054049126257 \t 3.818910153309129\n",
      "98     \t [0.15719875 0.58205408 0.8629761 ]. \t  3.816661295445352 \t 3.818910153309129\n",
      "99     \t [0.15795793 0.58209591 0.86309227]. \t  3.8164815373293846 \t 3.818910153309129\n",
      "100    \t [0.15914732 0.58217724 0.86327859]. \t  3.8161562472351718 \t 3.818910153309129\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [0.         1.         0.01931001]. \t  0.0004972996933881742 \t 1.9592421489197056\n",
      "2      \t [0.34155946 0.37570237 0.83160779]. \t  \u001b[92m2.885477546124352\u001b[0m \t 2.885477546124352\n",
      "3      \t [0.76036847 0.         1.        ]. \t  0.09054622014135268 \t 2.885477546124352\n",
      "4      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.885477546124352\n",
      "5      \t [0.46914601 0.56021435 1.        ]. \t  2.0853288885681445 \t 2.885477546124352\n",
      "6      \t [0.         0.54374871 0.81170505]. \t  \u001b[92m3.660638002449315\u001b[0m \t 3.660638002449315\n",
      "7      \t [1.         0.33058721 0.36974605]. \t  0.15255624639888474 \t 3.660638002449315\n",
      "8      \t [0.         0.54340061 1.        ]. \t  2.046064909593504 \t 3.660638002449315\n",
      "9      \t [0.         0.18640403 0.53588919]. \t  0.23927673585976372 \t 3.660638002449315\n",
      "10     \t [1.         0.70327517 1.        ]. \t  1.6746208344687308 \t 3.660638002449315\n",
      "11     \t [0.         0.85172912 0.53142955]. \t  2.896708294023016 \t 3.660638002449315\n",
      "12     \t [0.88405881 0.33194123 0.        ]. \t  0.03332111685903707 \t 3.660638002449315\n",
      "13     \t [0.         0.61588206 0.38672121]. \t  0.6860212395119168 \t 3.660638002449315\n",
      "14     \t [1.         0.35916783 1.        ]. \t  1.315340680147185 \t 3.660638002449315\n",
      "15     \t [0.6680651  0.43153136 0.67122011]. \t  1.630695344246502 \t 3.660638002449315\n",
      "16     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.660638002449315\n",
      "17     \t [0.         1.         0.63898034]. \t  2.0894916283282017 \t 3.660638002449315\n",
      "18     \t [0.28138991 0.         0.        ]. \t  0.09992439519125558 \t 3.660638002449315\n",
      "19     \t [0.         0.77303149 0.81314768]. \t  2.475339445951238 \t 3.660638002449315\n",
      "20     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.660638002449315\n",
      "21     \t [0.18949011 0.57369813 0.70557179]. \t  2.6028606582877916 \t 3.660638002449315\n",
      "22     \t [0.49855733 1.         0.        ]. \t  0.00020460392978034032 \t 3.660638002449315\n",
      "23     \t [0.42213024 0.         0.67118537]. \t  0.17504591382505866 \t 3.660638002449315\n",
      "24     \t [0.27878429 1.         0.43263425]. \t  1.5007000488044884 \t 3.660638002449315\n",
      "25     \t [0.56270583 0.5118461  0.        ]. \t  0.022076379297169022 \t 3.660638002449315\n",
      "26     \t [0.64309599 1.         1.        ]. \t  0.32916944613698507 \t 3.660638002449315\n",
      "27     \t [0.41168425 0.19554533 1.        ]. \t  0.5420471766437158 \t 3.660638002449315\n",
      "28     \t [1.         0.         0.81144963]. \t  0.24250997136226993 \t 3.660638002449315\n",
      "29     \t [0.         0.25150386 0.88038896]. \t  1.5584147581138346 \t 3.660638002449315\n",
      "30     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.660638002449315\n",
      "31     \t [0.         1.         0.30856907]. \t  0.4007247099777058 \t 3.660638002449315\n",
      "32     \t [1.         0.         0.09775839]. \t  0.11146112297198171 \t 3.660638002449315\n",
      "33     \t [0.55646917 1.         0.7075649 ]. \t  0.9216283605175435 \t 3.660638002449315\n",
      "34     \t [1.         0.93164216 0.41232887]. \t  0.1485343098683766 \t 3.660638002449315\n",
      "35     \t [1.         0.71360179 0.        ]. \t  0.0010311941477696952 \t 3.660638002449315\n",
      "36     \t [1.         0.60913671 0.78048435]. \t  2.9781479466080527 \t 3.660638002449315\n",
      "37     \t [0.23023681 0.88142745 0.        ]. \t  0.0006018988939470847 \t 3.660638002449315\n",
      "38     \t [0.4541946  0.69113228 0.79673618]. \t  2.9947699598247834 \t 3.660638002449315\n",
      "39     \t [0.41670422 0.19357908 0.        ]. \t  0.10981500711276505 \t 3.660638002449315\n",
      "40     \t [0.         0.         0.10210567]. \t  0.2556836196797167 \t 3.660638002449315\n",
      "41     \t [0.         0.62103622 0.73207113]. \t  2.824993255042226 \t 3.660638002449315\n",
      "42     \t [0.73270083 0.53253678 0.90373466]. \t  3.5281902563430365 \t 3.660638002449315\n",
      "43     \t [0.19360861 0.         0.27307703]. \t  0.7946760652193893 \t 3.660638002449315\n",
      "44     \t [0.79517476 0.58351588 0.91098407]. \t  3.4393661064685 \t 3.660638002449315\n",
      "45     \t [1.         0.44004361 0.83257179]. \t  3.2615502783789942 \t 3.660638002449315\n",
      "46     \t [0.64334668 0.4876792  0.89154888]. \t  3.5121722572493996 \t 3.660638002449315\n",
      "47     \t [0.83573014 0.53101883 0.88707781]. \t  3.628449143620206 \t 3.660638002449315\n",
      "48     \t [0.81251777 0.5398962  0.89198011]. \t  3.6152332797903632 \t 3.660638002449315\n",
      "49     \t [0.73434985 0.57652687 0.8998415 ]. \t  3.574049970482048 \t 3.660638002449315\n",
      "50     \t [0.82433112 0.52962812 0.88833987]. \t  3.621669588686066 \t 3.660638002449315\n",
      "51     \t [0.79012592 0.36883606 0.89259368]. \t  2.596863561803674 \t 3.660638002449315\n",
      "52     \t [0.70695363 0.59436761 0.89116112]. \t  3.6140393502385404 \t 3.660638002449315\n",
      "53     \t [0.71853357 0.58726099 0.89135013]. \t  3.626104900038346 \t 3.660638002449315\n",
      "54     \t [0.7346823  0.58480172 0.89280156]. \t  3.616193902330367 \t 3.660638002449315\n",
      "55     \t [0.75170469 0.58239111 0.89357579]. \t  3.609924243759216 \t 3.660638002449315\n",
      "56     \t [0.6708933  0.57015576 0.88116013]. \t  \u001b[92m3.726903568889976\u001b[0m \t 3.726903568889976\n",
      "57     \t [0.66004174 0.56847253 0.8790251 ]. \t  \u001b[92m3.741775167818381\u001b[0m \t 3.741775167818381\n",
      "58     \t [0.65157974 0.5668867  0.87733736]. \t  \u001b[92m3.752948147830329\u001b[0m \t 3.752948147830329\n",
      "59     \t [0.64546619 0.56596103 0.87608729]. \t  \u001b[92m3.760524639769201\u001b[0m \t 3.760524639769201\n",
      "60     \t [0.64027986 0.56494081 0.87504914]. \t  \u001b[92m3.766748822515277\u001b[0m \t 3.766748822515277\n",
      "61     \t [0.63643926 0.56413187 0.87425866]. \t  \u001b[92m3.7712934470946626\u001b[0m \t 3.7712934470946626\n",
      "62     \t [0.63329454 0.56355297 0.87364008]. \t  \u001b[92m3.7747354027598945\u001b[0m \t 3.7747354027598945\n",
      "63     \t [0.63081996 0.56293656 0.87308462]. \t  \u001b[92m3.777711834383984\u001b[0m \t 3.777711834383984\n",
      "64     \t [0.62859887 0.56251291 0.87261187]. \t  \u001b[92m3.780150207269484\u001b[0m \t 3.780150207269484\n",
      "65     \t [0.6270173  0.56206418 0.87236279]. \t  \u001b[92m3.7816383655163626\u001b[0m \t 3.7816383655163626\n",
      "66     \t [0.62549085 0.5616521  0.87198682]. \t  \u001b[92m3.7834875946114366\u001b[0m \t 3.7834875946114366\n",
      "67     \t [0.62402711 0.56137501 0.87170069]. \t  \u001b[92m3.784926628987977\u001b[0m \t 3.784926628987977\n",
      "68     \t [0.62293895 0.5610389  0.87147797]. \t  \u001b[92m3.786076328784035\u001b[0m \t 3.786076328784035\n",
      "69     \t [0.62188288 0.56084125 0.87126805]. \t  \u001b[92m3.787097812755461\u001b[0m \t 3.787097812755461\n",
      "70     \t [0.62108978 0.56056456 0.871098  ]. \t  \u001b[92m3.787953158536534\u001b[0m \t 3.787953158536534\n",
      "71     \t [0.62022069 0.56038    0.87091352]. \t  \u001b[92m3.788822188244232\u001b[0m \t 3.788822188244232\n",
      "72     \t [0.61954716 0.56029844 0.87076594]. \t  \u001b[92m3.78947773450113\u001b[0m \t 3.78947773450113\n",
      "73     \t [0.61846042 0.56000496 0.8705573 ]. \t  \u001b[92m3.7905027104350295\u001b[0m \t 3.7905027104350295\n",
      "74     \t [0.61821915 0.55979568 0.87048064]. \t  \u001b[92m3.7908766274435908\u001b[0m \t 3.7908766274435908\n",
      "75     \t [0.61750762 0.55973401 0.87037994]. \t  \u001b[92m3.7913782543438987\u001b[0m \t 3.7913782543438987\n",
      "76     \t [0.61707896 0.55959333 0.87024547]. \t  \u001b[92m3.7919380085093812\u001b[0m \t 3.7919380085093812\n",
      "77     \t [0.6165599  0.55943178 0.87017153]. \t  \u001b[92m3.7923446617979284\u001b[0m \t 3.7923446617979284\n",
      "78     \t [0.6160098  0.55933671 0.87008681]. \t  \u001b[92m3.7927639450713495\u001b[0m \t 3.7927639450713495\n",
      "79     \t [0.61584742 0.55921099 0.87003556]. \t  \u001b[92m3.792998845655636\u001b[0m \t 3.792998845655636\n",
      "80     \t [0.61539378 0.55900925 0.86999352]. \t  \u001b[92m3.793302741515035\u001b[0m \t 3.793302741515035\n",
      "81     \t [0.61520572 0.55897766 0.86994079]. \t  \u001b[92m3.793513261203721\u001b[0m \t 3.793513261203721\n",
      "82     \t [0.61470993 0.55896406 0.86983371]. \t  \u001b[92m3.793949920720249\u001b[0m \t 3.793949920720249\n",
      "83     \t [0.61443288 0.55876658 0.86973983]. \t  \u001b[92m3.794353275406404\u001b[0m \t 3.794353275406404\n",
      "84     \t [0.61404656 0.55850692 0.86969802]. \t  \u001b[92m3.7946492871687876\u001b[0m \t 3.7946492871687876\n",
      "85     \t [0.61397913 0.55860328 0.86971057]. \t  3.794600139762254 \t 3.7946492871687876\n",
      "86     \t [0.61358512 0.55851868 0.86958291]. \t  \u001b[92m3.7950872092744112\u001b[0m \t 3.7950872092744112\n",
      "87     \t [0.6131117  0.55836866 0.86954939]. \t  \u001b[92m3.795343318537047\u001b[0m \t 3.795343318537047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.61297611 0.55832083 0.86943287]. \t  \u001b[92m3.795719869699248\u001b[0m \t 3.795719869699248\n",
      "89     \t [0.6128774  0.558178   0.86944945]. \t  \u001b[92m3.7957384605528977\u001b[0m \t 3.7957384605528977\n",
      "90     \t [0.61259347 0.55819136 0.86941656]. \t  \u001b[92m3.795896640577507\u001b[0m \t 3.795896640577507\n",
      "91     \t [0.61247561 0.55811624 0.86934679]. \t  \u001b[92m3.7961423647223\u001b[0m \t 3.7961423647223\n",
      "92     \t [0.61242371 0.55808257 0.86934573]. \t  \u001b[92m3.7961673829402955\u001b[0m \t 3.7961673829402955\n",
      "93     \t [0.61195054 0.55801464 0.86925665]. \t  \u001b[92m3.796550136078568\u001b[0m \t 3.796550136078568\n",
      "94     \t [0.6118627  0.55793173 0.86929609]. \t  3.7964839814184517 \t 3.796550136078568\n",
      "95     \t [0.61157182 0.55799326 0.86919783]. \t  \u001b[92m3.7968116021283715\u001b[0m \t 3.7968116021283715\n",
      "96     \t [0.6115471  0.55784013 0.86915943]. \t  \u001b[92m3.796964747769561\u001b[0m \t 3.796964747769561\n",
      "97     \t [0.61136085 0.55781433 0.86916988]. \t  \u001b[92m3.796987921019186\u001b[0m \t 3.796987921019186\n",
      "98     \t [0.6110772  0.55767129 0.86909546]. \t  \u001b[92m3.797298853729808\u001b[0m \t 3.797298853729808\n",
      "99     \t [0.61099006 0.55767253 0.86904569]. \t  \u001b[92m3.7974565391653883\u001b[0m \t 3.7974565391653883\n",
      "100    \t [0.61081913 0.55750538 0.86908639]. \t  3.7974270789072957 \t 3.7974565391653883\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.69508376]. \t  \u001b[92m1.5604262051784055\u001b[0m \t 1.5604262051784055\n",
      "2      \t [0.0972586  0.63869247 1.        ]. \t  \u001b[92m1.971971736762121\u001b[0m \t 1.971971736762121\n",
      "3      \t [0.         0.5023081  0.80191163]. \t  \u001b[92m3.512988997148147\u001b[0m \t 3.512988997148147\n",
      "4      \t [0.         0.00460834 0.49501327]. \t  0.1449988871788916 \t 3.512988997148147\n",
      "5      \t [0.         0.71549649 0.36677121]. \t  0.7998946681922094 \t 3.512988997148147\n",
      "6      \t [0.        0.1343753 1.       ]. \t  0.3310577801880279 \t 3.512988997148147\n",
      "7      \t [1.         0.31847833 0.80546835]. \t  2.196173913474011 \t 3.512988997148147\n",
      "8      \t [1.         1.         0.56520551]. \t  0.2658289322331248 \t 3.512988997148147\n",
      "9      \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.512988997148147\n",
      "10     \t [1.         0.54358975 1.        ]. \t  1.9978390273717634 \t 3.512988997148147\n",
      "11     \t [1.         0.67126865 0.        ]. \t  0.0016623153622765947 \t 3.512988997148147\n",
      "12     \t [1.         0.60024388 0.64470961]. \t  1.147468966447964 \t 3.512988997148147\n",
      "13     \t [0.84361757 1.         0.        ]. \t  7.411185366126819e-05 \t 3.512988997148147\n",
      "14     \t [ 1.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.030954717033005123 \t 3.512988997148147\n",
      "15     \t [0.75847725 0.38925627 1.        ]. \t  1.5088537966425501 \t 3.512988997148147\n",
      "16     \t [0.         0.74632752 0.90003763]. \t  2.5912769359086396 \t 3.512988997148147\n",
      "17     \t [0.         0.37101937 0.59676697]. \t  0.8618497886287371 \t 3.512988997148147\n",
      "18     \t [ 0.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  0.0002735367680454458 \t 3.512988997148147\n",
      "19     \t [1.         0.         0.53478071]. \t  0.06784185349730848 \t 3.512988997148147\n",
      "20     \t [0.20874096 1.         0.38582863]. \t  1.0547948200853758 \t 3.512988997148147\n",
      "21     \t [0.55933177 0.         0.        ]. \t  0.0917066284777491 \t 3.512988997148147\n",
      "22     \t [0.         1.         0.39142302]. \t  1.1086444092318746 \t 3.512988997148147\n",
      "23     \t [0.16765554 0.71381955 0.75754067]. \t  2.7400366836153562 \t 3.512988997148147\n",
      "24     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.512988997148147\n",
      "25     \t [0.48243525 0.82433163 0.        ]. \t  0.0009465433087391665 \t 3.512988997148147\n",
      "26     \t [0.         0.44825436 1.        ]. \t  1.7896837666655026 \t 3.512988997148147\n",
      "27     \t [0.         0.         0.79315256]. \t  0.24735725524664387 \t 3.512988997148147\n",
      "28     \t [0.         0.71887676 0.68242601]. \t  2.4889335681057343 \t 3.512988997148147\n",
      "29     \t [0.29574326 0.44820571 0.90466784]. \t  3.194332946117912 \t 3.512988997148147\n",
      "30     \t [0.40754575 0.6717256  0.90728593]. \t  3.169908314220114 \t 3.512988997148147\n",
      "31     \t [0.343354   1.         0.92880772]. \t  0.5338265091093268 \t 3.512988997148147\n",
      "32     \t [0.68495097 0.52051204 0.84363969]. \t  \u001b[92m3.7509778354332126\u001b[0m \t 3.7509778354332126\n",
      "33     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.7509778354332126\n",
      "34     \t [1.        0.        0.8050153]. \t  0.2434925601543584 \t 3.7509778354332126\n",
      "35     \t [0.76374781 0.         0.21760523]. \t  0.5073032982551688 \t 3.7509778354332126\n",
      "36     \t [1.         1.         0.17582294]. \t  0.0032810623431356115 \t 3.7509778354332126\n",
      "37     \t [0.80155163 0.5875824  0.86513192]. \t  3.705206295721563 \t 3.7509778354332126\n",
      "38     \t [0.12475962 0.39246914 0.83925641]. \t  3.0253236012152525 \t 3.7509778354332126\n",
      "39     \t [0.5650728  0.564466   0.83991864]. \t  \u001b[92m3.7980912938039593\u001b[0m \t 3.7980912938039593\n",
      "40     \t [0.76064888 0.45759577 0.78636763]. \t  3.1408218679463817 \t 3.7980912938039593\n",
      "41     \t [0.60272845 0.57795419 0.85950283]. \t  3.794771062439238 \t 3.7980912938039593\n",
      "42     \t [0.69779236 0.60159242 0.89112254]. \t  3.596476461819535 \t 3.7980912938039593\n",
      "43     \t [0.52943405 0.55271365 0.84789768]. \t  \u001b[92m3.831033268170515\u001b[0m \t 3.831033268170515\n",
      "44     \t [0.         1.         0.95690763]. \t  0.45493745209264125 \t 3.831033268170515\n",
      "45     \t [0.15435271 0.56030599 0.85079532]. \t  \u001b[92m3.849077827290843\u001b[0m \t 3.849077827290843\n",
      "46     \t [0.22007427 0.         0.18657352]. \t  0.6711250668205371 \t 3.849077827290843\n",
      "47     \t [0.48304202 0.54644358 0.85855156]. \t  3.8410580725856653 \t 3.849077827290843\n",
      "48     \t [0.53035952 0.55177046 0.8588168 ]. \t  3.8339712780538884 \t 3.849077827290843\n",
      "49     \t [0.19301788 0.55850702 0.85547928]. \t  \u001b[92m3.854402298397439\u001b[0m \t 3.854402298397439\n",
      "50     \t [0.36408124 0.54973232 0.85687512]. \t  \u001b[92m3.8571920484480833\u001b[0m \t 3.8571920484480833\n",
      "51     \t [0.62198621 1.         0.16196105]. \t  0.011592251605852179 \t 3.8571920484480833\n",
      "52     \t [0.58067848 0.56411432 0.85336417]. \t  3.8188673786742444 \t 3.8571920484480833\n",
      "53     \t [0.58020219 0.56355116 0.85387287]. \t  3.8196802715281697 \t 3.8571920484480833\n",
      "54     \t [0.58224397 0.56327033 0.85416529]. \t  3.81949329517842 \t 3.8571920484480833\n",
      "55     \t [0.36226399 0.55259088 0.85705252]. \t  \u001b[92m3.857878774649776\u001b[0m \t 3.857878774649776\n",
      "56     \t [0.19550958 0.55874112 0.85406822]. \t  3.8552462507563305 \t 3.857878774649776\n",
      "57     \t [0.35760711 0.55273407 0.8572673 ]. \t  \u001b[92m3.8580054564933293\u001b[0m \t 3.8580054564933293\n",
      "58     \t [0.36536659 0.55277767 0.85761986]. \t  3.8572679989627026 \t 3.8580054564933293\n",
      "59     \t [0.20557734 0.55797658 0.85467878]. \t  3.8562802836059653 \t 3.8580054564933293\n",
      "60     \t [0.35485562 0.55322901 0.85761915]. \t  3.857928353652444 \t 3.8580054564933293\n",
      "61     \t [0.35790032 0.55331562 0.85787502]. \t  3.8575496400075875 \t 3.8580054564933293\n",
      "62     \t [0.21514053 0.55736345 0.85527991]. \t  3.8570299948074513 \t 3.8580054564933293\n",
      "63     \t [0.35185454 0.55336282 0.85788387]. \t  3.8578620043656815 \t 3.8580054564933293\n",
      "64     \t [0.34667297 0.55357416 0.85798533]. \t  \u001b[92m3.8580311638307827\u001b[0m \t 3.8580311638307827\n",
      "65     \t [0.22384756 0.55689308 0.8557371 ]. \t  3.8575908206526335 \t 3.8580311638307827\n",
      "66     \t [0.3503905  0.55364508 0.85806018]. \t  3.8577966334066294 \t 3.8580311638307827\n",
      "67     \t [0.33985409 0.55389175 0.85808535]. \t  \u001b[92m3.858240900161666\u001b[0m \t 3.858240900161666\n",
      "68     \t [0.23205924 0.55650574 0.85609886]. \t  3.8580381100744696 \t 3.858240900161666\n",
      "69     \t [0.34856158 0.55371118 0.85832498]. \t  3.857630142232953 \t 3.858240900161666\n",
      "70     \t [0.33923893 0.55390157 0.85825753]. \t  3.8580955855791466 \t 3.858240900161666\n",
      "71     \t [0.24049621 0.55634069 0.85638953]. \t  \u001b[92m3.8584333707904777\u001b[0m \t 3.8584333707904777\n",
      "72     \t [0.34654045 0.55374358 0.85841452]. \t  3.8576324120596954 \t 3.8584333707904777\n",
      "73     \t [0.33408339 0.55408877 0.8584005 ]. \t  3.8581386903932477 \t 3.8584333707904777\n",
      "74     \t [0.24437856 0.55607469 0.85660761]. \t  \u001b[92m3.8585295482924935\u001b[0m \t 3.8585295482924935\n",
      "75     \t [0.34869586 0.55391588 0.85846298]. \t  3.8575004519728187 \t 3.8585295482924935\n",
      "76     \t [0.32743079 0.55415875 0.85824091]. \t  3.85849261915219 \t 3.8585295482924935\n",
      "77     \t [0.24990144 0.55569687 0.85679243]. \t  \u001b[92m3.858717213119158\u001b[0m \t 3.858717213119158\n",
      "78     \t [0.34768317 0.55398549 0.85849823]. \t  3.857514718469765 \t 3.858717213119158\n",
      "79     \t [0.32674006 0.55412368 0.85838663]. \t  3.8583579268792167 \t 3.858717213119158\n",
      "80     \t [0.25666341 0.55559637 0.85698188]. \t  \u001b[92m3.858912906609323\u001b[0m \t 3.858912906609323\n",
      "81     \t [0.35053204 0.55419834 0.85848673]. \t  3.8574080921534986 \t 3.858912906609323\n",
      "82     \t [0.2949536  0.55477033 0.85790779]. \t  \u001b[92m3.859130351901472\u001b[0m \t 3.859130351901472\n",
      "83     \t [0.35608693 0.55402582 0.8587049 ]. \t  3.856892445575411 \t 3.859130351901472\n",
      "84     \t [0.25802188 0.55515715 0.85723775]. \t  3.8587562948072667 \t 3.859130351901472\n",
      "85     \t [0.33822596 0.55420999 0.85862371]. \t  3.8577741766710187 \t 3.859130351901472\n",
      "86     \t [0.27809076 0.55498018 0.85745171]. \t  \u001b[92m3.8592851198254134\u001b[0m \t 3.8592851198254134\n",
      "87     \t [0.35214791 0.55408173 0.85873978]. \t  3.857058024410366 \t 3.8592851198254134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.2764101  0.55511077 0.85753488]. \t  3.859167714739823 \t 3.8592851198254134\n",
      "89     \t [0.33897066 0.55419332 0.85855997]. \t  3.8578144760072726 \t 3.8592851198254134\n",
      "90     \t [0.29265649 0.55464186 0.85792471]. \t  3.8590871413013357 \t 3.8592851198254134\n",
      "91     \t [0.34956866 0.55404728 0.85876677]. \t  3.8571489753588732 \t 3.8592851198254134\n",
      "92     \t [0.2704922  0.55523682 0.85756266]. \t  3.8589638862926443 \t 3.8592851198254134\n",
      "93     \t [0.33864237 0.55416945 0.85861961]. \t  3.8577614029012213 \t 3.8592851198254134\n",
      "94     \t [0.27544461 0.55500873 0.8577329 ]. \t  3.858951771459859 \t 3.8592851198254134\n",
      "95     \t [0.34427988 0.55422515 0.85858712]. \t  3.857582617849017 \t 3.8592851198254134\n",
      "96     \t [0.28419623 0.5549482  0.85781286]. \t  3.8590817705232174 \t 3.8592851198254134\n",
      "97     \t [0.34654389 0.55416672 0.85873859]. \t  3.857321945772823 \t 3.8592851198254134\n",
      "98     \t [0.28116009 0.55487722 0.85779539]. \t  3.8590331480591793 \t 3.8592851198254134\n",
      "99     \t [0.34721483 0.55408961 0.85873601]. \t  3.857290855202896 \t 3.8592851198254134\n",
      "100    \t [0.28518879 0.55492306 0.85791012]. \t  3.8590044185198016 \t 3.8592851198254134\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.53737891 0.         1.        ]. \t  0.09156156614336214 \t 2.5106636917702634\n",
      "2      \t [1.         0.90027332 1.        ]. \t  0.6734964986177479 \t 2.5106636917702634\n",
      "3      \t [0.33012041 0.57001516 1.        ]. \t  2.0877069422414736 \t 2.5106636917702634\n",
      "4      \t [1.         0.41150385 0.56853198]. \t  0.4954358651720518 \t 2.5106636917702634\n",
      "5      \t [0.81571138 0.48453022 1.        ]. \t  1.915203840984113 \t 2.5106636917702634\n",
      "6      \t [0.811189  0.        0.4892981]. \t  0.12829943486699516 \t 2.5106636917702634\n",
      "7      \t [0.         0.79902522 1.        ]. \t  1.218128772451556 \t 2.5106636917702634\n",
      "8      \t [0.06164208 0.45898806 0.81354094]. \t  \u001b[92m3.422977446216387\u001b[0m \t 3.422977446216387\n",
      "9      \t [0.        0.3793612 1.       ]. \t  1.454840608728857 \t 3.422977446216387\n",
      "10     \t [0.         0.56664222 0.38491361]. \t  0.5451065816638465 \t 3.422977446216387\n",
      "11     \t [1.         0.00827994 1.        ]. \t  0.09680119645211832 \t 3.422977446216387\n",
      "12     \t [0.31087881 0.29905613 0.65988723]. \t  1.0960627332294122 \t 3.422977446216387\n",
      "13     \t [0.86181331 0.17659498 0.        ]. \t  0.05459149471501942 \t 3.422977446216387\n",
      "14     \t [0.6918224 0.6126347 0.       ]. \t  0.007405312444266599 \t 3.422977446216387\n",
      "15     \t [0.         0.69839789 0.74075545]. \t  2.6864661481611307 \t 3.422977446216387\n",
      "16     \t [0.49239823 0.30499368 0.        ]. \t  0.07866514852684055 \t 3.422977446216387\n",
      "17     \t [0.5954056  0.85627465 1.        ]. \t  0.9154577861400317 \t 3.422977446216387\n",
      "18     \t [0.         0.40761033 0.60548406]. \t  1.0544726055931521 \t 3.422977446216387\n",
      "19     \t [0.         0.98362332 0.        ]. \t  0.00029373324552338733 \t 3.422977446216387\n",
      "20     \t [0.         0.         0.91925052]. \t  0.17190746168050053 \t 3.422977446216387\n",
      "21     \t [0.         0.65342772 0.        ]. \t  0.004561199895571071 \t 3.422977446216387\n",
      "22     \t [0.         0.60145592 0.95335512]. \t  2.844777311981009 \t 3.422977446216387\n",
      "23     \t [0.84747671 0.6060389  0.82940603]. \t  \u001b[92m3.5400769232135207\u001b[0m \t 3.5400769232135207\n",
      "24     \t [ 6.31125994e-01  0.00000000e+00 -6.93889390e-18]. \t  0.08318773586548522 \t 3.5400769232135207\n",
      "25     \t [0.86640046 0.75355568 0.77413483]. \t  2.036216628476589 \t 3.5400769232135207\n",
      "26     \t [1.         0.60386894 0.89917141]. \t  3.423141287348643 \t 3.5400769232135207\n",
      "27     \t [0.17781177 0.7061417  0.85902138]. \t  3.139967202937458 \t 3.5400769232135207\n",
      "28     \t [0.66252696 0.         0.78755699]. \t  0.2504251329417409 \t 3.5400769232135207\n",
      "29     \t [1.         0.60897582 0.99968728]. \t  1.9800993425529323 \t 3.5400769232135207\n",
      "30     \t [0.27287528 0.70226331 0.        ]. \t  0.003913425195575182 \t 3.5400769232135207\n",
      "31     \t [0.59038144 0.60616211 0.87610622]. \t  \u001b[92m3.687092875783365\u001b[0m \t 3.687092875783365\n",
      "32     \t [1.         0.         0.19656968]. \t  0.22722748060240916 \t 3.687092875783365\n",
      "33     \t [0.81569325 0.66727215 0.90883639]. \t  3.1039205359927324 \t 3.687092875783365\n",
      "34     \t [1.         0.45192794 0.88069431]. \t  3.2759754702403767 \t 3.687092875783365\n",
      "35     \t [0.25365398 0.43827801 0.88540747]. \t  3.281409431456435 \t 3.687092875783365\n",
      "36     \t [1.         0.63927287 0.83563344]. \t  3.3389982392510182 \t 3.687092875783365\n",
      "37     \t [0.16963427 0.56785782 0.83850707]. \t  \u001b[92m3.8281571194126895\u001b[0m \t 3.8281571194126895\n",
      "38     \t [0.25852526 0.         0.25401482]. \t  0.8364557765835209 \t 3.8281571194126895\n",
      "39     \t [0.42200152 0.58020775 0.85861615]. \t  3.8278453972806217 \t 3.8281571194126895\n",
      "40     \t [0.1906868  0.56669933 0.84347293]. \t  \u001b[92m3.842618035216954\u001b[0m \t 3.842618035216954\n",
      "41     \t [0.19666175 0.56616827 0.84589037]. \t  \u001b[92m3.8473813293035004\u001b[0m \t 3.8473813293035004\n",
      "42     \t [0.39331017 0.57580465 0.85622162]. \t  3.8398825745930276 \t 3.8473813293035004\n",
      "43     \t [0.21378603 0.56620502 0.84678356]. \t  \u001b[92m3.850057849917069\u001b[0m \t 3.850057849917069\n",
      "44     \t [0.36088045 0.57265808 0.85399691]. \t  3.8476133922511475 \t 3.850057849917069\n",
      "45     \t [0.21360725 0.56602444 0.84787746]. \t  \u001b[92m3.8513342246169957\u001b[0m \t 3.8513342246169957\n",
      "46     \t [0.22557166 0.56601775 0.84843237]. \t  \u001b[92m3.8527711503687545\u001b[0m \t 3.8527711503687545\n",
      "47     \t [0.3534886  0.57161395 0.85404002]. \t  3.8494466515849863 \t 3.8527711503687545\n",
      "48     \t [0.22301196 0.56585697 0.84894581]. \t  \u001b[92m3.853121752872494\u001b[0m \t 3.853121752872494\n",
      "49     \t [0.23485215 0.56608277 0.84927363]. \t  \u001b[92m3.8539859734711523\u001b[0m \t 3.8539859734711523\n",
      "50     \t [0.34001279 0.57068803 0.85356968]. \t  3.851347400389123 \t 3.8539859734711523\n",
      "51     \t [0.22863027 0.56579453 0.84958355]. \t  \u001b[92m3.8540146992750786\u001b[0m \t 3.8540146992750786\n",
      "52     \t [0.23852861 0.56593885 0.84982357]. \t  \u001b[92m3.8546730985196986\u001b[0m \t 3.8546730985196986\n",
      "53     \t [0.32805514 0.56965251 0.85312317]. \t  3.853023392756163 \t 3.8546730985196986\n",
      "54     \t [0.23182124 0.56576125 0.85002188]. \t  3.854514907895388 \t 3.8546730985196986\n",
      "55     \t [0.24143185 0.56602208 0.85019297]. \t  \u001b[92m3.8549796963076672\u001b[0m \t 3.8549796963076672\n",
      "56     \t [0.25199931 0.56631104 0.85044572]. \t  \u001b[92m3.855385788411469\u001b[0m \t 3.855385788411469\n",
      "57     \t [0.33372223 0.56986472 0.85370974]. \t  3.852579347331705 \t 3.855385788411469\n",
      "58     \t [0.23800201 0.56580652 0.85044109]. \t  3.8550798046336516 \t 3.855385788411469\n",
      "59     \t [0.24495696 0.56587755 0.85057364]. \t  \u001b[92m3.8554648999167895\u001b[0m \t 3.8554648999167895\n",
      "60     \t [0.25513087 0.56629324 0.85081107]. \t  \u001b[92m3.855695784010048\u001b[0m \t 3.855695784010048\n",
      "61     \t [0.32234114 0.56924901 0.85330458]. \t  3.8536796139909173 \t 3.855695784010048\n",
      "62     \t [0.24066759 0.56590427 0.85074069]. \t  3.8552935246932747 \t 3.855695784010048\n",
      "63     \t [0.         0.57381529 0.85044814]. \t  3.798510757972632 \t 3.855695784010048\n",
      "64     \t [0.39156854 0.57164645 0.85838319]. \t  3.8444567987663527 \t 3.855695784010048\n",
      "65     \t [0.18656381 0.56535943 0.85288092]. \t  3.8513208702608352 \t 3.855695784010048\n",
      "66     \t [0.34980053 0.56959659 0.85610097]. \t  3.8514884305467962 \t 3.855695784010048\n",
      "67     \t [0.20256608 0.56508076 0.85272402]. \t  3.853312286953642 \t 3.855695784010048\n",
      "68     \t [0.31737549 0.56812041 0.85469464]. \t  3.854829690683855 \t 3.855695784010048\n",
      "69     \t [0.21655086 0.56520746 0.85269006]. \t  3.8545442270803347 \t 3.855695784010048\n",
      "70     \t [0.30169576 0.5674801  0.85413019]. \t  \u001b[92m3.855811819692689\u001b[0m \t 3.855811819692689\n",
      "71     \t [0.22707526 0.56530594 0.85268159]. \t  3.855314350290056 \t 3.855811819692689\n",
      "72     \t [0.29754852 0.56724055 0.85407441]. \t  \u001b[92m3.8560640981580105\u001b[0m \t 3.8560640981580105\n",
      "73     \t [0.23070185 0.56548248 0.85264126]. \t  3.855445226387255 \t 3.8560640981580105\n",
      "74     \t [0.2939856  0.5669976  0.85388968]. \t  \u001b[92m3.856309908275181\u001b[0m \t 3.856309908275181\n",
      "75     \t [0.2342668  0.56554061 0.85279012]. \t  3.8556483764392975 \t 3.856309908275181\n",
      "76     \t [0.29191711 0.56698098 0.85388045]. \t  \u001b[92m3.856327309597994\u001b[0m \t 3.856327309597994\n",
      "77     \t [0.23629805 0.56532364 0.85275405]. \t  3.8559328917401934 \t 3.856327309597994\n",
      "78     \t [0.29321112 0.56703266 0.85397397]. \t  3.8562717391344234 \t 3.856327309597994\n",
      "79     \t [0.23679065 0.56549487 0.85283768]. \t  3.85584323115956 \t 3.856327309597994\n",
      "80     \t [0.290595   0.56703594 0.85390542]. \t  3.8562768809266736 \t 3.856327309597994\n",
      "81     \t [0.24185622 0.56557073 0.8528912 ]. \t  3.856089218646053 \t 3.856327309597994\n",
      "82     \t [0.28553495 0.56686618 0.85381082]. \t  \u001b[92m3.8564087499066413\u001b[0m \t 3.8564087499066413\n",
      "83     \t [0.2435449  0.56558259 0.85294888]. \t  3.8561746640629058 \t 3.8564087499066413\n",
      "84     \t [0.28804563 0.56710182 0.85382981]. \t  3.856221735556235 \t 3.8564087499066413\n",
      "85     \t [0.24010566 0.56557896 0.85303636]. \t  3.855982849906317 \t 3.8564087499066413\n",
      "86     \t [0.28742596 0.56687382 0.85384728]. \t  \u001b[92m3.8564098936973092\u001b[0m \t 3.8564098936973092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87     \t [0.23977583 0.56568855 0.85287897]. \t  3.8558829881137795 \t 3.8564098936973092\n",
      "88     \t [0.28966111 0.56707835 0.85389176]. \t  3.8562404667037216 \t 3.8564098936973092\n",
      "89     \t [0.24139792 0.56558129 0.85294189]. \t  3.856056108202318 \t 3.8564098936973092\n",
      "90     \t [0.28365    0.56692124 0.85384684]. \t  3.8563434410209 \t 3.8564098936973092\n",
      "91     \t [0.24497525 0.56551937 0.85296053]. \t  3.8562974922885376 \t 3.8564098936973092\n",
      "92     \t [0.28628157 0.56690804 0.85389437]. \t  3.856369392864541 \t 3.8564098936973092\n",
      "93     \t [0.24273132 0.56571647 0.85302846]. \t  3.856031435791767 \t 3.8564098936973092\n",
      "94     \t [0.28527616 0.56694144 0.85384812]. \t  3.8563397617196324 \t 3.8564098936973092\n",
      "95     \t [0.243297   0.56560219 0.85297223]. \t  3.856146895993646 \t 3.8564098936973092\n",
      "96     \t [0.28305785 0.56675243 0.85374642]. \t  \u001b[92m3.8564889114540337\u001b[0m \t 3.8564889114540337\n",
      "97     \t [0.24908573 0.56567386 0.85307573]. \t  3.856391102593519 \t 3.8564889114540337\n",
      "98     \t [0.27717938 0.56662699 0.85370295]. \t  \u001b[92m3.856519255855769\u001b[0m \t 3.856519255855769\n",
      "99     \t [0.24972681 0.56589456 0.85316691]. \t  3.8562550481699396 \t 3.856519255855769\n",
      "100    \t [0.28216275 0.56675592 0.85367573]. \t  3.856483132369261 \t 3.856519255855769\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0. 1. 1.]. \t  0.3302198606064223 \t 1.6237282255098657\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
      "3      \t [0.2359714  0.37886106 1.        ]. \t  1.47079872101792 \t 1.6237282255098657\n",
      "4      \t [0.32643603 0.07070557 0.73634002]. \t  0.4303437181263484 \t 1.6237282255098657\n",
      "5      \t [0.3915361  0.56959094 0.77298859]. \t  \u001b[92m3.3095754800874033\u001b[0m \t 3.3095754800874033\n",
      "6      \t [0.         0.6699257  0.54363267]. \t  2.124167867852087 \t 3.3095754800874033\n",
      "7      \t [1.         0.4551689  0.62515283]. \t  0.9974559577900148 \t 3.3095754800874033\n",
      "8      \t [0.45426372 0.42390628 0.43385035]. \t  0.38672427056860437 \t 3.3095754800874033\n",
      "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.3095754800874033\n",
      "10     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.3095754800874033\n",
      "11     \t [1.         1.         0.63268416]. \t  0.28470765194535397 \t 3.3095754800874033\n",
      "12     \t [0.1648422  0.97420491 0.        ]. \t  0.00032959166352881675 \t 3.3095754800874033\n",
      "13     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.3095754800874033\n",
      "14     \t [1.         0.59809071 0.99867121]. \t  2.013076218179682 \t 3.3095754800874033\n",
      "15     \t [0.         0.61892024 0.86841112]. \t  \u001b[92m3.654939781793269\u001b[0m \t 3.654939781793269\n",
      "16     \t [0.71773745 0.5424361  0.        ]. \t  0.013352244430738982 \t 3.654939781793269\n",
      "17     \t [1.         0.         0.81946114]. \t  0.24061911496562308 \t 3.654939781793269\n",
      "18     \t [0.         0.84844931 0.75302324]. \t  1.9277853238777747 \t 3.654939781793269\n",
      "19     \t [0.         0.66332689 0.        ]. \t  0.004122989594234941 \t 3.654939781793269\n",
      "20     \t [0.42559516 0.         0.        ]. \t  0.10126538165330659 \t 3.654939781793269\n",
      "21     \t [0.62994748 1.         0.        ]. \t  0.00015019111271676099 \t 3.654939781793269\n",
      "22     \t [0.         0.35355841 0.74229637]. \t  2.1298297179551238 \t 3.654939781793269\n",
      "23     \t [0.23693307 0.67729416 1.        ]. \t  1.8527475894809948 \t 3.654939781793269\n",
      "24     \t [0.82214867 0.70275707 0.81458168]. \t  2.8283126335988777 \t 3.654939781793269\n",
      "25     \t [0.65701588 0.         0.9614863 ]. \t  0.12936128381568832 \t 3.654939781793269\n",
      "26     \t [0.         0.56221031 1.        ]. \t  2.0573692619223958 \t 3.654939781793269\n",
      "27     \t [0.13828954 0.52608231 0.76024992]. \t  3.1829093204213765 \t 3.654939781793269\n",
      "28     \t [1.         0.75618314 0.8309285 ]. \t  2.370739489550881 \t 3.654939781793269\n",
      "29     \t [0.49497233 0.37864226 0.7996886 ]. \t  2.789146820312596 \t 3.654939781793269\n",
      "30     \t [0.65096617 0.65931203 0.96089919]. \t  2.540975985716513 \t 3.654939781793269\n",
      "31     \t [0.         0.56074943 0.73554104]. \t  2.889426079456905 \t 3.654939781793269\n",
      "32     \t [0.         0.         0.95058906]. \t  0.13938809108587183 \t 3.654939781793269\n",
      "33     \t [1.         0.42625758 0.88334544]. \t  3.082792425172099 \t 3.654939781793269\n",
      "34     \t [1.         0.19491821 0.97120218]. \t  0.6663248531798406 \t 3.654939781793269\n",
      "35     \t [0.24865026 0.75514263 0.17864562]. \t  0.04827087454990986 \t 3.654939781793269\n",
      "36     \t [0.         1.         0.08230998]. \t  0.0031483912168895263 \t 3.654939781793269\n",
      "37     \t [0.80843169 0.4776671  0.83867229]. \t  3.556979031904194 \t 3.654939781793269\n",
      "38     \t [0.56987175 0.56831453 0.83838153]. \t  \u001b[92m3.7873821215042454\u001b[0m \t 3.7873821215042454\n",
      "39     \t [0.         0.72558092 0.8977083 ]. \t  2.7902989645266203 \t 3.7873821215042454\n",
      "40     \t [0.66028725 0.54722423 0.82353377]. \t  3.7008138122233785 \t 3.7873821215042454\n",
      "41     \t [0.52613141 0.56012463 0.85109848]. \t  \u001b[92m3.833028286755076\u001b[0m \t 3.833028286755076\n",
      "42     \t [0.57444138 0.5586138  0.84188755]. \t  3.80584005796213 \t 3.833028286755076\n",
      "43     \t [0.73005411 0.52815557 0.83811017]. \t  3.7343261095531473 \t 3.833028286755076\n",
      "44     \t [0.56421363 0.55938411 0.84286563]. \t  3.8106501312858527 \t 3.833028286755076\n",
      "45     \t [0.59720496 0.55727701 0.83630632]. \t  3.781950882369957 \t 3.833028286755076\n",
      "46     \t [0.6086105  0.55470215 0.83578534]. \t  3.777754483815717 \t 3.833028286755076\n",
      "47     \t [0.61045372 0.55355105 0.8367266 ]. \t  3.7811922823871367 \t 3.833028286755076\n",
      "48     \t [0.52228899 0.55736093 0.85094623]. \t  \u001b[92m3.834974417752209\u001b[0m \t 3.834974417752209\n",
      "49     \t [0.59300122 0.5550092  0.83975702]. \t  3.796163820782696 \t 3.834974417752209\n",
      "50     \t [0.60116365 0.55386848 0.83893538]. \t  3.791568277002004 \t 3.834974417752209\n",
      "51     \t [0.60407132 0.55319963 0.83888517]. \t  3.7907433552914025 \t 3.834974417752209\n",
      "52     \t [0.51745278 0.55742443 0.85102031]. \t  \u001b[92m3.836029684038237\u001b[0m \t 3.836029684038237\n",
      "53     \t [0.5703019  0.55600963 0.84358024]. \t  3.812435995178612 \t 3.836029684038237\n",
      "54     \t [0.58629936 0.55482727 0.84137462]. \t  3.802883598788438 \t 3.836029684038237\n",
      "55     \t [0.59067183 0.55428502 0.84088218]. \t  3.8004388180113198 \t 3.836029684038237\n",
      "56     \t [0.59340078 0.55397848 0.84067418]. \t  3.7991574592408637 \t 3.836029684038237\n",
      "57     \t [0.59318835 0.55390183 0.84078459]. \t  3.7995609613334063 \t 3.836029684038237\n",
      "58     \t [0.59332494 0.55377667 0.8407386 ]. \t  3.799421292943922 \t 3.836029684038237\n",
      "59     \t [0.59058608 0.55366291 0.84115998]. \t  3.8014255028038044 \t 3.836029684038237\n",
      "60     \t [0.50068362 0.55738401 0.85241944]. \t  \u001b[92m3.840295421341058\u001b[0m \t 3.840295421341058\n",
      "61     \t [0.52615739 0.55716346 0.8493367 ]. \t  3.8326429640347346 \t 3.840295421341058\n",
      "62     \t [0.54003285 0.55692103 0.84755287]. \t  3.827239016874591 \t 3.840295421341058\n",
      "63     \t [0.54756964 0.55679175 0.84651858]. \t  3.8238296784525767 \t 3.840295421341058\n",
      "64     \t [0.55419308 0.55644533 0.84568057]. \t  3.8208249749048386 \t 3.840295421341058\n",
      "65     \t [0.55915666 0.55607711 0.84512103]. \t  3.8186257754901156 \t 3.840295421341058\n",
      "66     \t [0.56219513 0.55594295 0.84473802]. \t  3.8171178683330815 \t 3.840295421341058\n",
      "67     \t [0.56573535 0.55578792 0.84423528]. \t  3.8151862728827326 \t 3.840295421341058\n",
      "68     \t [0.56762201 0.55560119 0.84400942]. \t  3.8142605390858892 \t 3.840295421341058\n",
      "69     \t [0.5688356  0.55550005 0.84388194]. \t  3.8136915680133345 \t 3.840295421341058\n",
      "70     \t [0.57083813 0.555339   0.84363778]. \t  3.8126610158512495 \t 3.840295421341058\n",
      "71     \t [0.5722382  0.55523676 0.84349458]. \t  3.811992316106593 \t 3.840295421341058\n",
      "72     \t [0.57345852 0.55499722 0.84342465]. \t  3.811581014060951 \t 3.840295421341058\n",
      "73     \t [0.5741193  0.55512974 0.84324686]. \t  3.8109412668438742 \t 3.840295421341058\n",
      "74     \t [0.57376969 0.55502707 0.84332452]. \t  3.811251441507451 \t 3.840295421341058\n",
      "75     \t [0.57501488 0.55498591 0.84324901]. \t  3.8107549810386567 \t 3.840295421341058\n",
      "76     \t [0.57567411 0.55490551 0.84307101]. \t  3.810172437932192 \t 3.840295421341058\n",
      "77     \t [0.57555837 0.55481269 0.84313063]. \t  3.810376016212409 \t 3.840295421341058\n",
      "78     \t [0.57677505 0.55476452 0.84294431]. \t  3.8096108948533427 \t 3.840295421341058\n",
      "79     \t [0.57634547 0.55476667 0.84307702]. \t  3.810050632349589 \t 3.840295421341058\n",
      "80     \t [0.57616345 0.55471402 0.84306327]. \t  3.810080180230812 \t 3.840295421341058\n",
      "81     \t [0.5764492  0.55474838 0.84301039]. \t  3.809864913104577 \t 3.840295421341058\n",
      "82     \t [0.57650388 0.5547923  0.84300995]. \t  3.809836601643136 \t 3.840295421341058\n",
      "83     \t [0.57653703 0.55472096 0.84304536]. \t  3.8099355335163367 \t 3.840295421341058\n",
      "84     \t [0.57634885 0.5547445  0.84304418]. \t  3.8099756225934667 \t 3.840295421341058\n",
      "85     \t [0.57720858 0.55465601 0.84292553]. \t  3.809480673002722 \t 3.840295421341058\n",
      "86     \t [0.57662822 0.55464757 0.84304397]. \t  3.80992884635956 \t 3.840295421341058\n",
      "87     \t [0.57670143 0.55454209 0.84305271]. \t  3.809960259132055 \t 3.840295421341058\n",
      "88     \t [0.57712051 0.55461596 0.84298965]. \t  3.809673819117651 \t 3.840295421341058\n",
      "89     \t [0.57748692 0.55449324 0.84295719]. \t  3.809530542310973 \t 3.840295421341058\n",
      "90     \t [0.5771517  0.55453897 0.84305824]. \t  3.8098555496599507 \t 3.840295421341058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.57694473 0.55465598 0.84296072]. \t  3.809637697888219 \t 3.840295421341058\n",
      "92     \t [0.57658235 0.55464921 0.84305117]. \t  3.80995819966723 \t 3.840295421341058\n",
      "93     \t [0.57672035 0.55464987 0.84298871]. \t  3.8097679085210543 \t 3.840295421341058\n",
      "94     \t [0.57748591 0.5545047  0.84288794]. \t  3.8093560695124227 \t 3.840295421341058\n",
      "95     \t [0.57654026 0.55463771 0.84300769]. \t  3.8098657381993966 \t 3.840295421341058\n",
      "96     \t [0.5776753  0.55456891 0.84291678]. \t  3.8093596740034217 \t 3.840295421341058\n",
      "97     \t [0.5773139  0.55451945 0.84298022]. \t  3.809626082856793 \t 3.840295421341058\n",
      "98     \t [0.57742958 0.55459575 0.84294848]. \t  3.809495904210399 \t 3.840295421341058\n",
      "99     \t [0.57685767 0.5545655  0.84298307]. \t  3.8097412661587904 \t 3.840295421341058\n",
      "100    \t [0.57665142 0.55468149 0.84305163]. \t  3.8099319212666876 \t 3.840295421341058\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [2.77555756e-17 1.00000000e+00 1.00000000e+00]. \t  0.330219860606422 \t 0.8830091449513892\n",
      "2      \t [0.8682089  0.7417213  0.95045034]. \t  \u001b[92m2.1264387904556696\u001b[0m \t 2.1264387904556696\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.1264387904556696\n",
      "4      \t [0.97530694 0.03012382 1.        ]. \t  0.12229591394257099 \t 2.1264387904556696\n",
      "5      \t [0.36949704 0.43900263 1.        ]. \t  1.7761666044126876 \t 2.1264387904556696\n",
      "6      \t [1.         0.51911223 1.        ]. \t  1.9637148924452197 \t 2.1264387904556696\n",
      "7      \t [0.86534951 0.69940216 0.63263829]. \t  1.0105517600104656 \t 2.1264387904556696\n",
      "8      \t [0.49708497 0.94150442 1.        ]. \t  0.5280178690140848 \t 2.1264387904556696\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.1264387904556696\n",
      "10     \t [0.         0.60095183 0.69058673]. \t  \u001b[92m2.459425852396037\u001b[0m \t 2.459425852396037\n",
      "11     \t [0.         0.53915895 1.        ]. \t  2.0415695487065495 \t 2.459425852396037\n",
      "12     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.459425852396037\n",
      "13     \t [0.         0.5925833  0.39859723]. \t  0.6894480468650217 \t 2.459425852396037\n",
      "14     \t [0.         0.93838105 0.61110659]. \t  \u001b[92m2.6357114448318604\u001b[0m \t 2.6357114448318604\n",
      "15     \t [0.         0.10290431 0.98012835]. \t  0.30211208657810573 \t 2.6357114448318604\n",
      "16     \t [0.70150429 0.3935589  0.84680569]. \t  \u001b[92m3.016729431612091\u001b[0m \t 3.016729431612091\n",
      "17     \t [0.68287181 0.         0.57799802]. \t  0.10506988709569051 \t 3.016729431612091\n",
      "18     \t [0.7517811  0.36640363 1.        ]. \t  1.3871518267095562 \t 3.016729431612091\n",
      "19     \t [1.         0.11805665 0.70615505]. \t  0.5239339699306922 \t 3.016729431612091\n",
      "20     \t [1.         0.47782522 0.        ]. \t  0.009660604941000092 \t 3.016729431612091\n",
      "21     \t [0.36128104 0.71473463 0.78525316]. \t  2.805938931657158 \t 3.016729431612091\n",
      "22     \t [0.40697908 0.38759353 0.66628336]. \t  1.5217107951088553 \t 3.016729431612091\n",
      "23     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.016729431612091\n",
      "24     \t [0.80877038 0.26270916 0.        ]. \t  0.053066076508234826 \t 3.016729431612091\n",
      "25     \t [0.         1.         0.33215347]. \t  0.5582128802594851 \t 3.016729431612091\n",
      "26     \t [0.41370101 0.         0.94887877]. \t  0.14352768463876578 \t 3.016729431612091\n",
      "27     \t [1.         0.52590072 0.79354345]. \t  \u001b[92m3.2937247785100228\u001b[0m \t 3.2937247785100228\n",
      "28     \t [0.19719931 0.75044091 0.        ]. \t  0.002188301822514915 \t 3.2937247785100228\n",
      "29     \t [0.         0.         0.32082982]. \t  0.5325713091157402 \t 3.2937247785100228\n",
      "30     \t [0.         0.78322283 0.85755574]. \t  2.3973007579732073 \t 3.2937247785100228\n",
      "31     \t [0.12054813 1.         0.74745797]. \t  1.1617191678055598 \t 3.2937247785100228\n",
      "32     \t [0.77923593 0.49937167 0.66832234]. \t  1.6496589186880288 \t 3.2937247785100228\n",
      "33     \t [0.50170162 0.         0.07941408]. \t  0.2868406066854365 \t 3.2937247785100228\n",
      "34     \t [1.         0.74985617 0.86247211]. \t  2.522132364833073 \t 3.2937247785100228\n",
      "35     \t [0.59954582 0.80023457 0.80594582]. \t  2.0496595816625236 \t 3.2937247785100228\n",
      "36     \t [0.         0.34973503 0.86715888]. \t  2.5527539969005635 \t 3.2937247785100228\n",
      "37     \t [1.         0.35972695 0.87749485]. \t  2.5463352422857235 \t 3.2937247785100228\n",
      "38     \t [0.55371899 0.59344817 0.91205972]. \t  \u001b[92m3.465094054188797\u001b[0m \t 3.465094054188797\n",
      "39     \t [0.54659636 0.55979388 0.87401144]. \t  \u001b[92m3.7945202038545194\u001b[0m \t 3.7945202038545194\n",
      "40     \t [1.         0.52909316 0.27045288]. \t  0.0632929707898035 \t 3.7945202038545194\n",
      "41     \t [0.56826484 0.57265658 0.87605648]. \t  3.772112988434327 \t 3.7945202038545194\n",
      "42     \t [0.56773299 0.5726452  0.87579831]. \t  3.7732937566447267 \t 3.7945202038545194\n",
      "43     \t [0.53564632 0.64020264 0.96868227]. \t  2.510230462976354 \t 3.7945202038545194\n",
      "44     \t [0.55900178 0.54624204 0.8528785 ]. \t  \u001b[92m3.8276599124901116\u001b[0m \t 3.8276599124901116\n",
      "45     \t [0.         0.8150714  0.08564998]. \t  0.005696225711170059 \t 3.8276599124901116\n",
      "46     \t [0.58234641 0.55709875 0.85615381]. \t  3.8230884372818443 \t 3.8276599124901116\n",
      "47     \t [0.58255114 0.55713825 0.85617499]. \t  3.8230224809516002 \t 3.8276599124901116\n",
      "48     \t [0.58243207 0.55704734 0.85614908]. \t  3.8230847199353004 \t 3.8276599124901116\n",
      "49     \t [0.58244437 0.5571192  0.85615407]. \t  3.823057707832821 \t 3.8276599124901116\n",
      "50     \t [0.58252399 0.55707439 0.8561876 ]. \t  3.8230482405787507 \t 3.8276599124901116\n",
      "51     \t [0.58245045 0.55705348 0.85617882]. \t  3.8230742384826977 \t 3.8276599124901116\n",
      "52     \t [0.58240232 0.55710775 0.85614894]. \t  3.8230724334119355 \t 3.8276599124901116\n",
      "53     \t [0.58231425 0.55710984 0.85615858]. \t  3.8230920684354137 \t 3.8276599124901116\n",
      "54     \t [0.58233496 0.5571735  0.85618344]. \t  3.8230627103860946 \t 3.8276599124901116\n",
      "55     \t [0.58219258 0.55708866 0.85618246]. \t  3.823125542919542 \t 3.8276599124901116\n",
      "56     \t [0.58215978 0.55706059 0.85622366]. \t  3.823136780172077 \t 3.8276599124901116\n",
      "57     \t [0.5820981  0.55717033 0.85621512]. \t  3.8231173299439636 \t 3.8276599124901116\n",
      "58     \t [0.5821357  0.55712473 0.85623712]. \t  3.82311987268395 \t 3.8276599124901116\n",
      "59     \t [0.58210229 0.55713761 0.85619358]. \t  3.823130136079658 \t 3.8276599124901116\n",
      "60     \t [0.582042   0.55717855 0.85620547]. \t  3.823129735986844 \t 3.8276599124901116\n",
      "61     \t [1.        0.        0.1990793]. \t  0.2296175962032703 \t 3.8276599124901116\n",
      "62     \t [0.57324387 0.55649347 0.85626062]. \t  3.8254544989799313 \t 3.8276599124901116\n",
      "63     \t [0.57318326 0.55647899 0.85626854]. \t  3.82547152222974 \t 3.8276599124901116\n",
      "64     \t [0.57317914 0.55651742 0.85625203]. \t  3.8254648041209838 \t 3.8276599124901116\n",
      "65     \t [0.57312621 0.55644927 0.85624635]. \t  3.825497153926465 \t 3.8276599124901116\n",
      "66     \t [0.57308826 0.55650026 0.85625373]. \t  3.825490913600252 \t 3.8276599124901116\n",
      "67     \t [0.57307382 0.55649231 0.85622116]. \t  3.825502141652011 \t 3.8276599124901116\n",
      "68     \t [0.5732141  0.55645724 0.85627041]. \t  3.825469798696327 \t 3.8276599124901116\n",
      "69     \t [0.5731901  0.55648256 0.85623125]. \t  3.825475408074743 \t 3.8276599124901116\n",
      "70     \t [0.57311872 0.55646304 0.85622652]. \t  3.8254985833711856 \t 3.8276599124901116\n",
      "71     \t [0.57312011 0.55648593 0.85620956]. \t  3.825494810826779 \t 3.8276599124901116\n",
      "72     \t [0.57301381 0.55649909 0.85624166]. \t  3.825511073117288 \t 3.8276599124901116\n",
      "73     \t [0.57308447 0.55645552 0.85623834]. \t  3.825506780621593 \t 3.8276599124901116\n",
      "74     \t [0.57293198 0.55639593 0.85621541]. \t  3.8255631672910075 \t 3.8276599124901116\n",
      "75     \t [0.57319035 0.55649821 0.85617044]. \t  3.825481008133507 \t 3.8276599124901116\n",
      "76     \t [0.57305573 0.55643438 0.85620777]. \t  3.825524568140141 \t 3.8276599124901116\n",
      "77     \t [0.57297328 0.55641764 0.85618029]. \t  3.8255532563104877 \t 3.8276599124901116\n",
      "78     \t [0.57302214 0.55649682 0.85620432]. \t  3.825516018415471 \t 3.8276599124901116\n",
      "79     \t [0.57306914 0.55645992 0.85620956]. \t  3.825514104402674 \t 3.8276599124901116\n",
      "80     \t [0.57305468 0.55648587 0.85615653]. \t  3.8255189492290516 \t 3.8276599124901116\n",
      "81     \t [0.57300687 0.55650352 0.85620662]. \t  3.8255174238232 \t 3.8276599124901116\n",
      "82     \t [0.57309096 0.55640638 0.85624022]. \t  3.8255182495898263 \t 3.8276599124901116\n",
      "83     \t [0.57310261 0.55642273 0.85623149]. \t  3.8255125585944936 \t 3.8276599124901116\n",
      "84     \t [0.57305708 0.55645831 0.85622001]. \t  3.825515667776137 \t 3.8276599124901116\n",
      "85     \t [0.5731085  0.55649194 0.8562079 ]. \t  3.8254961941759626 \t 3.8276599124901116\n",
      "86     \t [0.57306704 0.55644093 0.85619024]. \t  3.8255229734541327 \t 3.8276599124901116\n",
      "87     \t [0.57302387 0.55635219 0.8562151 ]. \t  3.825553045939904 \t 3.8276599124901116\n",
      "88     \t [0.57303497 0.5564593  0.85621398]. \t  3.825521678843762 \t 3.8276599124901116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.57304699 0.55638122 0.85621985]. \t  3.825538978609236 \t 3.8276599124901116\n",
      "90     \t [0.57296328 0.55640106 0.85625694]. \t  3.825547163237179 \t 3.8276599124901116\n",
      "91     \t [0.57306811 0.55642595 0.85617726]. \t  3.8255288922325867 \t 3.8276599124901116\n",
      "92     \t [0.57303373 0.55639413 0.85619825]. \t  3.825542280298356 \t 3.8276599124901116\n",
      "93     \t [0.57307942 0.55644247 0.85617627]. \t  3.8255218537164213 \t 3.8276599124901116\n",
      "94     \t [0.5730347  0.55643997 0.856219  ]. \t  3.825526176275681 \t 3.8276599124901116\n",
      "95     \t [0.57314912 0.55640275 0.85619532]. \t  3.825512942828281 \t 3.8276599124901116\n",
      "96     \t [0.57308872 0.55641908 0.85623286]. \t  3.82551662182639 \t 3.8276599124901116\n",
      "97     \t [0.5729218  0.55636384 0.85621781]. \t  3.8255737896741007 \t 3.8276599124901116\n",
      "98     \t [0.57304917 0.5563992  0.85617828]. \t  3.825540485335404 \t 3.8276599124901116\n",
      "99     \t [0.57313193 0.55641773 0.85621822]. \t  3.825509179947015 \t 3.8276599124901116\n",
      "100    \t [0.57306159 0.55642539 0.85620112]. \t  3.8255267165466993 \t 3.8276599124901116\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
      "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
      "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
      "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
      "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
      "1      \t [0.76578211 1.         1.        ]. \t  0.3255963478421927 \t 1.6536488994056173\n",
      "2      \t [0.         1.         0.77490875]. \t  0.9670197324816105 \t 1.6536488994056173\n",
      "3      \t [0.         0.57095083 0.27797625]. \t  0.20156410116118267 \t 1.6536488994056173\n",
      "4      \t [0.84846455 0.3019285  0.81642026]. \t  \u001b[92m2.109618045606501\u001b[0m \t 2.109618045606501\n",
      "5      \t [ 1.00000000e+00 -5.55111512e-17  1.00000000e+00]. \t  0.08848201872702738 \t 2.109618045606501\n",
      "6      \t [1.         0.60782624 0.6241399 ]. \t  0.928908926738548 \t 2.109618045606501\n",
      "7      \t [-1.38777878e-17  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 2.109618045606501\n",
      "8      \t [0.58054252 0.51110668 1.        ]. \t  2.0157235098910102 \t 2.109618045606501\n",
      "9      \t [0.96569688 0.47398255 1.        ]. \t  1.852295635197436 \t 2.109618045606501\n",
      "10     \t [0.70605883 0.26523383 1.        ]. \t  0.8472921185634389 \t 2.109618045606501\n",
      "11     \t [0.         0.64369601 1.        ]. \t  1.9449316526833897 \t 2.109618045606501\n",
      "12     \t [1.         0.         0.49516286]. \t  0.07482061686103206 \t 2.109618045606501\n",
      "13     \t [ 0.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.06797411659013226 \t 2.109618045606501\n",
      "14     \t [0. 1. 1.]. \t  0.330219860606422 \t 2.109618045606501\n",
      "15     \t [0.         0.36153233 0.70732103]. \t  1.8353463804599066 \t 2.109618045606501\n",
      "16     \t [0.         0.67333361 0.77293227]. \t  \u001b[92m3.0003163814996094\u001b[0m \t 3.0003163814996094\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.0003163814996094\n",
      "18     \t [0.7576003  0.66608004 0.86749662]. \t  \u001b[92m3.3326962919729346\u001b[0m \t 3.3326962919729346\n",
      "19     \t [0.         0.         0.55371254]. \t  0.09985997262858029 \t 3.3326962919729346\n",
      "20     \t [0.63168759 0.         0.        ]. \t  0.08311418609957139 \t 3.3326962919729346\n",
      "21     \t [0.94942223 0.76529798 1.        ]. \t  1.3739298946571339 \t 3.3326962919729346\n",
      "22     \t [0.4895827  0.80435609 0.87525547]. \t  2.1411075624161517 \t 3.3326962919729346\n",
      "23     \t [0.7758736  0.83234974 0.7338476 ]. \t  1.292342962012666 \t 3.3326962919729346\n",
      "24     \t [0.63748407 0.         0.64424455]. \t  0.14709852671043633 \t 3.3326962919729346\n",
      "25     \t [0.2871484  0.         0.01323776]. \t  0.12327138583454023 \t 3.3326962919729346\n",
      "26     \t [0.67853291 1.         0.        ]. \t  0.00013075966859959186 \t 3.3326962919729346\n",
      "27     \t [0.7143167 0.7404235 1.       ]. \t  1.5381217151418796 \t 3.3326962919729346\n",
      "28     \t [0.         0.68704426 0.        ]. \t  0.003224349850420056 \t 3.3326962919729346\n",
      "29     \t [0.83249143 0.77551401 0.        ]. \t  0.0008606192028088776 \t 3.3326962919729346\n",
      "30     \t [0.         0.         0.97190431]. \t  0.1174363400530585 \t 3.3326962919729346\n",
      "31     \t [0.16733666 0.55759132 0.87548361]. \t  \u001b[92m3.799933123754829\u001b[0m \t 3.799933123754829\n",
      "32     \t [0.16130531 0.37849384 0.96269212]. \t  1.9626253109848435 \t 3.799933123754829\n",
      "33     \t [0.15581633 0.73757719 0.90058385]. \t  2.6934162157088566 \t 3.799933123754829\n",
      "34     \t [0.87006886 0.         0.1795392 ]. \t  0.32580994126932067 \t 3.799933123754829\n",
      "35     \t [1.         0.65440351 0.85703251]. \t  3.3126250597572513 \t 3.799933123754829\n",
      "36     \t [0.4351403  0.50455205 0.01150958]. \t  0.030949559865689646 \t 3.799933123754829\n",
      "37     \t [1.         1.         0.85905009]. \t  0.5399331189886423 \t 3.799933123754829\n",
      "38     \t [0.48441945 0.51307852 0.85604673]. \t  3.7869274708308236 \t 3.799933123754829\n",
      "39     \t [0.         0.5334845  0.87027821]. \t  3.7611464972430872 \t 3.799933123754829\n",
      "40     \t [1.         0.46702906 0.85352528]. \t  3.4455447110144544 \t 3.799933123754829\n",
      "41     \t [0.42262514 0.52636429 0.86173085]. \t  \u001b[92m3.8190804531600846\u001b[0m \t 3.8190804531600846\n",
      "42     \t [0.18700562 0.53968577 0.83908857]. \t  \u001b[92m3.830778646942316\u001b[0m \t 3.830778646942316\n",
      "43     \t [0.         0.58653818 0.86073755]. \t  3.7724539784348647 \t 3.830778646942316\n",
      "44     \t [0.39483668 0.52975333 0.86146276]. \t  3.828252282330993 \t 3.830778646942316\n",
      "45     \t [0.70657564 0.54402144 0.85073114]. \t  3.7843997451097975 \t 3.830778646942316\n",
      "46     \t [0.49271376 0.52817164 0.8566164 ]. \t  3.8199916348666663 \t 3.830778646942316\n",
      "47     \t [0.17342041 0.54146006 0.84450869]. \t  \u001b[92m3.8410620811110503\u001b[0m \t 3.8410620811110503\n",
      "48     \t [0.40192146 0.53041914 0.85929585]. \t  3.8322164330117134 \t 3.8410620811110503\n",
      "49     \t [0.         0.57887255 0.85946031]. \t  3.78818123018904 \t 3.8410620811110503\n",
      "50     \t [0.41139841 0.53038914 0.85871115]. \t  3.832087214689274 \t 3.8410620811110503\n",
      "51     \t [0.17398674 0.54221633 0.8467937 ]. \t  \u001b[92m3.8445558702519977\u001b[0m \t 3.8445558702519977\n",
      "52     \t [0.35978073 0.53178051 0.85822232]. \t  3.8384628555761955 \t 3.8445558702519977\n",
      "53     \t [0.17496174 0.54236725 0.84811999]. \t  \u001b[92m3.845965385829425\u001b[0m \t 3.845965385829425\n",
      "54     \t [0.36694325 0.53158228 0.85792269]. \t  3.838128638785103 \t 3.845965385829425\n",
      "55     \t [0.         0.57885133 0.85857664]. \t  3.789195093640646 \t 3.845965385829425\n",
      "56     \t [0.8002355  0.55837355 0.86466219]. \t  3.7488430582838115 \t 3.845965385829425\n",
      "57     \t [0.56407131 0.53433974 0.85487515]. \t  3.8166160055926257 \t 3.845965385829425\n",
      "58     \t [0.16192131 0.54417751 0.84887765]. \t  \u001b[92m3.8460387740590516\u001b[0m \t 3.8460387740590516\n",
      "59     \t [0.33689077 0.53179535 0.85627853]. \t  3.841166252903137 \t 3.8460387740590516\n",
      "60     \t [0.16256185 0.54425644 0.84951759]. \t  \u001b[92m3.8465412577647102\u001b[0m \t 3.8465412577647102\n",
      "61     \t [0.34003225 0.53124531 0.85608732]. \t  3.840330575596033 \t 3.8465412577647102\n",
      "62     \t [0.54052403 0.53397645 0.85592229]. \t  3.8207959794368347 \t 3.8465412577647102\n",
      "63     \t [0.33262171 0.53192245 0.85585974]. \t  3.841783790133062 \t 3.8465412577647102\n",
      "64     \t [0.15592362 0.54533807 0.84995933]. \t  3.8464681260350684 \t 3.8465412577647102\n",
      "65     \t [0.33503861 0.53192552 0.85581723]. \t  3.8417635238131504 \t 3.8465412577647102\n",
      "66     \t [0.16001803 0.54502324 0.85041246]. \t  \u001b[92m3.8470611304036413\u001b[0m \t 3.8470611304036413\n",
      "67     \t [0.32973496 0.53206351 0.85557962]. \t  3.8422591600270857 \t 3.8470611304036413\n",
      "68     \t [0.         0.57753028 0.85808068]. \t  3.79171168877503 \t 3.8470611304036413\n",
      "69     \t [0.33112706 0.53211444 0.8555164 ]. \t  3.842353557900697 \t 3.8470611304036413\n",
      "70     \t [0.15650655 0.545417   0.85068452]. \t  3.8468543829369937 \t 3.8470611304036413\n",
      "71     \t [0.33046595 0.5318403  0.85536636]. \t  3.842022474385303 \t 3.8470611304036413\n",
      "72     \t [0.53266344 0.53414895 0.85628791]. \t  3.822415182269763 \t 3.8470611304036413\n",
      "73     \t [0.15951841 0.54506152 0.85093447]. \t  \u001b[92m3.847145118121375\u001b[0m \t 3.847145118121375\n",
      "74     \t [0.32338093 0.53225292 0.85518121]. \t  3.8429097429011216 \t 3.847145118121375\n",
      "75     \t [0.32376271 0.53217531 0.85508608]. \t  3.842836041828788 \t 3.847145118121375\n",
      "76     \t [0.1539506  0.5457234  0.85108116]. \t  3.846733334969604 \t 3.847145118121375\n",
      "77     \t [0.31510325 0.53238782 0.85474641]. \t  3.843437811661711 \t 3.847145118121375\n",
      "78     \t [0.16667861 0.54398478 0.85112828]. \t  \u001b[92m3.8474758211380062\u001b[0m \t 3.8474758211380062\n",
      "79     \t [0.3196051  0.53219168 0.85510827]. \t  3.8428984419630385 \t 3.8474758211380062\n",
      "80     \t [0.31700358 0.5324437  0.85479798]. \t  3.8434867415297984 \t 3.8474758211380062\n",
      "81     \t [0.16268999 0.54465863 0.85121672]. \t  3.847398742627669 \t 3.8474758211380062\n",
      "82     \t [0.30981554 0.53279249 0.85459428]. \t  3.8441604963599767 \t 3.8474758211380062\n",
      "83     \t [0.16539163 0.54474194 0.85155076]. \t  \u001b[92m3.847914661104258\u001b[0m \t 3.847914661104258\n",
      "84     \t [0.31258509 0.5326698  0.85455941]. \t  3.8439809866275896 \t 3.847914661104258\n",
      "85     \t [0.31919717 0.5327341  0.85476745]. \t  3.843935290912275 \t 3.847914661104258\n",
      "86     \t [0.16420657 0.54497099 0.85158815]. \t  3.8479050953383007 \t 3.847914661104258\n",
      "87     \t [0.29469596 0.53369448 0.85404273]. \t  3.8456656851300712 \t 3.847914661104258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.17638877 0.54309339 0.85161865]. \t  \u001b[92m3.848206064717785\u001b[0m \t 3.848206064717785\n",
      "89     \t [0.30462348 0.5329613  0.85432493]. \t  3.8445335247893158 \t 3.848206064717785\n",
      "90     \t [0.52370579 0.53402176 0.85650851]. \t  3.8238388804894248 \t 3.848206064717785\n",
      "91     \t [0.30311891 0.53305917 0.85426033]. \t  3.8447033057684554 \t 3.848206064717785\n",
      "92     \t [0.3129786  0.53270612 0.8545306 ]. \t  3.8440492449498675 \t 3.848206064717785\n",
      "93     \t [0.16391659 0.54480449 0.85170339]. \t  3.8477477165664107 \t 3.848206064717785\n",
      "94     \t [0.3027205  0.53301675 0.85422056]. \t  3.8446525742533235 \t 3.848206064717785\n",
      "95     \t [0.16548139 0.54472519 0.85176881]. \t  3.8479311585151015 \t 3.848206064717785\n",
      "96     \t [0.29840081 0.53315426 0.85403934]. \t  3.844898962661738 \t 3.848206064717785\n",
      "97     \t [0.29853228 0.53316969 0.85409544]. \t  3.844902283901545 \t 3.848206064717785\n",
      "98     \t [0.16694098 0.54447779 0.85186764]. \t  3.8479692878792635 \t 3.848206064717785\n",
      "99     \t [0.27991521 0.53406584 0.85361133]. \t  3.8460490342111475 \t 3.848206064717785\n",
      "100    \t [0.17144887 0.54407901 0.85205343]. \t  \u001b[92m3.8483268638421717\u001b[0m \t 3.8483268638421717\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1029187088185965\n",
      "2      \t [1.         0.88930099 0.80880304]. \t  \u001b[92m1.1165195820686737\u001b[0m \t 1.1165195820686737\n",
      "3      \t [1.         0.58451132 0.44101669]. \t  0.13447590532690396 \t 1.1165195820686737\n",
      "4      \t [0.19008692 1.         0.98167298]. \t  0.387753684724477 \t 1.1165195820686737\n",
      "5      \t [0.44087449 0.         0.31526746]. \t  0.8017972526319075 \t 1.1165195820686737\n",
      "6      \t [0.75973493 0.80739088 1.        ]. \t  \u001b[92m1.167483258052\u001b[0m \t 1.167483258052\n",
      "7      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.167483258052\n",
      "8      \t [0.54323895 1.         0.67929033]. \t  1.0769919628569897 \t 1.167483258052\n",
      "9      \t [0.         0.69127306 0.71634607]. \t  \u001b[92m2.5925420130797248\u001b[0m \t 2.5925420130797248\n",
      "10     \t [0.         1.         0.48775081]. \t  2.1617412074538147 \t 2.5925420130797248\n",
      "11     \t [0.         0.50705253 0.9520173 ]. \t  \u001b[92m2.8129881316778564\u001b[0m \t 2.8129881316778564\n",
      "12     \t [0.         0.13890263 0.88519482]. \t  0.7190450817816185 \t 2.8129881316778564\n",
      "13     \t [0.         1.         0.14945775]. \t  0.018052387744349037 \t 2.8129881316778564\n",
      "14     \t [1.         0.46915356 1.        ]. \t  1.8280938193561438 \t 2.8129881316778564\n",
      "15     \t [0.5052876 0.        0.       ]. \t  0.09669703874725076 \t 2.8129881316778564\n",
      "16     \t [0.21171033 0.54153401 1.        ]. \t  2.0685381697544267 \t 2.8129881316778564\n",
      "17     \t [0.         0.76450127 1.        ]. \t  1.4080658024798658 \t 2.8129881316778564\n",
      "18     \t [0.         0.34836335 0.63626908]. \t  1.0873522935262694 \t 2.8129881316778564\n",
      "19     \t [0.30682263 0.61463748 0.44428252]. \t  1.0201577263862864 \t 2.8129881316778564\n",
      "20     \t [1.         0.         0.45875201]. \t  0.09752342477696714 \t 2.8129881316778564\n",
      "21     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.8129881316778564\n",
      "22     \t [0.         1.         0.73115853]. \t  1.2486249137663272 \t 2.8129881316778564\n",
      "23     \t [0.         0.73521145 0.35440191]. \t  0.7320847549141173 \t 2.8129881316778564\n",
      "24     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.8129881316778564\n",
      "25     \t [0.21433339 1.         0.43304588]. \t  1.5872776858423308 \t 2.8129881316778564\n",
      "26     \t [0.90778854 0.45831564 0.        ]. \t  0.015311699543874048 \t 2.8129881316778564\n",
      "27     \t [1.         0.58108625 0.84574642]. \t  \u001b[92m3.620366045038041\u001b[0m \t 3.620366045038041\n",
      "28     \t [0.62414964 1.         0.        ]. \t  0.0001525619433972274 \t 3.620366045038041\n",
      "29     \t [1.         0.         0.82057412]. \t  0.2402989388449061 \t 3.620366045038041\n",
      "30     \t [0.29142882 0.         1.        ]. \t  0.09163197322406522 \t 3.620366045038041\n",
      "31     \t [1.         0.71650455 0.97256982]. \t  1.9895617811760762 \t 3.620366045038041\n",
      "32     \t [1.         0.33899007 0.77036664]. \t  2.1777169341735414 \t 3.620366045038041\n",
      "33     \t [0.59378808 0.58244815 0.82537363]. \t  \u001b[92m3.693073566292566\u001b[0m \t 3.693073566292566\n",
      "34     \t [0.39768232 0.75236094 0.82132133]. \t  2.6506280113409275 \t 3.693073566292566\n",
      "35     \t [0.83012017 0.60729654 0.79033566]. \t  3.193420703510633 \t 3.693073566292566\n",
      "36     \t [0.62248775 0.51103342 0.90642109]. \t  3.4732168478574628 \t 3.693073566292566\n",
      "37     \t [0.60782937 0.50925269 0.7290023 ]. \t  2.612566656023013 \t 3.693073566292566\n",
      "38     \t [1.         0.69802022 0.        ]. \t  0.0012335690389050794 \t 3.693073566292566\n",
      "39     \t [0.52472878 0.         0.76687599]. \t  0.24706980748994403 \t 3.693073566292566\n",
      "40     \t [0.17580289 0.         0.1535212 ]. \t  0.5288266324814462 \t 3.693073566292566\n",
      "41     \t [0.21226434 0.53235865 0.83766596]. \t  \u001b[92m3.8207759235965346\u001b[0m \t 3.8207759235965346\n",
      "42     \t [0.78970372 0.         0.15211433]. \t  0.34433538359228166 \t 3.8207759235965346\n",
      "43     \t [0.84904833 0.52520513 0.88500609]. \t  3.6256945550662043 \t 3.8207759235965346\n",
      "44     \t [0.17474958 0.53868809 0.82932618]. \t  3.7964056184391275 \t 3.8207759235965346\n",
      "45     \t [0.32382119 0.52623265 0.83869683]. \t  3.8162649036102994 \t 3.8207759235965346\n",
      "46     \t [0.17491356 0.54485478 0.83231488]. \t  3.8129531733977453 \t 3.8207759235965346\n",
      "47     \t [0.20132797 0.54003314 0.83133558]. \t  3.8085838466021333 \t 3.8207759235965346\n",
      "48     \t [0.34309693 0.52882868 0.8422128 ]. \t  \u001b[92m3.8278872253858593\u001b[0m \t 3.8278872253858593\n",
      "49     \t [0.2147627  0.53950366 0.83191212]. \t  3.811410405458474 \t 3.8278872253858593\n",
      "50     \t [0.31928147 0.53182698 0.83924139]. \t  3.827043681466108 \t 3.8278872253858593\n",
      "51     \t [0.21431704 0.54071616 0.83269428]. \t  3.815328223888956 \t 3.8278872253858593\n",
      "52     \t [0.25167723 0.53722028 0.83339358]. \t  3.81647269312141 \t 3.8278872253858593\n",
      "53     \t [0.31315904 0.53336082 0.83905263]. \t  \u001b[92m3.8289221715914343\u001b[0m \t 3.8289221715914343\n",
      "54     \t [0.21569385 0.54141551 0.8335678 ]. \t  3.8190916881773282 \t 3.8289221715914343\n",
      "55     \t [0.23768982 0.53896837 0.83368146]. \t  3.818678703360624 \t 3.8289221715914343\n",
      "56     \t [0.30749583 0.53444988 0.83862662]. \t  \u001b[92m3.829415488822378\u001b[0m \t 3.829415488822378\n",
      "57     \t [0.22761953 0.54021525 0.83387091]. \t  3.819923621938812 \t 3.829415488822378\n",
      "58     \t [0.24789028 0.53845249 0.83430636]. \t  3.820733502277209 \t 3.829415488822378\n",
      "59     \t [0.2992585  0.53528285 0.83796755]. \t  3.8288616572892478 \t 3.829415488822378\n",
      "60     \t [0.70979588 1.         0.18311976]. \t  0.013983655164565411 \t 3.829415488822378\n",
      "61     \t [0.19756972 0.54474516 0.84046474]. \t  \u001b[92m3.8393202319236344\u001b[0m \t 3.8393202319236344\n",
      "62     \t [0.20755961 0.54300004 0.83986548]. \t  3.8377258786890716 \t 3.8393202319236344\n",
      "63     \t [0.21672758 0.5418873  0.83942519]. \t  3.8366077980381874 \t 3.8393202319236344\n",
      "64     \t [0.22577934 0.54082207 0.83918871]. \t  3.8358073546539577 \t 3.8393202319236344\n",
      "65     \t [0.23598918 0.53997919 0.83907498]. \t  3.8354143198684403 \t 3.8393202319236344\n",
      "66     \t [0.24376399 0.53953317 0.8390919 ]. \t  3.8354509047824292 \t 3.8393202319236344\n",
      "67     \t [0.26872834 0.53807653 0.83977218]. \t  3.836498982147698 \t 3.8393202319236344\n",
      "68     \t [0.23094521 0.54043467 0.83915211]. \t  3.835717267722351 \t 3.8393202319236344\n",
      "69     \t [0.23875156 0.53994534 0.83909967]. \t  3.835602799508914 \t 3.8393202319236344\n",
      "70     \t [0.27867413 0.53781456 0.84026581]. \t  3.837531107415777 \t 3.8393202319236344\n",
      "71     \t [0.23044099 0.54057661 0.83903331]. \t  3.8355245144541015 \t 3.8393202319236344\n",
      "72     \t [0.2407693  0.53964709 0.83906913]. \t  3.83535005016295 \t 3.8393202319236344\n",
      "73     \t [0.24556612 0.53964654 0.83923933]. \t  3.8360121005750707 \t 3.8393202319236344\n",
      "74     \t [0.24309797 0.5396018  0.83903406]. \t  3.8353420818098507 \t 3.8393202319236344\n",
      "75     \t [0.24985627 0.5391016  0.83914551]. \t  3.835438344952437 \t 3.8393202319236344\n",
      "76     \t [0.2380414  0.53998021 0.83911629]. \t  3.8356365904653993 \t 3.8393202319236344\n",
      "77     \t [0.28603469 0.5373702  0.84057719]. \t  3.8378005332408125 \t 3.8393202319236344\n",
      "78     \t [0.23692499 0.53987117 0.83905921]. \t  3.8353269681628612 \t 3.8393202319236344\n",
      "79     \t [0.24137801 0.539536   0.83901084]. \t  3.83512999238982 \t 3.8393202319236344\n",
      "80     \t [0.2447079  0.53946602 0.8390778 ]. \t  3.835397182986236 \t 3.8393202319236344\n",
      "81     \t [0.24913646 0.53935794 0.83917102]. \t  3.8357266385483153 \t 3.8393202319236344\n",
      "82     \t [0.24360542 0.53968113 0.83912879]. \t  3.835678563022237 \t 3.8393202319236344\n",
      "83     \t [0.24714275 0.53934975 0.83907882]. \t  3.8354013300319125 \t 3.8393202319236344\n",
      "84     \t [0.24518384 0.53946421 0.83902302]. \t  3.835283778887492 \t 3.8393202319236344\n",
      "85     \t [0.24354491 0.53982616 0.83889628]. \t  3.8352414510535446 \t 3.8393202319236344\n",
      "86     \t [0.28757839 0.53757017 0.84067521]. \t  3.8382468655558863 \t 3.8393202319236344\n",
      "87     \t [0.24411042 0.53961039 0.83903322]. \t  3.835399797909725 \t 3.8393202319236344\n",
      "88     \t [0.24318428 0.53970809 0.83903479]. \t  3.8354522335229326 \t 3.8393202319236344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.24569444 0.53947666 0.83905614]. \t  3.835402663847139 \t 3.8393202319236344\n",
      "90     \t [0.24691827 0.53914194 0.83894786]. \t  3.834858460778257 \t 3.8393202319236344\n",
      "91     \t [0.24423287 0.53945511 0.83893597]. \t  3.8350121287908 \t 3.8393202319236344\n",
      "92     \t [0.24808899 0.53935534 0.83906256]. \t  3.835410503123616 \t 3.8393202319236344\n",
      "93     \t [0.2490184  0.5395476  0.83894728]. \t  3.8353567897659806 \t 3.8393202319236344\n",
      "94     \t [0.24095876 0.53952058 0.83912149]. \t  3.835364243634891 \t 3.8393202319236344\n",
      "95     \t [0.25189008 0.5391194  0.8392443 ]. \t  3.835784241342698 \t 3.8393202319236344\n",
      "96     \t [0.2457288  0.53960296 0.8389482 ]. \t  3.8352614446313176 \t 3.8393202319236344\n",
      "97     \t [0.2461858  0.53946323 0.83907388]. \t  3.8354566798664345 \t 3.8393202319236344\n",
      "98     \t [0.24955652 0.5392671  0.83911907]. \t  3.8355270982611467 \t 3.8393202319236344\n",
      "99     \t [0.25248614 0.53896067 0.83912537]. \t  3.8353564847568826 \t 3.8393202319236344\n",
      "100    \t [0.24527859 0.53944123 0.83929135]. \t  3.8359218636097294 \t 3.8393202319236344\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0.24568981 1.         1.        ]. \t  0.3342480851084214 \t 0.687459437576373\n",
      "2      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.687459437576373\n",
      "3      \t [1.         1.         0.27943141]. \t  0.024468952610438074 \t 0.687459437576373\n",
      "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.687459437576373\n",
      "5      \t [0.43662259 0.54994772 1.        ]. \t  \u001b[92m2.0811324666172695\u001b[0m \t 2.0811324666172695\n",
      "6      \t [0.97711282 0.34924089 1.        ]. \t  1.2656171157894882 \t 2.0811324666172695\n",
      "7      \t [0.0792292  0.52172553 1.        ]. \t  2.0268918453509603 \t 2.0811324666172695\n",
      "8      \t [0.43839007 0.26981306 1.        ]. \t  0.8788174516155525 \t 2.0811324666172695\n",
      "9      \t [1.         0.22645336 0.        ]. \t  0.03148867050963373 \t 2.0811324666172695\n",
      "10     \t [1.         0.63684977 1.        ]. \t  1.9154346197252605 \t 2.0811324666172695\n",
      "11     \t [0.52519574 0.64141748 0.73016177]. \t  \u001b[92m2.5523754741481306\u001b[0m \t 2.5523754741481306\n",
      "12     \t [0.35034352 1.         0.        ]. \t  0.00025832376069695294 \t 2.5523754741481306\n",
      "13     \t [0.74266122 0.66732491 0.        ]. \t  0.0037884550923855346 \t 2.5523754741481306\n",
      "14     \t [0.24780425 0.91916162 0.49167976]. \t  2.478200243777824 \t 2.5523754741481306\n",
      "15     \t [0.53979798 1.         0.46302017]. \t  1.142151723118171 \t 2.5523754741481306\n",
      "16     \t [0.         0.89740387 0.67290051]. \t  2.2482021044840534 \t 2.5523754741481306\n",
      "17     \t [0.37043882 0.67385291 0.38531056]. \t  0.7317949871181855 \t 2.5523754741481306\n",
      "18     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.5523754741481306\n",
      "19     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.5523754741481306\n",
      "20     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.5523754741481306\n",
      "21     \t [0.6721597 0.        0.       ]. \t  0.07759383402641766 \t 2.5523754741481306\n",
      "22     \t [0.80988655 0.         0.68411154]. \t  0.18274414251191784 \t 2.5523754741481306\n",
      "23     \t [0.22855622 0.77248192 0.79764818]. \t  2.472429227991032 \t 2.5523754741481306\n",
      "24     \t [0.88016703 0.35823781 0.68852377]. \t  1.525511430096214 \t 2.5523754741481306\n",
      "25     \t [0.1327703  1.         0.66141671]. \t  1.9448672277658883 \t 2.5523754741481306\n",
      "26     \t [0.74129501 0.73584742 1.        ]. \t  1.5588497463571862 \t 2.5523754741481306\n",
      "27     \t [0.         1.         0.34042996]. \t  0.6221478055168937 \t 2.5523754741481306\n",
      "28     \t [0.         0.68690942 0.        ]. \t  0.003228900730978016 \t 2.5523754741481306\n",
      "29     \t [0.        0.8086721 1.       ]. \t  1.1649203653192388 \t 2.5523754741481306\n",
      "30     \t [0.52682276 0.24777191 0.        ]. \t  0.09169637840746668 \t 2.5523754741481306\n",
      "31     \t [1.         0.65181313 0.        ]. \t  0.0020489395877162117 \t 2.5523754741481306\n",
      "32     \t [0.         0.31234752 1.        ]. \t  1.088669000930373 \t 2.5523754741481306\n",
      "33     \t [0.75502145 0.44136702 0.8896786 ]. \t  \u001b[92m3.2283229037035603\u001b[0m \t 3.2283229037035603\n",
      "34     \t [0.         0.64324804 0.71901231]. \t  2.6866779534364076 \t 3.2283229037035603\n",
      "35     \t [0.77589455 0.44057402 1.        ]. \t  1.7558481787021993 \t 3.2283229037035603\n",
      "36     \t [0.3716168  0.         0.70340635]. \t  0.2055456137535999 \t 3.2283229037035603\n",
      "37     \t [1.         1.         0.73974926]. \t  0.380125810215993 \t 3.2283229037035603\n",
      "38     \t [0.66176335 0.27810382 0.8059397 ]. \t  1.897139859394938 \t 3.2283229037035603\n",
      "39     \t [0.07633321 0.         0.25960557]. \t  0.6735250118708582 \t 3.2283229037035603\n",
      "40     \t [1.         0.47194162 0.85355224]. \t  \u001b[92m3.470908013482993\u001b[0m \t 3.470908013482993\n",
      "41     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.470908013482993\n",
      "42     \t [1.         0.30374862 0.82925061]. \t  2.1083577865603615 \t 3.470908013482993\n",
      "43     \t [0.87974673 0.56167926 0.85591919]. \t  \u001b[92m3.7208660987634876\u001b[0m \t 3.7208660987634876\n",
      "44     \t [1.         0.55731376 0.85694444]. \t  3.670370920286234 \t 3.7208660987634876\n",
      "45     \t [0.88486592 0.5574962  0.85681226]. \t  \u001b[92m3.7216968063976394\u001b[0m \t 3.7216968063976394\n",
      "46     \t [0.89282927 0.55173087 0.85809564]. \t  3.7200363694690277 \t 3.7216968063976394\n",
      "47     \t [0.89849324 0.54861932 0.85880802]. \t  3.717379934079533 \t 3.7216968063976394\n",
      "48     \t [1.         0.55133152 0.86036671]. \t  3.671418214097156 \t 3.7216968063976394\n",
      "49     \t [0.90357344 0.54673794 0.8593237 ]. \t  3.7146296890355046 \t 3.7216968063976394\n",
      "50     \t [0.90691084 0.54557909 0.85966231]. \t  3.7126711333443145 \t 3.7216968063976394\n",
      "51     \t [0.90946293 0.54477622 0.85986382]. \t  3.711157708527971 \t 3.7216968063976394\n",
      "52     \t [1.         0.54802946 0.86237391]. \t  3.669548488843294 \t 3.7216968063976394\n",
      "53     \t [0.91180738 0.54420777 0.86006062]. \t  3.709786582736648 \t 3.7216968063976394\n",
      "54     \t [0.91362385 0.54369075 0.86022193]. \t  3.70865606942733 \t 3.7216968063976394\n",
      "55     \t [0.91514055 0.54342086 0.86033898]. \t  3.7077893607447345 \t 3.7216968063976394\n",
      "56     \t [0.91638618 0.54315465 0.86040528]. \t  3.707069169799229 \t 3.7216968063976394\n",
      "57     \t [0.91729812 0.54304253 0.86049421]. \t  3.706551943730635 \t 3.7216968063976394\n",
      "58     \t [0.91825197 0.54283328 0.86056399]. \t  3.705974766507328 \t 3.7216968063976394\n",
      "59     \t [0.91907612 0.54279302 0.86059843]. \t  3.705572918471039 \t 3.7216968063976394\n",
      "60     \t [0.91964585 0.54265905 0.86060862]. \t  3.7052451446888863 \t 3.7216968063976394\n",
      "61     \t [1.         0.54630874 0.8633989 ]. \t  3.6679036385115857 \t 3.7216968063976394\n",
      "62     \t [0.92084672 0.54252133 0.86068909]. \t  3.7045868081053674 \t 3.7216968063976394\n",
      "63     \t [0.92151911 0.54243583 0.86076038]. \t  3.7041899622475203 \t 3.7216968063976394\n",
      "64     \t [0.92187183 0.54245162 0.86077449]. \t  3.7040364064476368 \t 3.7216968063976394\n",
      "65     \t [0.922272   0.54237098 0.86077568]. \t  3.703816180571757 \t 3.7216968063976394\n",
      "66     \t [0.92271794 0.54233775 0.86082876]. \t  3.7035605185208342 \t 3.7216968063976394\n",
      "67     \t [0.92304117 0.54228018 0.86087024]. \t  3.703352038608166 \t 3.7216968063976394\n",
      "68     \t [0.92333296 0.54227639 0.86083931]. \t  3.703250922026448 \t 3.7216968063976394\n",
      "69     \t [0.92354553 0.5422454  0.8608303 ]. \t  3.70314864767698 \t 3.7216968063976394\n",
      "70     \t [0.92390236 0.54222212 0.86088361]. \t  3.702935865629715 \t 3.7216968063976394\n",
      "71     \t [0.92413907 0.54220511 0.86088768]. \t  3.7028203939346716 \t 3.7216968063976394\n",
      "72     \t [0.9243655  0.54219033 0.86088963]. \t  3.7027124553729918 \t 3.7216968063976394\n",
      "73     \t [0.92440271 0.54218206 0.86089592]. \t  3.7026860196302973 \t 3.7216968063976394\n",
      "74     \t [0.92471853 0.5421783  0.86088712]. \t  3.7025555672786234 \t 3.7216968063976394\n",
      "75     \t [0.92482596 0.54225662 0.86092937]. \t  3.7025203816306593 \t 3.7216968063976394\n",
      "76     \t [0.92493435 0.54215406 0.86091865]. \t  3.702420804949304 \t 3.7216968063976394\n",
      "77     \t [0.92512535 0.54216368 0.8609168 ]. \t  3.70234614091152 \t 3.7216968063976394\n",
      "78     \t [0.92517931 0.54214357 0.8609244 ]. \t  3.7023040747972913 \t 3.7216968063976394\n",
      "79     \t [0.92536343 0.54217918 0.86092588]. \t  3.7022453106212505 \t 3.7216968063976394\n",
      "80     \t [1.         0.54549836 0.86404835]. \t  3.6667332124509766 \t 3.7216968063976394\n",
      "81     \t [0.926607   0.54211166 0.86103582]. \t  3.7015720877491924 \t 3.7216968063976394\n",
      "82     \t [0.9266837  0.54205907 0.86099883]. \t  3.701539428441242 \t 3.7216968063976394\n",
      "83     \t [0.92674313 0.54203049 0.86099266]. \t  3.7015015897937107 \t 3.7216968063976394\n",
      "84     \t [0.92680886 0.5420726  0.86102855]. \t  3.701467535441667 \t 3.7216968063976394\n",
      "85     \t [0.9268014  0.54205907 0.86103995]. \t  3.701452240929177 \t 3.7216968063976394\n",
      "86     \t [0.92692291 0.54208306 0.86099691]. \t  3.7014528407337757 \t 3.7216968063976394\n",
      "87     \t [0.92693327 0.54211966 0.86103254]. \t  3.7014393349394723 \t 3.7216968063976394\n",
      "88     \t [0.92713486 0.54206637 0.86094329]. \t  3.7013981870943384 \t 3.7216968063976394\n",
      "89     \t [0.92712101 0.54206904 0.86099616]. \t  3.7013594723922987 \t 3.7216968063976394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.92705401 0.54210038 0.86100889]. \t  3.7013964134081174 \t 3.7216968063976394\n",
      "91     \t [0.92718075 0.54203819 0.8609961 ]. \t  3.701314712975964 \t 3.7216968063976394\n",
      "92     \t [0.9271714  0.54207782 0.86101902]. \t  3.70132295743582 \t 3.7216968063976394\n",
      "93     \t [0.92724402 0.54212265 0.8610148 ]. \t  3.701322943518292 \t 3.7216968063976394\n",
      "94     \t [0.92733108 0.5421115  0.86103585]. \t  3.7012598946929605 \t 3.7216968063976394\n",
      "95     \t [0.92739605 0.54211023 0.86103109]. \t  3.701235328221542 \t 3.7216968063976394\n",
      "96     \t [0.92745076 0.54212246 0.861028  ]. \t  3.7012219874285632 \t 3.7216968063976394\n",
      "97     \t [0.92743794 0.5421401  0.86103274]. \t  3.701234115789428 \t 3.7216968063976394\n",
      "98     \t [0.92754803 0.54212183 0.86104216]. \t  3.7011670581997422 \t 3.7216968063976394\n",
      "99     \t [0.92747101 0.54213213 0.86101437]. \t  3.701231249169894 \t 3.7216968063976394\n",
      "100    \t [0.92750821 0.5421964  0.86104611]. \t  3.701226278940529 \t 3.7216968063976394\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0.         0.10900896 1.        ]. \t  0.26593361472447297 \t 1.6482992955272024\n",
      "2      \t [0. 1. 1.]. \t  0.3302198606064214 \t 1.6482992955272024\n",
      "3      \t [1.         0.94730063 1.        ]. \t  0.483714859219772 \t 1.6482992955272024\n",
      "4      \t [0.65488653 0.34205759 1.        ]. \t  1.2612587342658739 \t 1.6482992955272024\n",
      "5      \t [0.        0.3080415 0.       ]. \t  0.05412311521719344 \t 1.6482992955272024\n",
      "6      \t [0.08113011 0.56047614 1.        ]. \t  \u001b[92m2.0685513046976256\u001b[0m \t 2.0685513046976256\n",
      "7      \t [0.         0.49581261 0.74620055]. \t  \u001b[92m2.911499293541158\u001b[0m \t 2.911499293541158\n",
      "8      \t [0.         0.16957599 0.50266913]. \t  0.21591480010310765 \t 2.911499293541158\n",
      "9      \t [1.         0.54944954 0.86079035]. \t  \u001b[92m3.67112331865979\u001b[0m \t 3.67112331865979\n",
      "10     \t [1.         0.28800361 1.        ]. \t  0.9379425900674229 \t 3.67112331865979\n",
      "11     \t [1.         0.80777025 0.60915893]. \t  0.5267478282686093 \t 3.67112331865979\n",
      "12     \t [1.         0.55043784 0.        ]. \t  0.005433036682319128 \t 3.67112331865979\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.67112331865979\n",
      "14     \t [0.85498523 0.64214368 1.        ]. \t  1.9316752994998498 \t 3.67112331865979\n",
      "15     \t [0.58547448 1.         0.        ]. \t  0.00016855890187195737 \t 3.67112331865979\n",
      "16     \t [0.         0.64990376 0.        ]. \t  0.004726928179547609 \t 3.67112331865979\n",
      "17     \t [0.27781249 0.39596865 0.7937462 ]. \t  2.9005646435742656 \t 3.67112331865979\n",
      "18     \t [0.         0.80456615 0.70939564]. \t  2.2878449125057627 \t 3.67112331865979\n",
      "19     \t [1.       0.584216 1.      ]. \t  2.003152707594791 \t 3.67112331865979\n",
      "20     \t [1.         0.86099516 0.        ]. \t  0.00016716753867180702 \t 3.67112331865979\n",
      "21     \t [1.         0.47740416 0.63907015]. \t  1.1631899848051896 \t 3.67112331865979\n",
      "22     \t [0.56691279 0.         0.        ]. \t  0.09090002116767983 \t 3.67112331865979\n",
      "23     \t [0.64719541 0.439523   0.        ]. \t  0.03286121646489877 \t 3.67112331865979\n",
      "24     \t [0.74601413 0.11094904 0.79021025]. \t  0.6550256424430205 \t 3.67112331865979\n",
      "25     \t [0.66228806 1.         0.72773849]. \t  0.6894155008894507 \t 3.67112331865979\n",
      "26     \t [0.95481164 0.         0.        ]. \t  0.03650720756252996 \t 3.67112331865979\n",
      "27     \t [0.77857501 0.43762319 0.81028377]. \t  3.223528911069846 \t 3.67112331865979\n",
      "28     \t [0.         0.57472553 0.38703369]. \t  0.5752843621124181 \t 3.67112331865979\n",
      "29     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.67112331865979\n",
      "30     \t [0.87036646 0.71711397 0.81679207]. \t  2.69881988618209 \t 3.67112331865979\n",
      "31     \t [0.08678022 1.         0.6551141 ]. \t  2.006939600107645 \t 3.67112331865979\n",
      "32     \t [0.49778947 0.56406958 0.85687604]. \t  \u001b[92m3.837479967408341\u001b[0m \t 3.837479967408341\n",
      "33     \t [1.         0.24409146 0.79849444]. \t  1.5361527754000937 \t 3.837479967408341\n",
      "34     \t [0.         0.39722627 0.91514359]. \t  2.6571529296184426 \t 3.837479967408341\n",
      "35     \t [0.22174608 0.63132828 0.81232859]. \t  3.5252007039020796 \t 3.837479967408341\n",
      "36     \t [0.50069903 0.56637025 0.79885106]. \t  3.546784235518648 \t 3.837479967408341\n",
      "37     \t [0.         0.64576267 0.87989761]. \t  3.4803004656336918 \t 3.837479967408341\n",
      "38     \t [0.         0.         0.80920435]. \t  0.2463032390927449 \t 3.837479967408341\n",
      "39     \t [0.63830062 0.         0.19640894]. \t  0.6032902014434952 \t 3.837479967408341\n",
      "40     \t [0.         1.         0.80254942]. \t  0.8465324866002466 \t 3.837479967408341\n",
      "41     \t [0.27869534 1.         0.273349  ]. \t  0.2184328446952011 \t 3.837479967408341\n",
      "42     \t [0.3180299  0.75891241 0.73726385]. \t  2.3616781798091004 \t 3.837479967408341\n",
      "43     \t [0.53112907 0.44706922 0.75376391]. \t  2.8004556674713843 \t 3.837479967408341\n",
      "44     \t [0.80870939 0.51991328 0.85620496]. \t  3.720409350392213 \t 3.837479967408341\n",
      "45     \t [0.77568885 0.52931136 0.85293403]. \t  3.7487450858720197 \t 3.837479967408341\n",
      "46     \t [0.14807814 0.58130455 0.82451393]. \t  3.753228552696666 \t 3.837479967408341\n",
      "47     \t [0.80291168 0.52509027 0.85369001]. \t  3.732575925106079 \t 3.837479967408341\n",
      "48     \t [0.14870724 0.58178538 0.82410292]. \t  3.750406311438752 \t 3.837479967408341\n",
      "49     \t [1.         0.         0.88360758]. \t  0.20240889293309525 \t 3.837479967408341\n",
      "50     \t [1.         1.         0.81009188]. \t  0.49479132265010967 \t 3.837479967408341\n",
      "51     \t [0.13940019 0.57824926 0.82120368]. \t  3.739870772407581 \t 3.837479967408341\n",
      "52     \t [0.78433467 0.52659039 0.84547379]. \t  3.732553822096177 \t 3.837479967408341\n",
      "53     \t [0.13944438 0.57851179 0.82115943]. \t  3.739217064654601 \t 3.837479967408341\n",
      "54     \t [0.78143833 0.52708829 0.84548831]. \t  3.7343828505951953 \t 3.837479967408341\n",
      "55     \t [0.13951351 0.57873076 0.82117734]. \t  3.7389722473966094 \t 3.837479967408341\n",
      "56     \t [0.78328487 0.52704132 0.84575401]. \t  3.734158967032103 \t 3.837479967408341\n",
      "57     \t [0.1395277  0.578836   0.82115693]. \t  3.7386924195274887 \t 3.837479967408341\n",
      "58     \t [0.78224321 0.52720247 0.84564395]. \t  3.7345671011602555 \t 3.837479967408341\n",
      "59     \t [0.13940056 0.57888693 0.82105192]. \t  3.7380160257228243 \t 3.837479967408341\n",
      "60     \t [0.77260005 0.52865331 0.84550903]. \t  3.7398619448279975 \t 3.837479967408341\n",
      "61     \t [0.13942841 0.57903942 0.82113286]. \t  3.7382128722390844 \t 3.837479967408341\n",
      "62     \t [0.79787008 0.52524807 0.84643686]. \t  3.7272869109935964 \t 3.837479967408341\n",
      "63     \t [0.13956517 0.57907137 0.82110198]. \t  3.738013909833371 \t 3.837479967408341\n",
      "64     \t [0.73341342 0.53387605 0.84488573]. \t  3.758637055630971 \t 3.837479967408341\n",
      "65     \t [0.78815566 0.52675004 0.84613646]. \t  3.732669173519058 \t 3.837479967408341\n",
      "66     \t [0.73760509 0.53327066 0.84494327]. \t  3.7566795228616217 \t 3.837479967408341\n",
      "67     \t [0.78925482 0.52691902 0.84597656]. \t  3.7322382399708087 \t 3.837479967408341\n",
      "68     \t [0.13804407 0.57945191 0.82117775]. \t  3.737551850601408 \t 3.837479967408341\n",
      "69     \t [0.74448827 0.53225108 0.8449825 ]. \t  3.7532504228151735 \t 3.837479967408341\n",
      "70     \t [0.7903054  0.52658909 0.846171  ]. \t  3.7317058989392264 \t 3.837479967408341\n",
      "71     \t [0.74218601 0.53263    0.84498153]. \t  3.7544761528041954 \t 3.837479967408341\n",
      "72     \t [0.78342932 0.52735637 0.84597406]. \t  3.7349948456355 \t 3.837479967408341\n",
      "73     \t [0.13757909 0.57963309 0.82121017]. \t  3.737349501501327 \t 3.837479967408341\n",
      "74     \t [0.74868732 0.53161843 0.84503197]. \t  3.751154725759325 \t 3.837479967408341\n",
      "75     \t [0.78387199 0.52727581 0.84594114]. \t  3.734653725902422 \t 3.837479967408341\n",
      "76     \t [0.74835144 0.53163757 0.84505907]. \t  3.7513493817132373 \t 3.837479967408341\n",
      "77     \t [0.78056842 0.52799289 0.84577763]. \t  3.736593243523198 \t 3.837479967408341\n",
      "78     \t [0.13726331 0.57971372 0.82125317]. \t  3.7373960402593553 \t 3.837479967408341\n",
      "79     \t [0.76555795 0.52958386 0.8454978 ]. \t  3.743611360446508 \t 3.837479967408341\n",
      "80     \t [0.77571881 0.52823724 0.8458466 ]. \t  3.7388079398896457 \t 3.837479967408341\n",
      "81     \t [0.13731586 0.57975452 0.82127674]. \t  3.7374638938077203 \t 3.837479967408341\n",
      "82     \t [0.778966   0.52793739 0.84584247]. \t  3.7372076901373985 \t 3.837479967408341\n",
      "83     \t [0.74236427 0.53236427 0.84506356]. \t  3.7542767040226495 \t 3.837479967408341\n",
      "84     \t [0.776111   0.52830807 0.84571168]. \t  3.738514584872003 \t 3.837479967408341\n",
      "85     \t [0.74817891 0.53154834 0.84501032]. \t  3.751199527560298 \t 3.837479967408341\n",
      "86     \t [0.78003228 0.52785823 0.845876  ]. \t  3.7367737889911017 \t 3.837479967408341\n",
      "87     \t [0.13707887 0.57978454 0.8213154 ]. \t  3.737584102929625 \t 3.837479967408341\n",
      "88     \t [0.75738319 0.53038282 0.84519851]. \t  3.746932182854532 \t 3.837479967408341\n",
      "89     \t [0.78025481 0.52768806 0.84605452]. \t  3.7367747802220945 \t 3.837479967408341\n",
      "90     \t [0.74888019 0.53112958 0.84497362]. \t  3.7503651636604824 \t 3.837479967408341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.77195798 0.52864265 0.84567129]. \t  3.7403883286853894 \t 3.837479967408341\n",
      "92     \t [0.13688882 0.5797396  0.8213435 ]. \t  3.737780934248713 \t 3.837479967408341\n",
      "93     \t [0.7717894  0.52881617 0.84558315]. \t  3.7405225101894524 \t 3.837479967408341\n",
      "94     \t [0.77384787 0.52864451 0.84570505]. \t  3.739785855145973 \t 3.837479967408341\n",
      "95     \t [0.75405403 0.53069996 0.84536372]. \t  3.748827546591571 \t 3.837479967408341\n",
      "96     \t [0.78284285 0.52751615 0.84605327]. \t  3.735590496620025 \t 3.837479967408341\n",
      "97     \t [0.13710528 0.57987713 0.82135413]. \t  3.737641736460905 \t 3.837479967408341\n",
      "98     \t [0.74978845 0.53132698 0.84524782]. \t  3.75085795414233 \t 3.837479967408341\n",
      "99     \t [0.76810297 0.52933725 0.84532999]. \t  3.74204873172223 \t 3.837479967408341\n",
      "100    \t [0.76753917 0.52924123 0.84562352]. \t  3.742691764442564 \t 3.837479967408341\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [0.96511888 1.         1.        ]. \t  0.3183339348317999 \t 2.6697919207500047\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.6697919207500047\n",
      "3      \t [0.5519274  1.         0.62394447]. \t  1.3085469584230638 \t 2.6697919207500047\n",
      "4      \t [0.45774814 0.63834184 1.        ]. \t  1.9868458559946462 \t 2.6697919207500047\n",
      "5      \t [0.         0.36912095 0.49347072]. \t  0.4078917651854062 \t 2.6697919207500047\n",
      "6      \t [0.         0.90332539 0.57668746]. \t  \u001b[92m2.9167039304994606\u001b[0m \t 2.9167039304994606\n",
      "7      \t [1.         0.42622808 0.98902564]. \t  1.8164397293240166 \t 2.9167039304994606\n",
      "8      \t [0.         0.64920661 0.80337614]. \t  \u001b[92m3.339506766943253\u001b[0m \t 3.339506766943253\n",
      "9      \t [1.         0.73281076 0.67675863]. \t  1.1011545837206727 \t 3.339506766943253\n",
      "10     \t [1.         0.05330579 1.        ]. \t  0.15426969275275365 \t 3.339506766943253\n",
      "11     \t [0.         0.56953433 1.        ]. \t  2.0579274893415325 \t 3.339506766943253\n",
      "12     \t [0.68108159 0.65804203 0.81657642]. \t  3.249602349870072 \t 3.339506766943253\n",
      "13     \t [0.68862454 0.3761704  1.        ]. \t  1.4461008241204067 \t 3.339506766943253\n",
      "14     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.339506766943253\n",
      "15     \t [0.         0.         0.93989196]. \t  0.1505669195340882 \t 3.339506766943253\n",
      "16     \t [0.         0.71875803 0.49343669]. \t  2.110610518032938 \t 3.339506766943253\n",
      "17     \t [0.4769218 1.        0.       ]. \t  0.00021331588682850934 \t 3.339506766943253\n",
      "18     \t [0.8993566  0.71356216 1.        ]. \t  1.6464216294507745 \t 3.339506766943253\n",
      "19     \t [0.         0.         0.68322366]. \t  0.1815703646458458 \t 3.339506766943253\n",
      "20     \t [1.         0.44940379 0.83442564]. \t  3.323967218810252 \t 3.339506766943253\n",
      "21     \t [1.        0.1592544 0.7902597]. \t  0.9084655378655979 \t 3.339506766943253\n",
      "22     \t [0.14004155 1.         0.31649933]. \t  0.46470888396255805 \t 3.339506766943253\n",
      "23     \t [0.31271687 0.46271519 0.        ]. \t  0.035197871080799296 \t 3.339506766943253\n",
      "24     \t [0.29959311 0.7680471  0.        ]. \t  0.001903386044353711 \t 3.339506766943253\n",
      "25     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.339506766943253\n",
      "26     \t [0.19003445 0.         0.        ]. \t  0.09288939120704078 \t 3.339506766943253\n",
      "27     \t [1.         0.45631871 0.68209707]. \t  1.6884967718386372 \t 3.339506766943253\n",
      "28     \t [0.31300064 0.52741981 0.6948418 ]. \t  2.359814688895805 \t 3.339506766943253\n",
      "29     \t [0.         0.82022238 0.85054549]. \t  2.0529151327995985 \t 3.339506766943253\n",
      "30     \t [0.54490083 1.         1.        ]. \t  0.33146468295354825 \t 3.339506766943253\n",
      "31     \t [0.55170759 0.60184625 0.        ]. \t  0.010185547260463721 \t 3.339506766943253\n",
      "32     \t [0.         0.34313343 0.83137437]. \t  2.5375054312738516 \t 3.339506766943253\n",
      "33     \t [1.         0.66221684 0.86716159]. \t  3.2629365406068254 \t 3.339506766943253\n",
      "34     \t [0.55232081 0.73300577 0.5782433 ]. \t  1.6283794547782993 \t 3.339506766943253\n",
      "35     \t [0.         1.         0.68261812]. \t  1.678832427476765 \t 3.339506766943253\n",
      "36     \t [0.88275975 0.52923637 0.83243327]. \t  \u001b[92m3.654867224684655\u001b[0m \t 3.654867224684655\n",
      "37     \t [0.84382454 0.82557525 0.82196244]. \t  1.7491811230406327 \t 3.654867224684655\n",
      "38     \t [0.87999555 0.49439042 0.84342044]. \t  3.60959788530446 \t 3.654867224684655\n",
      "39     \t [1.         0.54707323 0.85025936]. \t  \u001b[92m3.6670361527864626\u001b[0m \t 3.6670361527864626\n",
      "40     \t [0.89529051 0.51816558 0.84233499]. \t  \u001b[92m3.667642656520333\u001b[0m \t 3.667642656520333\n",
      "41     \t [0.82816776 0.         0.90599171]. \t  0.185119082925169 \t 3.667642656520333\n",
      "42     \t [1.         0.5255399  0.84754274]. \t  3.64520804604708 \t 3.667642656520333\n",
      "43     \t [0.         0.62502307 0.69925998]. \t  2.5468255119900864 \t 3.667642656520333\n",
      "44     \t [0.96674946 1.         0.        ]. \t  4.403897517918598e-05 \t 3.667642656520333\n",
      "45     \t [0.59430687 0.5479032  0.84400574]. \t  \u001b[92m3.8077988086254555\u001b[0m \t 3.8077988086254555\n",
      "46     \t [0.59382907 0.54774029 0.84404507]. \t  \u001b[92m3.807983386777806\u001b[0m \t 3.807983386777806\n",
      "47     \t [0.60740018 0.54624145 0.8444408 ]. \t  3.80479643109508 \t 3.807983386777806\n",
      "48     \t [0.61859355 0.54472002 0.84485328]. \t  3.802037103462074 \t 3.807983386777806\n",
      "49     \t [0.62664682 0.54361334 0.84513815]. \t  3.799870994574684 \t 3.807983386777806\n",
      "50     \t [0.63196167 0.542949   0.84529073]. \t  3.7983357993441964 \t 3.807983386777806\n",
      "51     \t [0.63529522 0.54264044 0.84537694]. \t  3.797388649543322 \t 3.807983386777806\n",
      "52     \t [0.63708512 0.54274343 0.84536724]. \t  3.7969086210941634 \t 3.807983386777806\n",
      "53     \t [0.63795281 0.54290567 0.84525102]. \t  3.7965106989403 \t 3.807983386777806\n",
      "54     \t [0.46402332 0.65838504 0.82347323]. \t  3.386084527621632 \t 3.807983386777806\n",
      "55     \t [0.69108901 0.52942741 0.84912118]. \t  3.7734964119458954 \t 3.807983386777806\n",
      "56     \t [0.68289594 0.53142616 0.8484517 ]. \t  3.777955922059084 \t 3.807983386777806\n",
      "57     \t [0.67728161 0.532678   0.84798661]. \t  3.7806252484892218 \t 3.807983386777806\n",
      "58     \t [0.6739261  0.53346728 0.84765032]. \t  3.7821033773909187 \t 3.807983386777806\n",
      "59     \t [0.67088157 0.53417714 0.84740247]. \t  3.783462282422329 \t 3.807983386777806\n",
      "60     \t [0.66838911 0.53469573 0.8471698 ]. \t  3.7844214419980364 \t 3.807983386777806\n",
      "61     \t [0.66723076 0.53512334 0.84707148]. \t  3.785070329108023 \t 3.807983386777806\n",
      "62     \t [0.66589702 0.53545775 0.84691869]. \t  3.7855715869036546 \t 3.807983386777806\n",
      "63     \t [0.66486808 0.53575831 0.84683397]. \t  3.786046847227708 \t 3.807983386777806\n",
      "64     \t [0.66394991 0.536011   0.84671379]. \t  3.786374213568688 \t 3.807983386777806\n",
      "65     \t [0.6635338  0.53610971 0.84661217]. \t  3.7864248076541704 \t 3.807983386777806\n",
      "66     \t [0.66269892 0.53622676 0.84658295]. \t  3.7867384124485888 \t 3.807983386777806\n",
      "67     \t [0.66223543 0.53643962 0.84651056]. \t  3.786959133410033 \t 3.807983386777806\n",
      "68     \t [0.66175262 0.53651937 0.846472  ]. \t  3.7871128094092494 \t 3.807983386777806\n",
      "69     \t [0.66156165 0.53668334 0.84640653]. \t  3.787212175165009 \t 3.807983386777806\n",
      "70     \t [0.6613778  0.53679825 0.84636156]. \t  3.787296516684312 \t 3.807983386777806\n",
      "71     \t [0.66114005 0.53691107 0.84629627]. \t  3.7873577531089797 \t 3.807983386777806\n",
      "72     \t [0.66088147 0.53706739 0.84633411]. \t  3.7876453825626704 \t 3.807983386777806\n",
      "73     \t [0.66062209 0.53697492 0.84627105]. \t  3.787526204008329 \t 3.807983386777806\n",
      "74     \t [0.66030522 0.53710338 0.84622049]. \t  3.7876486940958842 \t 3.807983386777806\n",
      "75     \t [0.659926   0.53700765 0.84627121]. \t  3.7877632158481997 \t 3.807983386777806\n",
      "76     \t [0.66016974 0.53710597 0.84614221]. \t  3.7875515050263275 \t 3.807983386777806\n",
      "77     \t [0.66025909 0.53725131 0.84616828]. \t  3.7877036727466327 \t 3.807983386777806\n",
      "78     \t [0.65987935 0.53738075 0.84613656]. \t  3.7878758979885276 \t 3.807983386777806\n",
      "79     \t [0.65986124 0.53729648 0.84613697]. \t  3.787806412325234 \t 3.807983386777806\n",
      "80     \t [0.6595678  0.53744606 0.84611076]. \t  3.7879802024605866 \t 3.807983386777806\n",
      "81     \t [0.65954287 0.53736888 0.84615929]. \t  3.788005968177864 \t 3.807983386777806\n",
      "82     \t [0.65970112 0.53740207 0.84610703]. \t  3.787894686273737 \t 3.807983386777806\n",
      "83     \t [0.65951412 0.53743823 0.84612848]. \t  3.7880210940317953 \t 3.807983386777806\n",
      "84     \t [0.65930367 0.53749742 0.84609551]. \t  3.7880766804991244 \t 3.807983386777806\n",
      "85     \t [0.65938955 0.5374424  0.84607535]. \t  3.7879658957241378 \t 3.807983386777806\n",
      "86     \t [0.65941851 0.53749023 0.84605064]. \t  3.7879549575712392 \t 3.807983386777806\n",
      "87     \t [0.65952998 0.5375933  0.84604912]. \t  3.7880100140687967 \t 3.807983386777806\n",
      "88     \t [0.65930318 0.53752424 0.84607426]. \t  3.788062117938002 \t 3.807983386777806\n",
      "89     \t [0.65903424 0.5375308  0.84600544]. \t  3.7880226498918455 \t 3.807983386777806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.6592453  0.53764103 0.84603897]. \t  3.7881179618112606 \t 3.807983386777806\n",
      "91     \t [0.65914799 0.53769114 0.84598569]. \t  3.788093385790167 \t 3.807983386777806\n",
      "92     \t [0.65907475 0.5376746  0.84604247]. \t  3.788204309530483 \t 3.807983386777806\n",
      "93     \t [0.65920268 0.53764111 0.84602682]. \t  3.7881085567866464 \t 3.807983386777806\n",
      "94     \t [0.65896937 0.53768126 0.84598719]. \t  3.788140551235163 \t 3.807983386777806\n",
      "95     \t [0.65922592 0.53767225 0.84603406]. \t  3.7881420640233494 \t 3.807983386777806\n",
      "96     \t [0.65928574 0.5375866  0.8460046 ]. \t  3.787995578542035 \t 3.807983386777806\n",
      "97     \t [0.65917308 0.53767952 0.84600955]. \t  3.788119406433305 \t 3.807983386777806\n",
      "98     \t [0.65888077 0.53770734 0.84597843]. \t  3.7881735340600917 \t 3.807983386777806\n",
      "99     \t [0.6589035  0.53771516 0.84598771]. \t  3.788190577050645 \t 3.807983386777806\n",
      "100    \t [0.658813   0.53772583 0.84597977]. \t  3.7882121830480626 \t 3.807983386777806\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [-1.38777878e-17  1.00000000e+00  8.84086822e-02]. \t  0.003726710701834314 \t 2.610000357863649\n",
      "4      \t [1.         1.         0.41368529]. \t  0.13136624134920175 \t 2.610000357863649\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
      "6      \t [1.         0.36533738 0.        ]. \t  0.01915968188553927 \t 2.610000357863649\n",
      "7      \t [0.28966236 1.         0.55688309]. \t  2.3432566414168874 \t 2.610000357863649\n",
      "8      \t [1.         0.37436793 0.95403318]. \t  1.978869863261818 \t 2.610000357863649\n",
      "9      \t [0.         1.         0.57285977]. \t  2.475820428212603 \t 2.610000357863649\n",
      "10     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.610000357863649\n",
      "11     \t [0.19156711 0.47168698 0.        ]. \t  0.03038046920801749 \t 2.610000357863649\n",
      "12     \t [1.         0.40573598 0.57527651]. \t  0.5342485864080402 \t 2.610000357863649\n",
      "13     \t [1.         0.62942842 1.        ]. \t  1.93393752962067 \t 2.610000357863649\n",
      "14     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.610000357863649\n",
      "15     \t [0.         0.63986821 0.76144089]. \t  \u001b[92m3.047650529276181\u001b[0m \t 3.047650529276181\n",
      "16     \t [0.         0.44286414 1.        ]. \t  1.7667903316265952 \t 3.047650529276181\n",
      "17     \t [0.52001324 0.         0.        ]. \t  0.09547666395922927 \t 3.047650529276181\n",
      "18     \t [0.         0.40296028 0.50631177]. \t  0.5175699973469354 \t 3.047650529276181\n",
      "19     \t [0.64042758 1.         0.48375449]. \t  0.9482590362061885 \t 3.047650529276181\n",
      "20     \t [0.28601019 0.73783404 0.94145895]. \t  2.324944967789259 \t 3.047650529276181\n",
      "21     \t [0.         0.68494918 1.        ]. \t  1.7988015907277102 \t 3.047650529276181\n",
      "22     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.047650529276181\n",
      "23     \t [0.36948396 1.         0.83137349]. \t  0.7260671108605683 \t 3.047650529276181\n",
      "24     \t [0.38896022 0.23686634 1.        ]. \t  0.71870188071345 \t 3.047650529276181\n",
      "25     \t [0.18161319 0.58120239 0.77907544]. \t  \u001b[92m3.4011973121493018\u001b[0m \t 3.4011973121493018\n",
      "26     \t [1.         1.         0.72709304]. \t  0.3613595800744478 \t 3.4011973121493018\n",
      "27     \t [0.11479026 0.85547394 0.75929182]. \t  1.9043431742429073 \t 3.4011973121493018\n",
      "28     \t [0.23223164 1.         0.22927197]. \t  0.10217544709829682 \t 3.4011973121493018\n",
      "29     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.4011973121493018\n",
      "30     \t [0.73429631 0.5964621  0.        ]. \t  0.007926771165522062 \t 3.4011973121493018\n",
      "31     \t [0.70180891 0.56596235 1.        ]. \t  2.0660870644291105 \t 3.4011973121493018\n",
      "32     \t [0.         0.62719849 0.        ]. \t  0.005926691610171986 \t 3.4011973121493018\n",
      "33     \t [0.75775609 0.18465762 0.        ]. \t  0.07115667175809572 \t 3.4011973121493018\n",
      "34     \t [1.         0.         0.31133801]. \t  0.24933825780663388 \t 3.4011973121493018\n",
      "35     \t [0.38737318 0.73199719 0.65209683]. \t  2.223503366661042 \t 3.4011973121493018\n",
      "36     \t [0.13671796 0.36054777 0.79098613]. \t  2.58361610398969 \t 3.4011973121493018\n",
      "37     \t [0.22288836 0.50413772 1.        ]. \t  2.0024080488049614 \t 3.4011973121493018\n",
      "38     \t [0.13518058 0.59651732 0.6613645 ]. \t  2.2751171479603713 \t 3.4011973121493018\n",
      "39     \t [0.        0.        0.7339175]. \t  0.22530511654704122 \t 3.4011973121493018\n",
      "40     \t [0.67812679 0.33294093 0.84791658]. \t  2.4502589343794883 \t 3.4011973121493018\n",
      "41     \t [0.43534259 0.50348837 0.85250408]. \t  \u001b[92m3.7645623518798303\u001b[0m \t 3.7645623518798303\n",
      "42     \t [0.44162495 0.50161041 0.85316377]. \t  3.757128969603895 \t 3.7645623518798303\n",
      "43     \t [0.43628688 0.50277241 0.85256785]. \t  3.76196468918687 \t 3.7645623518798303\n",
      "44     \t [0.39985451 0.36580088 0.78414505]. \t  2.5941101818239476 \t 3.7645623518798303\n",
      "45     \t [0.47987751 0.54326161 0.85991203]. \t  \u001b[92m3.838086553834019\u001b[0m \t 3.838086553834019\n",
      "46     \t [0.47104801 0.53905031 0.86096578]. \t  3.8341186233877624 \t 3.838086553834019\n",
      "47     \t [0.46558442 0.53654234 0.86152758]. \t  3.8310831457473444 \t 3.838086553834019\n",
      "48     \t [0.         0.89476401 0.38443944]. \t  1.2102525754844757 \t 3.838086553834019\n",
      "49     \t [0.46737972 0.54175737 0.86431212]. \t  3.831266628442375 \t 3.838086553834019\n",
      "50     \t [0.46515577 0.54051747 0.86448907]. \t  3.829992236444263 \t 3.838086553834019\n",
      "51     \t [0.4633171  0.53957331 0.86460506]. \t  3.828997111835539 \t 3.838086553834019\n",
      "52     \t [0.46226711 0.53895094 0.8646059 ]. \t  3.8284496723520083 \t 3.838086553834019\n",
      "53     \t [0.46135206 0.53854788 0.86463374]. \t  3.8280487883581684 \t 3.838086553834019\n",
      "54     \t [0.46071236 0.53834104 0.86462213]. \t  3.8279180651123723 \t 3.838086553834019\n",
      "55     \t [0.45997377 0.53789132 0.86461564]. \t  3.827496752585325 \t 3.838086553834019\n",
      "56     \t [0.45967148 0.53776767 0.86459933]. \t  3.8274235466384554 \t 3.838086553834019\n",
      "57     \t [0.45920825 0.53753748 0.86467683]. \t  3.8270282733617593 \t 3.838086553834019\n",
      "58     \t [0.45904523 0.53743206 0.86459103]. \t  3.8271134819234214 \t 3.838086553834019\n",
      "59     \t [0.45875312 0.53727846 0.86454115]. \t  3.827072949777619 \t 3.838086553834019\n",
      "60     \t [0.45848956 0.53726143 0.86453451]. \t  3.827099952686884 \t 3.838086553834019\n",
      "61     \t [0.45833056 0.5371267  0.86454648]. \t  3.826925420031049 \t 3.838086553834019\n",
      "62     \t [0.45802082 0.53704264 0.86453707]. \t  3.8268801654502056 \t 3.838086553834019\n",
      "63     \t [0.45802517 0.53705885 0.86453407]. \t  3.826906665653879 \t 3.838086553834019\n",
      "64     \t [0.45789756 0.53699222 0.86449533]. \t  3.82692570127349 \t 3.838086553834019\n",
      "65     \t [0.45788252 0.53699995 0.86446586]. \t  3.8270031085580007 \t 3.838086553834019\n",
      "66     \t [0.45768117 0.53689031 0.86445574]. \t  3.826912520175349 \t 3.838086553834019\n",
      "67     \t [0.45760055 0.53695407 0.86439963]. \t  3.8271278825620305 \t 3.838086553834019\n",
      "68     \t [0.45754934 0.53683505 0.86442109]. \t  3.826936106662709 \t 3.838086553834019\n",
      "69     \t [0.45740456 0.53680347 0.86440277]. \t  3.8269547585950474 \t 3.838086553834019\n",
      "70     \t [0.45754692 0.53680826 0.86442801]. \t  3.826886984219956 \t 3.838086553834019\n",
      "71     \t [0.4573622  0.53676724 0.86436465]. \t  3.826998393746406 \t 3.838086553834019\n",
      "72     \t [0.45728196 0.53673594 0.86439781]. \t  3.8268950727080315 \t 3.838086553834019\n",
      "73     \t [0.45717137 0.53683712 0.86437093]. \t  3.827097192358709 \t 3.838086553834019\n",
      "74     \t [0.45729346 0.53687816 0.86432372]. \t  3.8272381575240186 \t 3.838086553834019\n",
      "75     \t [0.45721752 0.53670416 0.86437141]. \t  3.82692102684791 \t 3.838086553834019\n",
      "76     \t [0.45712648 0.53659756 0.86435888]. \t  3.8268234323315506 \t 3.838086553834019\n",
      "77     \t [0.45702245 0.5366373  0.86434549]. \t  3.82691704787745 \t 3.838086553834019\n",
      "78     \t [0.45709608 0.53670551 0.86429489]. \t  3.827106714726386 \t 3.838086553834019\n",
      "79     \t [0.45700511 0.5366477  0.8642973 ]. \t  3.8270387824241046 \t 3.838086553834019\n",
      "80     \t [0.45698804 0.53667405 0.86433743]. \t  3.8269862316481094 \t 3.838086553834019\n",
      "81     \t [0.45694982 0.5366519  0.864328  ]. \t  3.826983444512573 \t 3.838086553834019\n",
      "82     \t [0.45699642 0.53663438 0.86433418]. \t  3.8269415160443874 \t 3.838086553834019\n",
      "83     \t [0.45693574 0.53665934 0.86426422]. \t  3.8271349999795574 \t 3.838086553834019\n",
      "84     \t [0.45697336 0.53671829 0.86426688]. \t  3.8271998533685005 \t 3.838086553834019\n",
      "85     \t [0.45691393 0.5366655  0.86425934]. \t  3.827156285233781 \t 3.838086553834019\n",
      "86     \t [0.45700934 0.53657561 0.86420962]. \t  3.827137785455712 \t 3.838086553834019\n",
      "87     \t [0.45690645 0.53653886 0.86425255]. \t  3.8270094290104 \t 3.838086553834019\n",
      "88     \t [0.45684953 0.53661608 0.86425768]. \t  3.8271046109362024 \t 3.838086553834019\n",
      "89     \t [0.45681785 0.53668259 0.86426882]. \t  3.827169364012499 \t 3.838086553834019\n",
      "90     \t [0.45682565 0.53665078 0.86421543]. \t  3.827244505120496 \t 3.838086553834019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.45671981 0.53661238 0.86423543]. \t  3.8271647360754852 \t 3.838086553834019\n",
      "92     \t [0.45676258 0.53658881 0.86422706]. \t  3.8271474380928656 \t 3.838086553834019\n",
      "93     \t [0.45679266 0.53658014 0.86425995]. \t  3.8270605331663257 \t 3.838086553834019\n",
      "94     \t [0.45678198 0.53656133 0.86427506]. \t  3.8270045017553223 \t 3.838086553834019\n",
      "95     \t [0.45680239 0.53651044 0.86424637]. \t  3.8269992614782566 \t 3.838086553834019\n",
      "96     \t [0.45677753 0.53652518 0.86421962]. \t  3.8270799055171536 \t 3.838086553834019\n",
      "97     \t [0.45682861 0.53649152 0.86425329]. \t  3.8269563985036315 \t 3.838086553834019\n",
      "98     \t [0.45669388 0.53645454 0.86427577]. \t  3.8268759791815175 \t 3.838086553834019\n",
      "99     \t [0.45677292 0.5366207  0.86420184]. \t  3.827242176744384 \t 3.838086553834019\n",
      "100    \t [0.45690674 0.53663978 0.86417821]. \t  3.8273014128113303 \t 3.838086553834019\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0.07913247 1.         1.        ]. \t  0.33219768312887366 \t 1.540625560354162\n",
      "2      \t [0.68730556 0.         1.        ]. \t  0.09097719266474015 \t 1.540625560354162\n",
      "3      \t [0.05658884 1.         0.11164268]. \t  0.007165204956116788 \t 1.540625560354162\n",
      "4      \t [0.62238326 1.         0.92454933]. \t  0.5249052215695602 \t 1.540625560354162\n",
      "5      \t [0.27335469 0.         0.60345865]. \t  0.12236823544553098 \t 1.540625560354162\n",
      "6      \t [0.3798089  0.60020245 0.63914556]. \t  \u001b[92m1.9369853688894156\u001b[0m \t 1.9369853688894156\n",
      "7      \t [0.01455295 0.86107983 0.56376885]. \t  \u001b[92m3.0096553916835234\u001b[0m \t 3.0096553916835234\n",
      "8      \t [1.         0.         0.51721981]. \t  0.0685525096394875 \t 3.0096553916835234\n",
      "9      \t [0.         1.         0.55500699]. \t  2.490661511904847 \t 3.0096553916835234\n",
      "10     \t [0.         0.57918877 0.79270515]. \t  \u001b[92m3.4919007970873177\u001b[0m \t 3.4919007970873177\n",
      "11     \t [0.        0.2966819 1.       ]. \t  1.004649182314381 \t 3.4919007970873177\n",
      "12     \t [0.76524461 0.         0.        ]. \t  0.06382341046302777 \t 3.4919007970873177\n",
      "13     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.4919007970873177\n",
      "14     \t [0.         0.47161876 0.61204222]. \t  1.3601646669845835 \t 3.4919007970873177\n",
      "15     \t [ 0.00000000e+00 -1.11022302e-16  0.00000000e+00]. \t  0.06797411659013226 \t 3.4919007970873177\n",
      "16     \t [0.72234904 0.60604916 1.        ]. \t  2.0332213892612696 \t 3.4919007970873177\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.4919007970873177\n",
      "18     \t [0.18955634 0.61925214 1.        ]. \t  2.0274284684057644 \t 3.4919007970873177\n",
      "19     \t [0.26615506 0.51476512 0.        ]. \t  0.023421782806436465 \t 3.4919007970873177\n",
      "20     \t [0.         0.73161943 1.        ]. \t  1.5816970342989176 \t 3.4919007970873177\n",
      "21     \t [1.         0.31698102 1.        ]. \t  1.0892169873735658 \t 3.4919007970873177\n",
      "22     \t [0.2118618  1.         0.73618833]. \t  1.2176784727158412 \t 3.4919007970873177\n",
      "23     \t [0.63291074 1.         0.        ]. \t  0.00014898295574489886 \t 3.4919007970873177\n",
      "24     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.4919007970873177\n",
      "25     \t [1.         0.         0.13707305]. \t  0.1587354216029019 \t 3.4919007970873177\n",
      "26     \t [0.37410881 0.         0.16741546]. \t  0.646451862733445 \t 3.4919007970873177\n",
      "27     \t [0.85705766 1.         0.26704488]. \t  0.04005902188937761 \t 3.4919007970873177\n",
      "28     \t [0.         0.90421072 0.80513089]. \t  1.4161943080047703 \t 3.4919007970873177\n",
      "29     \t [0.         0.40372263 0.05533474]. \t  0.07608774145533398 \t 3.4919007970873177\n",
      "30     \t [1.         0.63874178 0.89306679]. \t  3.319293307566799 \t 3.4919007970873177\n",
      "31     \t [0.77517791 0.         0.76549421]. \t  0.243767218197464 \t 3.4919007970873177\n",
      "32     \t [1.         0.48538836 0.77080304]. \t  2.9688875128108845 \t 3.4919007970873177\n",
      "33     \t [0.14893665 1.         0.32030057]. \t  0.4895043499606671 \t 3.4919007970873177\n",
      "34     \t [0.         0.         0.41457308]. \t  0.3068714331007062 \t 3.4919007970873177\n",
      "35     \t [0.50490481 0.22757237 0.        ]. \t  0.09815532427836018 \t 3.4919007970873177\n",
      "36     \t [1.         0.68989449 1.        ]. \t  1.7326409227336774 \t 3.4919007970873177\n",
      "37     \t [0.         0.80746675 0.22218157]. \t  0.10586129746160332 \t 3.4919007970873177\n",
      "38     \t [0.3819549 0.8521873 1.       ]. \t  0.94341817363282 \t 3.4919007970873177\n",
      "39     \t [0.86249454 0.57648503 0.83395463]. \t  \u001b[92m3.6479593748204526\u001b[0m \t 3.6479593748204526\n",
      "40     \t [0.86337273 0.69594657 0.85030777]. \t  3.0470478358035886 \t 3.6479593748204526\n",
      "41     \t [0.88714119 0.49729687 0.84424314]. \t  3.6184784722276753 \t 3.6479593748204526\n",
      "42     \t [0.92529661 0.53805603 0.86304131]. \t  \u001b[92m3.696763276432456\u001b[0m \t 3.696763276432456\n",
      "43     \t [0.76434497 0.54106352 0.79527337]. \t  3.420969491077022 \t 3.696763276432456\n",
      "44     \t [0.91088315 0.53631645 0.85271642]. \t  \u001b[92m3.704535050441831\u001b[0m \t 3.704535050441831\n",
      "45     \t [0.91023533 0.53825229 0.85155501]. \t  \u001b[92m3.7054302692126053\u001b[0m \t 3.7054302692126053\n",
      "46     \t [0.90974686 0.53944089 0.85076275]. \t  \u001b[92m3.7056323019660087\u001b[0m \t 3.7056323019660087\n",
      "47     \t [0.9094309  0.54026342 0.85022648]. \t  3.705631368532729 \t 3.7056323019660087\n",
      "48     \t [0.90915376 0.54083242 0.84989707]. \t  \u001b[92m3.705644078224176\u001b[0m \t 3.705644078224176\n",
      "49     \t [0.90894987 0.54128952 0.84958437]. \t  3.7055418535984783 \t 3.705644078224176\n",
      "50     \t [0.90878979 0.54164894 0.84934987]. \t  3.70545150142765 \t 3.705644078224176\n",
      "51     \t [1.         0.53798405 0.86736756]. \t  3.6556375253249365 \t 3.705644078224176\n",
      "52     \t [0.91674845 0.54288157 0.85187417]. \t  \u001b[92m3.7057310491412676\u001b[0m \t 3.7057310491412676\n",
      "53     \t [0.91665886 0.54299346 0.85166011]. \t  3.7056008215047442 \t 3.7057310491412676\n",
      "54     \t [0.91651361 0.54312476 0.85155606]. \t  3.705610075101031 \t 3.7057310491412676\n",
      "55     \t [0.91642359 0.54316673 0.8513949 ]. \t  3.7054925996876653 \t 3.7057310491412676\n",
      "56     \t [0.91628526 0.54324934 0.85133716]. \t  3.7055227630000283 \t 3.7057310491412676\n",
      "57     \t [0.91625273 0.54335116 0.85119759]. \t  3.705421157736442 \t 3.7057310491412676\n",
      "58     \t [0.91609029 0.54340836 0.85106051]. \t  3.7053560918766992 \t 3.7057310491412676\n",
      "59     \t [0.91615048 0.5434563  0.8510252 ]. \t  3.7053074986420196 \t 3.7057310491412676\n",
      "60     \t [0.91605303 0.54355971 0.85098752]. \t  3.7053451250380425 \t 3.7057310491412676\n",
      "61     \t [0.91601294 0.54352129 0.85090767]. \t  3.7052526091215023 \t 3.7057310491412676\n",
      "62     \t [0.91610774 0.54351218 0.85092952]. \t  3.705234264786445 \t 3.7057310491412676\n",
      "63     \t [0.91596356 0.54359266 0.85086216]. \t  3.70524633351676 \t 3.7057310491412676\n",
      "64     \t [0.91592277 0.54361574 0.85075485]. \t  3.705141703787968 \t 3.7057310491412676\n",
      "65     \t [0.91580868 0.5436018  0.85074706]. \t  3.7051762100748125 \t 3.7057310491412676\n",
      "66     \t [0.91590262 0.54361946 0.85072107]. \t  3.705110087920481 \t 3.7057310491412676\n",
      "67     \t [0.91586371 0.54366713 0.85070094]. \t  3.705119705995316 \t 3.7057310491412676\n",
      "68     \t [0.91578739 0.54365682 0.85060855]. \t  3.7050331759975696 \t 3.7057310491412676\n",
      "69     \t [0.91582376 0.5436525  0.85062681]. \t  3.7050388592923675 \t 3.7057310491412676\n",
      "70     \t [0.9158539  0.5437442  0.85059272]. \t  3.705016478772426 \t 3.7057310491412676\n",
      "71     \t [0.91591382 0.54372936 0.85065103]. \t  3.7050586197554494 \t 3.7057310491412676\n",
      "72     \t [0.91578551 0.54372811 0.85057347]. \t  3.7050157910474817 \t 3.7057310491412676\n",
      "73     \t [0.91583421 0.54380922 0.8506013 ]. \t  3.7050593762312096 \t 3.7057310491412676\n",
      "74     \t [0.91577281 0.54378861 0.85053631]. \t  3.704995869465447 \t 3.7057310491412676\n",
      "75     \t [0.91567548 0.54370517 0.85049862]. \t  3.7049594198861304 \t 3.7057310491412676\n",
      "76     \t [0.91572751 0.54382121 0.85052092]. \t  3.70500747484509 \t 3.7057310491412676\n",
      "77     \t [0.91573219 0.54374658 0.85048364]. \t  3.704930684094392 \t 3.7057310491412676\n",
      "78     \t [0.91578676 0.54379219 0.85048396]. \t  3.7049239199509634 \t 3.7057310491412676\n",
      "79     \t [0.91565279 0.54377008 0.85046439]. \t  3.704948691706579 \t 3.7057310491412676\n",
      "80     \t [0.91574755 0.54378921 0.85046637]. \t  3.704917118204702 \t 3.7057310491412676\n",
      "81     \t [0.91571249 0.54381135 0.85046963]. \t  3.704944438255091 \t 3.7057310491412676\n",
      "82     \t [0.91560964 0.54381243 0.850423  ]. \t  3.70492886752105 \t 3.7057310491412676\n",
      "83     \t [0.91574376 0.54378066 0.85047231]. \t  3.7049233645895665 \t 3.7057310491412676\n",
      "84     \t [0.91574903 0.54385687 0.85048529]. \t  3.7049650473993725 \t 3.7057310491412676\n",
      "85     \t [0.91568494 0.54379596 0.85042912]. \t  3.7048983476074673 \t 3.7057310491412676\n",
      "86     \t [0.91565793 0.54382916 0.85045608]. \t  3.7049568756851126 \t 3.7057310491412676\n",
      "87     \t [0.91568476 0.54393196 0.85042742]. \t  3.704944280857404 \t 3.7057310491412676\n",
      "88     \t [0.91568973 0.54384486 0.85040202]. \t  3.704878387807582 \t 3.7057310491412676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.91574923 0.54385227 0.85039232]. \t  3.7048425817093635 \t 3.7057310491412676\n",
      "90     \t [0.91564974 0.54393544 0.85036537]. \t  3.7048794636480817 \t 3.7057310491412676\n",
      "91     \t [0.91564147 0.54387569 0.85039636]. \t  3.7049027632530676 \t 3.7057310491412676\n",
      "92     \t [0.91556814 0.54386553 0.85038067]. \t  3.7049103878768355 \t 3.7057310491412676\n",
      "93     \t [0.91571935 0.54386371 0.85040722]. \t  3.7048790347568348 \t 3.7057310491412676\n",
      "94     \t [0.91559408 0.54389374 0.85031375]. \t  3.704820876388226 \t 3.7057310491412676\n",
      "95     \t [0.91571919 0.54384786 0.85035737]. \t  3.7048081770469867 \t 3.7057310491412676\n",
      "96     \t [0.91572908 0.54384298 0.85039208]. \t  3.704847696590552 \t 3.7057310491412676\n",
      "97     \t [0.91565853 0.54394081 0.85036275]. \t  3.7048740739725208 \t 3.7057310491412676\n",
      "98     \t [0.91567291 0.54384061 0.85037904]. \t  3.7048540886700456 \t 3.7057310491412676\n",
      "99     \t [0.91573969 0.54391412 0.85038647]. \t  3.7048607952733557 \t 3.7057310491412676\n",
      "100    \t [0.91572002 0.54380805 0.85041078]. \t  3.7048636254711025 \t 3.7057310491412676\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.04419337 0.         1.        ]. \t  0.09059079717419466 \t 3.8084053754826726\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.8084053754826726\n",
      "3      \t [0.         0.3148688  0.24628265]. \t  0.4510537954108189 \t 3.8084053754826726\n",
      "4      \t [0.         0.50217815 1.        ]. \t  1.973150664835381 \t 3.8084053754826726\n",
      "5      \t [0.40635646 0.16302212 0.6604343 ]. \t  0.5560774145124691 \t 3.8084053754826726\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "7      \t [0.89791876 0.90462651 0.87968685]. \t  1.1378620907320154 \t 3.8084053754826726\n",
      "8      \t [0.         0.76613592 0.57757679]. \t  2.7724500514481316 \t 3.8084053754826726\n",
      "9      \t [1.         0.50339632 0.29970315]. \t  0.07699739498303583 \t 3.8084053754826726\n",
      "10     \t [0.42200288 0.88944826 0.74707193]. \t  1.5141392566763687 \t 3.8084053754826726\n",
      "11     \t [1.         0.56442159 1.        ]. \t  2.0086737823856975 \t 3.8084053754826726\n",
      "12     \t [0.52886699 0.43538119 0.        ]. \t  0.03942850234162349 \t 3.8084053754826726\n",
      "13     \t [0.30060657 0.5270571  0.5028818 ]. \t  0.940369793369894 \t 3.8084053754826726\n",
      "14     \t [0.94213603 1.         0.        ]. \t  4.919141654532678e-05 \t 3.8084053754826726\n",
      "15     \t [0.        0.        0.5854281]. \t  0.10318711884481228 \t 3.8084053754826726\n",
      "16     \t [0.5888379  0.70551738 1.        ]. \t  1.7240135889108943 \t 3.8084053754826726\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "18     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "19     \t [1.         0.         0.55830465]. \t  0.07277051915406726 \t 3.8084053754826726\n",
      "20     \t [0.38186915 0.32413454 1.        ]. \t  1.1704210534346395 \t 3.8084053754826726\n",
      "21     \t [1.         0.7203769  0.71520547]. \t  1.54164527689387 \t 3.8084053754826726\n",
      "22     \t [0.59547341 1.         0.        ]. \t  0.00016439955243124086 \t 3.8084053754826726\n",
      "23     \t [0.         0.61676345 0.        ]. \t  0.006560285319143317 \t 3.8084053754826726\n",
      "24     \t [0.18408722 0.7651818  0.99690409]. \t  1.4573655061417445 \t 3.8084053754826726\n",
      "25     \t [0.         1.         0.62798778]. \t  2.1817073717747935 \t 3.8084053754826726\n",
      "26     \t [0.83066131 0.65508492 0.        ]. \t  0.0034569828281253196 \t 3.8084053754826726\n",
      "27     \t [0.         0.76179685 0.83700524]. \t  2.607330371341866 \t 3.8084053754826726\n",
      "28     \t [0.65721929 0.         0.        ]. \t  0.07967877859774529 \t 3.8084053754826726\n",
      "29     \t [0.         0.25571787 0.8147724 ]. \t  1.6956792936601939 \t 3.8084053754826726\n",
      "30     \t [0.79654997 1.         0.35330492]. \t  0.18372387727449108 \t 3.8084053754826726\n",
      "31     \t [0.         1.         0.28121394]. \t  0.26169501334991585 \t 3.8084053754826726\n",
      "32     \t [0.73949093 0.65645102 0.75340598]. \t  2.558066175105763 \t 3.8084053754826726\n",
      "33     \t [1.         0.75677986 1.        ]. \t  1.4106922528731751 \t 3.8084053754826726\n",
      "34     \t [0.16152265 0.35653034 0.92481515]. \t  2.234003069104817 \t 3.8084053754826726\n",
      "35     \t [0.66884172 0.         0.76891063]. \t  0.24649519283562904 \t 3.8084053754826726\n",
      "36     \t [0.         0.53546965 0.71031397]. \t  2.5641018410759004 \t 3.8084053754826726\n",
      "37     \t [1.         1.         0.34679662]. \t  0.06487906617633497 \t 3.8084053754826726\n",
      "38     \t [0.79163928 0.54476166 0.91975863]. \t  3.3515705955411157 \t 3.8084053754826726\n",
      "39     \t [0.46442328 0.53761152 0.86175765]. \t  \u001b[92m3.8321689322192767\u001b[0m \t 3.8321689322192767\n",
      "40     \t [0.18947819 0.67422256 0.7848296 ]. \t  3.1167002138761357 \t 3.8321689322192767\n",
      "41     \t [0.49959462 0.52065066 0.86504163]. \t  3.790162532496516 \t 3.8321689322192767\n",
      "42     \t [0.43547226 0.55066169 0.86899895]. \t  3.828568057456179 \t 3.8321689322192767\n",
      "43     \t [0.44120306 0.54721526 0.86886185]. \t  3.826764629803803 \t 3.8321689322192767\n",
      "44     \t [0.58420823 0.49765299 0.8721631 ]. \t  3.680157807754341 \t 3.8321689322192767\n",
      "45     \t [0.39834434 0.5585162  0.8692847 ]. \t  3.831756097403568 \t 3.8321689322192767\n",
      "46     \t [0.40432214 0.5560093  0.86900767]. \t  \u001b[92m3.832392561905847\u001b[0m \t 3.832392561905847\n",
      "47     \t [0.4097944  0.55401158 0.86887541]. \t  3.8322312676470243 \t 3.832392561905847\n",
      "48     \t [0.41443501 0.55265772 0.86879248]. \t  3.8318465599135827 \t 3.832392561905847\n",
      "49     \t [0.41860701 0.55139284 0.86873381]. \t  3.831316433787886 \t 3.832392561905847\n",
      "50     \t [0.42197579 0.55039725 0.86868774]. \t  3.830803877227419 \t 3.832392561905847\n",
      "51     \t [0.42477544 0.5497061  0.86873147]. \t  3.830128245908276 \t 3.832392561905847\n",
      "52     \t [0.42712727 0.54911042 0.86869119]. \t  3.8297573912827385 \t 3.832392561905847\n",
      "53     \t [0.42860862 0.54862955 0.86871708]. \t  3.8293026347205386 \t 3.832392561905847\n",
      "54     \t [0.42793831 0.54872389 0.86873181]. \t  3.8293714902352987 \t 3.832392561905847\n",
      "55     \t [0.50258503 0.53276612 0.86932158]. \t  3.801623831046078 \t 3.832392561905847\n",
      "56     \t [0.40053495 0.55310764 0.8687736 ]. \t  \u001b[92m3.833179146964964\u001b[0m \t 3.833179146964964\n",
      "57     \t [0.4069678  0.55201023 0.86873129]. \t  3.8325368935762194 \t 3.833179146964964\n",
      "58     \t [0.41264395 0.55102122 0.86878494]. \t  3.83159126512862 \t 3.833179146964964\n",
      "59     \t [0.41699618 0.55034927 0.86866651]. \t  3.831321613339922 \t 3.833179146964964\n",
      "60     \t [0.42067022 0.54969056 0.86873516]. \t  3.8305087346527227 \t 3.833179146964964\n",
      "61     \t [0.42487157 0.5488561  0.8687739 ]. \t  3.8296121165995367 \t 3.833179146964964\n",
      "62     \t [0.42762887 0.54846781 0.86876285]. \t  3.829182394666384 \t 3.833179146964964\n",
      "63     \t [0.49526851 0.53484264 0.86946848]. \t  3.8053710110089796 \t 3.833179146964964\n",
      "64     \t [0.40303068 0.55206504 0.8686729 ]. \t  3.8330544643204725 \t 3.833179146964964\n",
      "65     \t [0.40808686 0.55137584 0.86867256]. \t  3.832441617638601 \t 3.833179146964964\n",
      "66     \t [0.41098964 0.55098151 0.86872456]. \t  3.831908046322643 \t 3.833179146964964\n",
      "67     \t [0.41539637 0.55018814 0.86864109]. \t  3.831484464489408 \t 3.833179146964964\n",
      "68     \t [0.41909628 0.54967593 0.86879861]. \t  3.830457897657908 \t 3.833179146964964\n",
      "69     \t [0.42329061 0.54892673 0.8687413 ]. \t  3.829899284391482 \t 3.833179146964964\n",
      "70     \t [0.42412597 0.54870083 0.86879802]. \t  3.8295369223574665 \t 3.833179146964964\n",
      "71     \t [0.42290765 0.54900092 0.86876077]. \t  3.8299117775423497 \t 3.833179146964964\n",
      "72     \t [0.42592898 0.54852448 0.86879055]. \t  3.829295754160068 \t 3.833179146964964\n",
      "73     \t [0.4277881  0.5481218  0.86876796]. \t  3.8289733816832268 \t 3.833179146964964\n",
      "74     \t [0.42803155 0.54814373 0.86885014]. \t  3.828708971545046 \t 3.833179146964964\n",
      "75     \t [0.42394095 0.54873588 0.86880286]. \t  3.8295571704715794 \t 3.833179146964964\n",
      "76     \t [0.49050823 0.53630097 0.86961837]. \t  3.8075996791912687 \t 3.833179146964964\n",
      "77     \t [0.40520116 0.55150508 0.86865966]. \t  3.832761926404051 \t 3.833179146964964\n",
      "78     \t [0.40810804 0.55111439 0.86869571]. \t  3.8322877889795066 \t 3.833179146964964\n",
      "79     \t [0.41220592 0.55045786 0.86868297]. \t  3.8317449593257797 \t 3.833179146964964\n",
      "80     \t [0.41513023 0.55007612 0.86870799]. \t  3.8312623347644346 \t 3.833179146964964\n",
      "81     \t [0.41653186 0.54976756 0.86876114]. \t  3.830848257533648 \t 3.833179146964964\n",
      "82     \t [0.4196155  0.54926649 0.86876439]. \t  3.830336031957329 \t 3.833179146964964\n",
      "83     \t [0.42050713 0.549237   0.86873632]. \t  3.830324181572887 \t 3.833179146964964\n",
      "84     \t [0.42323197 0.54876012 0.86876294]. \t  3.8297596909043294 \t 3.833179146964964\n",
      "85     \t [0.42189133 0.54892113 0.86877519]. \t  3.8299282440226907 \t 3.833179146964964\n",
      "86     \t [0.42231314 0.54893507 0.86885845]. \t  3.829639391223834 \t 3.833179146964964\n",
      "87     \t [0.42400582 0.54862282 0.86875139]. \t  3.8296528769839866 \t 3.833179146964964\n",
      "88     \t [0.42353759 0.54849967 0.86878581]. \t  3.8295322494141564 \t 3.833179146964964\n",
      "89     \t [0.42714065 0.54814252 0.86880926]. \t  3.8289228600964957 \t 3.833179146964964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.42668391 0.54817531 0.86885445]. \t  3.8288470257470584 \t 3.833179146964964\n",
      "91     \t [0.42428682 0.54849373 0.86879846]. \t  3.8294176820909804 \t 3.833179146964964\n",
      "92     \t [0.42611291 0.54826802 0.86881283]. \t  3.829079398195193 \t 3.833179146964964\n",
      "93     \t [0.42421842 0.54870246 0.86879364]. \t  3.829542069033038 \t 3.833179146964964\n",
      "94     \t [0.4253127  0.54829262 0.86881557]. \t  3.8291626800001852 \t 3.833179146964964\n",
      "95     \t [0.42841827 0.54798205 0.86878269]. \t  3.828790883483731 \t 3.833179146964964\n",
      "96     \t [0.42400595 0.54859439 0.86882451]. \t  3.8294153752900564 \t 3.833179146964964\n",
      "97     \t [0.42420455 0.54844457 0.86873849]. \t  3.82958413420741 \t 3.833179146964964\n",
      "98     \t [0.48080388 0.53841374 0.86945515]. \t  3.8121623915054115 \t 3.833179146964964\n",
      "99     \t [0.40823215 0.55082619 0.8687617 ]. \t  3.8319803854229004 \t 3.833179146964964\n",
      "100    \t [0.41050974 0.55046229 0.86869871]. \t  3.831847376301763 \t 3.833179146964964\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.         0.09733471 1.        ]. \t  0.23944474729704915 \t 3.1179188940604616\n",
      "2      \t [1.         0.94389885 1.        ]. \t  0.4961646173755303 \t 3.1179188940604616\n",
      "3      \t [0.85534446 0.10745361 1.        ]. \t  0.2608122520788004 \t 3.1179188940604616\n",
      "4      \t [0.00185123 0.77957555 1.        ]. \t  1.3257371358537284 \t 3.1179188940604616\n",
      "5      \t [0.47323635 0.7556654  1.        ]. \t  1.4741211951397921 \t 3.1179188940604616\n",
      "6      \t [1.         0.1170886  0.41488242]. \t  0.16692702985106284 \t 3.1179188940604616\n",
      "7      \t [0.         0.54344276 0.53756459]. \t  1.2540745406991987 \t 3.1179188940604616\n",
      "8      \t [0.44646759 0.13630996 0.56906916]. \t  0.24674912416572378 \t 3.1179188940604616\n",
      "9      \t [1.         0.59646324 0.65849829]. \t  1.3119808142783622 \t 3.1179188940604616\n",
      "10     \t [0.38027753 0.60561048 0.52197748]. \t  1.3392529594012552 \t 3.1179188940604616\n",
      "11     \t [0.91429022 0.         0.        ]. \t  0.04188951673078877 \t 3.1179188940604616\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.1179188940604616\n",
      "13     \t [0.86035479 1.         0.67595732]. \t  0.45893307685215945 \t 3.1179188940604616\n",
      "14     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1179188940604616\n",
      "15     \t [0.86774071 0.49067998 0.94622218]. \t  2.819197270142811 \t 3.1179188940604616\n",
      "16     \t [0.         0.48027557 0.86533426]. \t  \u001b[92m3.59726552477002\u001b[0m \t 3.59726552477002\n",
      "17     \t [0.49453935 1.         0.        ]. \t  0.0002062352302237528 \t 3.59726552477002\n",
      "18     \t [0.73571777 0.39872695 0.7429244 ]. \t  2.3765287897582565 \t 3.59726552477002\n",
      "19     \t [0.50583294 0.         0.        ]. \t  0.09665380941362732 \t 3.59726552477002\n",
      "20     \t [0.         0.28245743 0.76638788]. \t  1.7682838612169216 \t 3.59726552477002\n",
      "21     \t [0.30502237 0.42395469 0.93775669]. \t  2.633726201305368 \t 3.59726552477002\n",
      "22     \t [0.         0.44471039 1.        ]. \t  1.774711751854404 \t 3.59726552477002\n",
      "23     \t [0.         0.99232633 0.65416985]. \t  1.995377043552126 \t 3.59726552477002\n",
      "24     \t [0.         0.59237121 0.        ]. \t  0.008264397724795414 \t 3.59726552477002\n",
      "25     \t [1.        0.3269051 0.       ]. \t  0.02284823154157678 \t 3.59726552477002\n",
      "26     \t [0.37817806 0.         0.91653983]. \t  0.17762938317741067 \t 3.59726552477002\n",
      "27     \t [1.         0.41986007 1.        ]. \t  1.6239713530773774 \t 3.59726552477002\n",
      "28     \t [0.12220347 0.74482048 0.77430652]. \t  2.6250193289463235 \t 3.59726552477002\n",
      "29     \t [0.61982786 0.34005468 1.        ]. \t  1.2523512343598826 \t 3.59726552477002\n",
      "30     \t [0.67189205 1.         0.91624606]. \t  0.5365924095568178 \t 3.59726552477002\n",
      "31     \t [1.         0.         0.77022092]. \t  0.24028078285900611 \t 3.59726552477002\n",
      "32     \t [1.        0.6974936 1.       ]. \t  1.7001845777779883 \t 3.59726552477002\n",
      "33     \t [0.         0.68078645 0.80443658]. \t  3.160993642357898 \t 3.59726552477002\n",
      "34     \t [0.31945499 1.         0.32549779]. \t  0.463064570900234 \t 3.59726552477002\n",
      "35     \t [0.         0.26581507 0.        ]. \t  0.06246892403295176 \t 3.59726552477002\n",
      "36     \t [0.         1.         0.83510951]. \t  0.7469010692003779 \t 3.59726552477002\n",
      "37     \t [0.77398533 0.71893599 0.8077925 ]. \t  2.667521791215144 \t 3.59726552477002\n",
      "38     \t [0.1953467  0.45647725 0.7806111 ]. \t  3.178972753983364 \t 3.59726552477002\n",
      "39     \t [0.1310997  0.57432574 0.87282265]. \t  \u001b[92m3.7950175848491607\u001b[0m \t 3.7950175848491607\n",
      "40     \t [0.         0.         0.80575486]. \t  0.2467875594675885 \t 3.7950175848491607\n",
      "41     \t [0.         0.57757272 0.83379585]. \t  3.7596349721224027 \t 3.7950175848491607\n",
      "42     \t [0.10174121 0.55369567 0.84438352]. \t  \u001b[92m3.8339364108984793\u001b[0m \t 3.8339364108984793\n",
      "43     \t [0.10073113 0.55367104 0.84412245]. \t  3.833310966828081 \t 3.8339364108984793\n",
      "44     \t [0.09987799 0.55372345 0.84391313]. \t  3.832795451052879 \t 3.8339364108984793\n",
      "45     \t [0.09897776 0.55375448 0.84372845]. \t  3.8322978489119195 \t 3.8339364108984793\n",
      "46     \t [0.         0.57511888 0.83423319]. \t  3.7647800458933824 \t 3.8339364108984793\n",
      "47     \t [0.09441385 0.55394869 0.8429922 ]. \t  3.830015427714777 \t 3.8339364108984793\n",
      "48     \t [0.09356202 0.55400711 0.8428353 ]. \t  3.8295417190932404 \t 3.8339364108984793\n",
      "49     \t [0.09296335 0.55419704 0.84280546]. \t  3.8293556782678824 \t 3.8339364108984793\n",
      "50     \t [0.09242499 0.5541282  0.84266828]. \t  3.828979024505492 \t 3.8339364108984793\n",
      "51     \t [0.09202173 0.55421696 0.84261997]. \t  3.828799168410552 \t 3.8339364108984793\n",
      "52     \t [0.09162907 0.55426962 0.84260838]. \t  3.8286862263442396 \t 3.8339364108984793\n",
      "53     \t [0.09125126 0.55431465 0.84250923]. \t  3.8284163406916774 \t 3.8339364108984793\n",
      "54     \t [0.09098307 0.55438635 0.84256574]. \t  3.828457380183411 \t 3.8339364108984793\n",
      "55     \t [0.09073709 0.5543957  0.84252107]. \t  3.828316894022019 \t 3.8339364108984793\n",
      "56     \t [0.09052389 0.55443218 0.84246263]. \t  3.8281591140430673 \t 3.8339364108984793\n",
      "57     \t [0.09030973 0.55445726 0.8424898 ]. \t  3.8281581033582746 \t 3.8339364108984793\n",
      "58     \t [0.09008919 0.55451596 0.84248845]. \t  3.8281035811960766 \t 3.8339364108984793\n",
      "59     \t [0.0900198  0.55450654 0.84244766]. \t  3.8280115314830914 \t 3.8339364108984793\n",
      "60     \t [0.08979956 0.55453394 0.84244444]. \t  3.827952875118667 \t 3.8339364108984793\n",
      "61     \t [0.08962247 0.55459037 0.84242491]. \t  3.8278747304324923 \t 3.8339364108984793\n",
      "62     \t [0.08949515 0.55460976 0.84238709]. \t  3.8277740274542627 \t 3.8339364108984793\n",
      "63     \t [0.08945877 0.55460039 0.84240034]. \t  3.827789722950979 \t 3.8339364108984793\n",
      "64     \t [0.08936043 0.55464675 0.8424575 ]. \t  3.827872093255994 \t 3.8339364108984793\n",
      "65     \t [0.08920549 0.55463837 0.84240299]. \t  3.827733660457738 \t 3.8339364108984793\n",
      "66     \t [0.08914607 0.55465731 0.84235999]. \t  3.8276394790264403 \t 3.8339364108984793\n",
      "67     \t [0.08924532 0.5546888  0.84238771]. \t  3.8277152588320726 \t 3.8339364108984793\n",
      "68     \t [0.08900469 0.55466772 0.84235941]. \t  3.8276041826621245 \t 3.8339364108984793\n",
      "69     \t [0.08887656 0.5547219  0.84230192]. \t  3.8274658898393925 \t 3.8339364108984793\n",
      "70     \t [0.08902686 0.55470298 0.84236288]. \t  3.827616176729145 \t 3.8339364108984793\n",
      "71     \t [0.08887253 0.55471688 0.84233726]. \t  3.8275309916093168 \t 3.8339364108984793\n",
      "72     \t [0.08883371 0.55475087 0.84234169]. \t  3.8275298918656233 \t 3.8339364108984793\n",
      "73     \t [0.08878725 0.5547128  0.84231873]. \t  3.827475644063835 \t 3.8339364108984793\n",
      "74     \t [0.08870815 0.55474569 0.84226838]. \t  3.8273620545146056 \t 3.8339364108984793\n",
      "75     \t [0.08865496 0.5547359  0.84233117]. \t  3.8274668132673106 \t 3.8339364108984793\n",
      "76     \t [0.08863626 0.55475479 0.84228852]. \t  3.827382417746033 \t 3.8339364108984793\n",
      "77     \t [0.08858213 0.55475352 0.84231762]. \t  3.8274237882405453 \t 3.8339364108984793\n",
      "78     \t [0.0885254  0.5547468  0.84233703]. \t  3.827446269756836 \t 3.8339364108984793\n",
      "79     \t [0.08849478 0.5548077  0.84239797]. \t  3.827552193278528 \t 3.8339364108984793\n",
      "80     \t [0.08849742 0.55473037 0.84227849]. \t  3.8273298346809037 \t 3.8339364108984793\n",
      "81     \t [0.08841717 0.55478563 0.84231968]. \t  3.827387472836655 \t 3.8339364108984793\n",
      "82     \t [0.08839651 0.55479825 0.84233942]. \t  3.8274193032780777 \t 3.8339364108984793\n",
      "83     \t [0.08848129 0.55473955 0.84239313]. \t  3.8275400126678587 \t 3.8339364108984793\n",
      "84     \t [0.08849459 0.55484001 0.84228161]. \t  3.8273347095532677 \t 3.8339364108984793\n",
      "85     \t [0.08829888 0.55482604 0.84231228]. \t  3.827344685704671 \t 3.8339364108984793\n",
      "86     \t [0.08849084 0.55477695 0.8423165 ]. \t  3.8273994722042226 \t 3.8339364108984793\n",
      "87     \t [0.08852664 0.55480403 0.84228927]. \t  3.8273570685145373 \t 3.8339364108984793\n",
      "88     \t [0.08845159 0.55484432 0.84234187]. \t  3.827437057096101 \t 3.8339364108984793\n",
      "89     \t [0.08827074 0.55485758 0.84227671]. \t  3.8272709011927546 \t 3.8339364108984793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.08832967 0.55481253 0.84234432]. \t  3.8274121374719368 \t 3.8339364108984793\n",
      "91     \t [0.08835229 0.55492281 0.84233581]. \t  3.827400859886634 \t 3.8339364108984793\n",
      "92     \t [0.08824753 0.55484922 0.84238957]. \t  3.8274762033348977 \t 3.8339364108984793\n",
      "93     \t [0.08827405 0.55491777 0.8423155 ]. \t  3.8273438831659075 \t 3.8339364108984793\n",
      "94     \t [0.08822601 0.55481422 0.8423348 ]. \t  3.827369106922003 \t 3.8339364108984793\n",
      "95     \t [0.08811907 0.55485902 0.84230551]. \t  3.827287995254687 \t 3.8339364108984793\n",
      "96     \t [0.08822005 0.5548398  0.84232715]. \t  3.8273532250530664 \t 3.8339364108984793\n",
      "97     \t [0.08810053 0.55483717 0.8423086 ]. \t  3.827289412126921 \t 3.8339364108984793\n",
      "98     \t [0.08818243 0.55483722 0.84237183]. \t  3.8274274241016113 \t 3.8339364108984793\n",
      "99     \t [0.08824683 0.55488604 0.84233582]. \t  3.827375594564052 \t 3.8339364108984793\n",
      "100    \t [0.08818167 0.55484807 0.84225109]. \t  3.827201084938117 \t 3.8339364108984793\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0.05220471 1.         0.        ]. \t  0.00028352878334813653 \t 1.1210522139432408\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1210522139432408\n",
      "3      \t [0.74856642 1.         0.16109072]. \t  0.007368202388762191 \t 1.1210522139432408\n",
      "4      \t [0.56797341 0.75051254 1.        ]. \t  \u001b[92m1.4972449202582547\u001b[0m \t 1.4972449202582547\n",
      "5      \t [0.45381459 1.         0.82333968]. \t  0.7086038980193539 \t 1.4972449202582547\n",
      "6      \t [1.         0.56294332 1.        ]. \t  \u001b[92m2.008466631651834\u001b[0m \t 2.008466631651834\n",
      "7      \t [1.         0.23291404 1.        ]. \t  0.6750255070163653 \t 2.008466631651834\n",
      "8      \t [0.7795995  0.51407227 1.        ]. \t  1.9981155982760377 \t 2.008466631651834\n",
      "9      \t [0.48036792 0.47295133 1.        ]. \t  1.9109686095974583 \t 2.008466631651834\n",
      "10     \t [0.         0.37916692 1.        ]. \t  1.4538012480649765 \t 2.008466631651834\n",
      "11     \t [0.         0.82629294 0.63265533]. \t  \u001b[92m2.7123118575709073\u001b[0m \t 2.7123118575709073\n",
      "12     \t [0.         0.61078799 0.3662934 ]. \t  0.5502743027007143 \t 2.7123118575709073\n",
      "13     \t [1.00000000e+00 1.11022302e-16 0.00000000e+00]. \t  0.03095471703300515 \t 2.7123118575709073\n",
      "14     \t [0.         1.         0.45732341]. \t  1.8582452383594201 \t 2.7123118575709073\n",
      "15     \t [0.         0.65791684 0.93666266]. \t  \u001b[92m2.8859341716575138\u001b[0m \t 2.8859341716575138\n",
      "16     \t [0.         0.53653652 0.76914703]. \t  \u001b[92m3.261470465731078\u001b[0m \t 3.261470465731078\n",
      "17     \t [0.18706674 0.64514969 0.79484434]. \t  \u001b[92m3.34065447986638\u001b[0m \t 3.34065447986638\n",
      "18     \t [0.06990413 0.         0.        ]. \t  0.07819399243179133 \t 3.34065447986638\n",
      "19     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.34065447986638\n",
      "20     \t [0.52122098 0.         0.        ]. \t  0.09537175310766459 \t 3.34065447986638\n",
      "21     \t [1.         0.5139744  0.84627088]. \t  \u001b[92m3.6209647730128314\u001b[0m \t 3.6209647730128314\n",
      "22     \t [1.         0.21991904 0.6573988 ]. \t  0.6997049680186235 \t 3.6209647730128314\n",
      "23     \t [0.45081411 1.         0.        ]. \t  0.0002235443559905803 \t 3.6209647730128314\n",
      "24     \t [1.         1.         0.55657597]. \t  0.2623113886387092 \t 3.6209647730128314\n",
      "25     \t [1.        0.4856373 0.6646003]. \t  1.4814437855648075 \t 3.6209647730128314\n",
      "26     \t [0.66779023 0.40287394 0.81652436]. \t  3.04145524832235 \t 3.6209647730128314\n",
      "27     \t [0.         0.31085186 0.77405168]. \t  2.042230155818909 \t 3.6209647730128314\n",
      "28     \t [0.54227668 0.60130434 0.83176313]. \t  \u001b[92m3.6899734971605294\u001b[0m \t 3.6899734971605294\n",
      "29     \t [0.28359099 0.40368971 0.81150812]. \t  3.0631705619913205 \t 3.6899734971605294\n",
      "30     \t [1.         0.38250618 0.86959728]. \t  2.7881903753674018 \t 3.6899734971605294\n",
      "31     \t [1.         0.         0.85628124]. \t  0.2231463427466992 \t 3.6899734971605294\n",
      "32     \t [0.         0.31134947 0.        ]. \t  0.053438090330407015 \t 3.6899734971605294\n",
      "33     \t [1.         0.73790108 0.84649591]. \t  2.6052965738562843 \t 3.6899734971605294\n",
      "34     \t [0.        1.        0.7417688]. \t  1.1696292381881686 \t 3.6899734971605294\n",
      "35     \t [0.         0.         0.97692103]. \t  0.1124023422362413 \t 3.6899734971605294\n",
      "36     \t [0.50786588 0.58549074 0.82420526]. \t  \u001b[92m3.706761703043678\u001b[0m \t 3.706761703043678\n",
      "37     \t [0.80562486 0.5794492  0.83862882]. \t  3.6863812263928266 \t 3.706761703043678\n",
      "38     \t [0.56341046 0.58849816 0.83031432]. \t  \u001b[92m3.7152729272499467\u001b[0m \t 3.7152729272499467\n",
      "39     \t [0.69343768 0.64872138 0.84330263]. \t  3.447580026906602 \t 3.7152729272499467\n",
      "40     \t [0.23348898 0.59680923 0.87514562]. \t  \u001b[92m3.7547736480868306\u001b[0m \t 3.7547736480868306\n",
      "41     \t [0.41001591 0.57009177 0.83183742]. \t  \u001b[92m3.796875450194382\u001b[0m \t 3.796875450194382\n",
      "42     \t [0.41279362 0.56998611 0.83221205]. \t  \u001b[92m3.7981085050998087\u001b[0m \t 3.7981085050998087\n",
      "43     \t [0.2348465  0.58641646 0.86132503]. \t  \u001b[92m3.819540669110359\u001b[0m \t 3.819540669110359\n",
      "44     \t [0.81308394 0.52669166 0.81974742]. \t  3.608407830008626 \t 3.819540669110359\n",
      "45     \t [0.37500895 0.57349532 0.83517919]. \t  3.8106552088425945 \t 3.819540669110359\n",
      "46     \t [0.37283906 0.57352652 0.8354898 ]. \t  3.8119939766251196 \t 3.819540669110359\n",
      "47     \t [0.25526848 0.57964318 0.84596812]. \t  \u001b[92m3.83425436374746\u001b[0m \t 3.83425436374746\n",
      "48     \t [0.37582101 0.57308074 0.83560453]. \t  3.8126503086080277 \t 3.83425436374746\n",
      "49     \t [0.36608536 0.57403709 0.83611057]. \t  3.814232229855668 \t 3.83425436374746\n",
      "50     \t [0.25601537 0.57878687 0.84429479]. \t  3.8331236418553902 \t 3.83425436374746\n",
      "51     \t [0.36281662 0.57426933 0.83637628]. \t  3.81516878054275 \t 3.83425436374746\n",
      "52     \t [0.35235399 0.57528276 0.83695209]. \t  3.8166727660882085 \t 3.83425436374746\n",
      "53     \t [0.25924944 0.57846781 0.84269943]. \t  3.830746581841994 \t 3.83425436374746\n",
      "54     \t [0.35714376 0.57461242 0.83673464]. \t  3.816477817278139 \t 3.83425436374746\n",
      "55     \t [0.34825948 0.57544543 0.83720286]. \t  3.8176362495158314 \t 3.83425436374746\n",
      "56     \t [0.33915202 0.57601644 0.83759756]. \t  3.8188261889905446 \t 3.83425436374746\n",
      "57     \t [0.25922322 0.57770165 0.84235325]. \t  3.831299095767166 \t 3.83425436374746\n",
      "58     \t [0.34843236 0.5750106  0.83730786]. \t  3.818616778265565 \t 3.83425436374746\n",
      "59     \t [0.34043353 0.57549885 0.8376966 ]. \t  3.8198238343885373 \t 3.83425436374746\n",
      "60     \t [0.3319876  0.57656885 0.83806824]. \t  3.8199707292439102 \t 3.83425436374746\n",
      "61     \t [0.2698178  0.57845034 0.84193107]. \t  3.829259273857482 \t 3.83425436374746\n",
      "62     \t [0.3499718  0.57467872 0.83732552]. \t  3.8190201074258034 \t 3.83425436374746\n",
      "63     \t [0.34128226 0.57502808 0.83789891]. \t  3.8210823534758624 \t 3.83425436374746\n",
      "64     \t [0.33460672 0.57571179 0.83811617]. \t  3.821255757014422 \t 3.83425436374746\n",
      "65     \t [0.27176023 0.57803484 0.84137311]. \t  3.8287120186321344 \t 3.83425436374746\n",
      "66     \t [0.33968431 0.57535188 0.83787769]. \t  3.820666586213421 \t 3.83425436374746\n",
      "67     \t [0.33849992 0.57523036 0.83817924]. \t  3.8218578161585763 \t 3.83425436374746\n",
      "68     \t [0.33783808 0.57530019 0.83835444]. \t  3.822324618835758 \t 3.83425436374746\n",
      "69     \t [0.28252078 0.57791155 0.84104667]. \t  3.8280679378629108 \t 3.83425436374746\n",
      "70     \t [0.34280822 0.57471673 0.83758886]. \t  3.8204660975603906 \t 3.83425436374746\n",
      "71     \t [0.33009081 0.57592888 0.83831956]. \t  3.821873324944028 \t 3.83425436374746\n",
      "72     \t [0.33396358 0.57544527 0.83846548]. \t  3.822744971855081 \t 3.83425436374746\n",
      "73     \t [0.33026913 0.57563526 0.83861049]. \t  3.823158797008917 \t 3.83425436374746\n",
      "74     \t [0.33407734 0.57509204 0.83839617]. \t  3.8230706557387113 \t 3.83425436374746\n",
      "75     \t [0.27093661 0.57753204 0.84118944]. \t  3.8291151779724033 \t 3.83425436374746\n",
      "76     \t [0.33318706 0.57536309 0.83819007]. \t  3.8221252220900093 \t 3.83425436374746\n",
      "77     \t [0.32926515 0.57575216 0.83856145]. \t  3.8229123831214626 \t 3.83425436374746\n",
      "78     \t [0.33035368 0.57539494 0.83851707]. \t  3.823251480121474 \t 3.83425436374746\n",
      "79     \t [0.3270957  0.57577252 0.83854966]. \t  3.823005662454176 \t 3.83425436374746\n",
      "80     \t [0.33056311 0.57540987 0.83832768]. \t  3.822662281360783 \t 3.83425436374746\n",
      "81     \t [0.2818695  0.57768551 0.84077231]. \t  3.8277988065513995 \t 3.83425436374746\n",
      "82     \t [0.33857474 0.57508973 0.83807834]. \t  3.82176436846712 \t 3.83425436374746\n",
      "83     \t [0.32918999 0.57585849 0.83874831]. \t  3.8232882058620543 \t 3.83425436374746\n",
      "84     \t [0.3223888  0.57608391 0.83896758]. \t  3.824030942394462 \t 3.83425436374746\n",
      "85     \t [0.33571343 0.57510801 0.83833417]. \t  3.8227330881532735 \t 3.83425436374746\n",
      "86     \t [0.32316373 0.57605933 0.83913087]. \t  3.8244721035705878 \t 3.83425436374746\n",
      "87     \t [0.32884171 0.57529339 0.8383427 ]. \t  3.823012581806447 \t 3.83425436374746\n",
      "88     \t [0.33062524 0.5750414  0.83851762]. \t  3.8237673675384185 \t 3.83425436374746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.28192992 0.57743227 0.84083713]. \t  3.828365023262937 \t 3.83425436374746\n",
      "90     \t [0.33299    0.57529618 0.83836374]. \t  3.822753015310444 \t 3.83425436374746\n",
      "91     \t [0.32788933 0.57552764 0.83881497]. \t  3.82408274976698 \t 3.83425436374746\n",
      "92     \t [0.33253682 0.57545171 0.83851943]. \t  3.8230041268581862 \t 3.83425436374746\n",
      "93     \t [0.31675435 0.57627999 0.83910337]. \t  3.8244528181597426 \t 3.83425436374746\n",
      "94     \t [0.32605256 0.57593029 0.83884437]. \t  3.82367514795006 \t 3.83425436374746\n",
      "95     \t [0.32976598 0.57576369 0.83895089]. \t  3.823965703624851 \t 3.83425436374746\n",
      "96     \t [0.32239602 0.57584499 0.83891102]. \t  3.8242437731249947 \t 3.83425436374746\n",
      "97     \t [0.32283621 0.57606496 0.83887833]. \t  3.823780820534952 \t 3.83425436374746\n",
      "98     \t [0.33603282 0.57505955 0.8383253 ]. \t  3.8227540165376506 \t 3.83425436374746\n",
      "99     \t [0.28016493 0.57738307 0.84067958]. \t  3.828091687698717 \t 3.83425436374746\n",
      "100    \t [0.32648835 0.57529459 0.83867923]. \t  3.824151727558076 \t 3.83425436374746\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [1.         0.99504596 1.        ]. \t  0.33051039191831405 \t 2.524990008735946\n",
      "3      \t [0.         0.68600028 1.        ]. \t  1.7944423505823233 \t 2.524990008735946\n",
      "4      \t [0.55664087 0.50391138 1.        ]. \t  2.0005693523381023 \t 2.524990008735946\n",
      "5      \t [1.         0.18022319 0.17234696]. \t  0.22198129430365957 \t 2.524990008735946\n",
      "6      \t [1.         0.50551321 0.91515217]. \t  \u001b[92m3.2479412341907308\u001b[0m \t 3.2479412341907308\n",
      "7      \t [1.         0.58751606 0.58468956]. \t  0.6298611715088367 \t 3.2479412341907308\n",
      "8      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.2479412341907308\n",
      "9      \t [0.         0.40105078 0.9396782 ]. \t  2.405252149642792 \t 3.2479412341907308\n",
      "10     \t [0.09330184 1.         0.        ]. \t  0.00028846231730619224 \t 3.2479412341907308\n",
      "11     \t [1.         0.35430711 1.        ]. \t  1.289325944812616 \t 3.2479412341907308\n",
      "12     \t [0.80315033 0.         0.        ]. \t  0.058071041632539545 \t 3.2479412341907308\n",
      "13     \t [0.         0.         0.91269761]. \t  0.17850475857879586 \t 3.2479412341907308\n",
      "14     \t [0.43315752 0.90951796 0.8077743 ]. \t  1.2851901425282777 \t 3.2479412341907308\n",
      "15     \t [1.         0.         0.54398065]. \t  0.0689808043615408 \t 3.2479412341907308\n",
      "16     \t [0.23985296 0.55774634 0.73648413]. \t  2.9285847907747398 \t 3.2479412341907308\n",
      "17     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.2479412341907308\n",
      "18     \t [0.83879604 0.73077623 1.        ]. \t  1.5717564284604402 \t 3.2479412341907308\n",
      "19     \t [0.32402531 0.         1.        ]. \t  0.09168639842404466 \t 3.2479412341907308\n",
      "20     \t [0.         1.         0.50990218]. \t  2.331848595194349 \t 3.2479412341907308\n",
      "21     \t [0.53360729 0.57990039 0.63165083]. \t  1.5886377336784392 \t 3.2479412341907308\n",
      "22     \t [0.63133888 1.         0.        ]. \t  0.00014962351091238744 \t 3.2479412341907308\n",
      "23     \t [0.245791   1.         0.25872938]. \t  0.17484341874014078 \t 3.2479412341907308\n",
      "24     \t [ 0.00000000e+00 -2.77555756e-17  3.72056291e-01]. \t  0.4189075171200356 \t 3.2479412341907308\n",
      "25     \t [0.95246715 0.52264761 0.        ]. \t  0.008152911336835426 \t 3.2479412341907308\n",
      "26     \t [0.         0.27204184 0.66305922]. \t  0.9878182691686371 \t 3.2479412341907308\n",
      "27     \t [0.2011274  0.48754019 1.        ]. \t  1.9547441222124418 \t 3.2479412341907308\n",
      "28     \t [0.         0.87661718 0.76642044]. \t  1.711633033588132 \t 3.2479412341907308\n",
      "29     \t [0.47086975 0.         0.75562171]. \t  0.2426992261208335 \t 3.2479412341907308\n",
      "30     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.2479412341907308\n",
      "31     \t [1.         0.31405643 0.79063633]. \t  2.098959670465309 \t 3.2479412341907308\n",
      "32     \t [1.         0.76877151 0.85112603]. \t  2.323469076901747 \t 3.2479412341907308\n",
      "33     \t [0.        1.        0.1981341]. \t  0.05460193460747318 \t 3.2479412341907308\n",
      "34     \t [0.73841179 0.         0.9842183 ]. \t  0.10570504042417429 \t 3.2479412341907308\n",
      "35     \t [0.84357141 0.56507589 0.85983152]. \t  \u001b[92m3.732442996998576\u001b[0m \t 3.732442996998576\n",
      "36     \t [0.23141873 0.         0.14660339]. \t  0.5322699266242767 \t 3.732442996998576\n",
      "37     \t [0.12426706 1.         0.69800982]. \t  1.582320565705602 \t 3.732442996998576\n",
      "38     \t [0.80515674 1.         0.83446478]. \t  0.5688645571807993 \t 3.732442996998576\n",
      "39     \t [0.         0.57271727 0.7162506 ]. \t  2.6799816636758798 \t 3.732442996998576\n",
      "40     \t [0.84599235 0.56144268 0.85994404]. \t  \u001b[92m3.73453161425678\u001b[0m \t 3.73453161425678\n",
      "41     \t [0.68262284 0.58778871 0.86365532]. \t  \u001b[92m3.7472308039709303\u001b[0m \t 3.7472308039709303\n",
      "42     \t [0.84103833 0.56063052 0.86178822]. \t  3.735764990429395 \t 3.7472308039709303\n",
      "43     \t [0.83296973 0.56132098 0.86307328]. \t  3.7371227852708184 \t 3.7472308039709303\n",
      "44     \t [0.8271648  0.56176733 0.86375241]. \t  3.738205154075982 \t 3.7472308039709303\n",
      "45     \t [0.6637364  0.58144012 0.86413106]. \t  \u001b[92m3.766501173820461\u001b[0m \t 3.766501173820461\n",
      "46     \t [0.79024545 0.57019823 0.86430231]. \t  3.7429271916827846 \t 3.766501173820461\n",
      "47     \t [0.79429209 0.56827295 0.86460676]. \t  3.7433392279209974 \t 3.766501173820461\n",
      "48     \t [0.79573518 0.5672673  0.86476377]. \t  3.743684993063844 \t 3.766501173820461\n",
      "49     \t [0.79635171 0.56675587 0.86483058]. \t  3.743892671922773 \t 3.766501173820461\n",
      "50     \t [0.79620421 0.56646021 0.8649167 ]. \t  3.7441150979952997 \t 3.766501173820461\n",
      "51     \t [0.7966454  0.56617591 0.86494488]. \t  3.744192847517303 \t 3.766501173820461\n",
      "52     \t [0.67086951 0.57631341 0.86431828]. \t  \u001b[92m3.7734673725916137\u001b[0m \t 3.7734673725916137\n",
      "53     \t [0.7603653  0.57207839 0.86485117]. \t  3.7502490072149177 \t 3.7734673725916137\n",
      "54     \t [0.77279106 0.56981673 0.86504001]. \t  3.7485731735488352 \t 3.7734673725916137\n",
      "55     \t [0.77738833 0.56876484 0.86507413]. \t  3.7481481241481696 \t 3.7734673725916137\n",
      "56     \t [0.77973108 0.56834126 0.86511258]. \t  3.747739501433236 \t 3.7734673725916137\n",
      "57     \t [0.7810202  0.56797185 0.86508112]. \t  3.7477405702381072 \t 3.7734673725916137\n",
      "58     \t [0.78188567 0.56767417 0.86511541]. \t  3.747702796799324 \t 3.7734673725916137\n",
      "59     \t [0.78210692 0.56752781 0.86510321]. \t  3.74780001297227 \t 3.7734673725916137\n",
      "60     \t [0.89290989 0.         0.17889824]. \t  0.30268061995431206 \t 3.7734673725916137\n",
      "61     \t [0.77223224 0.56460359 0.86763715]. \t  3.749346072676287 \t 3.7734673725916137\n",
      "62     \t [0.7745314  0.56446033 0.86759768]. \t  3.748750381596921 \t 3.7734673725916137\n",
      "63     \t [0.77614592 0.56441994 0.86755402]. \t  3.74831407563731 \t 3.7734673725916137\n",
      "64     \t [0.77729105 0.56442133 0.86745942]. \t  3.7481128707077733 \t 3.7734673725916137\n",
      "65     \t [0.77822345 0.56448574 0.86737405]. \t  3.7479121858578632 \t 3.7734673725916137\n",
      "66     \t [0.77864722 0.56458379 0.86733517]. \t  3.747763149370842 \t 3.7734673725916137\n",
      "67     \t [0.77893533 0.56458423 0.86726861]. \t  3.747799127412461 \t 3.7734673725916137\n",
      "68     \t [0.7796272  0.56464892 0.86722514]. \t  3.7475913673224213 \t 3.7734673725916137\n",
      "69     \t [0.77930425 0.5648923  0.86710826]. \t  3.747735856213245 \t 3.7734673725916137\n",
      "70     \t [0.7798762  0.56484682 0.86707479]. \t  3.747641255835064 \t 3.7734673725916137\n",
      "71     \t [0.77973672 0.56495618 0.86699602]. \t  3.7477540916964487 \t 3.7734673725916137\n",
      "72     \t [0.78029356 0.56481671 0.86701857]. \t  3.747632957795637 \t 3.7734673725916137\n",
      "73     \t [0.78014633 0.56496633 0.86696657]. \t  3.7476598626225535 \t 3.7734673725916137\n",
      "74     \t [0.78018933 0.56504177 0.86692302]. \t  3.7476656061437947 \t 3.7734673725916137\n",
      "75     \t [0.77985063 0.56521    0.86685643]. \t  3.747768444766023 \t 3.7734673725916137\n",
      "76     \t [0.78003682 0.56509683 0.8668297 ]. \t  3.747854790140385 \t 3.7734673725916137\n",
      "77     \t [0.77989508 0.56519634 0.8668102 ]. \t  3.747855339373319 \t 3.7734673725916137\n",
      "78     \t [0.78039592 0.56516786 0.86682006]. \t  3.74768482138038 \t 3.7734673725916137\n",
      "79     \t [0.78044957 0.56517833 0.86682997]. \t  3.7476373123036817 \t 3.7734673725916137\n",
      "80     \t [0.78026579 0.56521921 0.86676576]. \t  3.7477910986744876 \t 3.7734673725916137\n",
      "81     \t [0.78057774 0.56510617 0.86678686]. \t  3.7477397159778323 \t 3.7734673725916137\n",
      "82     \t [0.77995192 0.56539461 0.86666939]. \t  3.747931376670536 \t 3.7734673725916137\n",
      "83     \t [0.78051808 0.56523947 0.86670805]. \t  3.7477959379107233 \t 3.7734673725916137\n",
      "84     \t [0.78038556 0.56531003 0.86670949]. \t  3.747777099402402 \t 3.7734673725916137\n",
      "85     \t [0.78004432 0.56536201 0.86668998]. \t  3.747888496837534 \t 3.7734673725916137\n",
      "86     \t [0.78151118 0.56520529 0.86663873]. \t  3.7476090460668257 \t 3.7734673725916137\n",
      "87     \t [0.78048221 0.56527223 0.86670744]. \t  3.747780660374689 \t 3.7734673725916137\n",
      "88     \t [0.78005076 0.56540382 0.86664061]. \t  3.7479434504911984 \t 3.7734673725916137\n",
      "89     \t [0.78050691 0.56533668 0.86663584]. \t  3.7478519996111768 \t 3.7734673725916137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.78031094 0.56538286 0.86663647]. \t  3.7478785074925884 \t 3.7734673725916137\n",
      "91     \t [0.78035305 0.56538125 0.86661927]. \t  3.747897970703443 \t 3.7734673725916137\n",
      "92     \t [0.78089372 0.56534945 0.86661939]. \t  3.7477355502268748 \t 3.7734673725916137\n",
      "93     \t [0.78073691 0.56540052 0.8665884 ]. \t  3.747804078026988 \t 3.7734673725916137\n",
      "94     \t [0.77955814 0.56544077 0.86654957]. \t  3.74825712231872 \t 3.7734673725916137\n",
      "95     \t [0.78063562 0.5654195  0.86658265]. \t  3.747833651368808 \t 3.7734673725916137\n",
      "96     \t [0.77970222 0.56551026 0.8665578 ]. \t  3.7481278589953644 \t 3.7734673725916137\n",
      "97     \t [0.78072151 0.56554997 0.86650931]. \t  3.7478238769965997 \t 3.7734673725916137\n",
      "98     \t [0.78117101 0.5653451  0.86657037]. \t  3.7477348916769406 \t 3.7734673725916137\n",
      "99     \t [0.78152844 0.5651687  0.86663131]. \t  3.747649499580642 \t 3.7734673725916137\n",
      "100    \t [0.77898967 0.56549822 0.86648066]. \t  3.7485354048781243 \t 3.7734673725916137\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [0.89514735 1.         0.        ]. \t  6.020754658262274e-05 \t 0.675391399411646\n",
      "2      \t [0.44077581 1.         1.        ]. \t  0.33323999610717187 \t 0.675391399411646\n",
      "3      \t [0.72556914 0.         0.        ]. \t  0.06980800021743609 \t 0.675391399411646\n",
      "4      \t [0.63866395 1.         0.53801488]. \t  \u001b[92m1.1114121878394092\u001b[0m \t 1.1114121878394092\n",
      "5      \t [0.98950045 1.         0.74652684]. \t  0.3957001855165442 \t 1.1114121878394092\n",
      "6      \t [0.38991945 1.         0.24810059]. \t  0.12058517536162469 \t 1.1114121878394092\n",
      "7      \t [0.24921866 1.         0.70526056]. \t  \u001b[92m1.4412297178034208\u001b[0m \t 1.4412297178034208\n",
      "8      \t [0.         1.         0.96173128]. \t  0.4412289630992171 \t 1.4412297178034208\n",
      "9      \t [0.         0.         0.27324951]. \t  0.5793364867314592 \t 1.4412297178034208\n",
      "10     \t [0.         1.         0.59852558]. \t  \u001b[92m2.3805064811439824\u001b[0m \t 2.3805064811439824\n",
      "11     \t [0.         1.         0.36881243]. \t  0.8747362265813862 \t 2.3805064811439824\n",
      "12     \t [1.         0.30199965 1.        ]. \t  1.0102322159250443 \t 2.3805064811439824\n",
      "13     \t [0.         0.74680389 0.65722821]. \t  \u001b[92m2.537374868130347\u001b[0m \t 2.537374868130347\n",
      "14     \t [0.97435631 0.71153486 1.        ]. \t  1.6420404057380196 \t 2.537374868130347\n",
      "15     \t [0.         0.46514764 0.71809593]. \t  2.461620693053005 \t 2.537374868130347\n",
      "16     \t [0.         0.26874223 0.88622961]. \t  1.6882593740986056 \t 2.537374868130347\n",
      "17     \t [0.94693675 1.         1.        ]. \t  0.31906860750553534 \t 2.537374868130347\n",
      "18     \t [0.67885018 0.52167621 1.        ]. \t  2.027126700936364 \t 2.537374868130347\n",
      "19     \t [0.         0.51361488 0.93929056]. \t  \u001b[92m3.0322074499757683\u001b[0m \t 3.0322074499757683\n",
      "20     \t [1.         0.33850862 0.16367679]. \t  0.13447222923602892 \t 3.0322074499757683\n",
      "21     \t [0.19648926 0.47215358 1.        ]. \t  1.9033938698409412 \t 3.0322074499757683\n",
      "22     \t [0.         0.38233289 1.        ]. \t  1.470697063005163 \t 3.0322074499757683\n",
      "23     \t [0.5826073  0.55456565 0.        ]. \t  0.015123165989666431 \t 3.0322074499757683\n",
      "24     \t [1.         0.         0.53883807]. \t  0.0682172860933208 \t 3.0322074499757683\n",
      "25     \t [1.        1.        0.2608418]. \t  0.01784359785529254 \t 3.0322074499757683\n",
      "26     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.0322074499757683\n",
      "27     \t [1.         0.45913598 0.80884161]. \t  \u001b[92m3.2507692601358675\u001b[0m \t 3.2507692601358675\n",
      "28     \t [0.12193132 0.         0.        ]. \t  0.08514942763263768 \t 3.2507692601358675\n",
      "29     \t [0.52185559 0.71377107 0.88619956]. \t  2.965400741047576 \t 3.2507692601358675\n",
      "30     \t [0.         0.         0.65079553]. \t  0.1497174067855223 \t 3.2507692601358675\n",
      "31     \t [1.         0.28923443 0.62054326]. \t  0.6696290721210865 \t 3.2507692601358675\n",
      "32     \t [1.         0.69928498 0.        ]. \t  0.0012159329396379745 \t 3.2507692601358675\n",
      "33     \t [0.84218744 0.70432473 0.82396595]. \t  2.8696543770772696 \t 3.2507692601358675\n",
      "34     \t [0.33254958 0.         0.18197969]. \t  0.6982282410713588 \t 3.2507692601358675\n",
      "35     \t [1.         0.63426709 0.75981442]. \t  2.585505745339754 \t 3.2507692601358675\n",
      "36     \t [0.14449206 0.60518956 0.83375777]. \t  \u001b[92m3.729874486413774\u001b[0m \t 3.729874486413774\n",
      "37     \t [0.81606954 0.45068847 0.8535922 ]. \t  3.419013837700222 \t 3.729874486413774\n",
      "38     \t [0.44128081 0.48161201 0.85059173]. \t  3.6736165645888406 \t 3.729874486413774\n",
      "39     \t [0.7041498  0.         0.77980042]. \t  0.24879330722690188 \t 3.729874486413774\n",
      "40     \t [0.         0.67917161 0.84880268]. \t  3.31976372145773 \t 3.729874486413774\n",
      "41     \t [0.21075838 0.54833936 0.8307617 ]. \t  \u001b[92m3.81244914195046\u001b[0m \t 3.81244914195046\n",
      "42     \t [0.3866575  0.56170652 0.83893829]. \t  \u001b[92m3.8336235478191543\u001b[0m \t 3.8336235478191543\n",
      "43     \t [0.2012273  0.54882654 0.83682224]. \t  3.832503822950735 \t 3.8336235478191543\n",
      "44     \t [1.         0.50370675 0.89530063]. \t  3.434186883077677 \t 3.8336235478191543\n",
      "45     \t [0.35331505 0.56248533 0.84190103]. \t  \u001b[92m3.84418588059723\u001b[0m \t 3.84418588059723\n",
      "46     \t [0.21341942 0.56206758 0.83980615]. \t  3.8402829346972265 \t 3.84418588059723\n",
      "47     \t [0.39985139 0.54925996 0.84340801]. \t  \u001b[92m3.845332217860157\u001b[0m \t 3.845332217860157\n",
      "48     \t [0.24016226 0.45618126 0.84639875]. \t  3.533013997484984 \t 3.845332217860157\n",
      "49     \t [0.24911667 0.58482449 0.84576828]. \t  3.8239010362940116 \t 3.845332217860157\n",
      "50     \t [0.29335574 0.571576   0.84348537]. \t  3.8421550440097594 \t 3.845332217860157\n",
      "51     \t [0.41193439 0.55314678 0.84371145]. \t  3.8448464005349328 \t 3.845332217860157\n",
      "52     \t [0.26224513 0.57151275 0.84291906]. \t  3.8412044090400648 \t 3.845332217860157\n",
      "53     \t [0.29251579 0.56653705 0.84320947]. \t  \u001b[92m3.8469187545043484\u001b[0m \t 3.8469187545043484\n",
      "54     \t [0.31868537 0.56240137 0.84349904]. \t  \u001b[92m3.849752317902909\u001b[0m \t 3.849752317902909\n",
      "55     \t [0.25397301 0.56699758 0.8423182 ]. \t  3.8445276167168636 \t 3.849752317902909\n",
      "56     \t [0.39449511 0.55271063 0.84436814]. \t  3.848218617823325 \t 3.849752317902909\n",
      "57     \t [0.28323299 0.56407267 0.84300352]. \t  3.8485559925229422 \t 3.849752317902909\n",
      "58     \t [0.30188321 0.5618661  0.84343635]. \t  \u001b[92m3.8505060896337513\u001b[0m \t 3.8505060896337513\n",
      "59     \t [0.29715365 0.56197125 0.84334609]. \t  3.8503775763014043 \t 3.8505060896337513\n",
      "60     \t [0.33000894 0.55828486 0.84388552]. \t  \u001b[92m3.851848710719999\u001b[0m \t 3.851848710719999\n",
      "61     \t [0.25476124 0.56361257 0.84211209]. \t  3.846738714011755 \t 3.851848710719999\n",
      "62     \t [0.35146625 0.55577209 0.8443018 ]. \t  \u001b[92m3.851874958786354\u001b[0m \t 3.851874958786354\n",
      "63     \t [0.27041508 0.56238099 0.84268733]. \t  3.848971917478458 \t 3.851874958786354\n",
      "64     \t [0.3115673  0.55931282 0.84364435]. \t  3.851822995685964 \t 3.851874958786354\n",
      "65     \t [0.27766979 0.56181936 0.84270233]. \t  3.849369547445159 \t 3.851874958786354\n",
      "66     \t [0.35326732 0.5550545  0.84446576]. \t  \u001b[92m3.852129478457803\u001b[0m \t 3.852129478457803\n",
      "67     \t [0.27402338 0.56136725 0.84278327]. \t  3.849745613236294 \t 3.852129478457803\n",
      "68     \t [0.30385581 0.55922021 0.84357051]. \t  3.8519412365468138 \t 3.852129478457803\n",
      "69     \t [0.29015385 0.56014397 0.84313435]. \t  3.8509836940652087 \t 3.852129478457803\n",
      "70     \t [0.33595309 0.5562987  0.84429579]. \t  \u001b[92m3.8527682082181887\u001b[0m \t 3.8527682082181887\n",
      "71     \t [0.2720458  0.56098094 0.84277683]. \t  3.8499060122741033 \t 3.8527682082181887\n",
      "72     \t [0.31273029 0.55835393 0.8438179 ]. \t  3.852438596810548 \t 3.8527682082181887\n",
      "73     \t [0.28342706 0.56022642 0.8431209 ]. \t  3.8509613307685266 \t 3.8527682082181887\n",
      "74     \t [0.34362037 0.55542056 0.84429586]. \t  3.8524502038296182 \t 3.8527682082181887\n",
      "75     \t [0.27724831 0.56038054 0.84294886]. \t  3.8505560968897745 \t 3.8527682082181887\n",
      "76     \t [0.29941146 0.55892205 0.84352965]. \t  3.8520698467778054 \t 3.8527682082181887\n",
      "77     \t [0.30137571 0.55881258 0.84355033]. \t  3.852106791742713 \t 3.8527682082181887\n",
      "78     \t [0.29452123 0.55910499 0.8434157 ]. \t  3.851873838659712 \t 3.8527682082181887\n",
      "79     \t [0.32619122 0.55667958 0.84409291]. \t  \u001b[92m3.852837507328925\u001b[0m \t 3.852837507328925\n",
      "80     \t [0.27252231 0.56018654 0.84285995]. \t  3.8504325401206816 \t 3.852837507328925\n",
      "81     \t [0.32477114 0.55679904 0.84417073]. \t  \u001b[92m3.8530130318834663\u001b[0m \t 3.8530130318834663\n",
      "82     \t [0.28553007 0.55980532 0.84315828]. \t  3.8512027337189405 \t 3.8530130318834663\n",
      "83     \t [0.29966892 0.55876105 0.84356731]. \t  3.8521897170896935 \t 3.8530130318834663\n",
      "84     \t [0.29685839 0.55842763 0.84353781]. \t  3.8522938449717947 \t 3.8530130318834663\n",
      "85     \t [0.32895091 0.55628527 0.84427442]. \t  \u001b[92m3.8531009989130207\u001b[0m \t 3.8531009989130207\n",
      "86     \t [0.27501104 0.55989355 0.84295596]. \t  3.850761966733675 \t 3.8531009989130207\n",
      "87     \t [0.31474967 0.55738067 0.84387952]. \t  3.8527591964679297 \t 3.8531009989130207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.28469564 0.55932523 0.84314371]. \t  3.85136494815604 \t 3.8531009989130207\n",
      "89     \t [0.31189782 0.55750884 0.84388967]. \t  3.852838580192509 \t 3.8531009989130207\n",
      "90     \t [0.28574636 0.55910642 0.84328843]. \t  3.851712813438623 \t 3.8531009989130207\n",
      "91     \t [0.34161141 0.55505429 0.8443846 ]. \t  3.852773499466527 \t 3.8531009989130207\n",
      "92     \t [0.28299469 0.55972818 0.8431813 ]. \t  3.851283608942738 \t 3.8531009989130207\n",
      "93     \t [0.30021374 0.55824598 0.84353477]. \t  3.852285951494296 \t 3.8531009989130207\n",
      "94     \t [0.2988073  0.55838136 0.8435158 ]. \t  3.8522354181010923 \t 3.8531009989130207\n",
      "95     \t [0.29689594 0.558425   0.84346293]. \t  3.8521566289036815 \t 3.8531009989130207\n",
      "96     \t [0.30632855 0.5578525  0.84376956]. \t  3.852689829668221 \t 3.8531009989130207\n",
      "97     \t [0.29660133 0.55840171 0.84341238]. \t  3.852075204378264 \t 3.8531009989130207\n",
      "98     \t [0.30335338 0.55791844 0.84361961]. \t  3.8524716788866558 \t 3.8531009989130207\n",
      "99     \t [0.30321147 0.5580412  0.84359128]. \t  3.852387802036411 \t 3.8531009989130207\n",
      "100    \t [0.28828031 0.55884268 0.84323812]. \t  3.8516979735046495 \t 3.8531009989130207\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [5.55111512e-17 1.00000000e+00 1.00000000e+00]. \t  0.330219860606422 \t 2.3951473341797507\n",
      "2      \t [0.         0.27009826 1.        ]. \t  0.8671866914779873 \t 2.3951473341797507\n",
      "3      \t [1.         0.644718   0.66885815]. \t  1.3183347555154663 \t 2.3951473341797507\n",
      "4      \t [0.40666224 0.63491801 0.78321051]. \t  \u001b[92m3.2315324059526427\u001b[0m \t 3.2315324059526427\n",
      "5      \t [0.60537792 0.51290999 1.        ]. \t  2.017529952225632 \t 3.2315324059526427\n",
      "6      \t [0.62136539 0.93627402 0.85709207]. \t  0.9889312058949565 \t 3.2315324059526427\n",
      "7      \t [0.5847218  0.62918615 0.43336444]. \t  0.59636283850733 \t 3.2315324059526427\n",
      "8      \t [ 1.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.030954717033005123 \t 3.2315324059526427\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.2315324059526427\n",
      "10     \t [0.1897144  0.63669964 1.        ]. \t  1.9861057540871971 \t 3.2315324059526427\n",
      "11     \t [0.05274489 0.         0.        ]. \t  0.0757566561839372 \t 3.2315324059526427\n",
      "12     \t [1.        0.4357227 0.       ]. \t  0.012856962270717436 \t 3.2315324059526427\n",
      "13     \t [1.         0.34147783 0.84955964]. \t  2.458860925757093 \t 3.2315324059526427\n",
      "14     \t [0.16829693 1.         0.66130936]. \t  1.930229287386405 \t 3.2315324059526427\n",
      "15     \t [1.         0.61204788 1.        ]. \t  1.969602456422221 \t 3.2315324059526427\n",
      "16     \t [0.39481823 0.2871046  0.        ]. \t  0.08761833149985217 \t 3.2315324059526427\n",
      "17     \t [0.         0.12537711 0.62701638]. \t  0.3311993591584969 \t 3.2315324059526427\n",
      "18     \t [0.26661465 0.4050574  0.77619799]. \t  2.8307767259573717 \t 3.2315324059526427\n",
      "19     \t [0.         0.51434917 0.81791621]. \t  \u001b[92m3.6552360177591083\u001b[0m \t 3.6552360177591083\n",
      "20     \t [1.         0.34931448 0.63185808]. \t  0.9003218170204749 \t 3.6552360177591083\n",
      "21     \t [0.94025837 1.         0.        ]. \t  4.960167311374681e-05 \t 3.6552360177591083\n",
      "22     \t [0.77678572 0.49037596 0.80127757]. \t  3.400102389854781 \t 3.6552360177591083\n",
      "23     \t [0.         0.74218231 0.83911236]. \t  2.788112406479081 \t 3.6552360177591083\n",
      "24     \t [0.66598145 0.         0.        ]. \t  0.078462056027835 \t 3.6552360177591083\n",
      "25     \t [0.77124047 0.26992002 0.8461386 ]. \t  1.8262704707087312 \t 3.6552360177591083\n",
      "26     \t [0.         0.56144965 1.        ]. \t  2.0571874003648127 \t 3.6552360177591083\n",
      "27     \t [0.50158113 1.         0.        ]. \t  0.00020337270244386157 \t 3.6552360177591083\n",
      "28     \t [0.86760477 0.42503889 0.98605373]. \t  1.8818913035908624 \t 3.6552360177591083\n",
      "29     \t [0.         1.         0.50433861]. \t  2.294116475886586 \t 3.6552360177591083\n",
      "30     \t [0.77198835 0.67222044 0.85559314]. \t  3.281718335944726 \t 3.6552360177591083\n",
      "31     \t [5.82608418e-06 3.75514358e-01 6.49055141e-01]. \t  1.31051708694276 \t 3.6552360177591083\n",
      "32     \t [0.         0.63390251 0.        ]. \t  0.005547754910034444 \t 3.6552360177591083\n",
      "33     \t [1.         0.         0.58301295]. \t  0.08473635010916605 \t 3.6552360177591083\n",
      "34     \t [0.28064599 0.         0.22655381]. \t  0.8105988485041825 \t 3.6552360177591083\n",
      "35     \t [0.         0.         0.79520184]. \t  0.24739374917047569 \t 3.6552360177591083\n",
      "36     \t [0.39140213 0.63882411 0.05673799]. \t  0.01814392106179456 \t 3.6552360177591083\n",
      "37     \t [0.99193866 0.76820774 0.87407086]. \t  2.3472110623663767 \t 3.6552360177591083\n",
      "38     \t [0.15001251 0.48233385 0.89953502]. \t  3.430776865063198 \t 3.6552360177591083\n",
      "39     \t [0.97396596 0.5520274  0.84518538]. \t  \u001b[92m3.6688817616660594\u001b[0m \t 3.6688817616660594\n",
      "40     \t [0.32959027 0.38738573 0.91247239]. \t  2.6400289696508636 \t 3.6688817616660594\n",
      "41     \t [0.62163616 0.49366306 0.82282163]. \t  3.615833419865573 \t 3.6688817616660594\n",
      "42     \t [0.16445252 0.61034483 0.84151965]. \t  \u001b[92m3.73585137015954\u001b[0m \t 3.73585137015954\n",
      "43     \t [0.92980285 0.50669172 0.83761653]. \t  3.6127684292546327 \t 3.73585137015954\n",
      "44     \t [0.         0.99999998 0.        ]. \t  0.00027353679487279015 \t 3.73585137015954\n",
      "45     \t [0.64603072 0.98356685 0.85169221]. \t  0.6965671981222739 \t 3.73585137015954\n",
      "46     \t [0.02705913 0.9252832  0.90498505]. \t  1.0243292766943146 \t 3.73585137015954\n",
      "47     \t [0.40824138 0.49126066 0.37660664]. \t  0.38730754260088057 \t 3.73585137015954\n",
      "48     \t [0.11224833 0.81636102 0.05510213]. \t  0.0032231561413997357 \t 3.73585137015954\n",
      "49     \t [0.18856892 0.58701826 0.71088235]. \t  2.6686946187175598 \t 3.73585137015954\n",
      "50     \t [0.27851208 0.8787004  0.92779283]. \t  1.2761988007569391 \t 3.73585137015954\n",
      "51     \t [0.81265955 0.41132749 0.84056276]. \t  3.135950624414653 \t 3.73585137015954\n",
      "52     \t [0.63427989 0.87306528 0.75324288]. \t  1.306823750539479 \t 3.73585137015954\n",
      "53     \t [0.71143955 0.50967573 0.03027368]. \t  0.027963169634076815 \t 3.73585137015954\n",
      "54     \t [0.54839175 0.7465166  0.49836605]. \t  1.3724653259441537 \t 3.73585137015954\n",
      "55     \t [0.72579525 0.35976761 0.33217317]. \t  0.3519326230068731 \t 3.73585137015954\n",
      "56     \t [0.917223   0.98846629 0.12487948]. \t  0.0015617372254431254 \t 3.73585137015954\n",
      "57     \t [0.55035505 0.35769172 0.58668171]. \t  0.6870939250800232 \t 3.73585137015954\n",
      "58     \t [0.02752459 0.61563494 0.42928929]. \t  0.9991490514822108 \t 3.73585137015954\n",
      "59     \t [0.83994469 0.13059362 0.40583657]. \t  0.297637982613456 \t 3.73585137015954\n",
      "60     \t [0.66539115 0.70870415 0.14928367]. \t  0.02179028217114431 \t 3.73585137015954\n",
      "61     \t [0.26527363 0.12271067 0.51708848]. \t  0.23156074888186506 \t 3.73585137015954\n",
      "62     \t [0.61867025 0.21017493 0.90488381]. \t  1.1280061567678494 \t 3.73585137015954\n",
      "63     \t [0.40202025 0.16095801 0.25146733]. \t  0.9713394518516686 \t 3.73585137015954\n",
      "64     \t [0.90472964 0.9160513  0.18428633]. \t  0.007771975311035936 \t 3.73585137015954\n",
      "65     \t [0.58464209 0.25549428 0.4077631 ]. \t  0.4304226878949818 \t 3.73585137015954\n",
      "66     \t [0.96757219 0.70456414 0.80806605]. \t  2.6887176263047983 \t 3.73585137015954\n",
      "67     \t [0.50159174 0.39568288 0.3605373 ]. \t  0.4049837552433268 \t 3.73585137015954\n",
      "68     \t [0.67924021 0.84651497 0.96716372]. \t  1.2267200011135135 \t 3.73585137015954\n",
      "69     \t [0.24570356 0.68828363 0.92009803]. \t  2.932787222999772 \t 3.73585137015954\n",
      "70     \t [0.1654261  0.25663803 0.20401237]. \t  0.6461403886104528 \t 3.73585137015954\n",
      "71     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.73585137015954\n",
      "72     \t [0.98804586 0.0449322  0.30230819]. \t  0.29004380755883535 \t 3.73585137015954\n",
      "73     \t [0.12133339 0.48852046 0.85843711]. \t  3.683986424697345 \t 3.73585137015954\n",
      "74     \t [0.19916068 0.6271735  0.40368678]. \t  0.8597929973599497 \t 3.73585137015954\n",
      "75     \t [0.1117425  0.28531176 0.48203229]. \t  0.3208871437866779 \t 3.73585137015954\n",
      "76     \t [0.43739805 0.42888133 0.1738385 ]. \t  0.29073845518451485 \t 3.73585137015954\n",
      "77     \t [0.4699236  0.89020772 0.37975838]. \t  0.8113985516385133 \t 3.73585137015954\n",
      "78     \t [0.4392073  0.78519199 0.10636872]. \t  0.010029344977685532 \t 3.73585137015954\n",
      "79     \t [0.21961907 0.6015288  0.09956847]. \t  0.04121021759401271 \t 3.73585137015954\n",
      "80     \t [0.36501478 0.61058063 0.78513565]. \t  3.3609733830481874 \t 3.73585137015954\n",
      "81     \t [0.49865486 0.51294938 0.08435451]. \t  0.07331448317596406 \t 3.73585137015954\n",
      "82     \t [0.78933727 0.90472402 0.82215516]. \t  1.126411336305204 \t 3.73585137015954\n",
      "83     \t [0.8777519  0.14790529 0.76351869]. \t  0.8108427181301892 \t 3.73585137015954\n",
      "84     \t [0.33935565 0.09674464 0.31095635]. \t  0.9396204683691252 \t 3.73585137015954\n",
      "85     \t [0.43361283 0.20523394 0.62461828]. \t  0.5329662580521853 \t 3.73585137015954\n",
      "86     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.73585137015954\n",
      "87     \t [0.99681669 0.19717048 0.35503832]. \t  0.23201799636579212 \t 3.73585137015954\n",
      "88     \t [0.52645571 0.75007053 0.81331628]. \t  2.5756383452991445 \t 3.73585137015954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.13150413 0.46151769 0.31307666]. \t  0.3390919336567982 \t 3.73585137015954\n",
      "90     \t [0.12310575 0.38906384 0.77574396]. \t  2.7006214372616513 \t 3.73585137015954\n",
      "91     \t [0.05598263 0.48270763 0.65166228]. \t  1.7811951220712343 \t 3.73585137015954\n",
      "92     \t [0.65259893 0.40543277 0.61169084]. \t  0.9410805313922265 \t 3.73585137015954\n",
      "93     \t [0.89712384 0.86740939 0.57199769]. \t  0.5705142395729424 \t 3.73585137015954\n",
      "94     \t [0.61636758 0.94645981 0.10071583]. \t  0.0030841750150836385 \t 3.73585137015954\n",
      "95     \t [0.49266886 0.21165411 0.91177336]. \t  1.110705187350084 \t 3.73585137015954\n",
      "96     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.73585137015954\n",
      "97     \t [0.52192988 0.23186286 0.10800905]. \t  0.38166872418543096 \t 3.73585137015954\n",
      "98     \t [0.30024739 0.21118253 0.95814266]. \t  0.8514100172171297 \t 3.73585137015954\n",
      "99     \t [0.8763737  0.52791731 0.09937568]. \t  0.0369334305681939 \t 3.73585137015954\n",
      "100    \t [0.69189166 0.22797366 0.90159894]. \t  1.2756340457508264 \t 3.73585137015954\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0.         0.95537217 1.        ]. \t  0.4721632346231306 \t 2.6229838112516717\n",
      "3      \t [0.44650782 0.5079681  1.        ]. \t  2.0158170778938795 \t 2.6229838112516717\n",
      "4      \t [0.29990631 0.74840943 0.57708454]. \t  2.5249934112546324 \t 2.6229838112516717\n",
      "5      \t [0.41258818 1.         0.65879956]. \t  1.5292881132677694 \t 2.6229838112516717\n",
      "6      \t [0.        0.636161  0.2259609]. \t  0.10720909579016796 \t 2.6229838112516717\n",
      "7      \t [0.04978533 0.55710297 0.75799245]. \t  \u001b[92m3.1663481571467003\u001b[0m \t 3.1663481571467003\n",
      "8      \t [0.         0.20723529 0.98447056]. \t  0.666486675685904 \t 3.1663481571467003\n",
      "9      \t [1.         1.         0.44669573]. \t  0.16924391660377788 \t 3.1663481571467003\n",
      "10     \t [0.         0.35456414 0.59654893]. \t  0.806874738505087 \t 3.1663481571467003\n",
      "11     \t [0.         0.85941559 0.60309758]. \t  2.8806519826668517 \t 3.1663481571467003\n",
      "12     \t [0.         0.59137579 1.        ]. \t  2.046781449148861 \t 3.1663481571467003\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1663481571467003\n",
      "14     \t [0.51320179 0.38553818 0.7833841 ]. \t  2.7319449838923386 \t 3.1663481571467003\n",
      "15     \t [0.6622743  0.53696027 0.5237463 ]. \t  0.6182478553194671 \t 3.1663481571467003\n",
      "16     \t [1.         0.55286416 0.95375908]. \t  2.8004874111517473 \t 3.1663481571467003\n",
      "17     \t [1.         0.72019678 0.7994586 ]. \t  2.468161395337108 \t 3.1663481571467003\n",
      "18     \t [0.22652932 0.75229813 0.81898659]. \t  2.694699893158549 \t 3.1663481571467003\n",
      "19     \t [0.         0.75432768 0.79493431]. \t  2.58568104024918 \t 3.1663481571467003\n",
      "20     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.1663481571467003\n",
      "21     \t [9.15698486e-09 1.00000000e+00 3.43742128e-01]. \t  0.6489951954160911 \t 3.1663481571467003\n",
      "22     \t [1.         0.32567445 0.87200609]. \t  2.247958883211814 \t 3.1663481571467003\n",
      "23     \t [2.93151504e-09 6.68476276e-01 5.36871504e-01]. \t  2.0806053472186696 \t 3.1663481571467003\n",
      "24     \t [0.21602951 0.7315963  0.        ]. \t  0.0027339001797266047 \t 3.1663481571467003\n",
      "25     \t [1.         0.         0.67008177]. \t  0.16389893111341908 \t 3.1663481571467003\n",
      "26     \t [0.22071556 0.28603862 0.83249739]. \t  2.013150866911896 \t 3.1663481571467003\n",
      "27     \t [0.1179571 0.        0.       ]. \t  0.08464544176203834 \t 3.1663481571467003\n",
      "28     \t [0.         1.         0.71703839]. \t  1.3635399883606292 \t 3.1663481571467003\n",
      "29     \t [0.65095623 0.         0.        ]. \t  0.08053727974763045 \t 3.1663481571467003\n",
      "30     \t [0.         0.7636578  0.00220742]. \t  0.0015038873547638368 \t 3.1663481571467003\n",
      "31     \t [1.50567870e-08 1.00000000e+00 4.02826829e-08]. \t  0.00027353711596750314 \t 3.1663481571467003\n",
      "32     \t [0.1268655  0.41178033 0.26678082]. \t  0.38181628623354774 \t 3.1663481571467003\n",
      "33     \t [0.95314159 0.63356991 0.63616313]. \t  1.0410885582216811 \t 3.1663481571467003\n",
      "34     \t [0.17651587 0.57877169 0.72584791]. \t  2.8245320419940176 \t 3.1663481571467003\n",
      "35     \t [0.68065875 0.06935073 0.27964859]. \t  0.727306180729379 \t 3.1663481571467003\n",
      "36     \t [0.44995398 0.76110441 0.21437761]. \t  0.07208527185093941 \t 3.1663481571467003\n",
      "37     \t [0.73667086 0.41211989 0.68574058]. \t  1.7285172452558117 \t 3.1663481571467003\n",
      "38     \t [0.14154474 0.71312893 0.82279243]. \t  3.038390692600929 \t 3.1663481571467003\n",
      "39     \t [0.85618534 0.25398188 0.5061637 ]. \t  0.2029365073720305 \t 3.1663481571467003\n",
      "40     \t [0.68891349 0.07170848 0.91273754]. \t  0.3644836785822796 \t 3.1663481571467003\n",
      "41     \t [0.70664598 0.00359851 0.80572512]. \t  0.2584865256522639 \t 3.1663481571467003\n",
      "42     \t [0.83720438 0.09152619 0.61561036]. \t  0.23362346564473038 \t 3.1663481571467003\n",
      "43     \t [0.50895923 0.32831698 0.62375505]. \t  0.8867725196045995 \t 3.1663481571467003\n",
      "44     \t [0.95112084 0.26166328 0.58067683]. \t  0.39697730342591064 \t 3.1663481571467003\n",
      "45     \t [0.57215748 0.81772148 0.08081551]. \t  0.004107945210182036 \t 3.1663481571467003\n",
      "46     \t [0.33948184 0.42579659 0.37398801]. \t  0.41242449144558546 \t 3.1663481571467003\n",
      "47     \t [0.45621961 0.62665144 0.30957697]. \t  0.25768188144896653 \t 3.1663481571467003\n",
      "48     \t [0.54383562 0.68843494 0.39754072]. \t  0.6071596355594916 \t 3.1663481571467003\n",
      "49     \t [0.90061552 0.28974133 0.84272881]. \t  1.9961683160107557 \t 3.1663481571467003\n",
      "50     \t [0.11943269 0.79208637 0.67105988]. \t  2.5604362371269542 \t 3.1663481571467003\n",
      "51     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.1663481571467003\n",
      "52     \t [0.57965593 0.71570533 0.65579483]. \t  1.7326335549163423 \t 3.1663481571467003\n",
      "53     \t [0.16520131 0.15697552 0.28504892]. \t  0.8630790546054824 \t 3.1663481571467003\n",
      "54     \t [0.89807939 0.20808284 0.13597905]. \t  0.2368611982249034 \t 3.1663481571467003\n",
      "55     \t [0.14556332 0.74958193 0.86294748]. \t  2.7389053935290635 \t 3.1663481571467003\n",
      "56     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.1663481571467003\n",
      "57     \t [0.93793433 0.96289914 0.62095157]. \t  0.41431461357234256 \t 3.1663481571467003\n",
      "58     \t [0.12547343 0.58598869 0.77601423]. \t  \u001b[92m3.3592035367884128\u001b[0m \t 3.3592035367884128\n",
      "59     \t [0.67647682 0.4645709  0.4712833 ]. \t  0.3295539844002202 \t 3.3592035367884128\n",
      "60     \t [0.1102904  0.72905619 0.7105759 ]. \t  2.5478252541413138 \t 3.3592035367884128\n",
      "61     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.3592035367884128\n",
      "62     \t [0.92199931 0.75913036 0.59764878]. \t  0.6646765953627011 \t 3.3592035367884128\n",
      "63     \t [0.19705171 0.73435699 0.7803774 ]. \t  2.712353150515283 \t 3.3592035367884128\n",
      "64     \t [0.94853201 0.29662511 0.52631384]. \t  0.23911710766300953 \t 3.3592035367884128\n",
      "65     \t [0.69982337 0.89273213 0.90401204]. \t  1.2222480714419688 \t 3.3592035367884128\n",
      "66     \t [0.84945765 0.68184342 0.73438583]. \t  2.0937597958104197 \t 3.3592035367884128\n",
      "67     \t [0.32353643 0.63389744 0.18376091]. \t  0.07949243946085771 \t 3.3592035367884128\n",
      "68     \t [0.91310694 0.30603364 0.66378493]. \t  1.0716646364880367 \t 3.3592035367884128\n",
      "69     \t [0.66611001 0.5483462  0.10832291]. \t  0.056965705841691784 \t 3.3592035367884128\n",
      "70     \t [0.86144225 0.25484801 0.5945089 ]. \t  0.4624660965502681 \t 3.3592035367884128\n",
      "71     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.3592035367884128\n",
      "72     \t [0.1452373  0.61405867 0.77570653]. \t  3.2976919975041223 \t 3.3592035367884128\n",
      "73     \t [0.89971495 0.71543465 0.28623959]. \t  0.053432034071667286 \t 3.3592035367884128\n",
      "74     \t [0.55324024 0.60212219 0.6342148 ]. \t  1.612388179004163 \t 3.3592035367884128\n",
      "75     \t [0.84542477 0.40008089 0.98842733]. \t  1.7162758555943898 \t 3.3592035367884128\n",
      "76     \t [0.2030483  0.60445376 0.6118741 ]. \t  2.008738081126147 \t 3.3592035367884128\n",
      "77     \t [0.83960497 0.7178898  0.51860813]. \t  0.5713437535535434 \t 3.3592035367884128\n",
      "78     \t [0.52897869 0.83294216 0.34163272]. \t  0.4510873530384205 \t 3.3592035367884128\n",
      "79     \t [0.33198665 0.15757304 0.07503607]. \t  0.32320648564614657 \t 3.3592035367884128\n",
      "80     \t [0.30448895 0.14236495 0.57839827]. \t  0.26961305933473323 \t 3.3592035367884128\n",
      "81     \t [0.45449509 0.58272908 0.81921715]. \t  \u001b[92m3.6973610072566836\u001b[0m \t 3.6973610072566836\n",
      "82     \t [0.5376522  0.29351357 0.06986797]. \t  0.2088508193654714 \t 3.6973610072566836\n",
      "83     \t [0.51902978 0.69461751 0.64907472]. \t  1.8627687898864986 \t 3.6973610072566836\n",
      "84     \t [0.15183179 0.68326864 0.73319422]. \t  2.740138846743 \t 3.6973610072566836\n",
      "85     \t [0.91910486 0.13245643 0.04176584]. \t  0.08746406265672467 \t 3.6973610072566836\n",
      "86     \t [0.43083179 0.01945127 0.15059802]. \t  0.597366371313457 \t 3.6973610072566836\n",
      "87     \t [0.88177672 0.84891331 0.85058481]. \t  1.6023093149208276 \t 3.6973610072566836\n",
      "88     \t [0.7690034  0.94956963 0.54386002]. \t  0.7981680382972645 \t 3.6973610072566836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.55206396 0.208606   0.5917476 ]. \t  0.40163011399776744 \t 3.6973610072566836\n",
      "90     \t [0.59454665 0.45537968 0.58468917]. \t  0.8630606019833844 \t 3.6973610072566836\n",
      "91     \t [0.40142707 0.84353954 0.4657078 ]. \t  1.8342725319658606 \t 3.6973610072566836\n",
      "92     \t [0.84081615 0.99488463 0.97315247]. \t  0.41314251161269006 \t 3.6973610072566836\n",
      "93     \t [0.67826294 0.93066493 0.87659274]. \t  0.9976574216271149 \t 3.6973610072566836\n",
      "94     \t [0.26765537 0.93506926 0.49235001]. \t  2.397794385279614 \t 3.6973610072566836\n",
      "95     \t [0.03870306 0.76250036 0.17795996]. \t  0.04580458941435675 \t 3.6973610072566836\n",
      "96     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.6973610072566836\n",
      "97     \t [0.37566745 0.03245529 0.93029623]. \t  0.22800922636108983 \t 3.6973610072566836\n",
      "98     \t [0.02696673 0.47445665 0.50263357]. \t  0.7518362739769882 \t 3.6973610072566836\n",
      "99     \t [0.32353418 0.39124557 0.82877661]. \t  3.019316440408473 \t 3.6973610072566836\n",
      "100    \t [0.13645963 0.68909868 0.32313889]. \t  0.45748233319260295 \t 3.6973610072566836\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [-5.55111512e-17  0.00000000e+00  1.00000000e+00]. \t  0.0902894676548261 \t 0.5647137279144399\n",
      "2      \t [0.79494645 0.57685733 1.        ]. \t  \u001b[92m2.0500741560229585\u001b[0m \t 2.0500741560229585\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.0500741560229585\n",
      "4      \t [0.1838557  0.72889059 1.        ]. \t  1.612792937063825 \t 2.0500741560229585\n",
      "5      \t [1.         0.24116425 1.        ]. \t  0.7117539812500708 \t 2.0500741560229585\n",
      "6      \t [0.51982552 0.86276163 1.        ]. \t  0.8851593253247967 \t 2.0500741560229585\n",
      "7      \t [1.         0.59103723 0.68705652]. \t  1.6932203094470708 \t 2.0500741560229585\n",
      "8      \t [0.         0.48177945 0.83149985]. \t  \u001b[92m3.6006259887914767\u001b[0m \t 3.6006259887914767\n",
      "9      \t [0.         0.69475383 0.64664047]. \t  2.449717917297722 \t 3.6006259887914767\n",
      "10     \t [0.         0.2546679  0.60188673]. \t  0.550219489957464 \t 3.6006259887914767\n",
      "11     \t [0.35765403 0.56390796 0.72466151]. \t  2.74173298897227 \t 3.6006259887914767\n",
      "12     \t [0.16598432 0.37477662 1.        ]. \t  1.4448190607174887 \t 3.6006259887914767\n",
      "13     \t [0.69549163 0.70204241 0.69312249]. \t  1.7522438198738801 \t 3.6006259887914767\n",
      "14     \t [0.80629046 0.39055863 0.68169943]. \t  1.586450834247917 \t 3.6006259887914767\n",
      "15     \t [1.         0.64086081 1.        ]. \t  1.904648354218178 \t 3.6006259887914767\n",
      "16     \t [1.         0.47773991 0.        ]. \t  0.009666546456399572 \t 3.6006259887914767\n",
      "17     \t [0.         0.73733936 0.85601982]. \t  2.8331139420258733 \t 3.6006259887914767\n",
      "18     \t [0.         0.46746188 1.        ]. \t  1.8650891865367123 \t 3.6006259887914767\n",
      "19     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.6006259887914767\n",
      "20     \t [1.        0.        0.6574404]. \t  0.1506754835931155 \t 3.6006259887914767\n",
      "21     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.6006259887914767\n",
      "22     \t [0.42745208 0.61055108 0.        ]. \t  0.01026480093572185 \t 3.6006259887914767\n",
      "23     \t [0.16816186 0.73019399 0.75311651]. \t  2.6372399121582495 \t 3.6006259887914767\n",
      "24     \t [0.12685762 0.37048938 0.76972391]. \t  2.513306320563275 \t 3.6006259887914767\n",
      "25     \t [0.6435829 0.        1.       ]. \t  0.09118953596901545 \t 3.6006259887914767\n",
      "26     \t [0.         0.52761646 0.68795763]. \t  2.292526411593663 \t 3.6006259887914767\n",
      "27     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.6006259887914767\n",
      "28     \t [0.17609027 0.         0.        ]. \t  0.09145630839388462 \t 3.6006259887914767\n",
      "29     \t [0.         0.         0.78941949]. \t  0.24716134619173563 \t 3.6006259887914767\n",
      "30     \t [0.         1.         0.67790516]. \t  1.7242272820534101 \t 3.6006259887914767\n",
      "31     \t [1.         0.77614935 0.34424128]. \t  0.07148615548920431 \t 3.6006259887914767\n",
      "32     \t [0.51670581 0.         0.        ]. \t  0.09576026477457089 \t 3.6006259887914767\n",
      "33     \t [0.49873788 0.50879429 0.88589919]. \t  \u001b[92m3.660585735587903\u001b[0m \t 3.660585735587903\n",
      "34     \t [0.43296316 0.59949777 0.85024551]. \t  \u001b[92m3.7764610350589756\u001b[0m \t 3.7764610350589756\n",
      "35     \t [1.         0.80713117 0.76896867]. \t  1.500114315063671 \t 3.7764610350589756\n",
      "36     \t [0.         1.         0.45584492]. \t  1.842056728915303 \t 3.7764610350589756\n",
      "37     \t [0.58854266 0.41900004 0.92397121]. \t  2.761297921200891 \t 3.7764610350589756\n",
      "38     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.7764610350589756\n",
      "39     \t [0.99999996 0.14769045 0.23244979]. \t  0.2892880727079052 \t 3.7764610350589756\n",
      "40     \t [0.         0.28095864 0.        ]. \t  0.05958079787362375 \t 3.7764610350589756\n",
      "41     \t [0.24040065 0.5979342  0.93973021]. \t  3.103387595412122 \t 3.7764610350589756\n",
      "42     \t [0.00757474 0.2833448  0.87186133]. \t  1.886878831923578 \t 3.7764610350589756\n",
      "43     \t [0.59794913 0.64044844 0.88919806]. \t  3.466907514378163 \t 3.7764610350589756\n",
      "44     \t [0.2983901  0.67222964 0.85834639]. \t  3.4077569257072478 \t 3.7764610350589756\n",
      "45     \t [0.49639723 0.63476874 0.90527985]. \t  3.394047631822924 \t 3.7764610350589756\n",
      "46     \t [0.45411088 0.59508763 0.96650425]. \t  2.671951945047823 \t 3.7764610350589756\n",
      "47     \t [0.89945758 0.51299447 0.85801683]. \t  3.6685791409780704 \t 3.7764610350589756\n",
      "48     \t [9.92134536e-13 9.48094719e-12 1.60393366e-01]. \t  0.41148296396972533 \t 3.7764610350589756\n",
      "49     \t [0.77969679 0.         0.21192876]. \t  0.4794614569090074 \t 3.7764610350589756\n",
      "50     \t [0.25969364 0.52902961 0.84973823]. \t  \u001b[92m3.8369322853351155\u001b[0m \t 3.8369322853351155\n",
      "51     \t [0.82785936 0.60520794 0.9016069 ]. \t  3.466015851127091 \t 3.8369322853351155\n",
      "52     \t [0.06911114 0.52506993 0.89631325]. \t  3.602499283983059 \t 3.8369322853351155\n",
      "53     \t [0.97243816 0.5286737  0.88156692]. \t  3.6026838385238253 \t 3.8369322853351155\n",
      "54     \t [0.95283373 0.50594766 0.85696194]. \t  3.6272277168104434 \t 3.8369322853351155\n",
      "55     \t [0.78036604 0.99056094 0.22594255]. \t  0.02670889612629775 \t 3.8369322853351155\n",
      "56     \t [0.83670017 0.69925873 0.89154338]. \t  2.978030687568044 \t 3.8369322853351155\n",
      "57     \t [0.36291565 0.60634218 0.88231056]. \t  3.6924544247186954 \t 3.8369322853351155\n",
      "58     \t [0.00812068 0.59682319 0.86887982]. \t  3.73270157983922 \t 3.8369322853351155\n",
      "59     \t [0.28321513 0.50327565 0.86146394]. \t  3.7581642824722348 \t 3.8369322853351155\n",
      "60     \t [0.20694285 0.51186598 0.85567081]. \t  3.789677047435778 \t 3.8369322853351155\n",
      "61     \t [0.18432046 0.49270155 0.86234314]. \t  3.7051817907753026 \t 3.8369322853351155\n",
      "62     \t [0.72461498 0.55008104 0.89588223]. \t  3.6202775977168518 \t 3.8369322853351155\n",
      "63     \t [0.34772657 0.63526902 0.87279512]. \t  3.6090340015861706 \t 3.8369322853351155\n",
      "64     \t [0.7047889  0.9977624  0.87487367]. \t  0.5963297208656796 \t 3.8369322853351155\n",
      "65     \t [0.27864228 0.58777322 0.84344024]. \t  3.8137286436938918 \t 3.8369322853351155\n",
      "66     \t [0.237519   0.62020935 0.84958679]. \t  3.7132381363798017 \t 3.8369322853351155\n",
      "67     \t [0.99498479 0.31728113 0.87472128]. \t  2.1579169025183687 \t 3.8369322853351155\n",
      "68     \t [0.34688249 0.46735149 0.86771699]. \t  3.5705717541078696 \t 3.8369322853351155\n",
      "69     \t [0.9446464  0.64162722 0.90109439]. \t  3.2725071171891273 \t 3.8369322853351155\n",
      "70     \t [0.30795902 0.54800624 0.81426055]. \t  3.7238754329067545 \t 3.8369322853351155\n",
      "71     \t [0.04278874 0.48416486 0.85163365]. \t  3.6523364322217025 \t 3.8369322853351155\n",
      "72     \t [0.19286379 0.66155319 0.83075219]. \t  3.445395577630209 \t 3.8369322853351155\n",
      "73     \t [0.10343915 0.64261531 0.86607861]. \t  3.5728676991924284 \t 3.8369322853351155\n",
      "74     \t [0.0937828  0.61594445 0.87005908]. \t  3.6884017797912274 \t 3.8369322853351155\n",
      "75     \t [0.74662635 0.58954708 0.84542765]. \t  3.7070140790356243 \t 3.8369322853351155\n",
      "76     \t [0.20972273 0.593682   0.82532506]. \t  3.735928122304564 \t 3.8369322853351155\n",
      "77     \t [0.23863015 0.59204473 0.86968537]. \t  3.7874536955201155 \t 3.8369322853351155\n",
      "78     \t [0.16027871 0.55694124 0.83317539]. \t  3.816515069930331 \t 3.8369322853351155\n",
      "79     \t [0.4355375  0.54754359 0.89048253]. \t  3.713800285982695 \t 3.8369322853351155\n",
      "80     \t [0.12126238 0.59471442 0.86922891]. \t  3.766855334801764 \t 3.8369322853351155\n",
      "81     \t [0.54758237 0.57382111 0.863957  ]. \t  3.8097549038613305 \t 3.8369322853351155\n",
      "82     \t [0.52337678 0.572788   0.9018212 ]. \t  3.6059766250623935 \t 3.8369322853351155\n",
      "83     \t [0.16394604 0.61591689 0.85099492]. \t  3.726265183042934 \t 3.8369322853351155\n",
      "84     \t [0.01540982 1.         0.2192624 ]. \t  0.08535586462185442 \t 3.8369322853351155\n",
      "85     \t [0.33823368 0.00296626 0.16657811]. \t  0.6458546360989943 \t 3.8369322853351155\n",
      "86     \t [0.26054219 0.62774684 0.85186876]. \t  3.6802344143886963 \t 3.8369322853351155\n",
      "87     \t [0.25538088 0.52097289 0.86034069]. \t  3.8124609993444105 \t 3.8369322853351155\n",
      "88     \t [0.04526332 0.53483161 0.79514675]. \t  3.537975140431649 \t 3.8369322853351155\n",
      "89     \t [0.12867569 0.6140194  0.89088936]. \t  3.596919807667831 \t 3.8369322853351155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.22236928 0.55235116 0.84878743]. \t  \u001b[92m3.856926445186091\u001b[0m \t 3.856926445186091\n",
      "91     \t [0.46642822 0.53798483 0.8852377 ]. \t  3.7372490242008842 \t 3.856926445186091\n",
      "92     \t [0.45347071 0.4992793  0.84919542]. \t  3.7471969204788125 \t 3.856926445186091\n",
      "93     \t [0.12790901 0.59730336 0.79982838]. \t  3.5528113482880306 \t 3.856926445186091\n",
      "94     \t [0.32250898 0.45224763 0.8026916 ]. \t  3.3443149127921905 \t 3.856926445186091\n",
      "95     \t [0.29219946 0.54066445 0.81482888]. \t  3.725162896747671 \t 3.856926445186091\n",
      "96     \t [0.26788769 0.57363023 0.85040279]. \t  3.8481037466355423 \t 3.856926445186091\n",
      "97     \t [0.82531367 0.52460042 0.84448228]. \t  3.712192099793732 \t 3.856926445186091\n",
      "98     \t [0.17207363 0.51920499 0.85439072]. \t  3.8064695259832257 \t 3.856926445186091\n",
      "99     \t [0.30441131 0.60621249 0.88492253]. \t  3.6794957935019257 \t 3.856926445186091\n",
      "100    \t [0.44711506 0.60559085 0.85921244]. \t  3.7553093492139897 \t 3.856926445186091\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [0.         1.         0.01930879]. \t  0.0004972811221163767 \t 1.9592421489197056\n",
      "2      \t [0.34155951 0.37570107 0.83160864]. \t  \u001b[92m2.885466839387301\u001b[0m \t 2.885466839387301\n",
      "3      \t [0.7603709 0.        1.       ]. \t  0.09054620424022429 \t 2.885466839387301\n",
      "4      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.885466839387301\n",
      "5      \t [0.46915575 0.56022262 1.        ]. \t  2.085330991508265 \t 2.885466839387301\n",
      "6      \t [0.         0.54374642 0.8117295 ]. \t  \u001b[92m3.6608037824648068\u001b[0m \t 3.6608037824648068\n",
      "7      \t [1.         0.33060953 0.36969732]. \t  0.15257416969683815 \t 3.6608037824648068\n",
      "8      \t [0.         0.54341267 1.        ]. \t  2.0460766779438724 \t 3.6608037824648068\n",
      "9      \t [0.         0.18634538 0.53585842]. \t  0.23918680876845355 \t 3.6608037824648068\n",
      "10     \t [1.         0.70327981 1.        ]. \t  1.6746000124634732 \t 3.6608037824648068\n",
      "11     \t [0.         0.85174815 0.5313975 ]. \t  2.8965603727050455 \t 3.6608037824648068\n",
      "12     \t [0.88433483 0.33178172 0.        ]. \t  0.03331551368880472 \t 3.6608037824648068\n",
      "13     \t [0.         0.61586397 0.38651747]. \t  0.684656255023168 \t 3.6608037824648068\n",
      "14     \t [1.         0.35920089 1.        ]. \t  1.3155173373766043 \t 3.6608037824648068\n",
      "15     \t [0.66807871 0.43143243 0.67120074]. \t  1.6301876277978304 \t 3.6608037824648068\n",
      "16     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.6608037824648068\n",
      "17     \t [0.         1.         0.63919173]. \t  2.087646591074984 \t 3.6608037824648068\n",
      "18     \t [0.28152043 0.         0.        ]. \t  0.09993123814664243 \t 3.6608037824648068\n",
      "19     \t [0.         0.77256785 0.80862433]. \t  2.4710399341482976 \t 3.6608037824648068\n",
      "20     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.6608037824648068\n",
      "21     \t [0.14208157 1.         0.40746804]. \t  1.3300495492622728 \t 3.6608037824648068\n",
      "22     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.6608037824648068\n",
      "23     \t [1.         0.68461922 0.23127582]. \t  0.020098581527145967 \t 3.6608037824648068\n",
      "24     \t [0.17057889 0.56892966 0.67680109]. \t  2.3181462611760386 \t 3.6608037824648068\n",
      "25     \t [0.57639064 0.76575798 0.        ]. \t  0.0016675307769606666 \t 3.6608037824648068\n",
      "26     \t [0.64243816 0.973641   0.98233611]. \t  0.47067236430591375 \t 3.6608037824648068\n",
      "27     \t [4.40966048e-08 2.65854548e-01 8.47656329e-01]. \t  1.782766685038717 \t 3.6608037824648068\n",
      "28     \t [0.47795091 0.         0.44653215]. \t  0.3292750121498166 \t 3.6608037824648068\n",
      "29     \t [0.65852044 0.33767986 0.18480943]. \t  0.3906986553651334 \t 3.6608037824648068\n",
      "30     \t [0.51496826 0.79027397 0.78346091]. \t  2.1065888169082414 \t 3.6608037824648068\n",
      "31     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.6608037824648068\n",
      "32     \t [1.         0.57227269 0.80287132]. \t  3.355287966151352 \t 3.6608037824648068\n",
      "33     \t [0.44741975 0.25057239 0.0020423 ]. \t  0.09948834535571728 \t 3.6608037824648068\n",
      "34     \t [0.36096356 0.02464945 1.        ]. \t  0.11941830054634542 \t 3.6608037824648068\n",
      "35     \t [1.         0.81607447 0.78092962]. \t  1.5212999014345707 \t 3.6608037824648068\n",
      "36     \t [1.         0.2705475  0.83271514]. \t  1.8019067968078417 \t 3.6608037824648068\n",
      "37     \t [0.66777497 0.99831002 0.09312717]. \t  0.0018157896568951044 \t 3.6608037824648068\n",
      "38     \t [8.22910510e-12 0.00000000e+00 2.62921876e-01]. \t  0.5795776272366936 \t 3.6608037824648068\n",
      "39     \t [0.28976058 0.00064687 0.23567666]. \t  0.8318758677457484 \t 3.6608037824648068\n",
      "40     \t [0.21480675 0.58207229 0.91665983]. \t  3.4426582362204985 \t 3.6608037824648068\n",
      "41     \t [0.14815742 0.55355534 0.84428545]. \t  \u001b[92m3.8429263391364796\u001b[0m \t 3.8429263391364796\n",
      "42     \t [1.00000000e+00 1.00000000e+00 4.21936544e-12]. \t  3.772718518368047e-05 \t 3.8429263391364796\n",
      "43     \t [0.08482858 0.82164654 0.49044133]. \t  2.6127388005142196 \t 3.8429263391364796\n",
      "44     \t [0.29782257 0.93516861 0.80293631]. \t  1.1952302694446941 \t 3.8429263391364796\n",
      "45     \t [0.33031573 0.36071192 0.87255146]. \t  2.678002114508356 \t 3.8429263391364796\n",
      "46     \t [0.43892031 0.77559639 0.60869333]. \t  2.1548089273312074 \t 3.8429263391364796\n",
      "47     \t [0.59205156 0.12922306 0.02366767]. \t  0.14491954357998088 \t 3.8429263391364796\n",
      "48     \t [0.77005454 0.80724671 0.03755781]. \t  0.001334095490997802 \t 3.8429263391364796\n",
      "49     \t [0.82214804 0.64135967 0.26016528]. \t  0.06292901647702147 \t 3.8429263391364796\n",
      "50     \t [0.28278184 0.04304465 0.36677558]. \t  0.6906797046127917 \t 3.8429263391364796\n",
      "51     \t [0.08966898 0.76237058 0.22546117]. \t  0.11425353032662568 \t 3.8429263391364796\n",
      "52     \t [0.91817613 0.35335043 0.51417888]. \t  0.24446144235633818 \t 3.8429263391364796\n",
      "53     \t [0.78613408 0.08167713 0.18378513]. \t  0.47525089404355164 \t 3.8429263391364796\n",
      "54     \t [0.29687815 0.87933742 0.87351696]. \t  1.4831575364258622 \t 3.8429263391364796\n",
      "55     \t [0.57568106 0.3194464  0.23303029]. \t  0.5669965466379389 \t 3.8429263391364796\n",
      "56     \t [0.25056999 0.23593882 0.47352493]. \t  0.33300666000299306 \t 3.8429263391364796\n",
      "57     \t [0.95419104 0.99391944 0.31777697]. \t  0.05673756586560111 \t 3.8429263391364796\n",
      "58     \t [0.68517702 0.04633665 0.7821367 ]. \t  0.3828038218861167 \t 3.8429263391364796\n",
      "59     \t [0.94862696 0.48475204 0.55087411]. \t  0.4477503855455977 \t 3.8429263391364796\n",
      "60     \t [0.60021514 0.45057802 0.73447275]. \t  2.547829917373072 \t 3.8429263391364796\n",
      "61     \t [0.00090387 0.86496889 0.4848036 ]. \t  2.5229968616770475 \t 3.8429263391364796\n",
      "62     \t [0.93209321 0.46337054 0.42189103]. \t  0.13049462361406547 \t 3.8429263391364796\n",
      "63     \t [0.85694658 0.84204688 0.7950423 ]. \t  1.4789700227686162 \t 3.8429263391364796\n",
      "64     \t [0.50397148 0.28079605 0.61976295]. \t  0.7173923433527241 \t 3.8429263391364796\n",
      "65     \t [0.35384538 0.30971668 0.65307572]. \t  1.0820119963262353 \t 3.8429263391364796\n",
      "66     \t [0.08100972 0.82894135 0.30922576]. \t  0.48637866568268806 \t 3.8429263391364796\n",
      "67     \t [0.01992128 0.15352265 0.25167697]. \t  0.6809044052266158 \t 3.8429263391364796\n",
      "68     \t [0.03434399 0.61503807 0.84049126]. \t  3.6869648987519668 \t 3.8429263391364796\n",
      "69     \t [0.05458561 0.38780484 0.02342249]. \t  0.06002266829451044 \t 3.8429263391364796\n",
      "70     \t [0.93801884 0.55742704 0.78516778]. \t  3.2134915628582204 \t 3.8429263391364796\n",
      "71     \t [0.34682653 0.0185079  0.84163519]. \t  0.2873547834147019 \t 3.8429263391364796\n",
      "72     \t [0.32959855 0.53863404 0.749242  ]. \t  3.0476588816382284 \t 3.8429263391364796\n",
      "73     \t [0.32780521 0.65929316 0.18393973]. \t  0.06929067415136254 \t 3.8429263391364796\n",
      "74     \t [0.65181106 0.60427478 0.28076896]. \t  0.13660250827562725 \t 3.8429263391364796\n",
      "75     \t [0.57431136 0.18655666 0.37108816]. \t  0.6173683076972666 \t 3.8429263391364796\n",
      "76     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8429263391364796\n",
      "77     \t [0.2200399  0.55184545 0.11254448]. \t  0.07176865340649807 \t 3.8429263391364796\n",
      "78     \t [0.79111777 0.49416059 0.83011944]. \t  3.600228146474148 \t 3.8429263391364796\n",
      "79     \t [0.72749109 0.29778109 0.07816875]. \t  0.16771807928609878 \t 3.8429263391364796\n",
      "80     \t [0.22225422 0.2908785  0.13896978]. \t  0.42332323504745456 \t 3.8429263391364796\n",
      "81     \t [0.75221482 0.35727869 0.83627775]. \t  2.6754626083668027 \t 3.8429263391364796\n",
      "82     \t [0.26462091 0.55019761 0.07359942]. \t  0.049031538521092516 \t 3.8429263391364796\n",
      "83     \t [0.47760278 0.03970655 0.12418723]. \t  0.4918340278072209 \t 3.8429263391364796\n",
      "84     \t [0.7042768  0.90279546 0.76947643]. \t  1.0802503344142702 \t 3.8429263391364796\n",
      "85     \t [0.93029315 0.72624329 0.18884353]. \t  0.013666408812849307 \t 3.8429263391364796\n",
      "86     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8429263391364796\n",
      "87     \t [0.97303439 0.44865581 0.9133195 ]. \t  2.998946085166498 \t 3.8429263391364796\n",
      "88     \t [0.79383933 0.96940233 0.75134196]. \t  0.6416687241195785 \t 3.8429263391364796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.07821525 0.28395456 0.59400979]. \t  0.5962106261528902 \t 3.8429263391364796\n",
      "90     \t [0.14238379 0.23918222 0.75471493]. \t  1.3953125447827626 \t 3.8429263391364796\n",
      "91     \t [0.12741762 0.71388527 0.48090979]. \t  2.0366496686570406 \t 3.8429263391364796\n",
      "92     \t [0.33022336 0.20393715 0.98524202]. \t  0.65687325947177 \t 3.8429263391364796\n",
      "93     \t [0.25767767 0.32707301 0.11423834]. \t  0.30731655934584096 \t 3.8429263391364796\n",
      "94     \t [0.83405762 0.85900289 0.44034565]. \t  0.4278160476836366 \t 3.8429263391364796\n",
      "95     \t [0.8249504  0.00771471 0.78485179]. \t  0.2666028021885188 \t 3.8429263391364796\n",
      "96     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8429263391364796\n",
      "97     \t [0.67750114 0.93713587 0.59103589]. \t  1.1358764267851724 \t 3.8429263391364796\n",
      "98     \t [0.17078721 0.27754479 0.64585559]. \t  0.9021637496207571 \t 3.8429263391364796\n",
      "99     \t [0.67232233 0.97100767 0.99615183]. \t  0.4304963204316718 \t 3.8429263391364796\n",
      "100    \t [0.89326156 0.34109651 0.11284493]. \t  0.1297525405107873 \t 3.8429263391364796\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.69508366]. \t  \u001b[92m1.5604271401562613\u001b[0m \t 1.5604271401562613\n",
      "2      \t [0.09725601 0.63869161 1.        ]. \t  \u001b[92m1.9719737923742677\u001b[0m \t 1.9719737923742677\n",
      "3      \t [0.         0.5023043  0.80190782]. \t  \u001b[92m3.5129476768802768\u001b[0m \t 3.5129476768802768\n",
      "4      \t [0.         0.00460562 0.49501578]. \t  0.14499425740251567 \t 3.5129476768802768\n",
      "5      \t [0.         0.71550468 0.36676047]. \t  0.7998184353572142 \t 3.5129476768802768\n",
      "6      \t [0.         0.13436464 1.        ]. \t  0.3310281445762389 \t 3.5129476768802768\n",
      "7      \t [1.         0.31851382 0.80546923]. \t  2.196489814064805 \t 3.5129476768802768\n",
      "8      \t [1.         1.         0.56521627]. \t  0.2658330099407987 \t 3.5129476768802768\n",
      "9      \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.5129476768802768\n",
      "10     \t [1.         0.54359029 1.        ]. \t  1.9978395219217062 \t 3.5129476768802768\n",
      "11     \t [1.         0.67118616 0.        ]. \t  0.0016638135309986078 \t 3.5129476768802768\n",
      "12     \t [1.         0.60032217 0.64464407]. \t  1.1466235204553663 \t 3.5129476768802768\n",
      "13     \t [0.84354617 1.         0.        ]. \t  7.413244622609359e-05 \t 3.5129476768802768\n",
      "14     \t [ 1.00000000e+00  0.00000000e+00 -1.11022302e-16]. \t  0.030954717033005095 \t 3.5129476768802768\n",
      "15     \t [0.75847368 0.38923686 1.        ]. \t  1.508752286848405 \t 3.5129476768802768\n",
      "16     \t [0.         0.746742   0.90175539]. \t  2.5763992296587412 \t 3.5129476768802768\n",
      "17     \t [0.         0.37079817 0.59711482]. \t  0.8635229544301204 \t 3.5129476768802768\n",
      "18     \t [-1.38777878e-17  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 3.5129476768802768\n",
      "19     \t [1.         0.         0.53500829]. \t  0.06785758401925182 \t 3.5129476768802768\n",
      "20     \t [0.20822087 1.         0.38624566]. \t  1.0595863447015943 \t 3.5129476768802768\n",
      "21     \t [0.55932983 0.         0.        ]. \t  0.09170683172348014 \t 3.5129476768802768\n",
      "22     \t [0.13408991 0.68846997 0.75729962]. \t  2.8635569962302876 \t 3.5129476768802768\n",
      "23     \t [0.         1.         0.37864454]. \t  0.973331290923864 \t 3.5129476768802768\n",
      "24     \t [-1.11022302e-16  0.00000000e+00  0.00000000e+00]. \t  0.06797411659013226 \t 3.5129476768802768\n",
      "25     \t [0.47379715 0.81984258 0.        ]. \t  0.001003401895604593 \t 3.5129476768802768\n",
      "26     \t [6.75608857e-09 3.70491115e-01 1.00000000e+00]. \t  1.4070846893163225 \t 3.5129476768802768\n",
      "27     \t [9.18620168e-09 0.00000000e+00 8.18813857e-01]. \t  0.24422414908238654 \t 3.5129476768802768\n",
      "28     \t [1.         0.31401713 0.        ]. \t  0.024078205139326393 \t 3.5129476768802768\n",
      "29     \t [0.32318748 0.50912733 0.90172814]. \t  \u001b[92m3.5393798927569957\u001b[0m \t 3.5393798927569957\n",
      "30     \t [0.39710523 0.70112469 0.92738229]. \t  2.7612448846461395 \t 3.5393798927569957\n",
      "31     \t [0.56907216 1.         0.22867115]. \t  0.05616650455868074 \t 3.5393798927569957\n",
      "32     \t [1.         1.         0.95275486]. \t  0.4371254165717936 \t 3.5393798927569957\n",
      "33     \t [0.31205983 0.23322101 1.        ]. \t  0.7015332116372851 \t 3.5393798927569957\n",
      "34     \t [0.74118804 0.51649725 0.84312219]. \t  \u001b[92m3.723526067387509\u001b[0m \t 3.723526067387509\n",
      "35     \t [0.8326493  0.64220408 0.9356974 ]. \t  2.944570741104587 \t 3.723526067387509\n",
      "36     \t [8.07487818e-01 0.00000000e+00 2.82462220e-09]. \t  0.05741521803429732 \t 3.723526067387509\n",
      "37     \t [0.1604721  0.40986701 0.8488245 ]. \t  3.1797814498443584 \t 3.723526067387509\n",
      "38     \t [0.59729613 0.59103355 0.79494808]. \t  3.417988208144956 \t 3.723526067387509\n",
      "39     \t [0.78447667 0.3410415  0.75397907]. \t  2.1144190504871396 \t 3.723526067387509\n",
      "40     \t [0.         0.73345548 0.732408  ]. \t  2.521307942779781 \t 3.723526067387509\n",
      "41     \t [0.03563184 0.99777013 0.91236659]. \t  0.5846614350563535 \t 3.723526067387509\n",
      "42     \t [0.49697738 0.52165524 0.87938773]. \t  \u001b[92m3.737105034544241\u001b[0m \t 3.737105034544241\n",
      "43     \t [0.89090209 0.5191714  0.86250249]. \t  3.6819726868169345 \t 3.737105034544241\n",
      "44     \t [0.99999997 1.         0.15242184]. \t  0.001923745168150364 \t 3.737105034544241\n",
      "45     \t [3.15540493e-02 5.91917675e-09 1.67090336e-01]. \t  0.45862670978499437 \t 3.737105034544241\n",
      "46     \t [0.39490507 0.49868253 0.81993836]. \t  3.662127348893379 \t 3.737105034544241\n",
      "47     \t [0.20841785 0.59175193 0.82559436]. \t  \u001b[92m3.7420933289386245\u001b[0m \t 3.7420933289386245\n",
      "48     \t [0.60007901 0.60582936 0.87155346]. \t  3.700699892319483 \t 3.7420933289386245\n",
      "49     \t [7.29626545e-01 5.35441933e-01 1.24080694e-08]. \t  0.013806162606305643 \t 3.7420933289386245\n",
      "50     \t [0.44551088 0.49851964 0.87022012]. \t  3.7104529891556943 \t 3.7420933289386245\n",
      "51     \t [0.73590812 0.59280829 0.8265764 ]. \t  3.6179550161420817 \t 3.7420933289386245\n",
      "52     \t [0.75564259 0.51640259 0.83975844]. \t  3.7098914909382468 \t 3.7420933289386245\n",
      "53     \t [0.22872296 0.58084762 0.89228037]. \t  3.686909788732777 \t 3.7420933289386245\n",
      "54     \t [0.16710845 0.5536774  0.88165761]. \t  \u001b[92m3.767324224889898\u001b[0m \t 3.767324224889898\n",
      "55     \t [0.6917764  0.57990414 0.83068504]. \t  3.6931976766216117 \t 3.767324224889898\n",
      "56     \t [0.70349766 0.54315535 0.86095189]. \t  \u001b[92m3.7846191840993066\u001b[0m \t 3.7846191840993066\n",
      "57     \t [0.203575   0.55628316 0.86840926]. \t  \u001b[92m3.8319440229197372\u001b[0m \t 3.8319440229197372\n",
      "58     \t [0.45251405 0.53069194 0.84603749]. \t  3.826420177848922 \t 3.8319440229197372\n",
      "59     \t [0.         0.56886329 0.79775385]. \t  3.5480942405190388 \t 3.8319440229197372\n",
      "60     \t [0.19420749 0.55732662 0.81099292]. \t  3.7019515227823594 \t 3.8319440229197372\n",
      "61     \t [0.46034125 0.53379066 0.88082437]. \t  3.7581602331884514 \t 3.8319440229197372\n",
      "62     \t [0.8249072  0.51898016 0.83829651]. \t  3.6855745075752595 \t 3.8319440229197372\n",
      "63     \t [0.73338877 0.56128897 0.80457053]. \t  3.514061397825079 \t 3.8319440229197372\n",
      "64     \t [0.09126638 0.53342651 0.86282355]. \t  3.8083394459995077 \t 3.8319440229197372\n",
      "65     \t [0.40204628 0.56808517 0.83665571]. \t  3.81886655318241 \t 3.8319440229197372\n",
      "66     \t [0.66655938 0.56624842 0.85422732]. \t  3.7931328612171007 \t 3.8319440229197372\n",
      "67     \t [0.63849938 0.50296667 0.90707924]. \t  3.435644596597211 \t 3.8319440229197372\n",
      "68     \t [0.8128187  0.54964688 0.86170033]. \t  3.7493543819827906 \t 3.8319440229197372\n",
      "69     \t [0.99021145 0.57173257 0.85201384]. \t  3.6560890252672635 \t 3.8319440229197372\n",
      "70     \t [1.         0.61421913 0.89723894]. \t  3.4023049493929403 \t 3.8319440229197372\n",
      "71     \t [0.52291315 0.52207886 0.8952701 ]. \t  3.6253489209988987 \t 3.8319440229197372\n",
      "72     \t [0.74378845 0.6014111  0.84037461]. \t  3.654249261841113 \t 3.8319440229197372\n",
      "73     \t [0.24029262 0.5295675  0.84033935]. \t  3.8248574561523787 \t 3.8319440229197372\n",
      "74     \t [0.35867675 0.60216802 0.89856512]. \t  3.592564688822986 \t 3.8319440229197372\n",
      "75     \t [0.84285238 0.52461577 0.84133533]. \t  3.6976913108093443 \t 3.8319440229197372\n",
      "76     \t [0.33080465 0.55713933 0.89117049]. \t  3.7168027104174484 \t 3.8319440229197372\n",
      "77     \t [0.28574165 0.59409854 0.84972207]. \t  3.806316897166872 \t 3.8319440229197372\n",
      "78     \t [0.86656847 0.67688136 0.83064491]. \t  3.119292729517962 \t 3.8319440229197372\n",
      "79     \t [0.21041529 0.55718008 0.86236294]. \t  \u001b[92m3.847951684013574\u001b[0m \t 3.847951684013574\n",
      "80     \t [0.33093161 0.61226635 0.81429751]. \t  3.6055525163642743 \t 3.847951684013574\n",
      "81     \t [0.69942732 0.56800427 0.86485214]. \t  3.775253199520877 \t 3.847951684013574\n",
      "82     \t [0.18723931 0.52728132 0.87270095]. \t  3.7831236259408825 \t 3.847951684013574\n",
      "83     \t [0.38008781 0.52276699 0.89747322]. \t  3.6192987421697254 \t 3.847951684013574\n",
      "84     \t [0.07680728 0.55414959 0.86913338]. \t  3.8063136428681528 \t 3.847951684013574\n",
      "85     \t [0.6251401  0.58748385 0.81282062]. \t  3.580530121875959 \t 3.847951684013574\n",
      "86     \t [0.00395456 0.57140021 0.8547066 ]. \t  3.8031105473606406 \t 3.847951684013574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87     \t [0.48501951 0.58256895 0.83946405]. \t  3.788527126643606 \t 3.847951684013574\n",
      "88     \t [0.26975594 0.51656287 0.88652498]. \t  3.687488990266769 \t 3.847951684013574\n",
      "89     \t [0.01145241 0.64330292 0.80261839]. \t  3.3690139922537727 \t 3.847951684013574\n",
      "90     \t [0.99877645 0.50844151 0.88096875]. \t  3.54517702341603 \t 3.847951684013574\n",
      "91     \t [0.08581628 0.49253797 0.89463015]. \t  3.5102417005988893 \t 3.847951684013574\n",
      "92     \t [0.97796091 0.         0.74670559]. \t  0.2306710952863526 \t 3.847951684013574\n",
      "93     \t [0.73955311 0.57672285 0.83124907]. \t  3.6845544708373774 \t 3.847951684013574\n",
      "94     \t [0.51106569 0.49257359 0.82948561]. \t  3.6687852784225665 \t 3.847951684013574\n",
      "95     \t [0.72370151 0.5154064  0.88829475]. \t  3.62007234293138 \t 3.847951684013574\n",
      "96     \t [0.6545895  0.57688861 0.87168801]. \t  3.760673899425855 \t 3.847951684013574\n",
      "97     \t [0.81409547 0.5939669  0.82395516]. \t  3.5639206426495873 \t 3.847951684013574\n",
      "98     \t [0.00307973 0.51657471 0.82973861]. \t  3.7185670463452465 \t 3.847951684013574\n",
      "99     \t [0.58798124 0.52707906 0.87484075]. \t  3.755688228388352 \t 3.847951684013574\n",
      "100    \t [0.18965355 0.5303132  0.87263533]. \t  3.789762071837187 \t 3.847951684013574\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.53737924 0.         1.        ]. \t  0.09156156531410853 \t 2.5106636917702634\n",
      "2      \t [1.         0.90027464 1.        ]. \t  0.6734906435150673 \t 2.5106636917702634\n",
      "3      \t [0.33011221 0.57001724 1.        ]. \t  2.0877065821628578 \t 2.5106636917702634\n",
      "4      \t [1.         0.41150242 0.56852685]. \t  0.49540299989929754 \t 2.5106636917702634\n",
      "5      \t [0.81572865 0.48454029 1.        ]. \t  1.9152322691493406 \t 2.5106636917702634\n",
      "6      \t [0.81118367 0.         0.48930135]. \t  0.12829727309624908 \t 2.5106636917702634\n",
      "7      \t [0.         0.79900976 1.        ]. \t  1.2182142021528377 \t 2.5106636917702634\n",
      "8      \t [0.06153998 0.45900333 0.81359998]. \t  \u001b[92m3.423365216695202\u001b[0m \t 3.423365216695202\n",
      "9      \t [0.         0.37928489 1.        ]. \t  1.454432392256655 \t 3.423365216695202\n",
      "10     \t [0.         0.56668641 0.38480624]. \t  0.5447049036711948 \t 3.423365216695202\n",
      "11     \t [1.         0.00809039 1.        ]. \t  0.0966036592797123 \t 3.423365216695202\n",
      "12     \t [0.31234187 0.29809728 0.6601504 ]. \t  1.0939448986589673 \t 3.423365216695202\n",
      "13     \t [0.86152092 0.17509316 0.        ]. \t  0.05473535679771781 \t 3.423365216695202\n",
      "14     \t [0.69134389 0.60949487 0.        ]. \t  0.007643209983707185 \t 3.423365216695202\n",
      "15     \t [0.         0.69843157 0.74053021]. \t  2.685191796922912 \t 3.423365216695202\n",
      "16     \t [0.49274787 0.3033747  0.        ]. \t  0.07912266426205035 \t 3.423365216695202\n",
      "17     \t [0.59445903 0.85493409 1.        ]. \t  0.9224867138679894 \t 3.423365216695202\n",
      "18     \t [0.         0.40711617 0.60273019]. \t  1.031558845369061 \t 3.423365216695202\n",
      "19     \t [0.         0.98086542 0.        ]. \t  0.0002973335051031929 \t 3.423365216695202\n",
      "20     \t [0.        0.        0.9198107]. \t  0.17133812679092725 \t 3.423365216695202\n",
      "21     \t [0.        0.6528906 0.       ]. \t  0.004586122844493949 \t 3.423365216695202\n",
      "22     \t [0.38242922 0.         0.72536955]. \t  0.22390853615161793 \t 3.423365216695202\n",
      "23     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.423365216695202\n",
      "24     \t [0.22065142 0.72794461 0.04124964]. \t  0.005697106461232178 \t 3.423365216695202\n",
      "25     \t [0.17571375 0.72453091 0.82039091]. \t  2.939460523413006 \t 3.423365216695202\n",
      "26     \t [0.81692264 0.63697062 0.83421207]. \t  \u001b[92m3.4292943811334275\u001b[0m \t 3.4292943811334275\n",
      "27     \t [0.86941029 0.80090451 0.7782484 ]. \t  1.6949463636839932 \t 3.4292943811334275\n",
      "28     \t [0.0785496  0.60814398 0.92366836]. \t  3.2791500158878986 \t 3.4292943811334275\n",
      "29     \t [0.99955826 0.5294849  0.86169813]. \t  \u001b[92m3.655010225294294\u001b[0m \t 3.655010225294294\n",
      "30     \t [6.69131845e-01 0.00000000e+00 5.77983397e-09]. \t  0.07802036746977072 \t 3.655010225294294\n",
      "31     \t [1.         0.63488461 0.95835447]. \t  2.5985288256404933 \t 3.655010225294294\n",
      "32     \t [0.9825324  0.64119854 0.78975886]. \t  2.941245243152489 \t 3.655010225294294\n",
      "33     \t [0.         0.         0.21859406]. \t  0.5399853673215321 \t 3.655010225294294\n",
      "34     \t [0.18480347 0.31159293 0.9100727 ]. \t  1.9596785779229993 \t 3.655010225294294\n",
      "35     \t [0.56839619 0.60422834 0.88163161]. \t  \u001b[92m3.67483053541043\u001b[0m \t 3.67483053541043\n",
      "36     \t [1.         0.         0.16548837]. \t  0.1934558280115685 \t 3.67483053541043\n",
      "37     \t [0.45190748 0.58145753 0.85799848]. \t  \u001b[92m3.82155418962038\u001b[0m \t 3.82155418962038\n",
      "38     \t [0.         0.60677085 0.87170326]. \t  3.6907097745193305 \t 3.82155418962038\n",
      "39     \t [0.78470965 0.6756411  0.88843313]. \t  3.192622553401318 \t 3.82155418962038\n",
      "40     \t [0.56839806 0.49435939 0.89273463]. \t  3.5460506240260274 \t 3.82155418962038\n",
      "41     \t [0.3852138  0.52344883 0.861765  ]. \t  3.815734497165148 \t 3.82155418962038\n",
      "42     \t [0.4998256  0.68577212 0.87437447]. \t  3.2466803872303664 \t 3.82155418962038\n",
      "43     \t [0.0873075  0.56674799 0.83862259]. \t  3.8138748893376646 \t 3.82155418962038\n",
      "44     \t [0.27569041 0.56127262 0.76789624]. \t  3.2902171557981417 \t 3.82155418962038\n",
      "45     \t [0.05810674 0.51735838 0.85758802]. \t  3.7747063441522646 \t 3.82155418962038\n",
      "46     \t [0.9999994  0.46335835 0.93572866]. \t  2.8027620468908796 \t 3.82155418962038\n",
      "47     \t [0.15336782 0.54256248 0.86222966]. \t  \u001b[92m3.833909933719887\u001b[0m \t 3.833909933719887\n",
      "48     \t [0.79052503 0.55636812 0.86191635]. \t  3.7566603118938224 \t 3.833909933719887\n",
      "49     \t [0.25985524 0.52303957 0.80815827]. \t  3.65689180998727 \t 3.833909933719887\n",
      "50     \t [0.97945179 0.57297862 0.84526062]. \t  3.645125427745423 \t 3.833909933719887\n",
      "51     \t [0.43336844 0.50240724 0.8551916 ]. \t  3.759708983888927 \t 3.833909933719887\n",
      "52     \t [0.37212681 0.51059707 0.8075559 ]. \t  3.617878953483486 \t 3.833909933719887\n",
      "53     \t [0.61695508 0.60090539 0.88475321]. \t  3.658209091454411 \t 3.833909933719887\n",
      "54     \t [0.16184387 0.58901879 0.88645656]. \t  3.7052065499523277 \t 3.833909933719887\n",
      "55     \t [0.33814094 0.5388838  0.87698943]. \t  3.792745909606981 \t 3.833909933719887\n",
      "56     \t [0.01513889 0.53816219 0.91142907]. \t  3.4625544623419366 \t 3.833909933719887\n",
      "57     \t [0.79669618 0.56852016 0.85035016]. \t  3.741442295734716 \t 3.833909933719887\n",
      "58     \t [0.00668549 0.48718495 0.84904449]. \t  3.6560653766697033 \t 3.833909933719887\n",
      "59     \t [0.72109551 0.53680745 0.86749658]. \t  3.76183393569078 \t 3.833909933719887\n",
      "60     \t [0.10845147 0.61945156 0.82283825]. \t  3.6250501958532473 \t 3.833909933719887\n",
      "61     \t [0.36768773 0.55688894 0.82602466]. \t  3.7868598194541505 \t 3.833909933719887\n",
      "62     \t [0.         0.62036731 0.8716549 ]. \t  3.639126371927121 \t 3.833909933719887\n",
      "63     \t [0.12963432 0.55076835 0.85642204]. \t  \u001b[92m3.843266530198156\u001b[0m \t 3.843266530198156\n",
      "64     \t [0.48409497 0.56947054 0.87169188]. \t  3.8073690584112034 \t 3.843266530198156\n",
      "65     \t [0.50206981 0.52974949 0.84959686]. \t  3.820743908935193 \t 3.843266530198156\n",
      "66     \t [0.56971276 0.49422248 0.84479273]. \t  3.7051282407317023 \t 3.843266530198156\n",
      "67     \t [0.38016627 0.61373523 0.86676068]. \t  3.7227598705954343 \t 3.843266530198156\n",
      "68     \t [1.         1.         0.89174128]. \t  0.5333130314127844 \t 3.843266530198156\n",
      "69     \t [0.80618619 0.56061566 0.82739061]. \t  3.6610569234819805 \t 3.843266530198156\n",
      "70     \t [0.74087462 0.6183894  0.8674642 ]. \t  3.6167930029156192 \t 3.843266530198156\n",
      "71     \t [0.60112754 0.49157714 0.84447687]. \t  3.6874788301145314 \t 3.843266530198156\n",
      "72     \t [0.00328616 0.53265249 0.85381374]. \t  3.7937482138783207 \t 3.843266530198156\n",
      "73     \t [0.28667082 0.48010677 0.86208475]. \t  3.655826726209798 \t 3.843266530198156\n",
      "74     \t [0.18871231 0.56982655 0.82247724]. \t  3.7640839941754987 \t 3.843266530198156\n",
      "75     \t [0.98389689 0.64127285 0.85388342]. \t  3.3971648305189674 \t 3.843266530198156\n",
      "76     \t [0.38994469 0.53083799 0.86894033]. \t  3.811626952813171 \t 3.843266530198156\n",
      "77     \t [1.01742907e-09 5.68133291e-01 9.25534971e-01]. \t  3.3012438120882717 \t 3.843266530198156\n",
      "78     \t [0.19159312 0.59681591 0.89878718]. \t  3.597457715056068 \t 3.843266530198156\n",
      "79     \t [0.25753667 0.58244486 0.82938498]. \t  3.781096467648544 \t 3.843266530198156\n",
      "80     \t [0.32808746 0.60432267 0.86560983]. \t  3.763492446307771 \t 3.843266530198156\n",
      "81     \t [0.69477389 0.51406093 0.88047631]. \t  3.6720414781536395 \t 3.843266530198156\n",
      "82     \t [0.67032664 0.63919199 0.86665292]. \t  3.535672078839898 \t 3.843266530198156\n",
      "83     \t [0.89645578 0.5763671  0.86370721]. \t  3.692744239708139 \t 3.843266530198156\n",
      "84     \t [0.         0.47784357 0.84932411]. \t  3.609306614958515 \t 3.843266530198156\n",
      "85     \t [0.50307243 0.49125161 0.86759742]. \t  3.6830469141102085 \t 3.843266530198156\n",
      "86     \t [0.06135247 0.57958923 0.82363057]. \t  3.7329974872563128 \t 3.843266530198156\n",
      "87     \t [0.83895042 0.54465273 0.84289063]. \t  3.721731509521644 \t 3.843266530198156\n",
      "88     \t [0.73662735 0.50880339 0.86011439]. \t  3.7128043671336286 \t 3.843266530198156\n",
      "89     \t [0.1146042  0.6012876  0.80969921]. \t  3.6126162993608557 \t 3.843266530198156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.77763142 0.52331722 0.82462624]. \t  3.6484848866777213 \t 3.843266530198156\n",
      "91     \t [0.61754506 0.51542371 0.86348641]. \t  3.7579744949066933 \t 3.843266530198156\n",
      "92     \t [0.83662438 0.51505061 0.8223202 ]. \t  3.5981122207834533 \t 3.843266530198156\n",
      "93     \t [0.48923693 0.56873296 0.83988858]. \t  3.8120825825083284 \t 3.843266530198156\n",
      "94     \t [0.7617484  0.5209347  0.84756012]. \t  3.734008204680812 \t 3.843266530198156\n",
      "95     \t [0.00883837 0.60135431 0.83189179]. \t  3.700634829907678 \t 3.843266530198156\n",
      "96     \t [0.60226808 0.56527437 0.90002825]. \t  3.6142165519154896 \t 3.843266530198156\n",
      "97     \t [0.39039719 0.55464894 0.85883543]. \t  \u001b[92m3.8544042581810087\u001b[0m \t 3.8544042581810087\n",
      "98     \t [0.26720097 0.53390784 0.82250852]. \t  3.7649593303662954 \t 3.8544042581810087\n",
      "99     \t [7.90488074e-01 6.32434999e-08 8.55525894e-01]. \t  0.22810171215408406 \t 3.8544042581810087\n",
      "100    \t [0.37780814 0.51867438 0.85024124]. \t  3.814026577268402 \t 3.8544042581810087\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.6237282255098657\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
      "3      \t [0.23596815 0.37885455 1.        ]. \t  1.4707632805021857 \t 1.6237282255098657\n",
      "4      \t [0.32644956 0.07071402 0.73633009]. \t  0.4303594727134669 \t 1.6237282255098657\n",
      "5      \t [0.39152734 0.56959173 0.77297457]. \t  \u001b[92m3.3094184194121046\u001b[0m \t 3.3094184194121046\n",
      "6      \t [0.         0.66994134 0.54362528]. \t  2.124240412560207 \t 3.3094184194121046\n",
      "7      \t [1.         0.45516096 0.62515783]. \t  0.9975020773857981 \t 3.3094184194121046\n",
      "8      \t [0.45427104 0.42391662 0.43378784]. \t  0.3866921187547138 \t 3.3094184194121046\n",
      "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.3094184194121046\n",
      "10     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.3094184194121046\n",
      "11     \t [1.         1.         0.63265651]. \t  0.28469885738342154 \t 3.3094184194121046\n",
      "12     \t [0.16490115 0.9741708  0.        ]. \t  0.000329648382280344 \t 3.3094184194121046\n",
      "13     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.3094184194121046\n",
      "14     \t [1.         0.59806293 0.99868159]. \t  2.012930894483042 \t 3.3094184194121046\n",
      "15     \t [0.         0.61883322 0.86844141]. \t  \u001b[92m3.655219709787853\u001b[0m \t 3.655219709787853\n",
      "16     \t [0.71758812 0.54242018 0.        ]. \t  0.013358226647675003 \t 3.655219709787853\n",
      "17     \t [1.         0.         0.81942657]. \t  0.2406288364379533 \t 3.655219709787853\n",
      "18     \t [0.         0.84841509 0.75351493]. \t  1.926462150154312 \t 3.655219709787853\n",
      "19     \t [0.         0.66334543 0.        ]. \t  0.004122205596902012 \t 3.655219709787853\n",
      "20     \t [0.42569101 0.         0.        ]. \t  0.10126207691401816 \t 3.655219709787853\n",
      "21     \t [0.63004889 1.         0.        ]. \t  0.00015014972754275242 \t 3.655219709787853\n",
      "22     \t [0.         0.35393236 0.74223059]. \t  2.131786631016053 \t 3.655219709787853\n",
      "23     \t [0.24064227 0.67585878 1.        ]. \t  1.858618939775221 \t 3.655219709787853\n",
      "24     \t [0.81028345 0.69831695 0.82719253]. \t  2.9545955764028693 \t 3.655219709787853\n",
      "25     \t [0.65648841 0.         0.95891767]. \t  0.13205215671075599 \t 3.655219709787853\n",
      "26     \t [0.70619157 0.33993458 0.79110312]. \t  2.3845781680091447 \t 3.655219709787853\n",
      "27     \t [0.14583259 0.48079575 0.76819457]. \t  3.15307920218296 \t 3.655219709787853\n",
      "28     \t [0.30656115 0.78418525 0.67705367]. \t  2.3471345152084404 \t 3.655219709787853\n",
      "29     \t [0.99999994 0.74380632 0.85393335]. \t  2.56841205375465 \t 3.655219709787853\n",
      "30     \t [0.65599954 0.87151726 0.70864741]. \t  1.2293183555220855 \t 3.655219709787853\n",
      "31     \t [0.81392239 0.         0.63596483]. \t  0.13459791752502845 \t 3.655219709787853\n",
      "32     \t [2.00757568e-11 5.13837444e-01 1.00000000e+00]. \t  2.000236358944962 \t 3.655219709787853\n",
      "33     \t [0.09454294 0.         0.96436676]. \t  0.1259602853864136 \t 3.655219709787853\n",
      "34     \t [1.         0.43565512 0.85729591]. \t  3.2486800634389272 \t 3.655219709787853\n",
      "35     \t [0.31240459 0.7263704  0.00442504]. \t  0.0032820738833648213 \t 3.655219709787853\n",
      "36     \t [0.9825228  0.29803906 0.96546033]. \t  1.313261169681342 \t 3.655219709787853\n",
      "37     \t [0.15635725 0.71874632 0.80027221]. \t  2.9117920652379103 \t 3.655219709787853\n",
      "38     \t [7.63459184e-01 2.05760135e-08 2.39691754e-06]. \t  0.06409682612914429 \t 3.655219709787853\n",
      "39     \t [0.         0.57768623 0.77672712]. \t  3.340657282649807 \t 3.655219709787853\n",
      "40     \t [0.54990826 0.57227597 0.78307074]. \t  3.3530222644019347 \t 3.655219709787853\n",
      "41     \t [0.23645787 0.87540558 0.23723033]. \t  0.14189501587699957 \t 3.655219709787853\n",
      "42     \t [0.58345875 0.57782182 0.80868207]. \t  3.585758229671717 \t 3.655219709787853\n",
      "43     \t [0.79181083 0.51399953 0.91842093]. \t  3.307382403567488 \t 3.655219709787853\n",
      "44     \t [0.32082843 0.60707346 0.84015789]. \t  \u001b[92m3.7471537684983316\u001b[0m \t 3.7471537684983316\n",
      "45     \t [0.11448403 0.53152793 0.85202134]. \t  \u001b[92m3.8227891349160736\u001b[0m \t 3.8227891349160736\n",
      "46     \t [0.12126894 0.60244484 0.81935302]. \t  3.6700907292820117 \t 3.8227891349160736\n",
      "47     \t [0.93000471 0.51747928 0.83852937]. \t  3.6405910312949272 \t 3.8227891349160736\n",
      "48     \t [0.89380502 0.51507532 0.86912521]. \t  3.656326348783573 \t 3.8227891349160736\n",
      "49     \t [0.82156478 0.55128562 0.80742435]. \t  3.5120049281792225 \t 3.8227891349160736\n",
      "50     \t [0.05086488 0.58321757 0.86421052]. \t  3.7887122805911706 \t 3.8227891349160736\n",
      "51     \t [0.81221309 0.52554342 0.85463588]. \t  3.7301786019234937 \t 3.8227891349160736\n",
      "52     \t [0.         0.         0.16466216]. \t  0.4226758424540686 \t 3.8227891349160736\n",
      "53     \t [0.78986596 0.50582462 0.86777442]. \t  3.670876024627669 \t 3.8227891349160736\n",
      "54     \t [0.98256827 0.48756252 0.83618939]. \t  3.523320001114249 \t 3.8227891349160736\n",
      "55     \t [0.5856452  0.62356707 0.89427545]. \t  3.5200559452008626 \t 3.8227891349160736\n",
      "56     \t [0.05652922 0.57622092 0.82931127]. \t  3.7631571232219314 \t 3.8227891349160736\n",
      "57     \t [0.91479155 0.51171388 0.85166098]. \t  3.659275519172261 \t 3.8227891349160736\n",
      "58     \t [0.5967447  0.53109382 0.86230594]. \t  3.7980351188968156 \t 3.8227891349160736\n",
      "59     \t [0.30983316 0.54263777 0.86647888]. \t  \u001b[92m3.83722875791526\u001b[0m \t 3.83722875791526\n",
      "60     \t [0.27358529 0.60648725 0.82194975]. \t  3.6792920873113713 \t 3.83722875791526\n",
      "61     \t [0.13008596 0.55398109 0.90854614]. \t  3.536109800999928 \t 3.83722875791526\n",
      "62     \t [0.60491553 0.58294925 0.86498095]. \t  3.778501987291195 \t 3.83722875791526\n",
      "63     \t [0.33211125 0.4287162  0.81744769]. \t  3.279798003262524 \t 3.83722875791526\n",
      "64     \t [0.56438443 0.53955909 0.87809297]. \t  3.7649783234419822 \t 3.83722875791526\n",
      "65     \t [0.01959051 0.62550864 0.85034983]. \t  3.651352388460514 \t 3.83722875791526\n",
      "66     \t [0.00445148 0.630332   0.86583621]. \t  3.6111525386865604 \t 3.83722875791526\n",
      "67     \t [0.17555335 0.62157021 0.8381856 ]. \t  3.683950582762198 \t 3.83722875791526\n",
      "68     \t [0.30395575 0.58088545 0.83345244]. \t  3.798881413316738 \t 3.83722875791526\n",
      "69     \t [0.71842608 0.56866578 0.85959434]. \t  3.773985313872086 \t 3.83722875791526\n",
      "70     \t [0.04259872 0.63979944 0.88563176]. \t  3.4954342120269404 \t 3.83722875791526\n",
      "71     \t [0.67681719 0.52661603 0.85870288]. \t  3.7745252454317226 \t 3.83722875791526\n",
      "72     \t [0.57250307 0.48977778 0.89327841]. \t  3.52096337039173 \t 3.83722875791526\n",
      "73     \t [0.01924337 0.56712374 0.80209471]. \t  3.5928191473013658 \t 3.83722875791526\n",
      "74     \t [0.40986174 0.49504099 0.8598248 ]. \t  3.727721661366961 \t 3.83722875791526\n",
      "75     \t [0.78515405 0.63934888 0.86334313]. \t  3.497917393336043 \t 3.83722875791526\n",
      "76     \t [0.0619389  0.59488189 0.8495422 ]. \t  3.7756407469698336 \t 3.83722875791526\n",
      "77     \t [0.52350357 0.49301625 0.84890562]. \t  3.7128242872194104 \t 3.83722875791526\n",
      "78     \t [0.79234094 0.53343622 0.8397368 ]. \t  3.7235930148215868 \t 3.83722875791526\n",
      "79     \t [0.54813149 0.4970873  0.8580276 ]. \t  3.721279611061185 \t 3.83722875791526\n",
      "80     \t [1. 1. 0.]. \t  3.7727185342822293e-05 \t 3.83722875791526\n",
      "81     \t [0.77052182 0.95424662 0.28007762]. \t  0.0793027583163215 \t 3.83722875791526\n",
      "82     \t [0.9833747  0.07992838 0.06819232]. \t  0.09673389388243601 \t 3.83722875791526\n",
      "83     \t [0.80347422 0.10640372 0.75266782]. \t  0.5886437441509206 \t 3.83722875791526\n",
      "84     \t [0.01984272 0.53509364 0.47590447]. \t  0.8991836318454244 \t 3.83722875791526\n",
      "85     \t [0.18697394 0.06951492 0.33744015]. \t  0.7658020018088214 \t 3.83722875791526\n",
      "86     \t [0.71236527 0.82625289 0.21164054]. \t  0.0330351757220587 \t 3.83722875791526\n",
      "87     \t [0.9501523  0.45799694 0.58188769]. \t  0.6322108433550255 \t 3.83722875791526\n",
      "88     \t [0.99273842 0.68766124 0.46204135]. \t  0.20391572856641502 \t 3.83722875791526\n",
      "89     \t [0.05298135 0.91785512 0.44484036]. \t  2.034715683423452 \t 3.83722875791526\n",
      "90     \t [0.85892235 0.69911034 0.40523649]. \t  0.230111965547196 \t 3.83722875791526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.46554557 0.35382268 0.5660885 ]. \t  0.5895364663931983 \t 3.83722875791526\n",
      "92     \t [0.71387159 0.68811182 0.42912745]. \t  0.4759020563078621 \t 3.83722875791526\n",
      "93     \t [0.25380393 0.09155608 0.337479  ]. \t  0.8262236815444031 \t 3.83722875791526\n",
      "94     \t [0.52257236 0.15567948 0.23155444]. \t  0.883761531629999 \t 3.83722875791526\n",
      "95     \t [0.60424375 0.72758491 0.61101768]. \t  1.5200447223288314 \t 3.83722875791526\n",
      "96     \t [0.66595097 0.76298908 0.1374925 ]. \t  0.012788849266881744 \t 3.83722875791526\n",
      "97     \t [0.53381469 0.51381329 0.15333844]. \t  0.1331021987449858 \t 3.83722875791526\n",
      "98     \t [0.43305606 0.25897579 0.04973795]. \t  0.19518771727017228 \t 3.83722875791526\n",
      "99     \t [0.09174291 0.95348489 0.19010536]. \t  0.0527072264754175 \t 3.83722875791526\n",
      "100    \t [0.04333211 0.42857133 0.76156424]. \t  2.811733943492713 \t 3.83722875791526\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.8830091449513892\n",
      "2      \t [0.86820976 0.74172137 0.95045093]. \t  \u001b[92m2.12643142846639\u001b[0m \t 2.12643142846639\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.12643142846639\n",
      "4      \t [0.97531123 0.03012177 1.        ]. \t  0.12229325864550886 \t 2.12643142846639\n",
      "5      \t [0.36949491 0.43899768 1.        ]. \t  1.7761444663362138 \t 2.12643142846639\n",
      "6      \t [1.         0.51911106 1.        ]. \t  1.96371271400507 \t 2.12643142846639\n",
      "7      \t [0.86537063 0.69940846 0.63262721]. \t  1.0104343771369524 \t 2.12643142846639\n",
      "8      \t [0.49707331 0.9415566  1.        ]. \t  0.5278158542557675 \t 2.12643142846639\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.12643142846639\n",
      "10     \t [0.         0.60093648 0.69058072]. \t  \u001b[92m2.4593570976957317\u001b[0m \t 2.4593570976957317\n",
      "11     \t [0.        0.5391564 1.       ]. \t  2.0415666410822966 \t 2.4593570976957317\n",
      "12     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4593570976957317\n",
      "13     \t [0.         0.59253246 0.39851396]. \t  0.6887926669054815 \t 2.4593570976957317\n",
      "14     \t [0.         0.93836897 0.61114774]. \t  \u001b[92m2.635480881728672\u001b[0m \t 2.635480881728672\n",
      "15     \t [0.         0.10294683 0.98012652]. \t  0.30223144408728597 \t 2.635480881728672\n",
      "16     \t [0.70163968 0.39333902 0.84695988]. \t  \u001b[92m3.014719359655036\u001b[0m \t 3.014719359655036\n",
      "17     \t [0.68280255 0.         0.57794282]. \t  0.1050646181278308 \t 3.014719359655036\n",
      "18     \t [0.75144135 0.3676548  1.        ]. \t  1.3939916126900427 \t 3.014719359655036\n",
      "19     \t [1.         0.11797529 0.70629784]. \t  0.5240161526942723 \t 3.014719359655036\n",
      "20     \t [1.         0.47731695 0.        ]. \t  0.009696035792418178 \t 3.014719359655036\n",
      "21     \t [0.35785029 0.71607219 0.78570726]. \t  2.8012352225523838 \t 3.014719359655036\n",
      "22     \t [0.4012986  0.37554029 0.66813839]. \t  1.4935031915218304 \t 3.014719359655036\n",
      "23     \t [0.8045357  0.25895064 0.        ]. \t  0.05424281793523187 \t 3.014719359655036\n",
      "24     \t [0.         1.         0.33523719]. \t  0.5815062675160388 \t 3.014719359655036\n",
      "25     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.014719359655036\n",
      "26     \t [0.41253724 0.         0.94737398]. \t  0.14512924222229898 \t 3.014719359655036\n",
      "27     \t [1.         0.48056556 0.76573594]. \t  2.894138934058688 \t 3.014719359655036\n",
      "28     \t [0.1703223  0.75813722 0.        ]. \t  0.001962753351919373 \t 3.014719359655036\n",
      "29     \t [0.         0.78648586 0.64670691]. \t  2.615034340259918 \t 3.014719359655036\n",
      "30     \t [0.         0.         0.33435743]. \t  0.5073649336768633 \t 3.014719359655036\n",
      "31     \t [0.10344992 1.         0.77505186]. \t  0.989881470610235 \t 3.014719359655036\n",
      "32     \t [0.63662486 0.586943   0.65874821]. \t  1.694619197291845 \t 3.014719359655036\n",
      "33     \t [1.         0.66909905 0.85959205]. \t  \u001b[92m3.213981574278944\u001b[0m \t 3.213981574278944\n",
      "34     \t [0.22320849 0.74271249 0.96637137]. \t  1.9853782697379228 \t 3.213981574278944\n",
      "35     \t [0.56818993 0.63265313 0.93130344]. \t  3.1017800888578657 \t 3.213981574278944\n",
      "36     \t [0.00357566 0.81369027 0.80636536]. \t  2.12290289873763 \t 3.213981574278944\n",
      "37     \t [0.51786535 0.         0.        ]. \t  0.09566145651104578 \t 3.213981574278944\n",
      "38     \t [0.99999998 0.8023603  0.82916663]. \t  1.9283967821908106 \t 3.213981574278944\n",
      "39     \t [0.77117197 0.         0.86389884]. \t  0.2226089506792358 \t 3.213981574278944\n",
      "40     \t [0.79177329 0.54476436 0.87339454]. \t  \u001b[92m3.728184346186906\u001b[0m \t 3.728184346186906\n",
      "41     \t [0.39710228 0.         0.23281005]. \t  0.8395560640978622 \t 3.728184346186906\n",
      "42     \t [0.229349   0.55045794 0.77657983]. \t  3.391620115471232 \t 3.728184346186906\n",
      "43     \t [0.7513287 0.        0.1833832]. \t  0.4552605297886627 \t 3.728184346186906\n",
      "44     \t [0.11704357 0.50995773 0.80766549]. \t  3.610202555333487 \t 3.728184346186906\n",
      "45     \t [0.92797937 0.56077271 0.78229307]. \t  3.177967365942206 \t 3.728184346186906\n",
      "46     \t [0.93909027 0.4719942  0.85805415]. \t  3.493139912932196 \t 3.728184346186906\n",
      "47     \t [0.10646367 0.30085614 0.86333646]. \t  2.100188937988357 \t 3.728184346186906\n",
      "48     \t [0.14332456 0.57850563 0.80483273]. \t  3.632302482435884 \t 3.728184346186906\n",
      "49     \t [0.19060912 0.5413007  0.80581955]. \t  3.660013791479833 \t 3.728184346186906\n",
      "50     \t [0.16120355 0.6954575  0.59340081]. \t  2.491414172154033 \t 3.728184346186906\n",
      "51     \t [0.40286212 0.57941246 0.81202679]. \t  3.6672549219615 \t 3.728184346186906\n",
      "52     \t [0.43877043 0.60913267 0.90013975]. \t  3.552253979860546 \t 3.728184346186906\n",
      "53     \t [0.84527734 0.57011107 0.87791283]. \t  3.6853122214850256 \t 3.728184346186906\n",
      "54     \t [0.99739391 0.37240559 0.89810064]. \t  2.535638067357522 \t 3.728184346186906\n",
      "55     \t [0.86226914 0.43007632 0.90634114]. \t  2.9782000272785782 \t 3.728184346186906\n",
      "56     \t [0.51866603 0.54593488 0.83646783]. \t  \u001b[92m3.8045473121396887\u001b[0m \t 3.8045473121396887\n",
      "57     \t [0.55479486 0.49091106 0.88741535]. \t  3.572907859214931 \t 3.8045473121396887\n",
      "58     \t [0.25553098 0.57942901 0.85173848]. \t  \u001b[92m3.839635202580555\u001b[0m \t 3.839635202580555\n",
      "59     \t [0.20482757 0.57186317 0.84931625]. \t  \u001b[92m3.846052724547246\u001b[0m \t 3.846052724547246\n",
      "60     \t [0.21829871 0.58297942 0.85185433]. \t  3.83122810254845 \t 3.846052724547246\n",
      "61     \t [0.92886336 0.56500441 0.83014687]. \t  3.6180906585323473 \t 3.846052724547246\n",
      "62     \t [0.74983922 0.66101297 0.85035126]. \t  3.362473855283838 \t 3.846052724547246\n",
      "63     \t [0.69469631 0.53422162 0.90911349]. \t  3.485759003063221 \t 3.846052724547246\n",
      "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.846052724547246\n",
      "65     \t [0.32102137 0.47669412 0.86462456]. \t  3.6324286954597236 \t 3.846052724547246\n",
      "66     \t [0.4560326  0.41527705 0.80267575]. \t  3.0951668602818123 \t 3.846052724547246\n",
      "67     \t [0.69520701 0.57043091 0.91089763]. \t  3.4832714008264176 \t 3.846052724547246\n",
      "68     \t [0.60349824 0.59811976 0.90167355]. \t  3.547356123324264 \t 3.846052724547246\n",
      "69     \t [0.91510425 0.51221984 0.82463941]. \t  3.5739192549472785 \t 3.846052724547246\n",
      "70     \t [0.22565378 0.55489021 0.8567725 ]. \t  \u001b[92m3.8570520060600515\u001b[0m \t 3.8570520060600515\n",
      "71     \t [0.19620681 0.553287   0.72587941]. \t  2.8071470043569997 \t 3.8570520060600515\n",
      "72     \t [0.29206328 0.52581084 0.83811974]. \t  3.814502756225837 \t 3.8570520060600515\n",
      "73     \t [0.01030337 0.82011915 0.35706277]. \t  0.8827697766089613 \t 3.8570520060600515\n",
      "74     \t [0.29016525 0.46670098 0.88314192]. \t  3.4886362959534996 \t 3.8570520060600515\n",
      "75     \t [2.28950265e-15 1.00000000e+00 1.57008929e-14]. \t  0.00027353676804558064 \t 3.8570520060600515\n",
      "76     \t [0.60008011 0.92496505 0.52429065]. \t  1.4087284930885242 \t 3.8570520060600515\n",
      "77     \t [0.2444321  0.50485488 0.50788661]. \t  0.8987089976929616 \t 3.8570520060600515\n",
      "78     \t [0.81769418 0.13001474 0.88255499]. \t  0.6765649630943437 \t 3.8570520060600515\n",
      "79     \t [0.11151363 0.36185785 0.05050592]. \t  0.1099957916682531 \t 3.8570520060600515\n",
      "80     \t [0.15873662 0.25637374 0.46219008]. \t  0.3342996983425587 \t 3.8570520060600515\n",
      "81     \t [0.12551094 0.68651564 0.87355364]. \t  3.261168261707135 \t 3.8570520060600515\n",
      "82     \t [0.08907019 0.52440296 0.83386307]. \t  3.775590223992559 \t 3.8570520060600515\n",
      "83     \t [0.42643516 0.33308287 0.34036965]. \t  0.5624461577601702 \t 3.8570520060600515\n",
      "84     \t [0.1897861  0.9599763  0.09354932]. \t  0.004927347337592565 \t 3.8570520060600515\n",
      "85     \t [0.04421126 0.59991414 0.27214862]. \t  0.1990397018294669 \t 3.8570520060600515\n",
      "86     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.8570520060600515\n",
      "87     \t [0.51712563 0.9804653  0.41579239]. \t  0.9115376651984629 \t 3.8570520060600515\n",
      "88     \t [0.77901981 0.30643979 0.83347927]. \t  2.1826575662159646 \t 3.8570520060600515\n",
      "89     \t [0.22800456 0.11021078 0.54544231]. \t  0.2056988119882285 \t 3.8570520060600515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.16234545 0.15090905 0.73449207]. \t  0.7693729649432159 \t 3.8570520060600515\n",
      "91     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.8570520060600515\n",
      "92     \t [0.54932523 0.36911071 0.26815621]. \t  0.4919621604955257 \t 3.8570520060600515\n",
      "93     \t [0.99313411 0.38895001 0.13157372]. \t  0.08546084765816847 \t 3.8570520060600515\n",
      "94     \t [0.21311319 0.7802126  0.09413853]. \t  0.009240500093104067 \t 3.8570520060600515\n",
      "95     \t [0.87256494 0.41979402 0.28258625]. \t  0.1933402220534644 \t 3.8570520060600515\n",
      "96     \t [0.5496978  0.8906903  0.01635096]. \t  0.0006243489958460381 \t 3.8570520060600515\n",
      "97     \t [0.67963353 0.3424413  0.72058914]. \t  1.830922880788777 \t 3.8570520060600515\n",
      "98     \t [0.79750165 0.33185519 0.4769283 ]. \t  0.2173532680358019 \t 3.8570520060600515\n",
      "99     \t [0.30148919 0.01337579 0.65699551]. \t  0.18123347020220654 \t 3.8570520060600515\n",
      "100    \t [0.55912399 0.9183065  0.48880292]. \t  1.4218084320990023 \t 3.8570520060600515\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
      "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
      "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
      "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
      "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
      "1      \t [0.76578315 1.         1.        ]. \t  0.3255963146009715 \t 1.6536488994056173\n",
      "2      \t [0.        1.        0.7749109]. \t  0.9670087296166714 \t 1.6536488994056173\n",
      "3      \t [0.         0.57095024 0.27797523]. \t  0.20156219910129247 \t 1.6536488994056173\n",
      "4      \t [0.84846708 0.30192678 0.81642154]. \t  \u001b[92m2.1096040059334884\u001b[0m \t 2.1096040059334884\n",
      "5      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.1096040059334884\n",
      "6      \t [1.        0.6078336 0.624137 ]. \t  0.9288723809519044 \t 2.1096040059334884\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.1096040059334884\n",
      "8      \t [0.58050907 0.51111631 1.        ]. \t  2.0157476629760196 \t 2.1096040059334884\n",
      "9      \t [0.9657221  0.47398867 1.        ]. \t  1.852310916350166 \t 2.1096040059334884\n",
      "10     \t [0.70593137 0.26514558 1.        ]. \t  0.8468592894783048 \t 2.1096040059334884\n",
      "11     \t [0.         0.64374786 1.        ]. \t  1.9447814984463552 \t 2.1096040059334884\n",
      "12     \t [1.         0.         0.49519207]. \t  0.07480842012965824 \t 2.1096040059334884\n",
      "13     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.1096040059334884\n",
      "14     \t [0. 1. 1.]. \t  0.330219860606422 \t 2.1096040059334884\n",
      "15     \t [0.         0.36120836 0.70724874]. \t  1.8327911265111776 \t 2.1096040059334884\n",
      "16     \t [0.         0.67364495 0.77309055]. \t  \u001b[92m2.999896870147583\u001b[0m \t 2.999896870147583\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.999896870147583\n",
      "18     \t [0.75702405 0.66539546 0.8667114 ]. \t  \u001b[92m3.3385385506098846\u001b[0m \t 3.3385385506098846\n",
      "19     \t [0.         0.         0.55450011]. \t  0.09973735099309451 \t 3.3385385506098846\n",
      "20     \t [0.63273875 0.         0.        ]. \t  0.08297627259937634 \t 3.3385385506098846\n",
      "21     \t [0.94798101 0.765865   1.        ]. \t  1.3711189835009363 \t 3.3385385506098846\n",
      "22     \t [0.48527823 0.80487025 0.87731126]. \t  2.1322328779263855 \t 3.3385385506098846\n",
      "23     \t [0.783894   0.83166249 0.73895147]. \t  1.3116618522613988 \t 3.3385385506098846\n",
      "24     \t [0.63455175 0.         0.6436945 ]. \t  0.14666516806632307 \t 3.3385385506098846\n",
      "25     \t [0.2869703  0.         0.01422774]. \t  0.12513115811273107 \t 3.3385385506098846\n",
      "26     \t [0.68217809 1.         0.        ]. \t  0.0001293395292828312 \t 3.3385385506098846\n",
      "27     \t [0.         0.77004059 0.60492182]. \t  2.7504816632345785 \t 3.3385385506098846\n",
      "28     \t [0.70406111 0.76211698 1.        ]. \t  1.4230539691829842 \t 3.3385385506098846\n",
      "29     \t [0.91506209 0.77178961 0.        ]. \t  0.000696080331131402 \t 3.3385385506098846\n",
      "30     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.3385385506098846\n",
      "31     \t [0.17841345 0.53140172 0.89697162]. \t  \u001b[92m3.6311578519846863\u001b[0m \t 3.6311578519846863\n",
      "32     \t [0.12279806 0.36139114 1.        ]. \t  1.3685355648122386 \t 3.6311578519846863\n",
      "33     \t [0.         0.99921209 0.17167768]. \t  0.030497089302198632 \t 3.6311578519846863\n",
      "34     \t [0.15251929 0.78439602 0.86909899]. \t  2.389390642327269 \t 3.6311578519846863\n",
      "35     \t [0.70592045 0.53694059 0.73455705]. \t  2.6508853116817117 \t 3.6311578519846863\n",
      "36     \t [0.9999999  0.84888307 0.86056654]. \t  1.5789167416812975 \t 3.6311578519846863\n",
      "37     \t [0.99776157 0.64222621 0.87827287]. \t  3.368594517628543 \t 3.6311578519846863\n",
      "38     \t [0.3670599  0.50120608 0.87261072]. \t  \u001b[92m3.7157803621050394\u001b[0m \t 3.7157803621050394\n",
      "39     \t [0.77380676 0.51891753 0.87410263]. \t  3.691767097289591 \t 3.7157803621050394\n",
      "40     \t [0.87460634 0.58919522 0.86276125]. \t  3.6748192876371895 \t 3.7157803621050394\n",
      "41     \t [8.24487137e-01 1.13099970e-07 1.97409415e-01]. \t  0.4041045982528039 \t 3.7157803621050394\n",
      "42     \t [1.46758089e-08 1.00000000e+00 1.38255160e-08]. \t  0.00027353688963388257 \t 3.7157803621050394\n",
      "43     \t [0.09794064 0.01275639 0.24237348]. \t  0.7065306144215252 \t 3.7157803621050394\n",
      "44     \t [0.02454027 0.11229068 0.87408474]. \t  0.6035591048625235 \t 3.7157803621050394\n",
      "45     \t [0.11322736 0.83179594 0.89396408]. \t  1.8419758353895992 \t 3.7157803621050394\n",
      "46     \t [0.13965536 0.24093939 0.65817463]. \t  0.8354675500972123 \t 3.7157803621050394\n",
      "47     \t [0.14591821 0.18953004 0.65808191]. \t  0.6362186078270452 \t 3.7157803621050394\n",
      "48     \t [0.45311091 0.40177007 0.27033025]. \t  0.4555297486971943 \t 3.7157803621050394\n",
      "49     \t [0.62218283 0.90900293 0.1277241 ]. \t  0.006534253992439146 \t 3.7157803621050394\n",
      "50     \t [0.7622295  0.81871884 0.17126316]. \t  0.013305911612558839 \t 3.7157803621050394\n",
      "51     \t [0.94887059 0.10427056 0.36931667]. \t  0.2693350594487597 \t 3.7157803621050394\n",
      "52     \t [0.37136239 0.91648361 0.59464345]. \t  2.392000869413017 \t 3.7157803621050394\n",
      "53     \t [5.75191493e-04 7.02981333e-01 8.18751706e-01]. \t  3.0704282744031817 \t 3.7157803621050394\n",
      "54     \t [0.7043113  0.26137456 0.97964802]. \t  0.9877612750800018 \t 3.7157803621050394\n",
      "55     \t [0.58822533 0.35373533 0.98274796]. \t  1.540692159340522 \t 3.7157803621050394\n",
      "56     \t [0.7112095  0.80676175 0.83426915]. \t  2.0265118381381173 \t 3.7157803621050394\n",
      "57     \t [0.51796176 0.7152021  0.35073211]. \t  0.4299390150832882 \t 3.7157803621050394\n",
      "58     \t [0.34888788 0.97358139 0.05275151]. \t  0.0013540786873147649 \t 3.7157803621050394\n",
      "59     \t [0.81918406 0.78267352 0.35913085]. \t  0.2003621949412156 \t 3.7157803621050394\n",
      "60     \t [0.79335955 0.03298455 0.85253868]. \t  0.3177628031295503 \t 3.7157803621050394\n",
      "61     \t [0.51219652 0.90003122 0.0227167 ]. \t  0.0007155287196951899 \t 3.7157803621050394\n",
      "62     \t [0.22317972 0.0667976  0.75480304]. \t  0.43744897063916954 \t 3.7157803621050394\n",
      "63     \t [0.00894797 0.86311035 0.32490426]. \t  0.5992491399772332 \t 3.7157803621050394\n",
      "64     \t [0.86282608 0.21285425 0.10259378]. \t  0.19445902589887212 \t 3.7157803621050394\n",
      "65     \t [0.90755475 0.97111228 0.97957777]. \t  0.4766153233078848 \t 3.7157803621050394\n",
      "66     \t [0.58185629 0.51786025 0.42086298]. \t  0.36884671397760366 \t 3.7157803621050394\n",
      "67     \t [0.51012351 0.3979206  0.57660706]. \t  0.7300217636525888 \t 3.7157803621050394\n",
      "68     \t [0.24130122 0.03297279 0.32166798]. \t  0.8129961054511182 \t 3.7157803621050394\n",
      "69     \t [0.77164609 0.05626549 0.04049691]. \t  0.12660461271326529 \t 3.7157803621050394\n",
      "70     \t [0.05313563 0.57318691 0.23474965]. \t  0.14576838971630784 \t 3.7157803621050394\n",
      "71     \t [0.43860812 0.29570706 0.13229548]. \t  0.41484876224588224 \t 3.7157803621050394\n",
      "72     \t [0.24463422 0.71318517 0.79012456]. \t  2.8933086874032927 \t 3.7157803621050394\n",
      "73     \t [0.63720446 0.48890602 0.68787638]. \t  1.9895817396735902 \t 3.7157803621050394\n",
      "74     \t [0.45144466 0.55258831 0.87575199]. \t  \u001b[92m3.8021350826596167\u001b[0m \t 3.8021350826596167\n",
      "75     \t [0.52796169 0.53199164 0.9145982 ]. \t  3.4501600929039613 \t 3.8021350826596167\n",
      "76     \t [0.51120923 0.35246941 0.6893926 ]. \t  1.5928196212322256 \t 3.8021350826596167\n",
      "77     \t [0.15043603 0.98776703 0.47746587]. \t  2.195165676259236 \t 3.8021350826596167\n",
      "78     \t [0.79929807 0.92451776 0.74737308]. \t  0.8272617348032159 \t 3.8021350826596167\n",
      "79     \t [0.59337992 0.13917599 0.07763026]. \t  0.2907462934900245 \t 3.8021350826596167\n",
      "80     \t [0.11193538 0.38235381 0.07991815]. \t  0.14179469135234987 \t 3.8021350826596167\n",
      "81     \t [0.72034395 0.6540787  0.73832796]. \t  2.3977008594632636 \t 3.8021350826596167\n",
      "82     \t [0.35452219 0.62967616 0.00217418]. \t  0.008904168090358431 \t 3.8021350826596167\n",
      "83     \t [0.27146486 0.74267653 0.18100121]. \t  0.05093524582366787 \t 3.8021350826596167\n",
      "84     \t [0.52519573 0.5521018  0.84112446]. \t  \u001b[92m3.818189709401497\u001b[0m \t 3.818189709401497\n",
      "85     \t [0.64144854 0.01569875 0.07184981]. \t  0.2295837295346077 \t 3.818189709401497\n",
      "86     \t [0.45353635 0.90024499 0.86312424]. \t  1.2996967483363713 \t 3.818189709401497\n",
      "87     \t [0.67793529 0.48675574 0.48690062]. \t  0.38896239080246947 \t 3.818189709401497\n",
      "88     \t [0.64175135 0.90497947 0.69370479]. \t  1.1379403889038424 \t 3.818189709401497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.6791164  0.55313926 0.8965976 ]. \t  3.627618240345153 \t 3.818189709401497\n",
      "90     \t [0.78575744 0.42016254 0.21131259]. \t  0.2184692038879679 \t 3.818189709401497\n",
      "91     \t [0.2128101  0.20207285 0.51922438]. \t  0.27836186776395766 \t 3.818189709401497\n",
      "92     \t [0.22109237 0.04755993 0.20649857]. \t  0.7988764965553357 \t 3.818189709401497\n",
      "93     \t [0.86752059 0.35948895 0.18470936]. \t  0.2153457044236414 \t 3.818189709401497\n",
      "94     \t [0.86645362 0.73534995 0.44053744]. \t  0.32533968898162047 \t 3.818189709401497\n",
      "95     \t [0.03979842 0.65364993 0.00761936]. \t  0.005595071720197461 \t 3.818189709401497\n",
      "96     \t [0.1348817  0.42480905 0.50048381]. \t  0.5879170108412282 \t 3.818189709401497\n",
      "97     \t [0.81979437 0.19768467 0.3570296 ]. \t  0.40530511115776674 \t 3.818189709401497\n",
      "98     \t [0.12922912 0.26204515 0.50371867]. \t  0.3109083246554116 \t 3.818189709401497\n",
      "99     \t [0.32246996 0.71313853 0.51126178]. \t  2.020487917978038 \t 3.818189709401497\n",
      "100    \t [0.1116215  0.11885075 0.12044446]. \t  0.4293323687653757 \t 3.818189709401497\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1029187088185965\n",
      "2      \t [1.         0.88930193 0.80880366]. \t  \u001b[92m1.1165151415157384\u001b[0m \t 1.1165151415157384\n",
      "3      \t [1.         0.58451295 0.4410129 ]. \t  0.13447145237511973 \t 1.1165151415157384\n",
      "4      \t [0.1900835  1.         0.98167119]. \t  0.38775892049899474 \t 1.1165151415157384\n",
      "5      \t [0.44087281 0.         0.31526593]. \t  0.801801346475015 \t 1.1165151415157384\n",
      "6      \t [0.75972805 0.80738254 1.        ]. \t  \u001b[92m1.1675299616353574\u001b[0m \t 1.1675299616353574\n",
      "7      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1675299616353574\n",
      "8      \t [0.54324326 1.         0.67929309]. \t  1.0769683076738872 \t 1.1675299616353574\n",
      "9      \t [0.         0.69127284 0.71634732]. \t  \u001b[92m2.592547735257158\u001b[0m \t 2.592547735257158\n",
      "10     \t [0.         1.         0.48774837]. \t  2.161719723936843 \t 2.592547735257158\n",
      "11     \t [0.         0.50704721 0.9520442 ]. \t  \u001b[92m2.8125356888485618\u001b[0m \t 2.8125356888485618\n",
      "12     \t [0.         0.13903823 0.88494162]. \t  0.7204268626164172 \t 2.8125356888485618\n",
      "13     \t [0.         1.         0.14954703]. \t  0.01809133737061053 \t 2.8125356888485618\n",
      "14     \t [1.         0.46915391 1.        ]. \t  1.8280950726006926 \t 2.8125356888485618\n",
      "15     \t [0.50530929 0.         0.        ]. \t  0.09669532199442461 \t 2.8125356888485618\n",
      "16     \t [0.2118284  0.54155226 1.        ]. \t  2.068565838048829 \t 2.8125356888485618\n",
      "17     \t [0.         0.76480526 1.        ]. \t  1.4064137803133927 \t 2.8125356888485618\n",
      "18     \t [0.         0.34854316 0.63610528]. \t  1.0866565074753427 \t 2.8125356888485618\n",
      "19     \t [0.30692531 0.61483026 0.4443785 ]. \t  1.0215559784183512 \t 2.8125356888485618\n",
      "20     \t [1.         0.         0.45898803]. \t  0.09733120194023713 \t 2.8125356888485618\n",
      "21     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.8125356888485618\n",
      "22     \t [0.         1.         0.73144531]. \t  1.246403496794569 \t 2.8125356888485618\n",
      "23     \t [0.         0.73433134 0.35584296]. \t  0.7429538425940581 \t 2.8125356888485618\n",
      "24     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.8125356888485618\n",
      "25     \t [0.21298568 1.         0.42368881]. \t  1.4795964545233804 \t 2.8125356888485618\n",
      "26     \t [0.8917254  0.46561251 0.        ]. \t  0.015325389219293774 \t 2.8125356888485618\n",
      "27     \t [1.         0.         0.93487264]. \t  0.15299582873043774 \t 2.8125356888485618\n",
      "28     \t [1.         0.65475536 0.91470154]. \t  \u001b[92m3.0625588868785547\u001b[0m \t 3.0625588868785547\n",
      "29     \t [0.68430693 1.         0.        ]. \t  0.00012851293714846674 \t 3.0625588868785547\n",
      "30     \t [0.68808053 0.         0.69742476]. \t  0.19789691580143018 \t 3.0625588868785547\n",
      "31     \t [1.         0.38840146 0.79010355]. \t  2.684703218548846 \t 3.0625588868785547\n",
      "32     \t [0.24377239 0.         1.        ]. \t  0.09151743033692988 \t 3.0625588868785547\n",
      "33     \t [0.42647025 0.70079849 0.83499798]. \t  \u001b[92m3.1330327124999893\u001b[0m \t 3.1330327124999893\n",
      "34     \t [0.26764739 0.8097426  0.79665571]. \t  2.154131442038133 \t 3.1330327124999893\n",
      "35     \t [0.69424867 0.58937545 0.79503179]. \t  \u001b[92m3.376931004139306\u001b[0m \t 3.376931004139306\n",
      "36     \t [0.59998379 0.47470446 0.66086841]. \t  1.6409682241434904 \t 3.376931004139306\n",
      "37     \t [1.83617617e-01 1.12968356e-08 1.10519545e-01]. \t  0.3763365788798481 \t 3.376931004139306\n",
      "38     \t [1.         0.79443154 0.        ]. \t  0.0003862360170632225 \t 3.376931004139306\n",
      "39     \t [0.1210631  0.487105   0.83846613]. \t  \u001b[92m3.6743263476426034\u001b[0m \t 3.6743263476426034\n",
      "40     \t [0.78149949 0.00972246 0.1241263 ]. \t  0.28916604231077836 \t 3.6743263476426034\n",
      "41     \t [0.65153985 0.51609367 0.91235161]. \t  3.418526131642012 \t 3.6743263476426034\n",
      "42     \t [0.89132638 0.57265299 0.9178792 ]. \t  3.3445018975302436 \t 3.6743263476426034\n",
      "43     \t [0.80495348 0.59263008 0.86239783]. \t  \u001b[92m3.693277314386332\u001b[0m \t 3.693277314386332\n",
      "44     \t [0.23733963 0.59026798 0.83350551]. \t  \u001b[92m3.7800942289225654\u001b[0m \t 3.7800942289225654\n",
      "45     \t [0.04312409 0.53788678 0.80260989]. \t  3.60341059411388 \t 3.7800942289225654\n",
      "46     \t [0.45924908 0.52383375 0.87776412]. \t  3.75507571449876 \t 3.7800942289225654\n",
      "47     \t [0.1288282  0.52863163 0.79327494]. \t  3.5331095362215263 \t 3.7800942289225654\n",
      "48     \t [0.1130231  0.5081149  0.84465645]. \t  3.762130344598458 \t 3.7800942289225654\n",
      "49     \t [1. 1. 0.]. \t  3.772718520138873e-05 \t 3.7800942289225654\n",
      "50     \t [0.38341112 0.43547532 0.66386309]. \t  1.6840253756341341 \t 3.7800942289225654\n",
      "51     \t [0.59006544 0.3371488  0.74327919]. \t  2.028629790617639 \t 3.7800942289225654\n",
      "52     \t [0.70426147 0.94977949 0.91975555]. \t  0.7859185485075025 \t 3.7800942289225654\n",
      "53     \t [0.29938903 0.33951197 0.10493827]. \t  0.2727704257910028 \t 3.7800942289225654\n",
      "54     \t [0.04227152 0.51632077 0.06614304]. \t  0.04442441130910269 \t 3.7800942289225654\n",
      "55     \t [0.9924857  0.16035798 0.36489196]. \t  0.2335742711477819 \t 3.7800942289225654\n",
      "56     \t [0.7384411  0.96115012 0.40008681]. \t  0.4147517720316982 \t 3.7800942289225654\n",
      "57     \t [0.44486902 0.573852   0.57591669]. \t  1.3456733234876415 \t 3.7800942289225654\n",
      "58     \t [0.22519599 0.91835787 0.61079726]. \t  2.704469251438837 \t 3.7800942289225654\n",
      "59     \t [0.        0.9999323 0.9998653]. \t  0.3307966764798626 \t 3.7800942289225654\n",
      "60     \t [0.29383637 0.66994699 0.20578115]. \t  0.08773904885755142 \t 3.7800942289225654\n",
      "61     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.7800942289225654\n",
      "62     \t [0.91427037 0.73030932 0.37220707]. \t  0.13958483090937676 \t 3.7800942289225654\n",
      "63     \t [0.87039675 0.75684018 0.12311133]. \t  0.005925869715045364 \t 3.7800942289225654\n",
      "64     \t [0.97772882 0.3891266  0.13536174]. \t  0.0931907695184537 \t 3.7800942289225654\n",
      "65     \t [0.62945952 0.81213003 0.07417604]. \t  0.0033818119848788857 \t 3.7800942289225654\n",
      "66     \t [0.63212291 0.13114456 0.48472706]. \t  0.24451925049178566 \t 3.7800942289225654\n",
      "67     \t [0.14245061 0.4693838  0.2147183 ]. \t  0.2462921799924161 \t 3.7800942289225654\n",
      "68     \t [0.34945726 0.51161119 0.28253862]. \t  0.28359048954848454 \t 3.7800942289225654\n",
      "69     \t [0.65959258 0.15693117 0.49238012]. \t  0.23201619425484993 \t 3.7800942289225654\n",
      "70     \t [0.07867937 0.90892995 0.60080682]. \t  2.896572730126719 \t 3.7800942289225654\n",
      "71     \t [0.91105214 0.64992004 0.97511737]. \t  2.303778064136172 \t 3.7800942289225654\n",
      "72     \t [0.61052577 0.39268847 0.46810742]. \t  0.31167649069899306 \t 3.7800942289225654\n",
      "73     \t [0.36830204 0.62088838 0.24672193]. \t  0.15347386046955724 \t 3.7800942289225654\n",
      "74     \t [0.00589812 0.27171843 0.98890298]. \t  0.9666106673936532 \t 3.7800942289225654\n",
      "75     \t [0.94822908 0.3459102  0.68891035]. \t  1.4637932477481637 \t 3.7800942289225654\n",
      "76     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.7800942289225654\n",
      "77     \t [0.1202519  0.93815514 0.4987862 ]. \t  2.6339673962723826 \t 3.7800942289225654\n",
      "78     \t [0.54455961 0.00965093 0.88019342]. \t  0.23392919690218134 \t 3.7800942289225654\n",
      "79     \t [0.94849578 0.41088567 0.6279293 ]. \t  0.9922720498965402 \t 3.7800942289225654\n",
      "80     \t [0.75053175 0.07049347 0.35769157]. \t  0.496737531071639 \t 3.7800942289225654\n",
      "81     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.7800942289225654\n",
      "82     \t [0.91763429 0.51515885 0.54189505]. \t  0.43106468558061406 \t 3.7800942289225654\n",
      "83     \t [0.60065582 0.55539493 0.33880207]. \t  0.24073711043874907 \t 3.7800942289225654\n",
      "84     \t [0.73643037 0.7558962  0.53479151]. \t  0.917505038546741 \t 3.7800942289225654\n",
      "85     \t [0.96033402 0.15484275 0.55027791]. \t  0.17937950478392242 \t 3.7800942289225654\n",
      "86     \t [0.72973343 0.7100578  0.55331377]. \t  0.9379670647842744 \t 3.7800942289225654\n",
      "87     \t [0.96856976 0.08974637 0.01914393]. \t  0.053200915564779426 \t 3.7800942289225654\n",
      "88     \t [0.95030821 0.19386301 0.97179269]. \t  0.6620893120506729 \t 3.7800942289225654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.62693302 0.84101297 0.42004557]. \t  0.7795783747167534 \t 3.7800942289225654\n",
      "90     \t [0.97250438 0.18173459 0.46635624]. \t  0.1399673786418343 \t 3.7800942289225654\n",
      "91     \t [0.21740244 0.349058   0.31633464]. \t  0.5423301983158865 \t 3.7800942289225654\n",
      "92     \t [0.9270585  0.78468855 0.49436225]. \t  0.3833080971914929 \t 3.7800942289225654\n",
      "93     \t [0.06804019 0.76189743 0.21971389]. \t  0.10216064197127789 \t 3.7800942289225654\n",
      "94     \t [0.89647184 0.77550589 0.61455622]. \t  0.7540772831785704 \t 3.7800942289225654\n",
      "95     \t [0.49508671 0.39037781 0.57475127]. \t  0.7079714096790843 \t 3.7800942289225654\n",
      "96     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.7800942289225654\n",
      "97     \t [0.71358188 0.73749073 0.55743709]. \t  1.0333471264077945 \t 3.7800942289225654\n",
      "98     \t [0.42325437 0.52968222 0.00214746]. \t  0.021975714792266725 \t 3.7800942289225654\n",
      "99     \t [0.55324297 0.28908356 0.80797698]. \t  2.0124792267115676 \t 3.7800942289225654\n",
      "100    \t [0.54656951 0.65652098 0.75129329]. \t  2.7054350876838593 \t 3.7800942289225654\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0.2456896 1.        1.       ]. \t  0.33424808430394015 \t 0.687459437576373\n",
      "2      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.687459437576373\n",
      "3      \t [1.         1.         0.27943008]. \t  0.024468416309198607 \t 0.687459437576373\n",
      "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.687459437576373\n",
      "5      \t [0.43662478 0.54994648 1.        ]. \t  \u001b[92m2.0811315183074326\u001b[0m \t 2.0811315183074326\n",
      "6      \t [0.97712019 0.34923978 1.        ]. \t  1.2656100223739448 \t 2.0811315183074326\n",
      "7      \t [0.07921776 0.52172559 1.        ]. \t  2.026890539916605 \t 2.0811315183074326\n",
      "8      \t [0.4384327  0.26975479 1.        ]. \t  0.8785203591882523 \t 2.0811315183074326\n",
      "9      \t [1.         0.22649961 0.        ]. \t  0.0314854820701764 \t 2.0811315183074326\n",
      "10     \t [1.         0.63682001 1.        ]. \t  1.9155126134123441 \t 2.0811315183074326\n",
      "11     \t [0.5251417  0.64139204 0.73003109]. \t  \u001b[92m2.5510754660642396\u001b[0m \t 2.5510754660642396\n",
      "12     \t [0.35041319 1.         0.        ]. \t  0.0002583029332755248 \t 2.5510754660642396\n",
      "13     \t [0.74269865 0.66733143 0.        ]. \t  0.0037878678507054634 \t 2.5510754660642396\n",
      "14     \t [0.24767662 0.91909616 0.4916185 ]. \t  2.4780283529652305 \t 2.5510754660642396\n",
      "15     \t [0.53961233 1.         0.46288579]. \t  1.1418435528342454 \t 2.5510754660642396\n",
      "16     \t [0.         0.89766419 0.67270143]. \t  2.249137546901632 \t 2.5510754660642396\n",
      "17     \t [0.37117958 0.67316347 0.38575041]. \t  0.7324216671299876 \t 2.5510754660642396\n",
      "18     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.5510754660642396\n",
      "19     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.5510754660642396\n",
      "20     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.5510754660642396\n",
      "21     \t [0.671924 0.       0.      ]. \t  0.07762710581083637 \t 2.5510754660642396\n",
      "22     \t [0.80976396 0.         0.68465307]. \t  0.18328842970497966 \t 2.5510754660642396\n",
      "23     \t [0.23045857 0.77764587 0.79680152]. \t  2.4281742699961093 \t 2.5510754660642396\n",
      "24     \t [0.87985404 0.35594686 0.68437397]. \t  1.4713803450343015 \t 2.5510754660642396\n",
      "25     \t [0.         1.         0.43148412]. \t  1.5651544527515644 \t 2.5510754660642396\n",
      "26     \t [0.74298036 0.50424324 1.        ]. \t  1.9813577209209778 \t 2.5510754660642396\n",
      "27     \t [0.         0.63013094 0.        ]. \t  0.005758296191958639 \t 2.5510754660642396\n",
      "28     \t [0.26046899 1.         0.69896933]. \t  1.4843039120622619 \t 2.5510754660642396\n",
      "29     \t [0.         0.83059661 1.        ]. \t  1.045370945222334 \t 2.5510754660642396\n",
      "30     \t [0.32108034 0.05086947 0.        ]. \t  0.11146232295877845 \t 2.5510754660642396\n",
      "31     \t [0.66117864 0.84559713 1.        ]. \t  0.9677889614997072 \t 2.5510754660642396\n",
      "32     \t [0.65542013 0.25456499 0.        ]. \t  0.07585233183685304 \t 2.5510754660642396\n",
      "33     \t [0.         0.515133   0.81795039]. \t  \u001b[92m3.6573403734516794\u001b[0m \t 3.6573403734516794\n",
      "34     \t [1.        0.6767047 0.       ]. \t  0.0015660970255837094 \t 3.6573403734516794\n",
      "35     \t [0.         0.29551176 0.84957163]. \t  2.0640574102143483 \t 3.6573403734516794\n",
      "36     \t [0.60265466 0.31278279 0.80241041]. \t  2.209756125471751 \t 3.6573403734516794\n",
      "37     \t [0.         0.69356071 0.62619229]. \t  2.439693769180224 \t 3.6573403734516794\n",
      "38     \t [0.         0.25304448 0.        ]. \t  0.06478198296715627 \t 3.6573403734516794\n",
      "39     \t [0.97937368 0.48131063 0.8397216 ]. \t  3.5092925284201013 \t 3.6573403734516794\n",
      "40     \t [ 3.30191268e-01 -1.73472348e-18  5.87475436e-01]. \t  0.11862027104238104 \t 3.6573403734516794\n",
      "41     \t [1.         0.2525132  0.79825779]. \t  1.6049881110952424 \t 3.6573403734516794\n",
      "42     \t [0.10954054 0.53068356 0.84826468]. \t  \u001b[92m3.8193731022614195\u001b[0m \t 3.8193731022614195\n",
      "43     \t [0.14806654 0.31308064 0.88452048]. \t  2.136625723793783 \t 3.8193731022614195\n",
      "44     \t [0.14904958 0.98989264 0.1936976 ]. \t  0.05259579240361601 \t 3.8193731022614195\n",
      "45     \t [0.31230029 0.50279904 0.80649758]. \t  3.59415037710926 \t 3.8193731022614195\n",
      "46     \t [0.51022822 0.00350185 0.22788038]. \t  0.7903425607010727 \t 3.8193731022614195\n",
      "47     \t [0.00204512 0.53637534 0.90481753]. \t  3.5269212761465907 \t 3.8193731022614195\n",
      "48     \t [0.04921229 0.49178842 0.86038596]. \t  3.67798694306888 \t 3.8193731022614195\n",
      "49     \t [0.21528783 0.59775634 0.7179797 ]. \t  2.73584785594404 \t 3.8193731022614195\n",
      "50     \t [0.91826938 0.5175478  0.88608238]. \t  3.575269829014075 \t 3.8193731022614195\n",
      "51     \t [0.34103848 0.47763601 0.87454306]. \t  3.5996859777604984 \t 3.8193731022614195\n",
      "52     \t [0.10416904 0.65341038 0.86492867]. \t  3.511841286870582 \t 3.8193731022614195\n",
      "53     \t [0.09608395 0.60493024 0.84109297]. \t  3.741876794515809 \t 3.8193731022614195\n",
      "54     \t [0.87156329 0.57386442 0.83438969]. \t  3.6512976012661476 \t 3.8193731022614195\n",
      "55     \t [0.99113659 0.50040342 0.74721172]. \t  2.6820935417902048 \t 3.8193731022614195\n",
      "56     \t [0.62283745 0.57932127 0.87298157]. \t  3.760947238900807 \t 3.8193731022614195\n",
      "57     \t [1.18721540e-09 1.00000000e+00 3.11124515e-10]. \t  0.0002735367709778672 \t 3.8193731022614195\n",
      "58     \t [0.30562469 0.08565643 0.2132352 ]. \t  0.8964390815744543 \t 3.8193731022614195\n",
      "59     \t [0.1711343  0.78616482 0.67357901]. \t  2.5332181225533814 \t 3.8193731022614195\n",
      "60     \t [0.79488288 0.55196623 0.07809319]. \t  0.030173806896773465 \t 3.8193731022614195\n",
      "61     \t [0.48915685 0.11463951 0.11542569]. \t  0.47932572537549323 \t 3.8193731022614195\n",
      "62     \t [0.10362265 0.99514782 0.20457993]. \t  0.06569264101769848 \t 3.8193731022614195\n",
      "63     \t [0.51847515 0.10331076 0.94773397]. \t  0.3949299809870835 \t 3.8193731022614195\n",
      "64     \t [0.51606021 0.93594717 0.69369794]. \t  1.3216254492453405 \t 3.8193731022614195\n",
      "65     \t [0.08292803 0.76258914 0.2812897 ]. \t  0.2935468645949054 \t 3.8193731022614195\n",
      "66     \t [0.89554193 0.60796097 0.29841   ]. \t  0.07109882839261406 \t 3.8193731022614195\n",
      "67     \t [0.99209761 0.52724509 0.59182   ]. \t  0.706813714232192 \t 3.8193731022614195\n",
      "68     \t [0.42643357 0.44394359 0.90144921]. \t  3.1966936344657384 \t 3.8193731022614195\n",
      "69     \t [0.42039382 0.75207267 0.10391814]. \t  0.012253459456886143 \t 3.8193731022614195\n",
      "70     \t [0.48486484 0.39706945 0.52374185]. \t  0.4903912089210618 \t 3.8193731022614195\n",
      "71     \t [0.01556112 0.32479479 0.52622145]. \t  0.40795320969965004 \t 3.8193731022614195\n",
      "72     \t [0.53269428 0.5021177  0.41613   ]. \t  0.38257475846236266 \t 3.8193731022614195\n",
      "73     \t [0.23292542 0.65429981 0.5694923 ]. \t  2.1114534977548267 \t 3.8193731022614195\n",
      "74     \t [0.22804912 0.77621284 0.62664208]. \t  2.68468494394334 \t 3.8193731022614195\n",
      "75     \t [0.77766985 0.26204123 0.97254898]. \t  1.0429693223013652 \t 3.8193731022614195\n",
      "76     \t [0.47931034 0.84014484 0.67227953]. \t  1.8305430269273972 \t 3.8193731022614195\n",
      "77     \t [0.75815597 0.23100177 0.46364472]. \t  0.2346363318083275 \t 3.8193731022614195\n",
      "78     \t [0.12906283 0.98224696 0.46590821]. \t  2.1055085231188135 \t 3.8193731022614195\n",
      "79     \t [0.75106957 0.15814369 0.38654257]. \t  0.42218406246182705 \t 3.8193731022614195\n",
      "80     \t [0.27461031 0.94774274 0.29159062]. \t  0.328639689249174 \t 3.8193731022614195\n",
      "81     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.8193731022614195\n",
      "82     \t [0.76246921 0.06395052 0.98874101]. \t  0.19521232108927497 \t 3.8193731022614195\n",
      "83     \t [0.76981789 0.64524744 0.58542349]. \t  0.9169360326409772 \t 3.8193731022614195\n",
      "84     \t [0.84248597 0.87114236 0.23971618]. \t  0.03217010731508287 \t 3.8193731022614195\n",
      "85     \t [0.60133007 0.4271783  0.83826588]. \t  3.3043282264695333 \t 3.8193731022614195\n",
      "86     \t [0.43393887 0.82305837 0.1625677 ]. \t  0.02601054601845941 \t 3.8193731022614195\n",
      "87     \t [0.98928277 0.83515542 0.82849782]. \t  1.6312442936979132 \t 3.8193731022614195\n",
      "88     \t [0.49240399 0.4472185  0.9468286 ]. \t  2.655421694864316 \t 3.8193731022614195\n",
      "89     \t [0.27537829 0.26613098 0.44075078]. \t  0.39781962342516103 \t 3.8193731022614195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.16053329 0.96913501 0.73523832]. \t  1.4046142063188611 \t 3.8193731022614195\n",
      "91     \t [0.29146938 0.27435274 0.85713041]. \t  1.8728266859564773 \t 3.8193731022614195\n",
      "92     \t [0.17780029 0.71406639 0.21052045]. \t  0.0887550125427909 \t 3.8193731022614195\n",
      "93     \t [0.99185561 0.82754114 0.53072786]. \t  0.3503385994046391 \t 3.8193731022614195\n",
      "94     \t [0.42975675 0.29917632 0.6583835 ]. \t  1.0741313974916602 \t 3.8193731022614195\n",
      "95     \t [0.47695731 0.57196415 0.97239096]. \t  2.587977947486678 \t 3.8193731022614195\n",
      "96     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.8193731022614195\n",
      "97     \t [0.09316869 0.31028097 0.07927709]. \t  0.1898574063366185 \t 3.8193731022614195\n",
      "98     \t [0.01379356 0.86797625 0.38832868]. \t  1.2760355953003144 \t 3.8193731022614195\n",
      "99     \t [0.85408766 0.561019   0.45399772]. \t  0.23760051016737732 \t 3.8193731022614195\n",
      "100    \t [0.21927488 0.77810318 0.82141228]. \t  2.469706919173057 \t 3.8193731022614195\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0.         0.10900885 1.        ]. \t  0.2659333352812696 \t 1.6482992955272024\n",
      "2      \t [0. 1. 1.]. \t  0.33021986060642144 \t 1.6482992955272024\n",
      "3      \t [1.         0.94730227 1.        ]. \t  0.48370889562804287 \t 1.6482992955272024\n",
      "4      \t [0.65489037 0.34205422 1.        ]. \t  1.261239801182174 \t 1.6482992955272024\n",
      "5      \t [0.         0.30803953 0.        ]. \t  0.054123523559270914 \t 1.6482992955272024\n",
      "6      \t [0.0811047  0.56047715 1.        ]. \t  \u001b[92m2.0685483929670867\u001b[0m \t 2.0685483929670867\n",
      "7      \t [0.        0.4957601 0.7461682]. \t  \u001b[92m2.910955300992803\u001b[0m \t 2.910955300992803\n",
      "8      \t [0.         0.16956756 0.50265023]. \t  0.21591710953899268 \t 2.910955300992803\n",
      "9      \t [1.         0.54945865 0.86079442]. \t  \u001b[92m3.6711212184075173\u001b[0m \t 3.6711212184075173\n",
      "10     \t [1.        0.2880023 1.       ]. \t  0.9379358931736621 \t 3.6711212184075173\n",
      "11     \t [1.         0.80777866 0.60915047]. \t  0.5267089967726875 \t 3.6711212184075173\n",
      "12     \t [1.        0.5504531 0.       ]. \t  0.00543232036780719 \t 3.6711212184075173\n",
      "13     \t [-1.11022302e-16  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 3.6711212184075173\n",
      "14     \t [0.85497479 0.64221855 1.        ]. \t  1.9314643945645846 \t 3.6711212184075173\n",
      "15     \t [0.58544154 1.         0.        ]. \t  0.00016857261876363828 \t 3.6711212184075173\n",
      "16     \t [0.         0.64989978 0.        ]. \t  0.004727117943817326 \t 3.6711212184075173\n",
      "17     \t [0.27764358 0.39563995 0.79390836]. \t  2.8991100603595306 \t 3.6711212184075173\n",
      "18     \t [-1.38777878e-17  8.05504461e-01  7.08898366e-01]. \t  2.2867214132252824 \t 3.6711212184075173\n",
      "19     \t [1.         0.58440622 1.        ]. \t  2.0030249564097384 \t 3.6711212184075173\n",
      "20     \t [1.         0.86100949 0.        ]. \t  0.00016713776767415163 \t 3.6711212184075173\n",
      "21     \t [0.         1.         0.52388524]. \t  2.410115796361618 \t 3.6711212184075173\n",
      "22     \t [1.         0.4752026  0.63507606]. \t  1.1168429979419416 \t 3.6711212184075173\n",
      "23     \t [0.58628752 0.         0.        ]. \t  0.08873153968617985 \t 3.6711212184075173\n",
      "24     \t [0.6430214  0.43460583 0.        ]. \t  0.03414612978275797 \t 3.6711212184075173\n",
      "25     \t [0.75101875 0.13149051 0.78497292]. \t  0.7592518988904973 \t 3.6711212184075173\n",
      "26     \t [0.7011562  1.         0.74513655]. \t  0.618224683057335 \t 3.6711212184075173\n",
      "27     \t [0.98610732 0.         0.        ]. \t  0.032607828492838324 \t 3.6711212184075173\n",
      "28     \t [0.76046604 0.45389298 0.80724719]. \t  3.3013651004605773 \t 3.6711212184075173\n",
      "29     \t [0.         0.59318635 0.37884473]. \t  0.5784926219771869 \t 3.6711212184075173\n",
      "30     \t [0.14447551 0.94698337 0.62992303]. \t  2.5286282993418965 \t 3.6711212184075173\n",
      "31     \t [0.86458989 0.68522532 0.85694912]. \t  3.1461754654985103 \t 3.6711212184075173\n",
      "32     \t [0.         0.3066956  0.88486902]. \t  2.0523209871367 \t 3.6711212184075173\n",
      "33     \t [0.4079644  0.56593286 0.824635  ]. \t  \u001b[92m3.7670102921509083\u001b[0m \t 3.7670102921509083\n",
      "34     \t [0.         1.         0.73857932]. \t  1.1926758407403604 \t 3.7670102921509083\n",
      "35     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.7670102921509083\n",
      "36     \t [4.37511734e-09 7.18196943e-09 7.79344637e-01]. \t  0.24579934581734364 \t 3.7670102921509083\n",
      "37     \t [0.51219762 0.99548668 0.32165281]. \t  0.3115630107554852 \t 3.7670102921509083\n",
      "38     \t [0.23233886 0.62715157 0.76985799]. \t  3.19846819234004 \t 3.7670102921509083\n",
      "39     \t [0.56308796 0.7089541  0.84732927]. \t  3.046534686735093 \t 3.7670102921509083\n",
      "40     \t [0.54933747 0.42846372 0.73472399]. \t  2.477414830414106 \t 3.7670102921509083\n",
      "41     \t [9.95506065e-01 1.17681188e-08 6.09163757e-01]. \t  0.10408135142140289 \t 3.7670102921509083\n",
      "42     \t [0.15263015 0.9646284  0.31523087]. \t  0.4918724842550882 \t 3.7670102921509083\n",
      "43     \t [0.99999967 0.71683669 0.8457105 ]. \t  2.7951833585639756 \t 3.7670102921509083\n",
      "44     \t [0.00000000e+00 9.99999891e-01 3.16420682e-08]. \t  0.00027353716703933117 \t 3.7670102921509083\n",
      "45     \t [0.55762613 0.67643008 0.35836798]. \t  0.3847512176610278 \t 3.7670102921509083\n",
      "46     \t [0.35695056 0.69715022 0.11117504]. \t  0.02162227862505147 \t 3.7670102921509083\n",
      "47     \t [0.14903979 0.14004251 0.06309594]. \t  0.24628545011331074 \t 3.7670102921509083\n",
      "48     \t [0.82405925 0.7268324  0.3799458 ]. \t  0.22291473364268177 \t 3.7670102921509083\n",
      "49     \t [0.19911323 0.00237939 0.87097552]. \t  0.22425533538853948 \t 3.7670102921509083\n",
      "50     \t [0.63719903 0.11168066 0.68479935]. \t  0.4601969693558089 \t 3.7670102921509083\n",
      "51     \t [0.17108634 0.36828815 0.71379018]. \t  1.9654707666452365 \t 3.7670102921509083\n",
      "52     \t [0.30217267 0.62936903 0.2550555 ]. \t  0.17144233029047232 \t 3.7670102921509083\n",
      "53     \t [0.4504184  0.03605091 0.84411583]. \t  0.3381776110807855 \t 3.7670102921509083\n",
      "54     \t [0.65381146 0.55497032 0.11298225]. \t  0.05763780126846553 \t 3.7670102921509083\n",
      "55     \t [0.25048847 0.35816706 0.75953867]. \t  2.3465209235283924 \t 3.7670102921509083\n",
      "56     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.7670102921509083\n",
      "57     \t [0.80933285 0.26153161 0.72892783]. \t  1.3799198877907282 \t 3.7670102921509083\n",
      "58     \t [0.69713192 0.06042533 0.31813701]. \t  0.6494525725605282 \t 3.7670102921509083\n",
      "59     \t [0.19125182 0.06925342 0.12400518]. \t  0.4802546651737729 \t 3.7670102921509083\n",
      "60     \t [0.26866794 0.08576328 0.09628928]. \t  0.3996322068042461 \t 3.7670102921509083\n",
      "61     \t [0.75257917 0.76666177 0.77475694]. \t  2.0249713671027507 \t 3.7670102921509083\n",
      "62     \t [0.72725406 0.24321588 0.42388974]. \t  0.3104960834888005 \t 3.7670102921509083\n",
      "63     \t [0.95058887 0.60792326 0.4672321 ]. \t  0.21497354455016082 \t 3.7670102921509083\n",
      "64     \t [0.03262922 0.14704709 0.86824414]. \t  0.8071026268017989 \t 3.7670102921509083\n",
      "65     \t [0.95386747 0.78640043 0.70238134]. \t  1.1284191028386925 \t 3.7670102921509083\n",
      "66     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.7670102921509083\n",
      "67     \t [0.31648226 0.21977504 0.63165128]. \t  0.6129235792428898 \t 3.7670102921509083\n",
      "68     \t [0.21123292 0.96019313 0.16994722]. \t  0.032336079936821825 \t 3.7670102921509083\n",
      "69     \t [0.59300626 0.38834017 0.69192603]. \t  1.7626767155205645 \t 3.7670102921509083\n",
      "70     \t [0.16132399 0.18463979 0.39212205]. \t  0.5455106698921409 \t 3.7670102921509083\n",
      "71     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.7670102921509083\n",
      "72     \t [0.97000739 0.8651114  0.1323205 ]. \t  0.0022642447975693637 \t 3.7670102921509083\n",
      "73     \t [0.88402471 0.55017171 0.74039155]. \t  2.627584398234073 \t 3.7670102921509083\n",
      "74     \t [0.20725764 0.44661577 0.43293425]. \t  0.47973243351049316 \t 3.7670102921509083\n",
      "75     \t [0.84983778 0.92072967 0.73290115]. \t  0.762527868336073 \t 3.7670102921509083\n",
      "76     \t [0.04172884 0.00472106 0.92828002]. \t  0.17150312598208348 \t 3.7670102921509083\n",
      "77     \t [0.41053179 0.47076279 0.77652987]. \t  3.191020238484077 \t 3.7670102921509083\n",
      "78     \t [0.96814049 0.77015168 0.31479389]. \t  0.05749856420429448 \t 3.7670102921509083\n",
      "79     \t [0.02051185 0.74371114 0.13909475]. \t  0.02227606750842887 \t 3.7670102921509083\n",
      "80     \t [0.32415789 0.63931755 0.71370113]. \t  2.6053987677052417 \t 3.7670102921509083\n",
      "81     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.7670102921509083\n",
      "82     \t [0.98790758 0.95210545 0.779161  ]. \t  0.6501774865339243 \t 3.7670102921509083\n",
      "83     \t [0.33721187 0.97906121 0.32228894]. \t  0.45450899396014893 \t 3.7670102921509083\n",
      "84     \t [0.16204421 0.44201374 0.31710038]. \t  0.37099875373032354 \t 3.7670102921509083\n",
      "85     \t [0.83615956 0.29385084 0.86273822]. \t  2.0094230244088163 \t 3.7670102921509083\n",
      "86     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.7670102921509083\n",
      "87     \t [0.20774061 0.64940324 0.57049668]. \t  2.1122396772161984 \t 3.7670102921509083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.4927525  0.5385138  0.27851717]. \t  0.2253183040613936 \t 3.7670102921509083\n",
      "89     \t [0.16429255 0.38111012 0.43333771]. \t  0.40029402072825343 \t 3.7670102921509083\n",
      "90     \t [0.96808327 0.58718331 0.88105544]. \t  3.592656151513638 \t 3.7670102921509083\n",
      "91     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.7670102921509083\n",
      "92     \t [0.45240892 0.93089545 0.88793639]. \t  1.0218699268839455 \t 3.7670102921509083\n",
      "93     \t [0.40131836 0.529441   0.64123593]. \t  1.7253094510661566 \t 3.7670102921509083\n",
      "94     \t [0.34324431 0.8824516  0.42989216]. \t  1.600752572429055 \t 3.7670102921509083\n",
      "95     \t [0.29793335 0.26841084 0.64559196]. \t  0.8646917429051725 \t 3.7670102921509083\n",
      "96     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.7670102921509083\n",
      "97     \t [0.0188978  0.69682049 0.34010822]. \t  0.5612207821390792 \t 3.7670102921509083\n",
      "98     \t [0.03290644 0.18995676 0.21230533]. \t  0.6180059499133492 \t 3.7670102921509083\n",
      "99     \t [0.20917549 0.28528179 0.46440661]. \t  0.3500468397911418 \t 3.7670102921509083\n",
      "100    \t [0.32680064 0.74037044 0.49418899]. \t  2.0226489388748736 \t 3.7670102921509083\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [0.96512046 1.         1.        ]. \t  0.3183338702581468 \t 2.6697919207500047\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.6697919207500047\n",
      "3      \t [0.55193248 1.         0.62394184]. \t  1.3085407596997056 \t 2.6697919207500047\n",
      "4      \t [0.45775057 0.63834348 1.        ]. \t  1.9868412418503971 \t 2.6697919207500047\n",
      "5      \t [0.         0.36911344 0.49346731]. \t  0.4078705468342739 \t 2.6697919207500047\n",
      "6      \t [0.         0.90336821 0.57666322]. \t  \u001b[92m2.9166553624575267\u001b[0m \t 2.9166553624575267\n",
      "7      \t [1.         0.42622848 0.98902394]. \t  1.8164671312088148 \t 2.9166553624575267\n",
      "8      \t [0.         0.64919713 0.80339342]. \t  \u001b[92m3.3396690641491635\u001b[0m \t 3.3396690641491635\n",
      "9      \t [1.         0.73284841 0.67672516]. \t  1.1007198574722874 \t 3.3396690641491635\n",
      "10     \t [1.         0.05321099 1.        ]. \t  0.15412458358472972 \t 3.3396690641491635\n",
      "11     \t [0.         0.56952745 1.        ]. \t  2.0579279795882806 \t 3.3396690641491635\n",
      "12     \t [0.68159768 0.65817201 0.81674982]. \t  3.2497563357842556 \t 3.3396690641491635\n",
      "13     \t [0.68908151 0.37592649 1.        ]. \t  1.4447433849242608 \t 3.3396690641491635\n",
      "14     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.3396690641491635\n",
      "15     \t [0.         0.         0.93977885]. \t  0.1506851005648569 \t 3.3396690641491635\n",
      "16     \t [0.         0.71897061 0.4926643 ]. \t  2.1050528136286073 \t 3.3396690641491635\n",
      "17     \t [0.47670685 1.         0.        ]. \t  0.000213401478761039 \t 3.3396690641491635\n",
      "18     \t [1.         0.71152257 1.        ]. \t  1.6369487479653115 \t 3.3396690641491635\n",
      "19     \t [0.         0.         0.69462795]. \t  0.1925738813475854 \t 3.3396690641491635\n",
      "20     \t [0.99484043 0.40237418 0.81803342]. \t  2.949418539811057 \t 3.3396690641491635\n",
      "21     \t [0.10540598 1.         0.35504066]. \t  0.7730269458263469 \t 3.3396690641491635\n",
      "22     \t [0.08576365 0.         0.        ]. \t  0.08038999981218503 \t 3.3396690641491635\n",
      "23     \t [0.29607492 0.62126578 0.        ]. \t  0.009215328117004111 \t 3.3396690641491635\n",
      "24     \t [1.         0.         0.71820499]. \t  0.21014295952526052 \t 3.3396690641491635\n",
      "25     \t [0.63843099 0.85440694 0.93712055]. \t  1.3842134102844637 \t 3.3396690641491635\n",
      "26     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.3396690641491635\n",
      "27     \t [0.35862246 0.5321103  0.74784793]. \t  3.0141907236115495 \t 3.3396690641491635\n",
      "28     \t [0.99999999 0.8298854  0.83267615]. \t  1.6898000977705834 \t 3.3396690641491635\n",
      "29     \t [0.         0.78974063 0.86258542]. \t  2.326206546967447 \t 3.3396690641491635\n",
      "30     \t [0.         0.99999989 0.71001626]. \t  1.4244010109362903 \t 3.3396690641491635\n",
      "31     \t [3.71670956e-08 3.78444299e-01 7.80939541e-01]. \t  2.6358998352548433 \t 3.3396690641491635\n",
      "32     \t [0.55190901 0.75831907 0.57873631]. \t  1.6853663376379822 \t 3.3396690641491635\n",
      "33     \t [0.00000000e+00 9.99999999e-01 3.06607186e-11]. \t  0.0002735367694700551 \t 3.3396690641491635\n",
      "34     \t [0.51438648 0.13364806 0.62148412]. \t  0.3430355524950182 \t 3.3396690641491635\n",
      "35     \t [0.06293522 0.83845774 0.66680146]. \t  2.518751420246468 \t 3.3396690641491635\n",
      "36     \t [0.82803501 0.75130533 0.12609098]. \t  0.0074436256194782115 \t 3.3396690641491635\n",
      "37     \t [0.9733965  0.04824388 0.53741531]. \t  0.09379129043720691 \t 3.3396690641491635\n",
      "38     \t [0.48614181 0.08087374 0.51203787]. \t  0.21111004333001018 \t 3.3396690641491635\n",
      "39     \t [0.88943879 0.76823097 0.37216405]. \t  0.16688068067390233 \t 3.3396690641491635\n",
      "40     \t [0.63740215 0.69850905 0.29943415]. \t  0.1626293940305324 \t 3.3396690641491635\n",
      "41     \t [0.01492616 0.79914268 0.07117118]. \t  0.004550701126035047 \t 3.3396690641491635\n",
      "42     \t [0.78730864 0.07818705 0.43092649]. \t  0.2718207112697513 \t 3.3396690641491635\n",
      "43     \t [0.71927048 0.24035136 0.84679427]. \t  1.5604251042519215 \t 3.3396690641491635\n",
      "44     \t [0.87898851 0.65115683 0.29056754]. \t  0.06469019948941754 \t 3.3396690641491635\n",
      "45     \t [0.23311641 0.99245468 0.17490467]. \t  0.03315245642980158 \t 3.3396690641491635\n",
      "46     \t [0.24500579 0.74221125 0.28871464]. \t  0.3054818590008818 \t 3.3396690641491635\n",
      "47     \t [0.63775282 0.84122823 0.98907381]. \t  1.0848821100342865 \t 3.3396690641491635\n",
      "48     \t [0.60697413 0.69697736 0.32770775]. \t  0.25036427211621354 \t 3.3396690641491635\n",
      "49     \t [0.87544484 0.55099794 0.57339059]. \t  0.6560061477708956 \t 3.3396690641491635\n",
      "50     \t [0.82259705 0.18715503 0.24239246]. \t  0.5043136364337744 \t 3.3396690641491635\n",
      "51     \t [0.75320002 0.16866405 0.87634165]. \t  0.9279718806791477 \t 3.3396690641491635\n",
      "52     \t [0.14283124 0.96982648 0.46066381]. \t  2.0942187826157297 \t 3.3396690641491635\n",
      "53     \t [0.02591159 0.27443151 0.46957918]. \t  0.2902708106392406 \t 3.3396690641491635\n",
      "54     \t [0.92469231 0.95738827 0.17634737]. \t  0.005448549580207616 \t 3.3396690641491635\n",
      "55     \t [0.52122534 0.89344579 0.92944922]. \t  1.1426180370132888 \t 3.3396690641491635\n",
      "56     \t [0.67393751 0.46596782 0.65886108]. \t  1.552198018826415 \t 3.3396690641491635\n",
      "57     \t [0.02708918 0.30626609 0.66384281]. \t  1.1499203951970487 \t 3.3396690641491635\n",
      "58     \t [0.83932096 0.82726693 0.50711291]. \t  0.6015910919907921 \t 3.3396690641491635\n",
      "59     \t [0.33381652 0.81356766 0.67690831]. \t  2.2464218528896813 \t 3.3396690641491635\n",
      "60     \t [0.44425173 0.05487545 0.10567923]. \t  0.4320337032660455 \t 3.3396690641491635\n",
      "61     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.3396690641491635\n",
      "62     \t [0.92301807 0.88585592 0.01980938]. \t  0.00024849928122262397 \t 3.3396690641491635\n",
      "63     \t [0.43194257 0.06871556 0.38614572]. \t  0.6366288501403745 \t 3.3396690641491635\n",
      "64     \t [0.31122698 0.46711811 0.02088006]. \t  0.04710443979578095 \t 3.3396690641491635\n",
      "65     \t [0.53570239 0.57571895 0.01116363]. \t  0.015773597038891984 \t 3.3396690641491635\n",
      "66     \t [0.04200811 0.25816403 0.12176415]. \t  0.31521747282416335 \t 3.3396690641491635\n",
      "67     \t [0.77935401 0.26930086 0.86935596]. \t  1.7638500802766433 \t 3.3396690641491635\n",
      "68     \t [0.43283647 0.76036953 0.41310426]. \t  1.0749063091057893 \t 3.3396690641491635\n",
      "69     \t [0.22385249 0.80812974 0.91181421]. \t  1.972010837229594 \t 3.3396690641491635\n",
      "70     \t [0.07828383 0.52821353 0.26729066]. \t  0.2197769127026234 \t 3.3396690641491635\n",
      "71     \t [0.25804814 0.13996467 0.81905318]. \t  0.8299592570505872 \t 3.3396690641491635\n",
      "72     \t [0.78420305 0.50520713 0.67589569]. \t  1.7511888039416958 \t 3.3396690641491635\n",
      "73     \t [0.23357874 0.73727731 0.3789872 ]. \t  0.9613318277059854 \t 3.3396690641491635\n",
      "74     \t [0.37514763 0.93238186 0.40027999]. \t  1.14811975387224 \t 3.3396690641491635\n",
      "75     \t [0.02769668 0.13806226 0.81265602]. \t  0.8084523199876115 \t 3.3396690641491635\n",
      "76     \t [0.63695442 0.19345711 0.67316354]. \t  0.7158065553199365 \t 3.3396690641491635\n",
      "77     \t [0.11305821 0.17562971 0.00982145]. \t  0.10865617664614267 \t 3.3396690641491635\n",
      "78     \t [0.5557356  0.45374772 0.24007706]. \t  0.29810480269679684 \t 3.3396690641491635\n",
      "79     \t [0.10194547 0.48337965 0.67262343]. \t  2.0130384067844784 \t 3.3396690641491635\n",
      "80     \t [0.3814367  0.81623411 0.17323498]. \t  0.03531819248838329 \t 3.3396690641491635\n",
      "81     \t [0.71959655 0.58015454 0.48968629]. \t  0.49473086569047553 \t 3.3396690641491635\n",
      "82     \t [0.92160388 0.16911816 0.78896864]. \t  0.9773645613769912 \t 3.3396690641491635\n",
      "83     \t [0.7813934  0.41506123 0.59324154]. \t  0.7473692311705328 \t 3.3396690641491635\n",
      "84     \t [0.93772354 0.22711269 0.86410544]. \t  1.3827203356587456 \t 3.3396690641491635\n",
      "85     \t [0.65901222 0.69788122 0.75645844]. \t  2.430455087973219 \t 3.3396690641491635\n",
      "86     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.3396690641491635\n",
      "87     \t [0.63723314 0.7948855  0.96558784]. \t  1.6083594950866844 \t 3.3396690641491635\n",
      "88     \t [0.72355056 0.33679754 0.23619852]. \t  0.4135885650455526 \t 3.3396690641491635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.18214649 0.81090828 0.03565757]. \t  0.002337368518537894 \t 3.3396690641491635\n",
      "90     \t [0.96134076 0.4161567  0.84804431]. \t  3.128094952430045 \t 3.3396690641491635\n",
      "91     \t [0.48019505 0.97477795 0.32725872]. \t  0.3799456516353434 \t 3.3396690641491635\n",
      "92     \t [0.67927896 0.80176736 0.89060317]. \t  2.078252412011598 \t 3.3396690641491635\n",
      "93     \t [0.65917194 0.14424592 0.1377917 ]. \t  0.46612716240634694 \t 3.3396690641491635\n",
      "94     \t [0.98883086 0.81042525 0.54541417]. \t  0.3860550087247636 \t 3.3396690641491635\n",
      "95     \t [0.28214584 0.3664615  0.5949526 ]. \t  0.8408457870693642 \t 3.3396690641491635\n",
      "96     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.3396690641491635\n",
      "97     \t [0.77153192 0.85421568 0.09593543]. \t  0.002562590911587586 \t 3.3396690641491635\n",
      "98     \t [0.13704579 0.51048051 0.96517546]. \t  2.626575319576149 \t 3.3396690641491635\n",
      "99     \t [0.23875162 0.45660836 0.35404049]. \t  0.39512902081682366 \t 3.3396690641491635\n",
      "100    \t [0.33108169 0.36912216 0.98071341]. \t  1.6709221372899323 \t 3.3396690641491635\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [0.         1.         0.08840389]. \t  0.003726219326288939 \t 2.610000357863649\n",
      "4      \t [1.         1.         0.41368439]. \t  0.1313652212203118 \t 2.610000357863649\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
      "6      \t [1.         0.36533768 0.        ]. \t  0.019159653422992654 \t 2.610000357863649\n",
      "7      \t [0.28966954 1.         0.55688454]. \t  2.343238878166749 \t 2.610000357863649\n",
      "8      \t [1.         0.37435725 0.9540355 ]. \t  1.9787640482364381 \t 2.610000357863649\n",
      "9      \t [0.         1.         0.57285943]. \t  2.4758211248168562 \t 2.610000357863649\n",
      "10     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.610000357863649\n",
      "11     \t [0.19155305 0.47168886 0.        ]. \t  0.030379613793761088 \t 2.610000357863649\n",
      "12     \t [1.         0.40570929 0.57525924]. \t  0.5341134608569804 \t 2.610000357863649\n",
      "13     \t [1.         0.62945073 1.        ]. \t  1.933884786217444 \t 2.610000357863649\n",
      "14     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.610000357863649\n",
      "15     \t [0.         0.63986519 0.76144802]. \t  \u001b[92m3.0477229740302754\u001b[0m \t 3.0477229740302754\n",
      "16     \t [0.         0.44275435 1.        ]. \t  1.7663166814876976 \t 3.0477229740302754\n",
      "17     \t [0.51998932 0.         0.        ]. \t  0.09547873428561021 \t 3.0477229740302754\n",
      "18     \t [0.         0.40289828 0.50630868]. \t  0.517396546907301 \t 3.0477229740302754\n",
      "19     \t [0.64029975 1.         0.48338841]. \t  0.9471219961575486 \t 3.0477229740302754\n",
      "20     \t [0.28635739 0.73732046 0.94164816]. \t  2.326819030962188 \t 3.0477229740302754\n",
      "21     \t [0.         0.68503131 1.        ]. \t  1.798461986722294 \t 3.0477229740302754\n",
      "22     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.0477229740302754\n",
      "23     \t [0.38417068 0.99436181 0.83398777]. \t  0.7450229029230726 \t 3.0477229740302754\n",
      "24     \t [0.38671892 0.23986681 1.        ]. \t  0.7326363159821659 \t 3.0477229740302754\n",
      "25     \t [0.18364581 0.59319131 0.76786106]. \t  \u001b[92m3.268329534395204\u001b[0m \t 3.268329534395204\n",
      "26     \t [0.71099039 0.27042326 0.        ]. \t  0.06522561812342177 \t 3.268329534395204\n",
      "27     \t [1.         1.         0.73736287]. \t  0.37646067467996114 \t 3.268329534395204\n",
      "28     \t [0.12412212 0.87425628 0.73784632]. \t  1.8929821661570267 \t 3.268329534395204\n",
      "29     \t [0.75142409 0.56668561 1.        ]. \t  2.058964021193554 \t 3.268329534395204\n",
      "30     \t [0.25625197 1.         0.2104806 ]. \t  0.06880524230917133 \t 3.268329534395204\n",
      "31     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.268329534395204\n",
      "32     \t [0.84081434 0.         0.48849201]. \t  0.12044165980455546 \t 3.268329534395204\n",
      "33     \t [0.         0.66460838 0.        ]. \t  0.004069117117174557 \t 3.268329534395204\n",
      "34     \t [8.90867977e-01 7.24577291e-01 5.18155541e-09]. \t  0.0013286844586681754 \t 3.268329534395204\n",
      "35     \t [1.         0.         0.24581882]. \t  0.2604184723164976 \t 3.268329534395204\n",
      "36     \t [0.07506889 0.         0.77928876]. \t  0.24732416427320053 \t 3.268329534395204\n",
      "37     \t [0.35746242 0.73898992 0.64314415]. \t  2.3072794093048543 \t 3.268329534395204\n",
      "38     \t [0.23071425 0.45723761 0.84260769]. \t  \u001b[92m3.5366963501854385\u001b[0m \t 3.5366963501854385\n",
      "39     \t [0.43549475 0.51451638 0.86140894]. \t  \u001b[92m3.7899809095822095\u001b[0m \t 3.7899809095822095\n",
      "40     \t [0.57240161 0.38950618 0.82484524]. \t  2.9781488932508897 \t 3.7899809095822095\n",
      "41     \t [0.29095318 0.32092887 0.84036205]. \t  2.3584862988960325 \t 3.7899809095822095\n",
      "42     \t [0.3618192  0.54621352 0.85267313]. \t  \u001b[92m3.857064911522959\u001b[0m \t 3.857064911522959\n",
      "43     \t [0.48200135 0.62917996 0.84300676]. \t  3.6293250291189123 \t 3.857064911522959\n",
      "44     \t [0.47737427 0.50702058 0.99921802]. \t  2.0263853793206006 \t 3.857064911522959\n",
      "45     \t [0.86613071 0.4080287  0.84631106]. \t  3.09614567554114 \t 3.857064911522959\n",
      "46     \t [0.77292986 0.53338191 0.8325803 ]. \t  3.703211581267123 \t 3.857064911522959\n",
      "47     \t [0.51686216 0.59547314 0.81787255]. \t  3.6361361194321926 \t 3.857064911522959\n",
      "48     \t [0.02143167 0.46589127 0.81452755]. \t  3.4547354078381023 \t 3.857064911522959\n",
      "49     \t [0.11426209 0.58168064 0.88830222]. \t  3.697907634602421 \t 3.857064911522959\n",
      "50     \t [0.66493919 0.02236047 0.9039821 ]. \t  0.23770703169882063 \t 3.857064911522959\n",
      "51     \t [0.23583286 0.55180261 0.83685406]. \t  3.8358976067261676 \t 3.857064911522959\n",
      "52     \t [0.13989507 0.48054837 0.90161422]. \t  3.400406567039254 \t 3.857064911522959\n",
      "53     \t [0.5111791  0.68212044 0.84907853]. \t  3.2889254769427283 \t 3.857064911522959\n",
      "54     \t [0.38986295 0.52899791 0.81242769]. \t  3.6863635906832815 \t 3.857064911522959\n",
      "55     \t [0.23577178 0.52439701 0.90632702]. \t  3.5317873453996995 \t 3.857064911522959\n",
      "56     \t [0.87438146 0.55299619 0.85486004]. \t  3.727007786061604 \t 3.857064911522959\n",
      "57     \t [0.14267947 0.49937013 0.82475109]. \t  3.6825141961190733 \t 3.857064911522959\n",
      "58     \t [0.44660655 0.59127984 0.7972218 ]. \t  3.503562591746211 \t 3.857064911522959\n",
      "59     \t [0.97886458 0.51205215 0.81735679]. \t  3.4992359860678026 \t 3.857064911522959\n",
      "60     \t [0.65101797 0.57024865 0.88380987]. \t  3.7178970718906834 \t 3.857064911522959\n",
      "61     \t [0.75214102 0.52029793 0.8212021 ]. \t  3.6325261178316084 \t 3.857064911522959\n",
      "62     \t [0.55469882 0.54897763 0.85721779]. \t  3.8293666595462295 \t 3.857064911522959\n",
      "63     \t [0.99673217 0.59663525 0.85441138]. \t  3.5965665351648743 \t 3.857064911522959\n",
      "64     \t [0.18764273 0.52880381 0.85311775]. \t  3.8302199371234913 \t 3.857064911522959\n",
      "65     \t [1. 0. 0.]. \t  0.03095471705739747 \t 3.857064911522959\n",
      "66     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.857064911522959\n",
      "67     \t [0.5441623  0.14054254 0.67376126]. \t  0.52555689094175 \t 3.857064911522959\n",
      "68     \t [0.24409945 0.746514   0.37579527]. \t  0.9428705896689451 \t 3.857064911522959\n",
      "69     \t [0.82250668 0.92070626 0.57262634]. \t  0.703700240487046 \t 3.857064911522959\n",
      "70     \t [0.80836442 0.11740861 0.54025285]. \t  0.1632805156707197 \t 3.857064911522959\n",
      "71     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.857064911522959\n",
      "72     \t [0.82454869 0.54477795 0.6759933 ]. \t  1.7256223502834849 \t 3.857064911522959\n",
      "73     \t [0.05346613 0.14611429 0.75967501]. \t  0.8007182747420258 \t 3.857064911522959\n",
      "74     \t [0.7160286  0.14338135 0.99987794]. \t  0.3590223569676292 \t 3.857064911522959\n",
      "75     \t [0.12756166 0.45359537 0.70827229]. \t  2.324034308079876 \t 3.857064911522959\n",
      "76     \t [0.63784989 0.52281331 0.76355988]. \t  3.089109868038915 \t 3.857064911522959\n",
      "77     \t [0.11502712 0.97940057 0.38983881]. \t  1.1868805497135508 \t 3.857064911522959\n",
      "78     \t [0.58754603 0.62029323 0.99348871]. \t  2.1370125930544903 \t 3.857064911522959\n",
      "79     \t [0.9654962  0.13244232 0.08087639]. \t  0.12090328344314215 \t 3.857064911522959\n",
      "80     \t [0.56999521 0.9768264  0.45899119]. \t  1.087354356012516 \t 3.857064911522959\n",
      "81     \t [0.01465277 0.63397872 0.83901536]. \t  3.591801365983141 \t 3.857064911522959\n",
      "82     \t [0.47352203 0.15129322 0.67759988]. \t  0.5793130146471712 \t 3.857064911522959\n",
      "83     \t [0.88256879 0.24667527 0.44384711]. \t  0.19112385305007243 \t 3.857064911522959\n",
      "84     \t [0.02025095 0.62794742 0.34387863]. \t  0.468892639341909 \t 3.857064911522959\n",
      "85     \t [0.66323936 0.67504463 0.77717188]. \t  2.786061946768787 \t 3.857064911522959\n",
      "86     \t [0.85617716 0.46750586 0.46817443]. \t  0.21337031939834197 \t 3.857064911522959\n",
      "87     \t [0.43523261 0.5618435  0.1594736 ]. \t  0.10387864219465551 \t 3.857064911522959\n",
      "88     \t [0.57278088 0.94051814 0.68332155]. \t  1.2067282037806892 \t 3.857064911522959\n",
      "89     \t [0.58636839 0.44979184 0.79863073]. \t  3.2622995894011355 \t 3.857064911522959\n",
      "90     \t [0.8312183  0.47740455 0.92330077]. \t  3.0908653462102595 \t 3.857064911522959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.28318428 0.71248563 0.53696315]. \t  2.2598699965857656 \t 3.857064911522959\n",
      "92     \t [0.1244726  0.52626388 0.13790217]. \t  0.09965785828484391 \t 3.857064911522959\n",
      "93     \t [0.38390603 0.72920448 0.50681676]. \t  1.9052606070746767 \t 3.857064911522959\n",
      "94     \t [0.12061518 0.47207798 0.68895717]. \t  2.1610137115993298 \t 3.857064911522959\n",
      "95     \t [0.96727165 0.74151729 0.17567326]. \t  0.00909902576108185 \t 3.857064911522959\n",
      "96     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.857064911522959\n",
      "97     \t [0.38754664 0.11656231 0.04666429]. \t  0.23190351032409634 \t 3.857064911522959\n",
      "98     \t [0.38700234 0.62982399 0.08687214]. \t  0.028972305826335615 \t 3.857064911522959\n",
      "99     \t [0.20002994 0.83101229 0.68104182]. \t  2.397476855481273 \t 3.857064911522959\n",
      "100    \t [0.14988495 0.10954133 0.79679232]. \t  0.6536039919046039 \t 3.857064911522959\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0.07913177 1.         1.        ]. \t  0.332197668425402 \t 1.540625560354162\n",
      "2      \t [0.68730545 0.         1.        ]. \t  0.0909771932339098 \t 1.540625560354162\n",
      "3      \t [0.05658752 1.         0.11164072]. \t  0.007164834472589218 \t 1.540625560354162\n",
      "4      \t [0.62238549 1.         0.92455521]. \t  0.5248929113076475 \t 1.540625560354162\n",
      "5      \t [0.27335201 0.         0.60346098]. \t  0.12236916866822137 \t 1.540625560354162\n",
      "6      \t [0.37981289 0.60019947 0.63914938]. \t  \u001b[92m1.9369966491257762\u001b[0m \t 1.9369966491257762\n",
      "7      \t [0.01453557 0.86108529 0.56377099]. \t  \u001b[92m3.0096265033004514\u001b[0m \t 3.0096265033004514\n",
      "8      \t [1.         0.         0.51722988]. \t  0.06855101097892474 \t 3.0096265033004514\n",
      "9      \t [0.        1.        0.5549949]. \t  2.490656480978106 \t 3.0096265033004514\n",
      "10     \t [0.         0.57916665 0.79272563]. \t  \u001b[92m3.4921151473593417\u001b[0m \t 3.4921151473593417\n",
      "11     \t [0.         0.29655645 1.        ]. \t  1.0039839967118966 \t 3.4921151473593417\n",
      "12     \t [0.76527156 0.         0.        ]. \t  0.06381931961505813 \t 3.4921151473593417\n",
      "13     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.4921151473593417\n",
      "14     \t [0.         0.47116465 0.6120311 ]. \t  1.3581664794032444 \t 3.4921151473593417\n",
      "15     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.4921151473593417\n",
      "16     \t [0.72110176 0.60615287 1.        ]. \t  2.0332373699002013 \t 3.4921151473593417\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.4921151473593417\n",
      "18     \t [0.18930791 0.61907525 1.        ]. \t  2.0277707351337386 \t 3.4921151473593417\n",
      "19     \t [0.26562165 0.51481524 0.        ]. \t  0.023404847582624722 \t 3.4921151473593417\n",
      "20     \t [0.         0.73162252 1.        ]. \t  1.5816813255423539 \t 3.4921151473593417\n",
      "21     \t [1.         0.31824874 1.        ]. \t  1.0959591783402034 \t 3.4921151473593417\n",
      "22     \t [0.19074752 0.96012319 0.73955105]. \t  1.4074710974939109 \t 3.4921151473593417\n",
      "23     \t [0.64761976 1.         0.        ]. \t  0.000143025432089795 \t 3.4921151473593417\n",
      "24     \t [1.        0.        0.1481096]. \t  0.17240082185792893 \t 3.4921151473593417\n",
      "25     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.4921151473593417\n",
      "26     \t [0.84415391 1.         0.25771065]. \t  0.0360434490362331 \t 3.4921151473593417\n",
      "27     \t [0.32560502 0.         0.15365772]. \t  0.5886451575689311 \t 3.4921151473593417\n",
      "28     \t [0.         0.68024661 0.11860655]. \t  0.0209898422720186 \t 3.4921151473593417\n",
      "29     \t [0.43317091 0.30906378 0.9305501 ]. \t  1.7748834142365693 \t 3.4921151473593417\n",
      "30     \t [0.         0.         0.87276756]. \t  0.21467756012468178 \t 3.4921151473593417\n",
      "31     \t [6.84912736e-09 8.36488057e-01 8.36488056e-01]. \t  1.917300464472247 \t 3.4921151473593417\n",
      "32     \t [0.81659609 0.3204685  0.81866347]. \t  2.292766041118082 \t 3.4921151473593417\n",
      "33     \t [0.97518667 0.60797034 0.79669756]. \t  3.1887489298251253 \t 3.4921151473593417\n",
      "34     \t [0.17011725 0.97932824 0.2722827 ]. \t  0.2427526183895619 \t 3.4921151473593417\n",
      "35     \t [0.         0.         0.29831758]. \t  0.5636063964488721 \t 3.4921151473593417\n",
      "36     \t [0.4289596 0.2501682 0.       ]. \t  0.0971409810471757 \t 3.4921151473593417\n",
      "37     \t [0.99429214 0.71194938 0.98908605]. \t  1.7890855670221537 \t 3.4921151473593417\n",
      "38     \t [0.69143978 0.65969836 0.82510249]. \t  3.2908888129426224 \t 3.4921151473593417\n",
      "39     \t [0.95240871 0.85168521 0.81739292]. \t  1.4590198873094615 \t 3.4921151473593417\n",
      "40     \t [0.56449376 0.63581622 0.8670616 ]. \t  \u001b[92m3.5842312653225354\u001b[0m \t 3.5842312653225354\n",
      "41     \t [0.43414743 0.82012562 0.92436913]. \t  1.7739179917134593 \t 3.5842312653225354\n",
      "42     \t [0.62254208 0.00388764 0.38948733]. \t  0.4659505740492604 \t 3.5842312653225354\n",
      "43     \t [0.58276729 0.47089066 0.69601991]. \t  2.0951921334187444 \t 3.5842312653225354\n",
      "44     \t [0.99897913 0.34791623 0.79107303]. \t  2.381636454351097 \t 3.5842312653225354\n",
      "45     \t [0.89196025 0.63099358 0.84676409]. \t  3.480191238714794 \t 3.5842312653225354\n",
      "46     \t [0.13831932 0.56162495 0.84586991]. \t  \u001b[92m3.8419099390250557\u001b[0m \t 3.8419099390250557\n",
      "47     \t [0.04659287 0.3839034  0.84791154]. \t  2.931778777902881 \t 3.8419099390250557\n",
      "48     \t [0.85029224 0.47746913 0.90671111]. \t  3.273734424265231 \t 3.8419099390250557\n",
      "49     \t [0.1280781  0.71043487 0.78441767]. \t  2.894436630705653 \t 3.8419099390250557\n",
      "50     \t [0.82601487 0.61188554 0.87635059]. \t  3.592051209019499 \t 3.8419099390250557\n",
      "51     \t [0.77466552 0.51472641 0.82920059]. \t  3.658761375513091 \t 3.8419099390250557\n",
      "52     \t [0.15869012 0.42539188 0.84517879]. \t  3.306204402569338 \t 3.8419099390250557\n",
      "53     \t [0.02949578 0.52997778 0.92497741]. \t  3.2843093921713233 \t 3.8419099390250557\n",
      "54     \t [0.80988227 0.57396531 0.87442095]. \t  3.706679989636221 \t 3.8419099390250557\n",
      "55     \t [0.78831765 0.61339268 0.82612554]. \t  3.5152079613001934 \t 3.8419099390250557\n",
      "56     \t [0.70285127 0.58917731 0.85457659]. \t  3.7391721711693338 \t 3.8419099390250557\n",
      "57     \t [0.54134615 0.55111158 0.84553353]. \t  3.8245307409040867 \t 3.8419099390250557\n",
      "58     \t [0.57100029 0.71412922 0.11645354]. \t  0.016504686611962843 \t 3.8419099390250557\n",
      "59     \t [0.52325185 0.61526784 0.82184117]. \t  3.5877434575736054 \t 3.8419099390250557\n",
      "60     \t [0.22307907 0.55979989 0.83730848]. \t  3.8354279025583025 \t 3.8419099390250557\n",
      "61     \t [0.18724942 0.5235326  0.81802404]. \t  3.719392829861624 \t 3.8419099390250557\n",
      "62     \t [0.91173321 0.56749295 0.82203114]. \t  3.571275009123716 \t 3.8419099390250557\n",
      "63     \t [0.46641273 0.58800702 0.90844736]. \t  3.5258889615723086 \t 3.8419099390250557\n",
      "64     \t [0.21978694 0.56140554 0.82618876]. \t  3.7918318922578527 \t 3.8419099390250557\n",
      "65     \t [0.16408977 0.59337217 0.8740827 ]. \t  3.760467292195661 \t 3.8419099390250557\n",
      "66     \t [0.58324905 0.66571806 0.87238852]. \t  3.3813130508984424 \t 3.8419099390250557\n",
      "67     \t [1.38737255e-09 5.69079206e-01 8.61581103e-01]. \t  3.7972635106041337 \t 3.8419099390250557\n",
      "68     \t [0.29148726 0.60149522 0.7994831 ]. \t  3.537688600963761 \t 3.8419099390250557\n",
      "69     \t [0.20610771 0.54167336 0.86818923]. \t  3.8249100181975373 \t 3.8419099390250557\n",
      "70     \t [0.13250116 0.58564284 0.82985387]. \t  3.7663806327468907 \t 3.8419099390250557\n",
      "71     \t [0.68972288 0.5979307  0.77144558]. \t  3.074797884679753 \t 3.8419099390250557\n",
      "72     \t [0.2949422  0.61369666 0.87519412]. \t  3.7008287547092187 \t 3.8419099390250557\n",
      "73     \t [0.3662879  0.6236053  0.84029481]. \t  3.6738729433577983 \t 3.8419099390250557\n",
      "74     \t [0.60767664 0.52111926 0.86551856]. \t  3.769805716921334 \t 3.8419099390250557\n",
      "75     \t [0.4871254  0.52984346 0.82683505]. \t  3.7571558209719513 \t 3.8419099390250557\n",
      "76     \t [0.78033333 0.56435585 0.91676593]. \t  3.3977313459139586 \t 3.8419099390250557\n",
      "77     \t [0.27905156 0.48833481 0.90169023]. \t  3.4538553286146687 \t 3.8419099390250557\n",
      "78     \t [0.29176936 0.57435684 0.85205994]. \t  \u001b[92m3.847981413179243\u001b[0m \t 3.847981413179243\n",
      "79     \t [0.23971743 0.57924799 0.87058792]. \t  3.8104225571908965 \t 3.847981413179243\n",
      "80     \t [0.36969825 0.62193064 0.88236617]. \t  3.631351212969145 \t 3.847981413179243\n",
      "81     \t [0.00223087 0.62100762 0.84302979]. \t  3.6566405900209893 \t 3.847981413179243\n",
      "82     \t [1.03855291e-08 1.95383893e-01 2.09120559e-02]. \t  0.10117977794270612 \t 3.847981413179243\n",
      "83     \t [0.86849217 0.5815052  0.86750985]. \t  3.6892206450215186 \t 3.847981413179243\n",
      "84     \t [0.45554389 0.56453606 0.87103392]. \t  3.817638523035912 \t 3.847981413179243\n",
      "85     \t [0.31987897 0.57677953 0.8572652 ]. \t  3.8429444041621084 \t 3.847981413179243\n",
      "86     \t [0.97932041 0.51371194 0.86888186]. \t  3.6176341285784863 \t 3.847981413179243\n",
      "87     \t [0.54219151 0.57182538 0.82481149]. \t  3.7271150611425483 \t 3.847981413179243\n",
      "88     \t [0.19333861 0.52943629 0.84441475]. \t  3.82755004937123 \t 3.847981413179243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.83745476 0.00731843 0.86691495]. \t  0.23612529762012008 \t 3.847981413179243\n",
      "90     \t [0.14392659 0.55259345 0.8598777 ]. \t  3.842484883393684 \t 3.847981413179243\n",
      "91     \t [0.15422424 0.51028812 0.86583017]. \t  3.757498895525971 \t 3.847981413179243\n",
      "92     \t [0.66882374 0.62941179 0.81375748]. \t  3.402796766106826 \t 3.847981413179243\n",
      "93     \t [0.00699248 0.48319873 0.84667419]. \t  3.6371181109678674 \t 3.847981413179243\n",
      "94     \t [0.15838102 0.53702434 0.90588317]. \t  3.5519377057061154 \t 3.847981413179243\n",
      "95     \t [0.18900851 0.54422568 0.87385412]. \t  3.804083153529043 \t 3.847981413179243\n",
      "96     \t [0.18487955 0.54099585 0.87645764]. \t  3.7882177287550896 \t 3.847981413179243\n",
      "97     \t [0.02767114 0.54582975 0.8676115 ]. \t  3.7935121405062504 \t 3.847981413179243\n",
      "98     \t [0.72665915 0.48520817 0.85434783]. \t  3.6328765874134885 \t 3.847981413179243\n",
      "99     \t [0.6198765  0.50286704 0.85537678]. \t  3.729780425727182 \t 3.847981413179243\n",
      "100    \t [0.7484816  0.568986   0.88041113]. \t  3.708972976074263 \t 3.847981413179243\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.04419304 0.         1.        ]. \t  0.09059079505432772 \t 3.8084053754826726\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.8084053754826726\n",
      "3      \t [0.         0.31486772 0.24627992]. \t  0.4510536979399161 \t 3.8084053754826726\n",
      "4      \t [0.         0.50217641 1.        ]. \t  1.9731462581685177 \t 3.8084053754826726\n",
      "5      \t [0.40639127 0.16296435 0.66045033]. \t  0.5559448572113304 \t 3.8084053754826726\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "7      \t [0.89774164 0.90459781 0.8796135 ]. \t  1.13817688027123 \t 3.8084053754826726\n",
      "8      \t [0.         0.76613789 0.57756763]. \t  2.7724578737068533 \t 3.8084053754826726\n",
      "9      \t [1.         0.50337288 0.29971729]. \t  0.0770084470047792 \t 3.8084053754826726\n",
      "10     \t [0.42188458 0.88947572 0.74701323]. \t  1.5143011871036198 \t 3.8084053754826726\n",
      "11     \t [1.         0.56450316 1.        ]. \t  2.0086827044209112 \t 3.8084053754826726\n",
      "12     \t [0.52910422 0.43531188 0.        ]. \t  0.03943688368234011 \t 3.8084053754826726\n",
      "13     \t [0.30074206 0.52708798 0.50279732]. \t  0.9400016632303546 \t 3.8084053754826726\n",
      "14     \t [0.9420973 1.        0.       ]. \t  4.9199853831229184e-05 \t 3.8084053754826726\n",
      "15     \t [0.         0.         0.58543795]. \t  0.10319059724249993 \t 3.8084053754826726\n",
      "16     \t [0.58857007 0.70658654 1.        ]. \t  1.7190227472391681 \t 3.8084053754826726\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "18     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "19     \t [1.         0.         0.55577618]. \t  0.07192675850461855 \t 3.8084053754826726\n",
      "20     \t [0.38277623 0.32362088 1.        ]. \t  1.1675695872229939 \t 3.8084053754826726\n",
      "21     \t [1.         0.72057673 0.71461696]. \t  1.5340814928616484 \t 3.8084053754826726\n",
      "22     \t [0.59561321 1.         0.        ]. \t  0.00016434148471123586 \t 3.8084053754826726\n",
      "23     \t [0.         0.61666467 0.        ]. \t  0.006566545425133654 \t 3.8084053754826726\n",
      "24     \t [0.20587101 0.78213985 0.99475806]. \t  1.3864159010282222 \t 3.8084053754826726\n",
      "25     \t [0.         1.         0.60190698]. \t  2.3620450058044877 \t 3.8084053754826726\n",
      "26     \t [0.82772314 0.64925347 0.        ]. \t  0.0037063133415056017 \t 3.8084053754826726\n",
      "27     \t [1.56290070e-10 7.90611566e-01 8.27827733e-01]. \t  2.336174554046693 \t 3.8084053754826726\n",
      "28     \t [0.65464333 0.         0.        ]. \t  0.0800330420056081 \t 3.8084053754826726\n",
      "29     \t [2.79005496e-08 2.61054462e-01 8.12045580e-01]. \t  1.739944459892243 \t 3.8084053754826726\n",
      "30     \t [0.79991792 1.         0.36441133]. \t  0.2064663689331181 \t 3.8084053754826726\n",
      "31     \t [0.         0.99999997 0.27880303]. \t  0.25151039414447074 \t 3.8084053754826726\n",
      "32     \t [1.         0.74880531 1.        ]. \t  1.4524141408584934 \t 3.8084053754826726\n",
      "33     \t [0.74258438 0.68894248 0.73657708]. \t  2.182363479464753 \t 3.8084053754826726\n",
      "34     \t [0.21680154 0.4224351  0.94380356]. \t  2.5352027278447244 \t 3.8084053754826726\n",
      "35     \t [0.64529412 0.         0.84034443]. \t  0.2390238237409696 \t 3.8084053754826726\n",
      "36     \t [0.00097687 0.51396486 0.74522362]. \t  2.945576452019224 \t 3.8084053754826726\n",
      "37     \t [1.         1.         0.30835869]. \t  0.038425388990621256 \t 3.8084053754826726\n",
      "38     \t [0.83644834 0.55224282 0.96003681]. \t  2.745491697765175 \t 3.8084053754826726\n",
      "39     \t [0.17785935 0.68774965 0.7583646 ]. \t  2.8708724634647362 \t 3.8084053754826726\n",
      "40     \t [0.42177013 0.6047287  0.87201929]. \t  3.7371524825698756 \t 3.8084053754826726\n",
      "41     \t [0.65940728 0.45904193 0.77633948]. \t  3.0752377920385197 \t 3.8084053754826726\n",
      "42     \t [0.43834334 0.51175497 0.86840719]. \t  3.7637132630054206 \t 3.8084053754826726\n",
      "43     \t [0.45165109 0.56499811 0.88088013]. \t  3.7750782219962375 \t 3.8084053754826726\n",
      "44     \t [0.08078928 0.99999981 0.78336721]. \t  0.9455747934816419 \t 3.8084053754826726\n",
      "45     \t [0.35766359 0.61351708 0.75977259]. \t  3.0926877879218706 \t 3.8084053754826726\n",
      "46     \t [0.98085136 0.53918381 0.8744958 ]. \t  3.644685447017437 \t 3.8084053754826726\n",
      "47     \t [1.         0.35803419 0.86238478]. \t  2.5884803880977 \t 3.8084053754826726\n",
      "48     \t [0.98892569 0.58008369 0.81652021]. \t  3.4670538586560715 \t 3.8084053754826726\n",
      "49     \t [0.62106236 0.58956594 0.85703334]. \t  3.7648587942791423 \t 3.8084053754826726\n",
      "50     \t [0.94067825 0.00282799 0.17395274]. \t  0.2534628597158042 \t 3.8084053754826726\n",
      "51     \t [0.56223847 0.51355713 0.86966909]. \t  3.747631758250024 \t 3.8084053754826726\n",
      "52     \t [1.00000000e+00 1.30287686e-01 2.43341579e-11]. \t  0.03543331361142402 \t 3.8084053754826726\n",
      "53     \t [0.73361601 0.53785826 0.8499492 ]. \t  3.7706363450326053 \t 3.8084053754826726\n",
      "54     \t [0.37810595 0.48547868 0.8545584 ]. \t  3.6944824384353008 \t 3.8084053754826726\n",
      "55     \t [0.58711545 0.54239742 0.86460212]. \t  \u001b[92m3.8094183267521577\u001b[0m \t 3.8094183267521577\n",
      "56     \t [0.50735776 0.55230575 0.89256634]. \t  3.692878413932031 \t 3.8094183267521577\n",
      "57     \t [0.25136302 0.53771766 0.9243765 ]. \t  3.3422781344023296 \t 3.8094183267521577\n",
      "58     \t [0.45172059 0.55617792 0.90706084]. \t  3.5679252395065983 \t 3.8094183267521577\n",
      "59     \t [0.99210371 0.47093638 0.82019429]. \t  3.3801904799045968 \t 3.8094183267521577\n",
      "60     \t [0.99999781 0.63076583 0.85976974]. \t  3.4546157626678786 \t 3.8094183267521577\n",
      "61     \t [0.58100829 0.51315963 0.84699064]. \t  3.7670478979711 \t 3.8094183267521577\n",
      "62     \t [0.36749301 0.5353084  0.87486028]. \t  3.797084616277852 \t 3.8094183267521577\n",
      "63     \t [1.         0.54333802 0.83014317]. \t  3.598823039239572 \t 3.8094183267521577\n",
      "64     \t [0.8603906  0.50887699 0.84673326]. \t  3.668531035209394 \t 3.8094183267521577\n",
      "65     \t [0.31829415 0.51272439 0.83250722]. \t  3.767160535122776 \t 3.8094183267521577\n",
      "66     \t [0.51038006 0.41054938 0.84960209]. \t  3.1903245654315744 \t 3.8094183267521577\n",
      "67     \t [0.85469566 0.56437582 0.85927447]. \t  3.728891979430636 \t 3.8094183267521577\n",
      "68     \t [0.73578907 0.53544141 0.86732277]. \t  3.756092954589885 \t 3.8094183267521577\n",
      "69     \t [1.00000000e+00 2.80354839e-08 2.55202368e-08]. \t  0.030954731733309444 \t 3.8094183267521577\n",
      "70     \t [0.73828086 0.51416956 0.16225059]. \t  0.10097721825159194 \t 3.8094183267521577\n",
      "71     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8094183267521577\n",
      "72     \t [0.68752985 0.91708869 0.32884826]. \t  0.23485860023972074 \t 3.8094183267521577\n",
      "73     \t [0.33441761 0.7498     0.57979193]. \t  2.4374668523466974 \t 3.8094183267521577\n",
      "74     \t [0.01821837 0.06125134 0.50305178]. \t  0.16691697532744534 \t 3.8094183267521577\n",
      "75     \t [0.6199999  0.67361749 0.6570084 ]. \t  1.6762379630964075 \t 3.8094183267521577\n",
      "76     \t [0.01607279 0.86491636 0.99912064]. \t  0.873550371675762 \t 3.8094183267521577\n",
      "77     \t [0.34516894 0.87245412 0.01261572]. \t  0.0008504359352652114 \t 3.8094183267521577\n",
      "78     \t [0.5402775  0.11084036 0.71951798]. \t  0.5495022323459491 \t 3.8094183267521577\n",
      "79     \t [0.9480081  0.94259231 0.74700629]. \t  0.6253523675698478 \t 3.8094183267521577\n",
      "80     \t [0.08310822 0.13166705 0.87448687]. \t  0.7073593697548204 \t 3.8094183267521577\n",
      "81     \t [0.9573711  0.34027874 0.32495091]. \t  0.2008090808223381 \t 3.8094183267521577\n",
      "82     \t [0.71889719 0.86154601 0.78677426]. \t  1.387810450846854 \t 3.8094183267521577\n",
      "83     \t [0.6266223  0.45742366 0.14465752]. \t  0.16529669971798924 \t 3.8094183267521577\n",
      "84     \t [0.1070112  0.18095404 0.22045493]. \t  0.7325303941307997 \t 3.8094183267521577\n",
      "85     \t [0.56404089 0.4851528  0.63595788]. \t  1.4085342250678154 \t 3.8094183267521577\n",
      "86     \t [0.43892116 0.26254843 0.78721818]. \t  1.7210008628480868 \t 3.8094183267521577\n",
      "87     \t [0.38558712 0.16581331 0.59887999]. \t  0.3512752105104248 \t 3.8094183267521577\n",
      "88     \t [0.03188105 0.53292955 0.25350763]. \t  0.18648847658079717 \t 3.8094183267521577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.83669151 0.28453506 0.13416878]. \t  0.2302791142968332 \t 3.8094183267521577\n",
      "90     \t [0.85768254 0.7871128  0.83599386]. \t  2.156419749248716 \t 3.8094183267521577\n",
      "91     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8094183267521577\n",
      "92     \t [0.81174416 0.74959993 0.9268094 ]. \t  2.313399694605151 \t 3.8094183267521577\n",
      "93     \t [0.23209014 0.82288377 0.68282029]. \t  2.3689475747338604 \t 3.8094183267521577\n",
      "94     \t [0.62209714 0.27693319 0.35783341]. \t  0.5164228460111759 \t 3.8094183267521577\n",
      "95     \t [0.78469275 0.42110863 0.58256948]. \t  0.6744696322787973 \t 3.8094183267521577\n",
      "96     \t [0.56581891 0.14315824 0.27505852]. \t  0.8834667125710743 \t 3.8094183267521577\n",
      "97     \t [0.11835136 0.17764058 0.08677275]. \t  0.30038574577333 \t 3.8094183267521577\n",
      "98     \t [0.62945792 0.1604025  0.22358598]. \t  0.7562538003237188 \t 3.8094183267521577\n",
      "99     \t [0.97546352 0.36504299 0.38991969]. \t  0.13791651637606442 \t 3.8094183267521577\n",
      "100    \t [0.73376058 0.30712204 0.79077983]. \t  2.0952712170015175 \t 3.8094183267521577\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.         0.09733363 1.        ]. \t  0.23944238092735717 \t 3.1179188940604616\n",
      "2      \t [1.         0.94389995 1.        ]. \t  0.49616054538243065 \t 3.1179188940604616\n",
      "3      \t [0.85534606 0.10745261 1.        ]. \t  0.26080988771850844 \t 3.1179188940604616\n",
      "4      \t [0.00184528 0.77957667 1.        ]. \t  1.3257303472110982 \t 3.1179188940604616\n",
      "5      \t [0.47323381 0.75567518 1.        ]. \t  1.4740680194641147 \t 3.1179188940604616\n",
      "6      \t [1.         0.11708725 0.41488025]. \t  0.16692951035196246 \t 3.1179188940604616\n",
      "7      \t [0.         0.54344949 0.53756035]. \t  1.2540914562612548 \t 3.1179188940604616\n",
      "8      \t [0.44646105 0.13629368 0.56906351]. \t  0.24672337166254313 \t 3.1179188940604616\n",
      "9      \t [1.         0.59647184 0.65849172]. \t  1.311885786857347 \t 3.1179188940604616\n",
      "10     \t [0.38055479 0.60527193 0.521651  ]. \t  1.3351710790762539 \t 3.1179188940604616\n",
      "11     \t [0.91470691 0.         0.        ]. \t  0.04183241627530189 \t 3.1179188940604616\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.1179188940604616\n",
      "13     \t [0.86056132 1.         0.67582757]. \t  0.45869201589623343 \t 3.1179188940604616\n",
      "14     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1179188940604616\n",
      "15     \t [0.86772207 0.49081406 0.94632875]. \t  2.8181034223332793 \t 3.1179188940604616\n",
      "16     \t [0.         0.47994795 0.86573121]. \t  \u001b[92m3.59440102620636\u001b[0m \t 3.59440102620636\n",
      "17     \t [0.49482005 1.         0.        ]. \t  0.0002061214489009716 \t 3.59440102620636\n",
      "18     \t [0.73290256 0.39530947 0.74478612]. \t  2.3796882719763683 \t 3.59440102620636\n",
      "19     \t [0.50766553 0.         0.        ]. \t  0.09650741978937791 \t 3.59440102620636\n",
      "20     \t [0.         0.28333537 0.76531985]. \t  1.7695874316822895 \t 3.59440102620636\n",
      "21     \t [0.3092289  0.42630582 0.93893098]. \t  2.6344892531649826 \t 3.59440102620636\n",
      "22     \t [0.         0.44430036 1.        ]. \t  1.772959621464111 \t 3.59440102620636\n",
      "23     \t [0.         0.99245697 0.65408568]. \t  1.9954374197690843 \t 3.59440102620636\n",
      "24     \t [0.         0.59309613 0.        ]. \t  0.008209002389077692 \t 3.59440102620636\n",
      "25     \t [1.         0.32731996 0.        ]. \t  0.022808442911577433 \t 3.59440102620636\n",
      "26     \t [0.37569331 0.         0.91820114]. \t  0.1759142686659238 \t 3.59440102620636\n",
      "27     \t [1.         0.40968705 1.        ]. \t  1.5754531665299463 \t 3.59440102620636\n",
      "28     \t [0.13109459 0.69117619 0.80150818]. \t  3.11232373726407 \t 3.59440102620636\n",
      "29     \t [0.6235122 0.3841026 1.       ]. \t  1.4939276244686142 \t 3.59440102620636\n",
      "30     \t [0.         1.         0.86633336]. \t  0.6744932614682198 \t 3.59440102620636\n",
      "31     \t [1.         0.         0.77856776]. \t  0.24236908519219072 \t 3.59440102620636\n",
      "32     \t [0.75134347 0.86652521 0.9020083 ]. \t  1.432200010864357 \t 3.59440102620636\n",
      "33     \t [0.75090784 0.20950367 0.12533792]. \t  0.3237372880915953 \t 3.59440102620636\n",
      "34     \t [1.         1.         0.03712155]. \t  0.00010176098424324013 \t 3.59440102620636\n",
      "35     \t [0.99853182 0.66603714 0.99999998]. \t  1.8254000111788118 \t 3.59440102620636\n",
      "36     \t [0.         1.         0.34142752]. \t  0.6301582293084128 \t 3.59440102620636\n",
      "37     \t [0.1350685  0.462777   0.83341084]. \t  3.5413606373672133 \t 3.59440102620636\n",
      "38     \t [0.01531287 0.70247407 0.77457815]. \t  2.8608437144912644 \t 3.59440102620636\n",
      "39     \t [3.24605390e-09 2.99196423e-01 0.00000000e+00]. \t  0.05593820779895772 \t 3.59440102620636\n",
      "40     \t [0.43006464 0.41590474 0.81483877]. \t  3.1695699058208726 \t 3.59440102620636\n",
      "41     \t [0.16218736 0.53608206 0.84926114]. \t  \u001b[92m3.837850880317825\u001b[0m \t 3.837850880317825\n",
      "42     \t [0. 1. 0.]. \t  0.00027353676921007184 \t 3.837850880317825\n",
      "43     \t [0.67649801 0.84149783 0.33888668]. \t  0.2836989245946835 \t 3.837850880317825\n",
      "44     \t [0.33218932 0.55692604 0.34266385]. \t  0.37144478515930707 \t 3.837850880317825\n",
      "45     \t [0.4600414  0.15492279 0.513307  ]. \t  0.2517280105725702 \t 3.837850880317825\n",
      "46     \t [0.68466348 0.98355411 0.32230445]. \t  0.19501261561535138 \t 3.837850880317825\n",
      "47     \t [0.9529959  0.15801898 0.74378317]. \t  0.8124898466909471 \t 3.837850880317825\n",
      "48     \t [0.48565841 0.5055407  0.52301778]. \t  0.7532456792463784 \t 3.837850880317825\n",
      "49     \t [0.19068887 0.54653876 0.22469722]. \t  0.17468431327761985 \t 3.837850880317825\n",
      "50     \t [0.2511088 0.3079115 0.6974723]. \t  1.4612392662713283 \t 3.837850880317825\n",
      "51     \t [0.58655025 0.79762814 0.20912458]. \t  0.04739341477913653 \t 3.837850880317825\n",
      "52     \t [0.35348394 0.54054037 0.79326966]. \t  3.5423436991380184 \t 3.837850880317825\n",
      "53     \t [0.54033536 0.55708343 0.80453791]. \t  3.5940782171099377 \t 3.837850880317825\n",
      "54     \t [0.51540621 0.82069364 0.35927333]. \t  0.5722574887864392 \t 3.837850880317825\n",
      "55     \t [0.12084503 0.23291589 0.02030243]. \t  0.11658770253805333 \t 3.837850880317825\n",
      "56     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.837850880317825\n",
      "57     \t [0.36108574 0.84814879 0.50563352]. \t  2.33289574863249 \t 3.837850880317825\n",
      "58     \t [0.02994214 0.90579619 0.36261184]. \t  0.9648630155089746 \t 3.837850880317825\n",
      "59     \t [0.468795   0.56654873 0.53786253]. \t  1.0723388479276335 \t 3.837850880317825\n",
      "60     \t [0.6745912  0.3495636  0.27472958]. \t  0.44668356148041866 \t 3.837850880317825\n",
      "61     \t [0.79147124 0.49478865 0.20856206]. \t  0.1315508901407916 \t 3.837850880317825\n",
      "62     \t [0.19529861 0.25854139 0.58264935]. \t  0.4881278080966765 \t 3.837850880317825\n",
      "63     \t [0.01115601 0.37581528 0.5813964 ]. \t  0.779550659614829 \t 3.837850880317825\n",
      "64     \t [0.23105329 0.41375037 0.8828008 ]. \t  3.1029376695836173 \t 3.837850880317825\n",
      "65     \t [0.36112329 0.01047067 0.55398452]. \t  0.1323014939872815 \t 3.837850880317825\n",
      "66     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.837850880317825\n",
      "67     \t [0.58547314 0.77591115 0.66615084]. \t  1.668138336827412 \t 3.837850880317825\n",
      "68     \t [0.39740304 0.88446466 0.49074221]. \t  2.0880523492333616 \t 3.837850880317825\n",
      "69     \t [0.38886039 0.25135511 0.94066634]. \t  1.2307586237532115 \t 3.837850880317825\n",
      "70     \t [0.75242046 0.91825839 0.37080074]. \t  0.309645020884253 \t 3.837850880317825\n",
      "71     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.837850880317825\n",
      "72     \t [0.77800432 0.25537753 0.17586429]. \t  0.38915595585963736 \t 3.837850880317825\n",
      "73     \t [0.49427874 0.06078479 0.09831404]. \t  0.39241495499378787 \t 3.837850880317825\n",
      "74     \t [0.2380556  0.22872624 0.76367594]. \t  1.3619292087267638 \t 3.837850880317825\n",
      "75     \t [0.69207192 0.83763329 0.14473628]. \t  0.00949174803665957 \t 3.837850880317825\n",
      "76     \t [0.14881374 0.11551966 0.23886853]. \t  0.8445449719001228 \t 3.837850880317825\n",
      "77     \t [0.81168948 0.57008726 0.64103874]. \t  1.298527861838243 \t 3.837850880317825\n",
      "78     \t [0.01114399 0.68792893 0.33358086]. \t  0.5015947856493921 \t 3.837850880317825\n",
      "79     \t [0.55316105 0.41412635 0.94449087]. \t  2.467181550680747 \t 3.837850880317825\n",
      "80     \t [0.49204468 0.87623059 0.93071709]. \t  1.267310045025293 \t 3.837850880317825\n",
      "81     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.837850880317825\n",
      "82     \t [0.26342989 0.42283618 0.15070141]. \t  0.25518962353945907 \t 3.837850880317825\n",
      "83     \t [0.0780557  0.49791145 0.34174264]. \t  0.34536256461160575 \t 3.837850880317825\n",
      "84     \t [0.89256904 0.0488158  0.85634456]. \t  0.36195246762087896 \t 3.837850880317825\n",
      "85     \t [0.60846194 0.72990991 0.70982028]. \t  1.9273562235638337 \t 3.837850880317825\n",
      "86     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.837850880317825\n",
      "87     \t [0.16752079 0.2572927  0.51443825]. \t  0.31906035279084605 \t 3.837850880317825\n",
      "88     \t [0.3287762  0.21881349 0.20110872]. \t  0.7875486857035024 \t 3.837850880317825\n",
      "89     \t [0.15046753 0.4255731  0.64371055]. \t  1.4833790492787362 \t 3.837850880317825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.33748504 0.79789433 0.85234081]. \t  2.2721529590135434 \t 3.837850880317825\n",
      "91     \t [0.84569178 0.9191447  0.79022502]. \t  0.9258210145201851 \t 3.837850880317825\n",
      "92     \t [0.17671046 0.56556359 0.67002609]. \t  2.2476536673077527 \t 3.837850880317825\n",
      "93     \t [0.18882945 0.49185784 0.18836753]. \t  0.1969260192761571 \t 3.837850880317825\n",
      "94     \t [0.66936525 0.53565213 0.74427858]. \t  2.8125319218943465 \t 3.837850880317825\n",
      "95     \t [0.92825386 0.75906919 0.23961872]. \t  0.024146434416881628 \t 3.837850880317825\n",
      "96     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.837850880317825\n",
      "97     \t [0.15529655 0.73583913 0.78478178]. \t  2.7239012189583907 \t 3.837850880317825\n",
      "98     \t [0.95993186 0.62888357 0.6627181 ]. \t  1.3205584756640187 \t 3.837850880317825\n",
      "99     \t [0.10159203 0.67178858 0.46480478]. \t  1.6291136170079699 \t 3.837850880317825\n",
      "100    \t [0.12728804 0.15490575 0.95500312]. \t  0.5730027320889911 \t 3.837850880317825\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0.05220315 1.         0.        ]. \t  0.00028352854528979776 \t 1.1210522139432408\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1210522139432408\n",
      "3      \t [0.74856878 1.         0.16108473]. \t  0.007367112926211671 \t 1.1210522139432408\n",
      "4      \t [0.56797556 0.75051147 1.        ]. \t  \u001b[92m1.4972505411588866\u001b[0m \t 1.4972505411588866\n",
      "5      \t [0.45381046 1.         0.82332646]. \t  0.7086269855052809 \t 1.4972505411588866\n",
      "6      \t [1.        0.5629405 1.       ]. \t  \u001b[92m2.0084661539383126\u001b[0m \t 2.0084661539383126\n",
      "7      \t [1.         0.23289447 1.        ]. \t  0.6749396364286011 \t 2.0084661539383126\n",
      "8      \t [0.77935835 0.51379687 1.        ]. \t  1.997578083767971 \t 2.0084661539383126\n",
      "9      \t [0.47987765 0.47300285 1.        ]. \t  1.9111694171530162 \t 2.0084661539383126\n",
      "10     \t [0.         0.37859285 1.        ]. \t  1.4507281766394489 \t 2.0084661539383126\n",
      "11     \t [0.         0.82740085 0.63221811]. \t  \u001b[92m2.715105226783768\u001b[0m \t 2.715105226783768\n",
      "12     \t [0.         0.61121829 0.36672978]. \t  0.5536529871685026 \t 2.715105226783768\n",
      "13     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.715105226783768\n",
      "14     \t [0.         1.         0.45669808]. \t  1.8514103682713363 \t 2.715105226783768\n",
      "15     \t [0.         0.65811561 0.93607408]. \t  \u001b[92m2.8926823701977247\u001b[0m \t 2.8926823701977247\n",
      "16     \t [0.         0.53590351 0.76867605]. \t  \u001b[92m3.255580692875059\u001b[0m \t 3.255580692875059\n",
      "17     \t [0.19131677 0.65199932 0.79818247]. \t  \u001b[92m3.3300010814001433\u001b[0m \t 3.3300010814001433\n",
      "18     \t [0.06594175 0.         0.        ]. \t  0.07763647258605057 \t 3.3300010814001433\n",
      "19     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.3300010814001433\n",
      "20     \t [0.52149133 0.         0.        ]. \t  0.09534817028416226 \t 3.3300010814001433\n",
      "21     \t [1.         0.51362733 0.84248533]. \t  \u001b[92m3.6122969008425176\u001b[0m \t 3.6122969008425176\n",
      "22     \t [1.         0.21685253 0.65837996]. \t  0.6945673714209586 \t 3.6122969008425176\n",
      "23     \t [0.44874158 1.         0.        ]. \t  0.00022434040215130603 \t 3.6122969008425176\n",
      "24     \t [1.         1.         0.55331796]. \t  0.26084766719317726 \t 3.6122969008425176\n",
      "25     \t [1.         0.51973455 0.63127425]. \t  1.0780799889125467 \t 3.6122969008425176\n",
      "26     \t [0.66708107 0.39586132 0.806683  ]. \t  2.9387972109753173 \t 3.6122969008425176\n",
      "27     \t [0.24374506 0.3721194  0.80402862]. \t  2.763058757295363 \t 3.6122969008425176\n",
      "28     \t [0.54698691 0.59312901 0.83637411]. \t  \u001b[92m3.734511337178215\u001b[0m \t 3.734511337178215\n",
      "29     \t [4.30245230e-08 3.15928035e-01 7.73099697e-01]. \t  2.07869330420818 \t 3.734511337178215\n",
      "30     \t [0.99999992 0.38009567 0.85223718]. \t  2.8109844427353705 \t 3.734511337178215\n",
      "31     \t [1.        0.        0.8588241]. \t  0.22146089784216028 \t 3.734511337178215\n",
      "32     \t [0.         0.31093055 0.        ]. \t  0.053525004494212436 \t 3.734511337178215\n",
      "33     \t [0.99474447 0.73145104 0.86477008]. \t  2.698057302024204 \t 3.734511337178215\n",
      "34     \t [7.66155025e-10 9.99999967e-01 7.02227564e-01]. \t  1.4943596121653715 \t 3.734511337178215\n",
      "35     \t [0.         0.         0.97712125]. \t  0.11220284047998204 \t 3.734511337178215\n",
      "36     \t [0.51730372 0.57738998 0.85345016]. \t  \u001b[92m3.8167817561187674\u001b[0m \t 3.8167817561187674\n",
      "37     \t [0.80642946 0.63118122 0.83220205]. \t  3.4556757266160556 \t 3.8167817561187674\n",
      "38     \t [0.50270643 0.72653397 0.77528744]. \t  2.553680678795565 \t 3.8167817561187674\n",
      "39     \t [0.22580487 0.52291128 0.89254165]. \t  3.655740109251648 \t 3.8167817561187674\n",
      "40     \t [0.15465376 0.59206484 0.88776904]. \t  3.6887449708821123 \t 3.8167817561187674\n",
      "41     \t [0.83619285 0.51334585 0.76359824]. \t  2.991273522167406 \t 3.8167817561187674\n",
      "42     \t [0.28539997 0.62664739 0.93656482]. \t  3.068900305033308 \t 3.8167817561187674\n",
      "43     \t [0.38920446 0.5661161  0.79855737]. \t  3.5785468551104933 \t 3.8167817561187674\n",
      "44     \t [0.22111792 0.48996467 0.78711858]. \t  3.387373804670617 \t 3.8167817561187674\n",
      "45     \t [0.18053328 0.64239143 0.82985594]. \t  3.555383915082971 \t 3.8167817561187674\n",
      "46     \t [0.43362631 0.54993453 0.78693922]. \t  3.4607409703790157 \t 3.8167817561187674\n",
      "47     \t [0.43821696 0.48847161 0.81320232]. \t  3.578897426737286 \t 3.8167817561187674\n",
      "48     \t [0.86159006 0.50053186 0.86346186]. \t  3.638190286750224 \t 3.8167817561187674\n",
      "49     \t [0.06038581 0.48710306 0.85396585]. \t  3.6692227475891332 \t 3.8167817561187674\n",
      "50     \t [0.23082093 0.51981067 0.85882528]. \t  3.810183995148995 \t 3.8167817561187674\n",
      "51     \t [1.54002066e-07 8.76630735e-01 1.86867143e-01]. \t  0.05168489325137806 \t 3.8167817561187674\n",
      "52     \t [0.        0.5492426 0.8077   ]. \t  3.6343601529275027 \t 3.8167817561187674\n",
      "53     \t [3.09964488e-04 7.04815963e-01 8.17080914e-01]. \t  3.0500871058741232 \t 3.8167817561187674\n",
      "54     \t [0.7236271  0.54891957 0.82602554]. \t  3.692794041819255 \t 3.8167817561187674\n",
      "55     \t [0.16938349 0.60577749 0.83759113]. \t  3.743067515554851 \t 3.8167817561187674\n",
      "56     \t [0.28436352 0.53660847 0.83885214]. \t  \u001b[92m3.8328246660815886\u001b[0m \t 3.8328246660815886\n",
      "57     \t [0.25219562 0.54255454 0.8005485 ]. \t  3.6199341386302537 \t 3.8328246660815886\n",
      "58     \t [0.09416128 0.54966927 0.80992235]. \t  3.6793311948967133 \t 3.8328246660815886\n",
      "59     \t [0.1238727  0.58448544 0.80479934]. \t  3.618925043179442 \t 3.8328246660815886\n",
      "60     \t [0.64667836 0.54550968 0.87736904]. \t  3.755722307324307 \t 3.8328246660815886\n",
      "61     \t [0.72331987 0.59059318 0.80312784]. \t  3.442106994740324 \t 3.8328246660815886\n",
      "62     \t [0.12583939 0.53821094 0.85233906]. \t  \u001b[92m3.8346421445733085\u001b[0m \t 3.8346421445733085\n",
      "63     \t [0.72160519 0.52892459 0.86315788]. \t  3.7596721837251215 \t 3.8346421445733085\n",
      "64     \t [0.70881154 0.56442765 0.89117796]. \t  3.6602456894915916 \t 3.8346421445733085\n",
      "65     \t [0.60981051 0.56835853 0.89210125]. \t  3.6753922927626665 \t 3.8346421445733085\n",
      "66     \t [0.56090139 0.55724634 0.83568322]. \t  3.789869645211792 \t 3.8346421445733085\n",
      "67     \t [0.40350852 0.48734537 0.83457703]. \t  3.6803365774908725 \t 3.8346421445733085\n",
      "68     \t [0.19674094 0.60121109 0.79585857]. \t  3.5158941191808015 \t 3.8346421445733085\n",
      "69     \t [0.53995552 0.56187834 0.78766428]. \t  3.422359447965519 \t 3.8346421445733085\n",
      "70     \t [0.71715237 0.53279554 0.83002213]. \t  3.7107165501048565 \t 3.8346421445733085\n",
      "71     \t [0.02625677 0.55786316 0.84645854]. \t  3.815792519370941 \t 3.8346421445733085\n",
      "72     \t [0.24878524 0.56947782 0.86426663]. \t  \u001b[92m3.840916042002484\u001b[0m \t 3.840916042002484\n",
      "73     \t [0.90801153 0.55723966 0.87437687]. \t  3.6828908763136394 \t 3.840916042002484\n",
      "74     \t [0.44711931 0.59018142 0.86390028]. \t  3.796465460184738 \t 3.840916042002484\n",
      "75     \t [0.91762721 0.50122639 0.85517429]. \t  3.627839130127172 \t 3.840916042002484\n",
      "76     \t [0.21774654 0.60955027 0.83948732]. \t  3.738703500869203 \t 3.840916042002484\n",
      "77     \t [0.39069351 0.4960901  0.82778033]. \t  3.69186292772273 \t 3.840916042002484\n",
      "78     \t [0.26065729 0.59572774 0.86982293]. \t  3.7786294814002206 \t 3.840916042002484\n",
      "79     \t [0.71754052 0.49921964 0.85972336]. \t  3.6872913140115235 \t 3.840916042002484\n",
      "80     \t [0.29583643 0.5269367  0.84031241]. \t  3.8218270984186113 \t 3.840916042002484\n",
      "81     \t [0.28142206 0.55189371 0.81134661]. \t  3.7058514209339886 \t 3.840916042002484\n",
      "82     \t [1.         0.60272525 0.8645279 ]. \t  3.57668869824294 \t 3.840916042002484\n",
      "83     \t [0.24626232 0.58181881 0.85524743]. \t  3.8349824367833127 \t 3.840916042002484\n",
      "84     \t [0.16883855 0.53646462 0.82004888]. \t  3.7482387849161896 \t 3.840916042002484\n",
      "85     \t [0.16730153 0.50270762 0.83847759]. \t  3.7439487842266477 \t 3.840916042002484\n",
      "86     \t [0.4117036  0.53357254 0.87764847]. \t  3.778861125878081 \t 3.840916042002484\n",
      "87     \t [0.55959664 0.5603704  0.8603501 ]. \t  3.8247030908359503 \t 3.840916042002484\n",
      "88     \t [0.36779511 0.5142616  0.85778853]. \t  3.7996384423780563 \t 3.840916042002484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.30152738 0.55281233 0.82864873]. \t  3.805875197613239 \t 3.840916042002484\n",
      "90     \t [0.61845989 0.56399105 0.90127266]. \t  3.6002267279696705 \t 3.840916042002484\n",
      "91     \t [0.03252927 0.50919283 0.83273744]. \t  3.7188391409059514 \t 3.840916042002484\n",
      "92     \t [0.62261106 0.57431313 0.8254504 ]. \t  3.7002447599010835 \t 3.840916042002484\n",
      "93     \t [0.28197727 0.50396939 0.85938561]. \t  3.7645483814549268 \t 3.840916042002484\n",
      "94     \t [0.52897407 0.54925181 0.88068636]. \t  3.7659958974425827 \t 3.840916042002484\n",
      "95     \t [0.8499885  0.49942563 0.89155455]. \t  3.505154897399043 \t 3.840916042002484\n",
      "96     \t [0.8909644  0.60674045 0.85682839]. \t  3.611240516138046 \t 3.840916042002484\n",
      "97     \t [0.78750166 0.5106615  0.8288577 ]. \t  3.6434110088599025 \t 3.840916042002484\n",
      "98     \t [0.283014   0.49718155 0.84471722]. \t  3.744005325689698 \t 3.840916042002484\n",
      "99     \t [0.01568044 0.56830684 0.81870837]. \t  3.7063505932228233 \t 3.840916042002484\n",
      "100    \t [0.23913699 0.53010755 0.83224022]. \t  3.802341092036368 \t 3.840916042002484\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [1.         0.99504682 1.        ]. \t  0.33050798529235154 \t 2.524990008735946\n",
      "3      \t [0.         0.68600033 1.        ]. \t  1.7944421634609722 \t 2.524990008735946\n",
      "4      \t [0.55663821 0.50390896 1.        ]. \t  2.0005635015490997 \t 2.524990008735946\n",
      "5      \t [1.         0.18022042 0.17234163]. \t  0.22197533017614318 \t 2.524990008735946\n",
      "6      \t [1.         0.50551013 0.91518513]. \t  \u001b[92m3.2475575316001972\u001b[0m \t 3.2475575316001972\n",
      "7      \t [1.         0.5875155  0.58468326]. \t  0.6298183503529091 \t 3.2475575316001972\n",
      "8      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.2475575316001972\n",
      "9      \t [0.         0.40098172 0.93974806]. \t  2.403850663863382 \t 3.2475575316001972\n",
      "10     \t [0.09308688 1.         0.        ]. \t  0.0002884435452873031 \t 3.2475575316001972\n",
      "11     \t [1.         0.35409407 1.        ]. \t  1.2881840947266074 \t 3.2475575316001972\n",
      "12     \t [0.80289462 0.         0.        ]. \t  0.05810973386210423 \t 3.2475575316001972\n",
      "13     \t [0.         0.         0.91232573]. \t  0.17887536103647605 \t 3.2475575316001972\n",
      "14     \t [0.43306589 0.90920436 0.80766248]. \t  1.2875448249227555 \t 3.2475575316001972\n",
      "15     \t [1.         0.         0.54381107]. \t  0.06895053166750933 \t 3.2475575316001972\n",
      "16     \t [0.23991498 0.55761109 0.73646571]. \t  2.9282831556737223 \t 3.2475575316001972\n",
      "17     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.2475575316001972\n",
      "18     \t [0.83880593 0.73080145 1.        ]. \t  1.5716274153023846 \t 3.2475575316001972\n",
      "19     \t [0.32378096 0.         1.        ]. \t  0.09168606283363934 \t 3.2475575316001972\n",
      "20     \t [0.         1.         0.50993251]. \t  2.3320443719521844 \t 3.2475575316001972\n",
      "21     \t [0.53901036 0.57526708 0.63223376]. \t  1.5766259433337437 \t 3.2475575316001972\n",
      "22     \t [0.63349806 1.         0.        ]. \t  0.00014874379605904543 \t 3.2475575316001972\n",
      "23     \t [0.24574932 1.         0.25846694]. \t  0.17403772302759665 \t 3.2475575316001972\n",
      "24     \t [0.         0.         0.36407565]. \t  0.4392415215450986 \t 3.2475575316001972\n",
      "25     \t [0.95644808 0.52463749 0.        ]. \t  0.007910851941348414 \t 3.2475575316001972\n",
      "26     \t [0.         0.26191287 0.65821475]. \t  0.91061111753003 \t 3.2475575316001972\n",
      "27     \t [0.19833687 0.39875973 0.98727452]. \t  1.755976896897547 \t 3.2475575316001972\n",
      "28     \t [0.01943084 0.78149108 0.75561002]. \t  2.331433614260389 \t 3.2475575316001972\n",
      "29     \t [0.6022577 0.        0.7852664]. \t  0.2508046017239304 \t 3.2475575316001972\n",
      "30     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.2475575316001972\n",
      "31     \t [0.99895974 0.38684106 0.75504949]. \t  2.3655868185657507 \t 3.2475575316001972\n",
      "32     \t [0.99999998 0.77894767 0.85647331]. \t  2.2349012296723427 \t 3.2475575316001972\n",
      "33     \t [0.26290395 0.77205105 0.90282632]. \t  2.3657880177099395 \t 3.2475575316001972\n",
      "34     \t [0.         1.         0.22716252]. \t  0.098902039304371 \t 3.2475575316001972\n",
      "35     \t [0.99999997 0.77022801 0.99999996]. \t  1.339213794358699 \t 3.2475575316001972\n",
      "36     \t [0.11765331 1.         0.58827663]. \t  2.5148933924006887 \t 3.2475575316001972\n",
      "37     \t [0.89547963 0.6045913  0.82085453]. \t  \u001b[92m3.4665222347379476\u001b[0m \t 3.4665222347379476\n",
      "38     \t [0.76388155 0.         0.9964469 ]. \t  0.0938128015462093 \t 3.4665222347379476\n",
      "39     \t [0.80451683 0.56366985 0.90615964]. \t  \u001b[92m3.5050593971274093\u001b[0m \t 3.5050593971274093\n",
      "40     \t [0.         0.99932551 0.7773396 ]. \t  0.9580972182248952 \t 3.5050593971274093\n",
      "41     \t [0.27660827 0.         0.09699447]. \t  0.35609587262373743 \t 3.5050593971274093\n",
      "42     \t [0.04968191 0.5769431  0.62752184]. \t  1.9687563675837685 \t 3.5050593971274093\n",
      "43     \t [0.13279126 0.86650818 0.59641589]. \t  3.004232789831125 \t 3.5050593971274093\n",
      "44     \t [0.79221563 0.56994792 0.8539359 ]. \t  \u001b[92m3.7453849562954726\u001b[0m \t 3.7453849562954726\n",
      "45     \t [0.85342784 0.45714755 0.86362571]. \t  3.4310195841454227 \t 3.7453849562954726\n",
      "46     \t [0.7044528  0.71705601 0.        ]. \t  0.002363285046313946 \t 3.7453849562954726\n",
      "47     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.7453849562954726\n",
      "48     \t [0.87319599 0.55755758 0.8559258 ]. \t  3.7262873266284062 \t 3.7453849562954726\n",
      "49     \t [1.         0.         0.82446898]. \t  0.23907006766505284 \t 3.7453849562954726\n",
      "50     \t [0.68157583 0.58573709 0.83131925]. \t  3.6863462106083364 \t 3.7453849562954726\n",
      "51     \t [0.84862983 0.67793811 0.85148879]. \t  3.2007980994817053 \t 3.7453849562954726\n",
      "52     \t [0.73047233 0.58662707 0.84846195]. \t  3.727785008238859 \t 3.7453849562954726\n",
      "53     \t [0.4655943  0.57329529 0.83902864]. \t  \u001b[92m3.8083519718979297\u001b[0m \t 3.8083519718979297\n",
      "54     \t [0.68655261 0.47258864 0.84533407]. \t  3.58096390901099 \t 3.8083519718979297\n",
      "55     \t [0.57907978 0.48073636 0.9026203 ]. \t  3.3937110918704416 \t 3.8083519718979297\n",
      "56     \t [0.62872303 0.62837177 0.85052197]. \t  3.60529633046477 \t 3.8083519718979297\n",
      "57     \t [7.57859867e-01 2.48769989e-07 1.94497412e-01]. \t  0.4724898871991849 \t 3.8083519718979297\n",
      "58     \t [0.68017572 0.55187965 0.86681449]. \t  3.7850442773935122 \t 3.8083519718979297\n",
      "59     \t [0.79350737 0.59879218 0.82944476]. \t  3.591082664407764 \t 3.8083519718979297\n",
      "60     \t [0.39027896 0.65545231 0.80313699]. \t  3.30189568012093 \t 3.8083519718979297\n",
      "61     \t [0.82240138 0.53421827 0.84675429]. \t  3.730609898034445 \t 3.8083519718979297\n",
      "62     \t [0.91865281 0.59578764 0.82759887]. \t  3.5343136974551483 \t 3.8083519718979297\n",
      "63     \t [0.87645461 0.50986913 0.84950572]. \t  3.6683136566515926 \t 3.8083519718979297\n",
      "64     \t [0.62862939 0.41088114 0.83441169]. \t  3.167566576463021 \t 3.8083519718979297\n",
      "65     \t [0.80961846 0.57623445 0.88725633]. \t  3.6429479175326582 \t 3.8083519718979297\n",
      "66     \t [0.70308766 0.49797967 0.89349822]. \t  3.527636323124246 \t 3.8083519718979297\n",
      "67     \t [0.25289115 0.69989549 0.69291913]. \t  2.494771730446058 \t 3.8083519718979297\n",
      "68     \t [0.75187725 0.62030597 0.85690451]. \t  3.6089997727174454 \t 3.8083519718979297\n",
      "69     \t [0.67478775 0.52553071 0.88899525]. \t  3.6529641889598476 \t 3.8083519718979297\n",
      "70     \t [0.64130579 0.59180651 0.82806353]. \t  3.6669608329284125 \t 3.8083519718979297\n",
      "71     \t [0.717548   0.64206644 0.78316383]. \t  3.0093439727430304 \t 3.8083519718979297\n",
      "72     \t [0.80196144 0.56701382 0.82649701]. \t  3.6494050810797134 \t 3.8083519718979297\n",
      "73     \t [0.94992091 0.55786121 0.87055988]. \t  3.6765076002326658 \t 3.8083519718979297\n",
      "74     \t [0.79947666 0.56012598 0.82975512]. \t  3.6776665954592196 \t 3.8083519718979297\n",
      "75     \t [0.88912945 0.54853318 0.94762724]. \t  2.9311735392403793 \t 3.8083519718979297\n",
      "76     \t [0.88331167 0.60419099 0.88224722]. \t  3.5750287098063076 \t 3.8083519718979297\n",
      "77     \t [0.67405335 0.51758349 0.90584153]. \t  3.488027597338727 \t 3.8083519718979297\n",
      "78     \t [0.70083441 0.51246696 0.86856134]. \t  3.7158654190060267 \t 3.8083519718979297\n",
      "79     \t [0.66223202 0.54100328 0.90628869]. \t  3.5325252495176356 \t 3.8083519718979297\n",
      "80     \t [0.7076945  0.55551048 0.83049328]. \t  3.7205360934649 \t 3.8083519718979297\n",
      "81     \t [0.53684284 0.65057741 0.83416512]. \t  3.460748749444207 \t 3.8083519718979297\n",
      "82     \t [0.69965927 0.59082477 0.886635  ]. \t  3.6534629006616206 \t 3.8083519718979297\n",
      "83     \t [0.89059765 0.52281871 0.88164367]. \t  3.6243431428906816 \t 3.8083519718979297\n",
      "84     \t [0.64888313 0.53415557 0.85992721]. \t  3.792678810729399 \t 3.8083519718979297\n",
      "85     \t [0.65906202 0.52960367 0.83542641]. \t  3.74969033841937 \t 3.8083519718979297\n",
      "86     \t [0.71990696 0.55497088 0.88775261]. \t  3.6824552509940345 \t 3.8083519718979297\n",
      "87     \t [0.50990504 0.50798049 0.84600198]. \t  3.7654845659765326 \t 3.8083519718979297\n",
      "88     \t [0.93611751 0.5539485  0.87919322]. \t  3.6523112908739033 \t 3.8083519718979297\n",
      "89     \t [0.82693744 0.54162089 0.91390793]. \t  3.406713221924355 \t 3.8083519718979297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.67693528 0.59511023 0.82093615]. \t  3.597844622053158 \t 3.8083519718979297\n",
      "91     \t [0.86922346 0.43971491 0.85382234]. \t  3.3291234027686722 \t 3.8083519718979297\n",
      "92     \t [0.81014152 0.60786244 0.86603965]. \t  3.637169334292773 \t 3.8083519718979297\n",
      "93     \t [0.53352565 0.56499296 0.8753765 ]. \t  3.788980336646898 \t 3.8083519718979297\n",
      "94     \t [0.57831859 0.53149904 0.8450732 ]. \t  3.801408567076799 \t 3.8083519718979297\n",
      "95     \t [0.4496689  0.44280534 0.91214114]. \t  3.0798546022163746 \t 3.8083519718979297\n",
      "96     \t [0.50022342 0.54704294 0.75925108]. \t  3.1070259488733916 \t 3.8083519718979297\n",
      "97     \t [0.83192047 0.60099477 0.86664626]. \t  3.652565823839403 \t 3.8083519718979297\n",
      "98     \t [0.32692265 0.63074226 0.84188908]. \t  3.6484987898276646 \t 3.8083519718979297\n",
      "99     \t [0.9023587  0.54834923 0.91047516]. \t  3.424869542794152 \t 3.8083519718979297\n",
      "100    \t [0.99007835 0.60102209 0.91414893]. \t  3.298435130870296 \t 3.8083519718979297\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [0.89514779 1.         0.        ]. \t  6.0207437068894756e-05 \t 0.675391399411646\n",
      "2      \t [0.44077582 1.         1.        ]. \t  0.33323999598862386 \t 0.675391399411646\n",
      "3      \t [0.72556975 0.         0.        ]. \t  0.0698079095886099 \t 0.675391399411646\n",
      "4      \t [0.63866865 1.         0.53801657]. \t  \u001b[92m1.111398338326787\u001b[0m \t 1.111398338326787\n",
      "5      \t [0.9895     1.         0.74653339]. \t  0.39571060086780663 \t 1.111398338326787\n",
      "6      \t [0.38991738 1.         0.24808862]. \t  0.12055913440959459 \t 1.111398338326787\n",
      "7      \t [0.24919743 1.         0.70526386]. \t  \u001b[92m1.4412225702226187\u001b[0m \t 1.4412225702226187\n",
      "8      \t [0.         1.         0.96174901]. \t  0.4411783183439743 \t 1.4412225702226187\n",
      "9      \t [0.         0.         0.27326175]. \t  0.579334007864959 \t 1.4412225702226187\n",
      "10     \t [0.         1.         0.59849268]. \t  \u001b[92m2.380679878948851\u001b[0m \t 2.380679878948851\n",
      "11     \t [0.         1.         0.36861579]. \t  0.8728182352225031 \t 2.380679878948851\n",
      "12     \t [1.         0.30148538 1.        ]. \t  1.0075474021957789 \t 2.380679878948851\n",
      "13     \t [0.         0.7468714  0.65724549]. \t  \u001b[92m2.5374008827798975\u001b[0m \t 2.5374008827798975\n",
      "14     \t [0.97451831 0.71140988 1.        ]. \t  1.6425911389594288 \t 2.5374008827798975\n",
      "15     \t [0.         0.46500911 0.71807299]. \t  2.460814752309674 \t 2.5374008827798975\n",
      "16     \t [0.         0.26918944 0.88614379]. \t  1.692745279543874 \t 2.5374008827798975\n",
      "17     \t [0.94694451 1.         1.        ]. \t  0.31906829721868263 \t 2.5374008827798975\n",
      "18     \t [0.67842352 0.52130764 1.        ]. \t  2.026506310843201 \t 2.5374008827798975\n",
      "19     \t [0.         0.51465399 0.9392382 ]. \t  \u001b[92m3.0359916324650467\u001b[0m \t 3.0359916324650467\n",
      "20     \t [1.         0.33759212 0.1638395 ]. \t  0.13515296025149373 \t 3.0359916324650467\n",
      "21     \t [0.19724091 0.47227774 1.        ]. \t  1.9038890863366293 \t 3.0359916324650467\n",
      "22     \t [0.        0.3814023 1.       ]. \t  1.4657401218659134 \t 3.0359916324650467\n",
      "23     \t [0.58320323 0.55397547 0.        ]. \t  0.015189260410125171 \t 3.0359916324650467\n",
      "24     \t [1.         0.         0.53903257]. \t  0.06824032827555662 \t 3.0359916324650467\n",
      "25     \t [1.         1.         0.26092114]. \t  0.017868422000380395 \t 3.0359916324650467\n",
      "26     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.0359916324650467\n",
      "27     \t [1.         0.45704674 0.80689845]. \t  \u001b[92m3.2268746408553484\u001b[0m \t 3.2268746408553484\n",
      "28     \t [0.1229082 0.        0.       ]. \t  0.08527253117070595 \t 3.2268746408553484\n",
      "29     \t [0.50299214 0.71560515 0.88286721]. \t  2.9670801673604164 \t 3.2268746408553484\n",
      "30     \t [0.99999998 0.62335498 0.73650162]. \t  2.3030262233422527 \t 3.2268746408553484\n",
      "31     \t [0.         0.         0.65619437]. \t  0.15491673446130416 \t 3.2268746408553484\n",
      "32     \t [1.         0.30685105 0.59749503]. \t  0.554311690658414 \t 3.2268746408553484\n",
      "33     \t [1.         0.63715501 0.        ]. \t  0.0023877323277276367 \t 3.2268746408553484\n",
      "34     \t [0.80965529 0.63599975 0.85024471]. \t  \u001b[92m3.496238918514142\u001b[0m \t 3.496238918514142\n",
      "35     \t [0.78222744 0.3732645  0.85115275]. \t  2.813017062040341 \t 3.496238918514142\n",
      "36     \t [0.1308358  0.58725117 0.82308549]. \t  \u001b[92m3.731488519598362\u001b[0m \t 3.731488519598362\n",
      "37     \t [0.39481924 0.5419751  0.85427047]. \t  \u001b[92m3.851459185752709\u001b[0m \t 3.851459185752709\n",
      "38     \t [0.18675979 0.39214023 0.85049689]. \t  3.0274816248219114 \t 3.851459185752709\n",
      "39     \t [0.29416199 0.59838815 0.8570637 ]. \t  3.794831155871813 \t 3.851459185752709\n",
      "40     \t [0.         0.75533329 0.8394246 ]. \t  2.668467114264704 \t 3.851459185752709\n",
      "41     \t [0.83257076 0.53228141 0.87182543]. \t  3.706049471922162 \t 3.851459185752709\n",
      "42     \t [0.13379374 0.6083358  0.77438055]. \t  3.2993366397876933 \t 3.851459185752709\n",
      "43     \t [0.971717   0.48442457 0.9179512 ]. \t  3.143217551399207 \t 3.851459185752709\n",
      "44     \t [0.44969035 0.5468803  0.814912  ]. \t  3.7057476013694295 \t 3.851459185752709\n",
      "45     \t [0.99998151 0.         0.83027796]. \t  0.2369299001331057 \t 3.851459185752709\n",
      "46     \t [0.57156043 0.55078191 0.82186336]. \t  3.718942283879795 \t 3.851459185752709\n",
      "47     \t [0.64363701 0.56294467 0.86378287]. \t  3.7977645132766638 \t 3.851459185752709\n",
      "48     \t [2.51263375e-01 2.16697137e-08 1.34953948e-01]. \t  0.4946748631705911 \t 3.851459185752709\n",
      "49     \t [0.55291595 0.59605646 0.82611354]. \t  3.674247735819701 \t 3.851459185752709\n",
      "50     \t [0.77622412 0.59772272 0.85519632]. \t  3.688160379656863 \t 3.851459185752709\n",
      "51     \t [0.58093162 0.52707011 0.85079252]. \t  3.800896913329003 \t 3.851459185752709\n",
      "52     \t [0.21951205 0.55564683 0.8667471 ]. \t  3.838512269864305 \t 3.851459185752709\n",
      "53     \t [0.55472384 0.54294219 0.878486  ]. \t  3.7683402088275466 \t 3.851459185752709\n",
      "54     \t [0.5385813  0.537793   0.84399947]. \t  3.815806592542139 \t 3.851459185752709\n",
      "55     \t [0.75604744 0.48743582 0.84301113]. \t  3.625979297747025 \t 3.851459185752709\n",
      "56     \t [0.49785084 0.47891102 0.85380939]. \t  3.652617709305358 \t 3.851459185752709\n",
      "57     \t [0.61433998 0.58797358 0.83757131]. \t  3.7323992589269173 \t 3.851459185752709\n",
      "58     \t [0.74181387 0.56463583 0.87632533]. \t  3.7327432448241327 \t 3.851459185752709\n",
      "59     \t [0.33566021 0.57681387 0.84819008]. \t  3.8401138413354348 \t 3.851459185752709\n",
      "60     \t [0.56672854 0.56948922 0.85920592]. \t  3.8164305965885927 \t 3.851459185752709\n",
      "61     \t [0.23341515 0.64051561 0.81569232]. \t  3.4995692111828873 \t 3.851459185752709\n",
      "62     \t [0.44748993 0.52633163 0.85634578]. \t  3.823127523918837 \t 3.851459185752709\n",
      "63     \t [0.15350649 0.5162277  0.79170126]. \t  3.4998770111265034 \t 3.851459185752709\n",
      "64     \t [0.2228989  0.5925615  0.86417633]. \t  3.799315818059108 \t 3.851459185752709\n",
      "65     \t [0.82194606 0.62700404 0.83302722]. \t  3.47544228643242 \t 3.851459185752709\n",
      "66     \t [0.46478485 0.58701188 0.87814947]. \t  3.7568740256893376 \t 3.851459185752709\n",
      "67     \t [0.43962848 0.54129425 0.84292926]. \t  3.835046427787292 \t 3.851459185752709\n",
      "68     \t [0.72840131 0.57290357 0.83569712]. \t  3.7173213676497046 \t 3.851459185752709\n",
      "69     \t [0.63821701 0.55635005 0.86257731]. \t  3.8040884470105745 \t 3.851459185752709\n",
      "70     \t [0.2286621  0.59112469 0.88090374]. \t  3.74134137355253 \t 3.851459185752709\n",
      "71     \t [0.         0.61126116 0.83526052]. \t  3.675448918085918 \t 3.851459185752709\n",
      "72     \t [0.01734228 0.58851742 0.85598816]. \t  3.778486843515699 \t 3.851459185752709\n",
      "73     \t [0.20494671 0.54585707 0.90041796]. \t  3.6240242933927242 \t 3.851459185752709\n",
      "74     \t [0.08042053 0.51857114 0.83791356]. \t  3.771972232272881 \t 3.851459185752709\n",
      "75     \t [0.45373265 0.58623397 0.86229666]. \t  3.807209145870114 \t 3.851459185752709\n",
      "76     \t [0.81772517 0.49902074 0.86488352]. \t  3.645481928679061 \t 3.851459185752709\n",
      "77     \t [0.21638706 0.50397297 0.82937073]. \t  3.7267206276391645 \t 3.851459185752709\n",
      "78     \t [0.49538918 0.55468843 0.87998858]. \t  3.7765416171234327 \t 3.851459185752709\n",
      "79     \t [0.39725421 0.52615467 0.77963121]. \t  3.378371769495169 \t 3.851459185752709\n",
      "80     \t [0.24144351 0.51696093 0.83242439]. \t  3.776690489000904 \t 3.851459185752709\n",
      "81     \t [0.1914493  0.54173381 0.82521261]. \t  3.7826566885562896 \t 3.851459185752709\n",
      "82     \t [0.68131934 0.61501785 0.86098447]. \t  3.657745016165207 \t 3.851459185752709\n",
      "83     \t [0.         0.50840256 0.86853407]. \t  3.7050461359229194 \t 3.851459185752709\n",
      "84     \t [0.42804968 0.57732045 0.85816384]. \t  3.8323299914409126 \t 3.851459185752709\n",
      "85     \t [0.09180287 0.57618841 0.87800989]. \t  3.761619827760823 \t 3.851459185752709\n",
      "86     \t [0.20359988 0.52524119 0.88731929]. \t  3.6983511255683688 \t 3.851459185752709\n",
      "87     \t [0.60272353 0.49761108 0.81673354]. \t  3.597967822284226 \t 3.851459185752709\n",
      "88     \t [0.46462218 0.46455989 0.88665077]. \t  3.4499832335539424 \t 3.851459185752709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.28248762 0.49883271 0.79023606]. \t  3.445596542807343 \t 3.851459185752709\n",
      "90     \t [0.35168179 0.4792474  0.82500792]. \t  3.612969563620175 \t 3.851459185752709\n",
      "91     \t [0.2173313  0.55758254 0.84414713]. \t  3.8510380908036503 \t 3.851459185752709\n",
      "92     \t [0.84141006 0.70499698 0.81609005]. \t  2.8110704291282325 \t 3.851459185752709\n",
      "93     \t [0.42603882 0.56789026 0.86243468]. \t  3.839847265227289 \t 3.851459185752709\n",
      "94     \t [0.62022077 0.59527223 0.82644439]. \t  3.654847687681074 \t 3.851459185752709\n",
      "95     \t [0.14220784 0.57222378 0.86508623]. \t  3.823796156174947 \t 3.851459185752709\n",
      "96     \t [0.41500635 0.60085238 0.80435402]. \t  3.5498651947587287 \t 3.851459185752709\n",
      "97     \t [0.65827314 0.54858658 0.8535718 ]. \t  3.8028008384195284 \t 3.851459185752709\n",
      "98     \t [0.28094534 0.56704517 0.85348634]. \t  \u001b[92m3.8562392576219713\u001b[0m \t 3.8562392576219713\n",
      "99     \t [0.35187913 0.54509214 0.90165918]. \t  3.6197106142506605 \t 3.8562392576219713\n",
      "100    \t [0.37655084 0.57882921 0.80215025]. \t  3.595796967855205 \t 3.8562392576219713\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.5679926917617815, -2.0641303202478944)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8324703257802128, -1.7992736738233843)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.126528058433538, -5.140706141792039)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.7284040295122587, -3.9193668624795746)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.6564561886948495, -4.21121668380296)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.073456228054626, -4.7824156297916485)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.7949255976967895, -3.667069349629895)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3489820184789103, -5.162389907294258)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.236843852828597, -3.1102391432412486)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.752468321723597, -2.492707746442)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9584055370519489, -3.137136916435786)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.676949595041634, -2.3458088460004944)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.90412688945273, -0.6479617229727552)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.701217407966093, -5.164645500308269)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8511977333207672, -4.213223587855063)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.5199520993161655, -2.930662520148231)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.545867525573378, -3.691718693655577)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.5569520783981012, -3.8229157514378107)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.415612396747107, -2.9108760353372376)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.637796576512354, -5.029704606495165)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.5679926917617815, -2.492707746442)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEYCAYAAAC0tfaFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3zb1bn48c+RbHnvPWPHzk4cZ5IEAiGBQEILgdKy7oUCLW2hUDpvezu5pYVCe6Gl3Lb8KIUWSGjTUjYEEkZYGU6cxFnO8t52vKek8/tDsmPHsi3bkhVLz/v10ivWdz6ynEdH53u+51Faa4QQQvgWg6cDEEIIMfEk+QshhA+S5C+EED5Ikr8QQvggSf5CCOGDJPkLIYQPkuQvvIZSKkMppZVSj561vFEp9d4ojxWulPpfpdRXXRrkGCmljEqpnyulSpRSp5VSzyulIj0dl5i8JPkL4dhC4JtAoKcDsbsJ+BHwNPBz4Abgp54MSExukvyFNzIqpQJ7H70LlVLTlFLblVJtSqlWpdQbSqk4+zqtlHpHKXVcKXUUeNe+2yNKqZ/ZH1op9V1767tYKXW7UmqjUqpZKVWglJptP9YypVSeUqrT/q1jo1IqqN95nldKva6UaldK7VBKTbeve9q+/X8rpeqVUhVKqTvscWwEZgD3A4fsy3rc/psUXkuSv/BGXwc6+j0i7MtvA6KAW7El0cuB6/vtdxHwAHAP8B37sj8Af+23zTrge/ZjPgk0YGuBzwG+bd/mTqAbW+v8L/ZzXNLvGF8AtgIPA0v7nQv7cc+zH8MIPKqUMmqte7TWhfbX9gZQaI9ViDGR5C+80QvA8n6PVvvyHwH/BcwHLrMvi+63306t9Z+11m8BefZlhVrrk/22+YnWehNwAFvL+x7gd2cd6w7gN8AK4AIH5/lAa/0b4Bf253FnxX+31voFbB8QQUBov3WvAlfZj/ePoX4BQozEz9MBCOEGVVrrT3ufKKUs9h//ge3D4AfAO8AqQPXbr96JY/d+kFiAdq21xX4O+z/KCHwAhAH3AXuA5886TzOA1rq7d7+zztFs/7f7zEtQc4F5WuuNQKH9AvbnlFL+Wmvp/hGjJslf+JJLgNPYuoJuty8z9ltv7fdzb+JdpJTKHcU5IoAlwF778W51cJ6xWAn8n/1D4AiwFvhQEr8YK+n2Eb7ku9i6UZ7E1jKvBeYNse0e4GNgA/BZZ0+gtW4A/gfIBP6E7dtEzzDncdYfgV9iu27xe2xdQtcPu4cQw1AypbMQQvgeafkLIYQPkuQvhBA+SJK/EEL4IEn+QgjhgybNUM/Y2FidkZHh6TCEEGJSycvLq9Nan30j4eRJ/hkZGezevdvTYQghxKSilCp2tFy6fYQQwgdJ8hdCCB8kyV8IIXzQpOnzF8Idenp6KCsro7Oz09OhCDEugYGBpKam4u/v79T2kvyFTysrKyMsLIyMjIzemTmFmHS01tTX11NWVkZmZqZT+0i3j/BpnZ2dxMTESOIXk5pSipiYmFF9g5XkL3yeJH7hDUb7d+yx5K+UekopVaOUKvBUDEII4as82fJ/GlsNVbfbX72Z9p66iTiVmPSecPFjZNXV1dx4441MnTqVRYsWsXz5cl588UUA3nvvPSIiIsjNzWXWrFncd999Do9RWFjI+vXrmTZtGgsXLuQLX/gC1dXVA/bPycnhkksuoaamBoCnn34apRTvvPNO33H+/e9/o5Ri8+bNg87xxS9+kczMTHJzc5k/fz5bt2516vWNR2NjI//3f//n1LarVq3quxG0qamJm2++mezsbLKysrjppps4ffo0AEVFRQQFBZGbm8vs2bO5+eab6enxvZo4Hkv+WusPsBW/drujdR+zqeC77Cp/hh5L+0ScUginaK3ZsGEDF154ISdPniQvL49NmzZRVlbWt83KlSvJz89n9+7dPPvss+zZs2fAMTo7O7niiiv42te+xrFjx9izZw933nkntbW1A/bfv38/S5Ys4fHHH+/bd968eWzatKnv+caNG5k/f/6Q8T788MPk5+fz6KOP8tWvftUlvwOz2TzkutEk//5uv/12pk6dyvHjxzlx4gTZ2dl88Ytf7FuflZVFfn4+Bw4coKysjL///e9jCX1SO6f7/JVSdyildiuldvf+IY+V2drD3qq32Vc9uEUjhKds27YNk8k0IJFOmTKFu+++e9C2ISEhLFq0iOPHjw9Y/vzzz7N8+XI++9kzBcdWrVrF3LlzB2yntaalpYWoqKi+ZStXrmTnzp309PTQ2trK8ePHyc0duWrl8uXLKS8v73uel5fHRRddxKJFi7jsssuorKwEYNeuXeTk5JCbm8t3v/vdvpiefvpprrzySlavXs2aNWsA2wfLkiVLyMnJ4ac//SkA3//+9zlx4kTf/s44fvw4eXl5/PjHP+5b9pOf/IR9+/Zx9OjRAdsajUaWLl064LX4inM6+Wutn9BaL9ZaL46LGzQv0ZgU1HwgrX9xzjh48CALFy50atv6+no+/fRT5syZM2B5QUEBixYtGnK/7du3k5ubS3p6Ou+88w633XZb3zqlFJdccglvvfUWL730EldeeaVTsbz55pts2LABsN0rcffdd7N582by8vK47bbb+OEPfwjArbfeyp/+9Cfy8/MxGgeWMd6zZw+bN2/m/fffZ8uWLRw7doydO3eSn59PXl4eH3zwAQ8++GBfK/3hhx8GGPHD6dChQ+Tm5g44n9FoZMGCBRw+fHjAtp2dnezYsYPLL5+QHuhzyjmd/N2h29LJ4brXPR2GEA7dddddzJ8/nyVLlvQt2759OwsWLGDt2rV8//vfH5T8R9Lb7VNaWsqtt97K9773vQHrr7/+ejZt2sSmTZu44YYbhj3Wd7/7XaZPn86NN97If/3XfwFw9OhRCgoKuPTSS8nNzeX++++nrKyMxsZGWlpaWL58OQA33njjgGNdeumlREdHA7Blyxa2bNnCggULWLhwIUeOHOHYsWMOY8jPzx/V63ek99tEQkICSUlJ5OTkjPuYk41P3uS1v3orc+KuxGgweToU4ePmzJnDP//5z77njz/+OHV1dSxevLhv2cqVK3n11VeHPcb777/v1PmuvPJKPve5zw1YtnTpUg4cOEBwcDDTp08fdv+HH36Ya6+9lscee4zbbruNvLw8tNbMmTOHTz75ZMC2jY2Nwx4rJCSk72etNT/4wQ/4yle+MmCboqIiJ17VQLNnzyY/Px+r1YrBYGvfWq1W9u3bx8KFC7FarX3fJurq6jj//PN5+eWXnf7W4y08OdRzI/AJMEMpVaaUun2izt3e08KxhndG3lAIN1u9ejWdnZ384Q9/6FvW3j66bskbb7yRjz/+mNdee61v2QcffEBBweBR1B9++CFZWVmDlj/44IP88pe/dPqcX//617Farbz11lvMmDGD2travuTf09PDwYMHiYyMJCwsjB07dgAMuLB8tssuu4ynnnqK1tZWAMrLy6mpqSEsLIyWlhan4wLIzs5mwYIF3H///X3L7r//ftasWUN6evqAbWNjY3nwwQd54IEHRnUOb+Cxlr/Wevjvl262r+otZsRcjlI+1/MlhnXHhJ5NKcW///1vvvnNb/LQQw8RFxdHSEgIv/rVr5w+RlBQEK+++ir33nsv9957L/7+/uTk5PDb3/6Wurq6vj5/rTURERE8+eSTg46xbt26Ucf9ox/9iIceeojLLruMzZs3c88999DU1ITZbObee+9lzpw5/PnPf+bLX/4yBoOBiy66iIiICIfHW7t2LYcPH+7rIgoNDeXZZ58lKyuL888/n7lz57Ju3ToefvhhcnNzR+z6eeqpp7j77rvJysqiubmZJUuW8MorrzjcdsOGDfzsZz9j+/btrFy5clS/h8lMaa09HYNTFi9erMdazOUfB7/F6c6aQctTw6dhMgYNWh5miiE38fME+IWP6Xxi8jh8+DCzZs3ydBheq7W1ldDQUMD27aKyspLf/va3ExrD0aNHueKKK/jd737H+vXrJ/TcE83R37NSKk9rvfjsbb2+z/+v++7gpaN/x2q1AJAcFsb8xNUAlDU7vqAEcLR+B0uSr2Rm7Hr5diDEGL322ms88MADmM1mpkyZwtNPPz3hMcyYMWPQ8FjhA8n/eMMR8iqa0UCnWVPT1sqvLy0mLGDKsPt1mtvZXrKJlu5alqbcOjHBCuFlrrvuOq677jpPhyEc8Prk/z8Xf8C8eFu3T4+lmW9veYW8ygOsyhg++fc6UvcJi5JukpFBQgiv4lP9Gf7GcC7LDuHVwibMVudGVHSa2znVuN3NkQkhxMTyqeQPsDxlNi3dcOq08zeKHK59z30BCSGEB/hc8o8KymZWrIG3T5Y4vU9l6ylOd5xyY1RCCDGxfC75K2Xg8uxU9lVbaOw86fR+h+u2uDEqIYSYWD6X/AGyohYS7A+7yp2vI1NYvwuzVYp8CyG8g9eP9nHE3xjCuuww/n20BZPxNZannY/JGDnsPt2WTvZUbmRGzBrCA1Jl7L+XeiLPuQIszrpjkXN3DP/iF7/g+eefx2g0YjAY+NOf/tQ3z01VVRVGo5HemW137txJUFAQ8+bNw2w2M2vWLJ555hmCg4MHHLOqqop7772XXbt2ERkZSUJCAo8++ijTp0/HaDQyb948tNYYjUZ+//vfs2LFCsB29+5NN93Es88+C9jm209KSuK8885zOMdQ77HMZjOZmZn87W9/IzJy+P9P49XY2Mjzzz/PnXfeOeK2oaGhfdNGlJWVcdddd3Ho0CEsFgvr16/nN7/5DQEBAaN6LR0dHVx++eVs27Zt0Gyl49XZ2cmFF15IV1cXZrOZa6+9tq+IT3d3N5dccgnbtm3Dz2986dtnM9jFGRezYUYYmw428v13XuO9otfYWb6FneVbOFK3Ha2tg/bJr9rKCwf/m2f23cHrx+5jZ/lfOHn6PZq7yrDqoQtSCDGcTz75hFdffZU9e/awf/9+3nnnHdLS0sjPzyc/P5+vfvWrfPOb3+x7bjKZCAoKIj8/n4KCAkwmE3/84x8HHFNrzdVXX82qVas4ceIEeXl5PPDAA1RXVwP07b9v3z4eeOABfvCDH/TtGxISQkFBAR0dHQC8/fbbpKSkDBl//1iio6MHFIsZK601Vuvg/4O9xlLkRWvNNddcw4YNGzh27BjHjh2jo6NjwCynzr6Wp556imuuucbliR8gICCAbdu2sW/fPvLz83nzzTf59NNPATCZTKxZs4YXXnhh3Ofx2eTvbwzj0qwr+fWlF3PhlGD+caiRv+TX8tTeWh75tGTY6wHdlk7Kmo+RX7WVd04+yaaC7/Pkni/y3P47+aTUtS1H4f0qKyuJjY3ta33GxsaSnJzs9P4rV64cdAfru+++i7+//4AiMfPnz3c4d01zc/OAAi8A69ev75sobuPGjSNO9dzr7CIvzz77LEuXLiU3N5evfOUrWCy2O+1//vOfM2PGDC644AJuuOEGfv3rX1NUVMSMGTO4+eabmTt3LqWlpUPuP5YiL9u2bSMwMJBbb7XdtGk0GnnkkUf461//2vfNYLjX0t9zzz3HVVddBdhKRiYkJPStW7RoEU1NTU7F5IhSqm9KjJ6eHnp6egYUZ9+wYQPPPffcmI/fy2eTf68QUzIbZl7N4+tv4g9X3MRj667E3wCH64ae+mEobT3NHKz9iC5zsxsiFd5q7dq1lJaWMn36dO68806np2cGW5fMG2+8wbx58wYsH6nAS0dHB7m5ucycOZMvfelLA6pewZk5/js7O9m/fz/nnXfeiLFYLBa2bt3aNzXy4cOHeeGFF/joo4/6irk899xz7Nq1i3/+85/s27ePN954g/5zdh07dow777yTgwcP0t7e7nB/YFCRl/Xr11NRUTFsfAcPHhz0OwkPDycjI2PQh+fZr6W/7u5uTp48SUZGBgARERG0t7f3laOcP38++/fvH7TfypUryc3NHfToX0O5//lzc3OJj4/n0ksvHfD7nzt3Lrt27Rr2tTrDJ/v8h+NvDGNFWgDvF59meap11H37Vm3heMO7zIm/yk0RCm8TGhpKXl4e27dv59133+W6667jwQcfHFBz9my9yRtsSeX220c3I3pv9wbYup1uvvlmCgoK+lqYOTk5FBUVsXHjxhEnQ+uNpby8nFmzZnHppZcCsHXrVvLy8voK03R0dBAfH09DQwNXXXUVgYGBBAYGDig/OWXKFJYtWzbs/o68/rprCjQN9Vr6q6urG3QdIDExkcrKStLS0jhy5AiJiYmD9tu+3fmbRY1GI/n5+TQ2NnL11VdTUFDQVwLTaDRiMploaWkhLCxslK/wDJ9v+TuyJDmZokZNS7fz9wL0d7T+IxdHJLyd0Whk1apV3Hffffz+978fUODFkd7knZ+fz2OPPYbJNHD6kTlz5pCXl+fUuZcvX05dXR1n18m+8sor+c53vjNil09vLMXFxWit+/rJtdbccsstfXEePXqUn/3sZ8Me6+wCL6PdfzizZ88e9Dtpbm6mqqqKGTNmDPtazn69nZ0DR/4lJydTUVHB5s2biY2NZdq0aYP2G03Lv1dkZCQXX3wxb7755oDlXV1dBAYGOv3aHZHk78CUyDkYFBypKxzT/nXtFdS3j77bSPimo0ePDihZmJ+fz5Qpzs09NZTVq1fT1dXFE0+cuQa1f/9+h63PI0eOYLFYiImJGbD8tttu46c//emgLqWhBAcH87vf/Y7f/OY3mM1m1qxZw+bNm6mpsU2n3tDQQHFxMeeffz6vvPIKnZ2dtLa2DlmlbKj9gTEVeVmzZg3t7e389a9/BWxdK9/+9rf5+te/TlDQwKndz34t/UVFRWGxWAZ8ACQnJ/P666/z0EMP8dRTTzk8//bt2/s+yPo/LrnkkgHb1dbW9lVB6+jo4O2332bmzJl96+vr64mNjcXf339Ur/9s0u3jgMkYwbJUf7aX1LN06EEOwzpav40VwYM//cW5zdmhma7U2trK3XffTWNjI35+fmRnZw9I2mOhlOLFF1/k3nvv5Ve/+hWBgYFkZGTw6KOPAgO7jbTWPPPMM4NGrqSmpnLPPfeM6rwLFiwgJyeHjRs38p//+Z/cf//9rF27FqvVir+/P48//jjLli3jyiuvJCcnh4SEBObNm+ewyMvs2bMd7j9lyhRiYmIGFHk5ePAgTz755LAXynt/J3fddRc///nPqa2t5brrrusrNj/Sa+lv7dq1fPjhh32JOzk5meeff55t27YRGxs7qt/Z2SorK7nllluwWCxYrVa+8IUv8JnPfKZv/bvvvssVV1wxrnOAjxdzGc6Ruu088mkJv1l7EaGm1FGfM8AYxH/kPC6zgZ7jpJiLZ/QWeWlvb+fCCy/kiSeeYOHChRMaw8cff8wNN9zAiy++OOpz79mzh0ceeYS//e1vbopuaNdccw0PPvigw3rLUszFBTIjZwMlFNYfYWHS6JN/l6WD4qaPmRq1yuWxCTHZ3XHHHRw6dIjOzk5uueWWCU/8ACtWrOjrRhqthQsXcvHFF2OxWNwy1n8o3d3dbNiwwWHiHy1J/kMI8IthSbIfH5bUMjO2CgA/Q+CIdwL3l1/1OhmRF2BQ8msWor/nn3/e0yGM22233Tbh5zSZTNx8880uOZZc8B3G8tQEDtZa+eZbW/nmW1v5xpuv0dY9/Fji/uraK8irGP/NGEII4Wo+0ST93OyH+n6uaNnL1pNP0GXpGHG/mbHL+OEFB7BoKx3mLn67o5TyllNMj3H+7su9VW+TGj6fpLDcMcUuhBDu4BMtf4Py63ukhi/h6lk/JSrQ8c0i/RkNgaRHLiEz6jxmxV5AXDAcb6gb9fm3nXpC7vo9h02WQQ9CDGe0f8c+0fI/W3hAKhtm3s+Bmhdp7Kyipaue0501dFuGnrJZKQMLkoLYV93G+lGO4GzraWbLiYdJDM12cFxFdFA6yWG5BPq5dyZEMVhgYCD19fXExMQMmD9FiMlEa019ff2obvzyyeQP4G8MZmHSTX3PeyztHKl7g/3VW2nrcdxKnxETzZYT5XSZGwjwix7V+SpbT1HZOnw1sPiQND47/T4ZHjqBUlNTKSsrG3R3qxCTTWBgIKmpzo9M9NnkfzZ/YzDzEj7H7LjP8n7xYxxv2Dtom9TwVKCcmrZTpEWMLvk7o6atlMN1rzM3foPLjy0c8/f3JzMz09NhCDHhfKLPfzSMBhO5iY6Tb3hAOoF+UNRY7bbz7618UyqGCSHcTpK/A9FBWSSEpA9ablAmchP8Kah138XbDnMrBTUvue34QggBkvyHNDvuYofLZ8VFUFBjwTzMxeHxyq96m27L4OISQgjhKh5L/kqpy5VSR5VSx5VS3/dUHEOZGnURAcagQcszIpIwW+F05/AXb8ej29LJvqrhp/QVQojx8EjyV0oZgceBdcBs4Aal1GxPxDIUo8HEjNjB1Ytig20XB0ubnb/TdywO1m6Xvn8hhNt4quW/FDiutT6pte4GNgHnXOmrWbFrBy3zM4YxM9bAkbrTbj13t6WTU6c/dOs5hBC+y1PJPwUo7fe8zL5sAKXUHUqp3Uqp3Z4Yhx0RmE5y2NRBy+fFh5Jf1YVVW916/qP1kvyFEO5xTl/w1Vo/obVerLVeHBcX55EYksNmDFqWFRVPUxe0dJc62MN1KlqO09JV5dZzCCF8k6eSfzmQ1u95qn3ZOScmKGPQsuSw6SjgWP3YyjyORmH9NrefQwjhezyV/HcB05RSmUopE3A98LKHYhlWTHDWoGUBflEsS/Xng+LRT/I2WoX1n8rEY0IIl/NI8tdam4GvA28Bh4G/a60PeiKWkYSaEh0O+VyWksTReistXWVuPX9LdwOVrfvdeg4hhO/xWJ+/1vp1rfV0rXWW1voXnorDGTHBSYOWTY2egwIK64+4/fxH6951+zmEEL5FJnZzQkxQKhUtJwcsMxmjWZriz/aSWhY5X9tlTI437KWi5d6+52kRM8lN3EB4QKJ7TyyE8FqS/J0QEzzF4fLlqYk8uqOU1q5yQgMGjVR1GY2Ftp4z1xeO1H3I0bqPmRazkISQwdckxiI1fCFhbnwNQohziyR/J8QEDR7rD5AZNQcopbDhMAuTJjZxaqwU1u+msH63S46XEVnA2qwfuORYQohz3zk9zv9cERWUiUEZBy0P9IthSbIfH5ZM/kIgRY0HqWs/6ukwhBATRJK/EwzKb8iavyvSEjlYa6W+3f0Xft1tV/kLng5BCDFBJPk7KSbYcbdOdlQuGZGK+97Po6hxxwRH5VqlzYVUybBSIXyCJH8nxQSlOVxu8ovg28uuYGGSPw98eJxPyt50+5w/7rS7YrOnQxBCTAC54OukmOCh67ya/CL4z5xrSAt/g6fz69H6LVakrZvA6FynouUkH5f+EZMxGICk0FxSwnM8HJUQwtUk+TspeogRP72MBj9WZ16BUq/zzL4GkkJ3kBk1uB7AZFBQc2Y2UaPayxXT/4vEULmnQAhvIt0+Tgr0iyTEP3zYbZQycNGUy1mR5s//fnqc0x3HJig697HoWt46/hKnO9xbv0AIMbGk5T8KMcHJtDUNX7zdaPDjxrmXU9nyKv/76U4unTq2mT+1hozIZNIi5mNQnv2M7rIU88bxN1iXvY4Av4AxHcOojGPeVwjhemqyzBi5ePFivXu3a25oGqu27ho6zbbkX9qcx87yV4bctrW7jD/s3k5d+9gu/vZYoK0HsqL8uHTqPHISZmI0eOpDwISt+Nr4zp8WnsasuFmkR6R7/ANNCF+hlMrTWi8etFyS/9iYrZ08t/9uuiwdbjpDDzVtO/j7wWIO1MDFGbFcP/cyN53LGbOBWJccKcQ/hNlxs5kZO5Mg/8EzpgohXGeo5C/NrzHyMwQyK26FG8/gT3zIBXxvxQa+cV4Q7xbVUVh/yo3nG4nrKoq19bSxq2IXzx14jo9LP3bZcYUQzpPkPw6z49ah3PwrbDeHsDjpYmbGwqaCnXRbzG4939AagE6XHtGqrRTUFHDy9MmRNxZCuJRc8B2HUFMimVFzOXnavXfFtpuj+M7ydL70SgnbTu3k8mx3fuMYTh6uby8Y+bhUkxL2RbkgLMQEkpb/OM2Nv3xCzmNQ5/EfOUZeKTzFqdO1HirtaAF6XPzopL1nBzvLpWCNEBNJWv7jlBiaQ2xwMnXtFW49T4/VxLWzcnnjWB4PfrSF6CAjs2KDSAz1x89gwM9gxKAUStm2z4hIISV8lltjcp1uDtdtJjs6i6SwaZ4ORgifIMnfBebFr+Xdoqfdfp7qthm8eF05e6qq2VNpZU9lK8caoMsMXRaw2EeVWjSEmqr5xeopfdM0nPs6efvkn4gIWAmA0WDkgvQLiAyM9HBcQngnGerpAlZtZlPBvbR2N3ooAo1RWVDK9l42dRbxrS07uXNxCvMTV3koprEIxHY/gU2AMYB109YRH+J4Om0hxMhkqKcbGZQfOQmXeDAChUX7Ybb6Y7b6E2Kaxsp0f/51pByLtduDcY1WF3DmprguSxevFr5KaVOp50ISwktJ8neRmbGXE+h37nSxXJY9m6pWOHn63Py25JjG9gFwhtlqZuuprZitnhriKoR3kuTvIn6GQObEXeTpMPrEBs0mJ8HIv48WofVkSpyD7yXotnRzouGEB2IRwntJ8nehOfGfwc9g8nQYgG2G0c9Oz+Z4g6aiJR+FFYUVW+v6XOZ4uozDdYcnOA4hvJuM9nGhQL8IZsYuo6DmA0+HAkBaeC6ZkYX8zwdHMaozxdk1atj9TEYjiSERJIVFEBccilKDtw8xmViZPg2Dg3Xj4/gu4pq2Gho6GogOinbx+YTwTZL8XSwn4SoO1X6EVVs8HQpK+XHHomUU1tu6TEzGHmKDT3PydCqNnUPXJug0m6lsbeJgbQXNXUNP6RDk58/SlKErnI3N0Oc7XHuY89PPd/H5hPBNkvxdLNSUwLToxRytPzeKuUcHTWVZqq0KmcLK5+e8So+ljRePrIQRvgEAWKxW9FldRVrDgx++yStHD7AoaYqLp5oeepbUYw3HOC/1PPwM8mcrxHhJn78b5CZuwJnEOtE0BvZVzSEupIGUMOdm6TTa7x7u//A3Grlyxnxq2lv4pMzVk7IN3fLvtnTLJHBCuMiEJ3+l1OeVUgeVUlal1KAbD7xBRGAaUy5KkisAACAASURBVKPOzaLnxxoyaO0OZkFSwbiOk5OQQkZkDK8dO0CPxZVdXBZg6HsTDtYc9NC8RkJ4F0+0/AuAa4Bz46qomyxIvNrTIThk1Ub2V88iOayGhJCaMR9HKcWGGfNp6Ghne8lxF0YIw7X+a9tryavMc/H5hPA9E955qrU+DDgcQeJNYoKzSY+YRUnTuTdE8UhdNgsSC7hwyg5q2oavztVhDiS/ag7dlsFDWGfGJjI9Op7Xjh2gvNlW4D3EFMD6aXMJ9PMfR4SdwNAXpPdU7iE2OJaMyIxxnEMI3+axuX2UUu8B39FaD3kLqlLqDuAOgPT09EXFxcUTFJ1rdFta6ehxfr4fq7aQV/l3Tp7e58aobGbGHiM38eCIVyZCTO20dQex7dT5VLcNnmOnqLGeJ/K2Y7HapmVo6upgdeZMvjBn0TiiywDSh93C3+DP1bOulonfhBjBuGv4KqUCgCigXmvdM8K27wCJDlb9UGv9kn2b9xgh+fd3Lk/s5mqF9W/xUckL9JwD8/LEBdexZupHhJraOFgzg+aukGG3f+ijUl49VsuPVq4jJTxqjGdNAGaMuFV0UDTXzr52jOcQwjcMlfyH7fZRShmB27C1vhdgG8JiVkrlAf8PeEZrbT17P621J2c5m/Smx1xGbHAWmw/9zNOhUNseyz8Pref89F3MSzgy4vZPxULmb/3YVLCbby2/ZIzde86Vi2zoaKCxs1Fa/0KMwUh9/nmAGXgZ+CXQDEQCc4EvA/cC890ZoK+KDsomOiiBho5qT4dCj9Wf94pW8FHJEgxq0Gd9H6U01899mW8tj+C+92vYVVHM0pSMMZxx6LH+ZyttKpXkL8QYjJT8v+SoW0Yp9abW+j6l1NzRnlApdTXwGBAHvKaUytdaXzba4/iCjMh550Ty79VjHfkibmlTEt9aVsNf8qPYfGgPtW0tAPgZDFyUMd3JC8Hd2IZ8Gkc+X3Mp8xLmOXFMIUR/Iw313KuUMimlLEqpWPvPM4ETAFrrUQ8W11q/qLVO1VoHaK0TJPEPbUrE5LsNoqQphfDATr62eCY9VgsvF+7n5cL9/OtIPltPjtxtdIZzXT+VLZUy3bMQYzBSy/8bwK+xTQXZvwnq3oK1AoDY4FkE+wfT3tPu6VCcVtqcjNawJrONyMBr6Z1F9Pc73+O94kIuy56Nn2HkFr0t+Q9/cRnAoi1UtFSQHjH86CAhxEAjtfwfB27FdqH369gu/t4CrHZzXAJQykhG5GxPhzEqneZAatpiSI8ox6AUBmXAoAyszpxJc1cnuytKnD2S0+csay4bW7BC+LBhk7/Wuktr/YzW2gB8gq0pth8omoDYBDAlYqGnQxi1kqYU4kPqCfQ7k8BnxyWRGBrOtlNHnJyeYXQXfYUQo+PU9A5KqW9hS/6/A74PPOXOoMQZyWGL8Heqm+TcUdKUglKQFn6md9CgFKszZlDc1MDJ03VOHKUVqLc/GhiuCE1TVxPNXc3jjFoI3+Ls3D7fAS4AWoBNwAa3RSQGMBpCSA2f6ukwRqW+I4q27iCmRJYPWL4sdSrB/ia2nnLmwm8zcND+KACOMdwHgLT+hRgdZ5O/AnozUBrg/JwFYtymRE62WykUJU3JpIZXYlBnZvwM8PPjgrQs9laV0tDRNspjVgGFDPUBUNosyV+I0XB2YrdfYmvxK2xdP990W0RikMzI8wmd3uVgjaaxs4GSplNUtNRgtnq+elivkqYUZsWd4Ob5m2nsjKCpMwyz1UhWVA/bijR7Krdy36p4NAbyq2bT2h3qxFF7B5wNnvqhoqUCs9UshV6EcJKz/1M+BuYBc4AjWusD7gtJnM3fGEdy2HUO1yWHwew4sFjbaeup71te0fI+u8q30WH2zPxAJU0pvF90HjHBjUQGNpEYWotBWUmPgDsWGvljXgvfXdHNouRutFZ8VLrEySNXA5nAwFlGzVYzZc1lMtOnEE5yNvlvAxZprf/hzmDE2BkNwYQHBPc9Dw+4iayoVPZVb3HyAuvI2nu66bY4d0OVxsDR+mzb9dqzZEV3EWB8idtfjuFf1/mTFV3EJ2ULsWpnL2yfxjb520BFjUWS/IVwkrPJfy/wK6XU+9gHYGutn3BbVMIFFP7GVSxO9mNx8iGXHLGwvpr3io6O+zihpgDWZc/hX0fy+cehXH5wQTFp4RUUN6U5eYR6HCX/kqYSrNqKQUl1UiFG4mzyv9D+b295Kg1I8p8ULsA2D9/4ZUUdZUfZKZd0Ja3OnMl7xYX8cXcxdy8NYHrMqVEk/9OAlbPHK3SaO6lqrSI5LHnc8Qnh7ZxK/vabvMSk5ZpZL42GxcyKe4c9leOvTuZvNHLVjFz+kv8xUb8CgypFsYkvzFnMBenZI+xtAZqwlZcYqKixSJK/EE5wKvkrpT4+a5EVKAd+qrUezWxdYlIzMjvuc+RX/RLr4DIOo7Y0JYNui5m2njpmxpzk1WOBbCzYRXpENOkR0SPs3cBQyX9F2opxxyaEt3O2Rd8ApAIl2Mb5KyAL+Kub4hLnqGD/mUyNmuWSYxmU4sIp01iXvZxvr4ji79eaCDUF8P/yPqTTPGyxOBxeSQZau1upbat1SXxCeDNnk38ycKnW+nrgMmwTrX8WKeTik+bGfw5GrP47OsfqM5kVd5ofrpxObXsLz+3fMcIcQJ2A49lOixqLXBqbEN7I2Qu+6cBFSqlKYAW2Vn/vdA/Cx8SHTCc1fDWnHRaaMWOb8XvgTWlWrek096CHuEP3eEMGS1Ly+c6KfXSa4cfvFlPdVoLR3jzpNJvosfijUCxJyeAz0+dhUPVA8KBjnWo8xew4x7OhBvsHj7G0pBDexakC7kqpHwH/w5l7638KTAOatNb3uC+8M3ypgPvkZ8E2F89e+rcPrNpKa3cXLV2d1LW3UtveSlVrE+09ttFDIf5tRAc1EuLfwqaDxZQ22T5AQkzt9Fj9KGtOoqWrk8N1VeQmpnFb7mUE+PWv4hXASN9IvjDnC1L2UfiUoQq4O5X87QfIAWYCR7XW+5RSyVrrCSvqIsl/MmoC/oFtfIBjFquVd4uOcvL00P30CxIPsDh5P8/uv4b2nkC2njrK5kN7SA2P5MsLLyAhNNy+ZRC2HsoEhvpSuyZzDVnRWWN8PUJMPkMlf2endI7CVtTlO8AKpdSCiUz8YrKKYKR7DIwGA2syZ5KTkDrkNsVNqSgFUyLLUUpxydSZ3LXkImrbW/jJe6/w4Idvsu3UETp6mrFVGN3BUMVg6jscXygWwtc4e8H3L9jm9ZkL5NifC+GEhdha5ENTSrEsdSqLkqY4XN/QEUlzVwhTIs7M3DkvIYX7Vn2Wa2YtwGy18sLBPJ47sMO+1oJtYNpgde2umepCiMnO2eR/MbaWfxfwPDDSXThC2JkA5yZtmxM/1M1ZiuLGNFLCq/AznBkCGhkYzGVZs/nRhes5Py2LAzUVWKy9XUw1OGr917dLy18IcD75FwKPYPuf/G1sFTaEcNIMIHbErQL9/IkNdjy1c1FjKn4GK2nhlQ7Xz4lLotPcQ1FTb3K34qj132HuoL3H8RBRIXyJs8n/NmzDPcF2k9eX3BOO8E4KWAdcgu3WkFQg0f6Ip//0zKnhg+/aBahqjaPTbCIj0nHRlhmxiSjgcG1Vv6XVOKoFLK1/IZyf2+cAsKz3uVLqwmE2F8KBIGzF4ByVpNTYJms7SWp4I/lVgxO8xkBJUwrpEeUorOiz2i2hpgDSI6I5XFfJZ6bP69sLSoHpA7at76gnLcLZSeSE8E7DtvyVUtcqpSqVUqVKqbVKqQCl1GPY5vcXwkUUEA0sJCEkFr8hCtYXNaYR6NdNUliNw/WzYhM5ebrurKkhqoEi4MxMpNLyF2Lkbp+HsfXv7wAeA14EvoqtrKMQLmbAaJhKUmiEw7VlzUmYrUamRhU7XD8rLgmr1hTW97/zWGPr+9+J7dKVWYZ7CsHIyT8FuAH4PDAF21DPVVrrn7g7MOGrppIS7vgOXLPVjxMNU5gWfQqTcXBN46yoOPwNRg7XVTnY24qtCPwJGjsbMVudq0gmhLcaKfn7AZ3adhtwB3Cj1voj94clfFcqqeHxQ649UDMTf6OFmbEnBq3zNxqZFhN/1kXfs1UD9TR0NIw/VCEmMWdG+/xUKfVLbBOn3KCU+qX9uRBuYCA6aC7B/iaHaxs6oqhoSWBu/FGUg2kjZsUmUtnaxOmO4YZzHqOuvdxF8QoxOY2U/EuAz2Hr+qkG1tt/vn6sJ1RKPayUOqKU2q+UelEpJbNsibNMJSXM8ZBPgAPVMwg1tTsc9jkrLgmAI/XDtf67qW9/f7xBCjGpDZv8tdYZWutMBw9H4/Wc9TYwV2udg+0K3A/GcSzhlVJIDY8bcm1JUwrNXaHMSxhcRC4lLJIwUwB5FSXD1gOo7ygEGl0RrBCT0khDPfcqpb6mlEo9a3maUurLSqkdQ+07FK31Fq1179W2T7Hd8SNEP0YSQ+cNuVZjoKBmBomhdcQFD5yrx6AUqzNncqCmnPeKCoc8RkNHG1o7qkcghG8YqdvnVmwjfUqUUqeVUiVKqRZsA6dvBu4d5/lvA94YaqVS6g6l1G6l1O7aWinN50vCAhYR5BfDUPPzH63Lotviz+XZ77EoaT9Bfmfu5L08ew45CSn8/VAexxsc3xNgtlrYWPA8Gw9sZOOBjWwv3i4jgIRPcbaYy1xsFbxigFpgu9b66DDbv4Pt3v2z/VBr/ZJ9mx8Ci4FrtBNByHz+vuet429R3HQS25/c4FZ8XHAdC5MKmBJZjsVq4IPi8zjWYOuR7Ojp5pcfvkmnuYfvrVhLWEAgACajH4a+Sl5hwIK+40UGRrImcw0xwTHufWFCTKBxFXNRSoVjq93bOzev1lr/bRzBfBH4CrBGa+3ULFuS/H3P3sq97KrYhW2K5qFHGEcENHNZ9nu0dQfz2rFL+pZXtDTy4Idv0WU506KfG5/M3Usvtj8zYGvTnPkCbFAG0iPSMShnp70an/iQeHIScibkXMI3DZX8na3h+yq2mr29NDCm5K+Uuhz4HnCRs4lf+KaE0AT7T0Zsk791O9yuqSucuvZo4kMG3rmbHBbJd1ZcyhH7TV/HG2rZX11Oa3cnoaZAbDd+tWH7BmBj1dYJLQBf1VolyV94hLPNm3nA5diqZQfhqGq2836P7X/b20qpfKXUH8dxLOHFYoP7TwM9fEGYlq5QQk1tg8b+p0dEszZrNmuzZrN+2lw0moKa/kXoWvCk9p52KTAjPMLZ5P8sZ0blaM4Uch81rXW21jpNa51rf3x1rMcS3s1kNBEdFG1/NkLy7w7FoDQhpqG/TKZHRBMeEMj+6v43eHk2+QOUNjmeploId3I2+d8MPIntO3InjiZJF8IN4kN6p3oY/stmS1cIAGGmtiG3MSjFvPgUDtZWYrZaevd0QZTjU9osyV9MvGH7/JVSvROhfw9ba793mMSYW/5CjEZ8SDxH6o4wUsu/uctWASwsoJXK1oQht8tJSOGj0hMcb6hlZmwi0A6Ycf7yl+tVt1bTZe4iwC/AYzEI3zPSX/wRBid6ZV/2hFsiEqKfMy3/4ZN/W08wWg/f8geYFZuEn8HA/upye/IHaAU8N8uIRlPWXEZWdJbHYhC+Z6Tkf/EI64Vwq6jAKPwN/vRYAznT7hjMqo209QQTFtA67PEC/PyYGZPI/uoyPj97IUopbF0/np1iqqSpRJK/mFDDJn+ttcx+JTxKKUVcSBwVLRVAIMNdbmrpCiHMNHzyB1vXz/MFFVS3tZAYGs650u+vtbZ/GAnhfhNzJ4sQ4+Bs109LdyhhAcN3+wDMS0gBYH91mX3JyB8Y7tZp7qS2XaYwERPHc1e5hHBSUmgShf6FQBRat9JhdnyzV0tXKCHRpzAoC1btuA4wQHRQCKnhUeyuKOaSqTMxqE6gBtvNZOMVCISMac9Tp08R4j+2fc9FAX4B+BkkxZyr5J0R57y0iDT+I+c/gENovZ0tJw5R3DS4Dm9LdwhKQYh/Oy3dYYMP1M+azBk8s+9TXik8wFUz5mMb2+AKBmAqkDzqPfdV72Nf9T4XxeF5GZEZrM1a6+kwxBCk20dMIhEopbg4cwbhAYO7gFrswz3DR7joC7A8dSrnp2Xx+rEC9vV1/7iCFTiObSK6wZXGfElRYxGVLZWeDkMMQVr+YhKJAGwzc67Nms2/j+T3u1nL1vIHCB1huCfYLiRfP3cxpc2n+cvej/nvleuIDxn+28LoVAENuKYryVn+2KqtBmKbVHf4ayQT4dOyT9kwc4NcyD4HSctfTCIh9CbT6KAQLpwyjbjgMOKCw4gNDqWtOxirVk61/MH2IfKVRSsxKMWjn25lV0URVidmuXVeN7bRSRP1aMY2/XUpkIet7MaZD0dPqG2v5cTpEx6NQTgmLX8xiShsrf8GALKj48mOto0E0lrzl/yPae0OGXGsf3+xwaHctXQVz+7fwZN7PuKt8EOsypiOyTj8f43IwCCmxwx9J7HnWbGV4K4BkoB4bN8KJt7O8p1kRmZiNEzktyAxEkn+YpI5k/z7U0oRFRhMS1eIU90+/WVFxfHjC9ezs7yIl4/u52/7R65OqoCHLv0c4fYiMeeuTuCU/REJRAHh2CbWnZgv/q3drWw5sYVQU+iEnM/bpIankhmV6fLjSvIXk0zEkGuig0Jo6Q4lPaJ8yG2GYlAGlqVOZUlyBnXtrcNOXlXSVM+f935MSVMDc+NHP6rHcxo5U7TegO0DIBrbB4J7E7NMXjd2AX4BkvyFGG4ahuigEFq6Qgj278SozFj06P+8jQYDCaHhw24TYW/tT77k358VaLI/TmG7WBzGmW8FIdgK6AhvJclfTDLDtfyDOVpva8GGmtpo6hp62/EI8jcRFxxKafNptxzfM3qwdaf171IzYRs5JDwrHFjq8qNK8heTTDy2borBibe35Q8QFuC+5A+QFhFNSdPgaw/epZuhSmeKieSe8iky1FNMMoqhWkFB/iZ6LLbKX85M8DYe6RFR1LW30t4jyVFMTpL8xSQ0BXA8zDLIPxaz1eD0WP+xSgu3fciUNnlT14/wJZL8xSTluPUfHRRCa3cIqeGVhAc0u+3s6RFRAJQ0e3vXj/BWkvzFJJUEpA5aGh0Uwp7KeYQFtPL52a9xXsoeTEbXd82EBwQRGRhEqdf3+wtvJRd8xSS2DCiw/2wFCokOCuH94kzKmxNZkrKPnITDRAc18sbx1S4/e1p4NCXS7SMmKUn+YhKLBi7s97yUqCDbXDYd5iA+KF5GR08g8xMPEWDspMvi2mGL6RHRFNRU0G0xjzgdhBDnGun2EV4kDj+DccB0z6ca0zAozZTI0d/1O5L0iCh78fXGkTcW4hwjyV94Edskb9FBZ6ph1bVH09odzJQIV87Zb5MeYRvx4/3j/YU3kuQvvEgcYLvT9wxFcWMqaRGVGJXZpWeLCgwmxN8kF33FpCTJX3iR3uQ/sA5uUWMqfgYLqeFVLj2bUor0iGhKvGqaB+Er5CqV8CKBQBjRQe0Dlla0JNBl9icjspTipsHDQ8cjLSKarSeP8Je9Hzu9j8GgSAuPIjs6npSwSIwGaYOJiTfhyV8p9XPgKmxj82qAL2qtKyY6DuGt4ogIaCbY39Q39YLGQElTCukR5SisaBd+4c1NTGVfVSnHT9c6vU+3xczHpScB8DcYMRltRU6MBgN3LFzJtJh4l8UnxFA80fJ/WGv9YwCl1D3AT4CveiAO4ZXiUOok2dHx7O9XmL24KZVpMUUkhNZR1eq65JoVFcf/XHzlqPdr6GjjREMtxU0N9NjrEO8qL+adU0ck+YsJMeHJX2vd/577EBi2boYQo2Tr9592VvIvbUrGYjUwJaLMpcl/rKKDQohOCWFJSkbfMpPByDunjtDU2UFEoOeLrwvv5pHORqXUL5RSpcBN2Fr+Q213h1Jqt1Jqd22t81+rhS+zJf+Y4FCiAs9c+O2x+lPWnMT0mJP4G3o8FdywLkjPxqo1n5Sd9HQowge4Jfkrpd5RShU4eFwFoLX+odY6DXgO+PpQx9FaP6G1Xqy1XhwXF+eOUIXX8ae32tfZ3Sd5lfMI9OtiYdIBD8Q1soTQcKZHx/NhyXG0li/Ewr3ckvy11pdorec6eLx01qbPAZ9zRwzCl9mSfnb0wAZDXXsMhfVTmRt/lPCAFk8ENqLz07OpbW+lsL7a06EILzfh3T5KqWn9nl4FHJnoGIS3syX9UFMgyWEDa/7uLM/Fog2cl7LHE4GNaGFSGsH+JraXHPd0KMLLeaLP/0F7F9B+YC3wDQ/EILzamRZ/dvTArp8OcxD5VXPIjCojOcy1N325gsnox9KUDPZWldLa3eXpcIQX88RoH+nmEW4Wg61dY2VqVCwNHW0D1lqsCXSai1ibtZ1j9VM43pBJdVssthKRnrcyPZv3igp5oWA3ty5YjkHJTWDC9eQOX+GFjNha/9WYjH6sSMtysM030Po1ZsXtY078McxWI1rbkn99RySvHL3UpTeDjUZqeBRXzZjPS0f3YTAobpm/TD4AhMtJ8hdeKgEY7qJpCkrdgaKTjp4d1LYfpamzg0C/dqbFFDElspyixrSJCnaQ9dPmorXm5cL9KBQ3z1+GQZ0b30yEd5DkL7yU4wLvgwUS5H8R6REX0R1q5uTpalq6/pechEMeTf4AV0yfhxXNq4UHyIiMYVXGdI/GI7yLfJcUXsrZ5H+GyejHzNgUqlqXkBhaR0KI528s/Oz0HFLCIsmrKPZ0KMLLSPIXXioYCB/TnooVdJpN5CQcdm1IY5SbmMqxhlpauzs9HYrwIpL8hRcbfesfIDEsnsO108iILD0nbgbLTUxDo9lf7fpSlMJ3SfIXXmxsyT/UFEhRYw5WbWBh0gHCA5oxGbvx1ByEaeFRRAUGk1/l+lKUwnfJBV/hxRLHvGd0UDKF9VOZFXec6TGnxnQMs9VIcWMqhfWZlDUnjXnoqFKK3MQ0Piw5TrfFjMko/23F+MlfkfBiUYAJ6B71nklhEbxftIRTjWkE+XUS6NeFyTi62UCD/TvIjCohK7qYTrOJbotp0DZaK5q6QmnoiKKhIxKz1fF/ycuzTbxbZKGu/QAr0mL79i1rTsSi5b+xGD35qxFeTGGb5G303SXJYRFoDJQ1J48rgo9KF5MeUU56RAUGZR203qisRAQ2kxJ2BKNh8PpeF2fAgx9CUeMhfrbqzPKy5kReP7aac+XuZDF5SPIXXi6BsST/UFMgYaZAWsY5wsaqjRQ1plPUmD7sdgor4QEtw34AzEvYxz8P17Auew1Gg4HU8EqWpe5ldtwxDtXKPQBidCT5Cy839n7/pLAIWuonZnilxkBTV8Sw28yKncoHxeU8f6CCmOAQIJBPyyKICNjNviozHebACYl1IvgZjOQmpuJnMHo6FK8lyV94uXhg6hDrqoD2IfdMDos8p+bVnxOXTJCfPy8X7newdu+Ex+Nu185ayKVZszwdhteS5C+8nD9wyRDrOoH3Acd3zyaFDt8Sn2gBfn78YvVVtPcMvIA9JbKU5al7aekKxqK9Y/T2PW908FHpXh5bfxyT0bevZwT6vQdMAy5w6XEl+QsfFghcBhQA+/ot7wCshAUEkhgaMSjZArT3dGO2WiYmzH5CTAGEmALOimUWde2auJD6CY/HXW5fGMjtL9fx7H4/rpkVMvIOXiwyMJ5gf9f/DiT5C8Fc+6PXfuBTAK6cMd/hHpUtTbxauB/toRu/BlLsq57j6SBcSmtNcthrPPKphTDTSpQPz2iam7iGpSkLXH5c7/iOKIRLzQWih90iKSyCRclTJiYcH6SU4rKs2VS0NFFQU+HpcLySJH8hBjEAF4641YLEtEE1goXrLEnOICowmC0nDnk6FK8k3T5COBQPzAKGntlTKcXqzJlsPpRHp3l0d/+KkRkNBtZMncnmQ3t4+OMtHqlmFuTnT1xIKPEhYYSbgvBE71NT5x6mRKwnIXRsc1UNRZK/EENaCjQA5iG3CPaHm+d/xuVntlgtHG+o4kBNCQ0drf3WaHovSPuClenZHG+opa27C60n/vpKbXsLh2or6fHAxf0zPmBewhVcnn25S48qyV+IIQUAV3nkzEYDzIi1PSpaKmjr7l+EvhvbPQrlQAlnzzZq1ZrtJcex6sn/ARHo58/XFo/cBedOVq1p6uzwWD2FGbHnsSJthcuPK8lfiHNccpij+YV6R/fUAx8AA6uOVbc1c6Suys2R+QaDUkQFBRMVFOyR80+PSSc8YGyFiYYjyV+ISS0G2AAcBGr6luYkJHKk7j0PxSRcyz03G0ryF2LSUwy8TwEiA2FKRADFTVL7d/JLdctRZainEF5qfqLjG9SEAEn+QnitxNBE4kPiPR2GOEdJ8hfCi81PkNa/cEz6/IXwYplRmXxp4Zc8HYZTOs2d7CzfSWF9oadD8QmS/IXwcp64M3Ysgv2DWZWxihkxM/io9CMaOho8HZJX81jyV0p9G/g1EKe1rvNUHEKIc0tSWBLXzr6W9p52atpqqGuvw2wd+i5rdzpce5geq3dO3eGR5K+USgPWYrs9UQghBgn2DyYjMoOMyAyPxdDS1cKpxlMeO787eer74CPA9zj7vnQhhDiHTIn03mm7Jzz5K6WuAsq11vuc2PYOpdRupdTu2trakTYXQgiXSgtP83QIbuOWbh+l1DtAooNVPwT+G1uXz4i01k8ATwAsXrxYviUIISZUkH8Q8SHx1LTVjLzxJOOW5K+1dlgxWyk1D8gE9tnLsqUCe5RSS7XWMguVEOKckx6R7pXJf0K7fbTWB7TW8VrrDK11BlAGLJTEL4Q4V6VHpHs6BLeYHAOAhRDCQ2KDYwn298x0zu7k0eRv/wYgY/yFEOc0b2z9S8tfCCFGMCXC+4Z8yvQOQggxgpTwFKICo9AuuDVJa41VW7Foi1OlNv0M7knTkvyFEGIEfgY/Pj/n854Ow6Wk20cIIXyQJH8hhPBBkvyFEMIHSfIXQggfJMlfCCF8kCR/rm7Y3gAACJRJREFUIYTwQZL8hRDCB0nyF0IIHyTJXwghfJDSenLUSFFK1QLFY9w9FvC1CeTkNfsGec2+YTyveYrWOu7shZMm+Y+HUmq31nqxp+OYSPKafYO8Zt/gjtcs3T5CCOGDJPkLIYQP8pXk/4SnA/AAec2+QV6zb3D5a/aJPn8hhBAD+UrLXwghRD+S/IUQwgd5ffJXSl2ulDqqlDqulPq+p+NxNaVUmlLqXaXUIaXUQaXUN+zLo5VSbyuljtn/jfJ0rK6mlDIqpfYqpV61P89USu2wv9cvKKVMno7RlZRSkUqpzUqpI0qpw0qp5d7+Piulvmn/uy5QSm1USgV62/uslHpKKVWjlCrot8zh+6psfmd/7fuVUgvHel6vTv5KKSPwOLAOmA3coJSa7dmoXM4MfFtrPRtYBtxlf43fB7ZqracBW+3Pvc03gMP9nv8KeERrnQ2cBm73SFTu81vgTa31TGA+ttfute+zUioFuAdYrLWeCxiB6/G+9/lp4PKzlg31vq4DptkfdwB/GOtJvTr5A0uB41rrk1rrbmATcJWHY3IprXWl1nqP/ecWbAkhBdvrfMa+2TPABs9E6B5KqVTgCuBJ+3MFrAY22zfxqteslIoALgT+DKC17tZaN+Ll7zO2OuNBSik/IBioxMveZ631B0DDWYuHel+vAv6qbT4FIpVSSWM5r7cn/xSgtN/zMvsyr6SUygAWADuABK11pX1VFZDgobDc5VHge4DV/jwGaNRam+3Pve29zgRqgb/Yu7qeVEqF4MXvs9a6HPg1UIIt6TcBeXj3+9xrqPfVZTnN25O/z1BKhQL/BO7VWjf3X6dt43m9ZkyvUuozQI3WOs/TsUwgP2Ah8Aet9QKgjbO6eLzwfY7C1tLNBJKBEAZ3j3g9d72v3p78y4G0fs9T7cu8ilLKH1vif05r/S/74urer4P2f2s8FZ8bnA9cqZQqwtaVtxpbf3ikvXsAvO+9LgPKtNY77M83Y/sw8Ob3+RLglNa6VmvdA/wL23vvze9zr6HeV5flNG9P/ruAafbRASZsF4te9nBMLmXv6/4zcFhr/b/9Vr0M3GL/+RbgpYmOzV201j/QWqdqrTOwvafbtNY3Ae8C19o387bXXAWUKqVm2BetAQ7hxe8ztu6eZUqpYPvfee9r9tr3uZ+h3teXgZvto36WAU39uodGR2vt1Q9gPVAInID/397dhVhRxnEc//4qXwoE2xIKisALUxRXo4RqL4SSyIsgLypIKLSgDOpGqazohegiTLuIXoxKVIr0oiLBAovKLLDATCXdCpToIqzFFxQj7N/FM7M7zZ45L7vqLju/Dyxnd848z/z/Z3b/Z+bZw/PwxEjHcxby6yHdEv4I/JB9LSSNgX8G/AxsA7pGOtazlP98YEv2/VRgJ/ALsBmYMNLxneFc5wDfZ+f6Q+DisX6egWeB/cBeYAMwYaydZ+A90v80/iHd4S2tOq+ASJ9g/BXYQ/ok1JCO6+kdzMxqaKwP+5iZWQMu/mZmNeTib2ZWQy7+ZmY15OJvZlZDLv5mZjXk4m9mVkMu/mY2bJJukrRhpOOw9rn4W0ckzZG0W9J8SZF9nZbUJ+npIfR3vqQl2dzt5efyY0xv0j7fZ2axn1Zti8+3c5xWcQ+lj0JfKyQNmpe9KrfhKMY9nJgb6AZ2nYF+7Bxx8bdOrQHeLPw8D7gM2Ag8I2lah/31kOYmmtTgua9JUxj0Nmmf73NpqZ922nZynLJy3EPpI/cWsETSjIq4yrkNRzHu4cRc1g3skjRB0jpJL2Tz8dgo5eJvbZM0izSXzseFzccj4jADMwuOl3SepJck/SnpL0mvSRov6Vql5fj+zpahu4WBBSt+ytYjKOohrdQ0rXCVuirrd3e2f77P+6V+im27JH0u6VR2h7KyyXHWFe5oQtI7Fe3Lced9TG+Ue/b6NcwhIvqAb4D7KuL6X26Sxikt/XdUaYnSBZLulXRC0g6l+f6rcu6PG1hcyLvqnFW97mWzSTNPfgpsi4iV4bljRjUXf+vEjcCxiDhU2LZT0kngeWB1ROwF7gceIs28eHP2+ChwN+l3rgdYDUwGlmX9zCPN4tjKCWABaVnOOwvbn2vSz5WkCdGmkyZEe7hJ/8tIV8PvAseyOBu1r4p7AY1zb5XDHtLr20g5t6XAIuAG0p3YRmAiaaWrV0irPlXlXBV31TlrFjPQP6X4VNIEZY9HxMaKPGwUuaD1Lmb9LgGOl7bdTho26IuIE9m2ucCBiPgCQNK3pGLzIHA5sJW0KtMa4I+szfGI+FfSYwwsUrKqQQybImKfpD7gwsL2fEWvvJ9im6OkorWWtCDIxKoEI+KkpOXAHcDCiNiTXemW25+sON6tFbm3yuEY0FURVjm32aRhmx2kN9NJDPwtb42II9mQS6Oc++POtueqztn2JjHnZpCmT+8CTlfkYKOMr/ytE4dJV8VFv0fEb4XCD7AbuDobMpgLXE9aWnIRaa3Sa4BPSHcLebG4IhseeZ00dXE+fXFZvn95SGFfqZ+iR4CZwAPAIdK0uA1JWkqaRng58J3SCmmN2pfjzn1UkXurHCYz8EZYVs5tP+kNbTHwFPB2IadTLXLujxsoxl11zprFnOsmDVvdRVpmcswsJTmWufhbJ7YDF0m6qsV+a4FXSatNbcseXwS+BK4j3SncRiqw+7KfNwNTI+JIRByMiIMMFLJ2nC72U3ruA2Acaa2DLtKV8pSKfp7MHl8mjYdvqWjfW3G8rypyb2UW6Uq+ndzWZjGtz+LtZfAd2aCYJU2h9HoX9q86Z+3oBvZGRC9pqGhTNhRko5jn87eOSNoBrI+IN0Y6lrFC0iTSP0vnRMSBkY7H6sFX/tapFQz+VIoNzz2kN1QXfjtnfOVvZlZDvvI3M6shF38zsxpy8TczqyEXfzOzGnLxNzOrIRd/M7MacvE3M6uh/wCIlpvnywbK5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Yellow')\n",
    "plt.plot(median_winner, color = 'Green')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Yellow', alpha=0.4, label='GP CBM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Green', alpha=0.4, label='STP CBM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
